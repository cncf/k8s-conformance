  I0512 14:53:24.546094      23 e2e.go:109] Starting e2e run "c7668d51-6bbe-406a-be04-bad37eb1c076" on Ginkgo node 1
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1747061603 - will randomize all specs

Will run 404 of 6605 specs
------------------------------
[ReportBeforeSuite] 
k8s.io/kubernetes/test/e2e/e2e_test.go:154
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
k8s.io/kubernetes/test/e2e/e2e.go:69
  I0512 14:53:25.043552 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 14:53:25.046255 23 helper.go:48] Waiting up to 30m0s for all (but 0) nodes to be schedulable
  I0512 14:53:25.223064 23 e2e.go:142] Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  I0512 14:53:25.247519 23 e2e.go:153] 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
  I0512 14:53:25.247634 23 e2e.go:153] 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
  I0512 14:53:25.247669 23 e2e.go:153] 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
  I0512 14:53:25.247694 23 e2e.go:153] 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
  I0512 14:53:25.247718 23 e2e.go:153] 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
  I0512 14:53:25.247739 23 e2e.go:153] 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  I0512 14:53:25.247761 23 e2e.go:153] 7 / 7 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
  I0512 14:53:25.247780 23 e2e.go:153] 3 / 3 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
  I0512 14:53:25.247801 23 e2e.go:153] 7 / 7 pods ready in namespace 'kube-system' in daemonset 'vsphere-csi-node' (0 seconds elapsed)
  I0512 14:53:25.247833 23 e2e.go:245] e2e test version: v1.31.4
  I0512 14:53:25.249930 23 e2e.go:254] kube-apiserver version: v1.31.4
  I0512 14:53:25.250165 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 14:53:25.260618 23 e2e.go:383] Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.217 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:70
  STEP: Creating a kubernetes client @ 05/12/25 14:53:25.52
  I0512 14:53:25.520402 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 14:53:25.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 14:53:25.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 14:53:25.573
  I0512 14:53:25.583971 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/12/25 14:53:34.726
  I0512 14:53:34.726633 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 create -f -'
  I0512 14:53:34.925972 23 builder.go:146] stderr: ""
  I0512 14:53:34.926029 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  I0512 14:53:34.926111 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 delete e2e-test-crd-publish-openapi-2617-crds test-foo'
  I0512 14:53:35.080766 23 builder.go:146] stderr: ""
  I0512 14:53:35.080825 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  I0512 14:53:35.080897 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 apply -f -'
  I0512 14:53:35.253422 23 builder.go:146] stderr: ""
  I0512 14:53:35.253636 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  I0512 14:53:35.253776 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 delete e2e-test-crd-publish-openapi-2617-crds test-foo'
  I0512 14:53:35.389059 23 builder.go:146] stderr: ""
  I0512 14:53:35.389124 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/12/25 14:53:35.389
  I0512 14:53:35.389337 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 create -f -'
  I0512 14:53:35.539235 23 builder.go:135] rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/12/25 14:53:35.539
  I0512 14:53:35.539485 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 create -f -'
  I0512 14:53:35.670436 23 builder.go:135] rc: 1
  I0512 14:53:35.670625 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 apply -f -'
  I0512 14:53:35.805193 23 builder.go:135] rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/12/25 14:53:35.805
  I0512 14:53:35.805516 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 create -f -'
  I0512 14:53:35.974824 23 builder.go:135] rc: 1
  I0512 14:53:35.974935 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 --namespace=crd-publish-openapi-3946 apply -f -'
  I0512 14:53:36.159587 23 builder.go:135] rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/12/25 14:53:36.159
  I0512 14:53:36.159828 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 explain e2e-test-crd-publish-openapi-2617-crds'
  I0512 14:53:36.314135 23 builder.go:146] stderr: ""
  I0512 14:53:36.314294 23 builder.go:147] stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2617-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/12/25 14:53:36.316
  I0512 14:53:36.316351 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 explain e2e-test-crd-publish-openapi-2617-crds.metadata'
  I0512 14:53:36.490152 23 builder.go:146] stderr: ""
  I0512 14:53:36.490485 23 builder.go:147] stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2617-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  I0512 14:53:36.490965 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 explain e2e-test-crd-publish-openapi-2617-crds.spec'
  I0512 14:53:36.645009 23 builder.go:146] stderr: ""
  I0512 14:53:36.645145 23 builder.go:147] stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2617-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  I0512 14:53:36.645455 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 explain e2e-test-crd-publish-openapi-2617-crds.spec.bars'
  I0512 14:53:36.788643 23 builder.go:146] stderr: ""
  I0512 14:53:36.788725 23 builder.go:147] stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-2617-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n  enum: Great, Down\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/12/25 14:53:36.788
  I0512 14:53:36.789027 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-3946 explain e2e-test-crd-publish-openapi-2617-crds.spec.bars2'
  I0512 14:53:36.961744 23 builder.go:135] rc: 1
  I0512 14:53:40.129967 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3946" for this suite. @ 05/12/25 14:53:40.15
• [14.646 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/daemon_set.go:890
  STEP: Creating a kubernetes client @ 05/12/25 14:53:40.166
  I0512 14:53:40.166810 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename daemonsets @ 05/12/25 14:53:40.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 14:53:40.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 14:53:40.215
  STEP: Creating simple DaemonSet "daemon-set" @ 05/12/25 14:53:40.286
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/25 14:53:40.294
  I0512 14:53:40.375102 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:40.375359 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:40.375828 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:40.422534 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:40.423149 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:41.311736 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:41.311848 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:41.312087 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:41.327506 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:41.328371 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:42.306759 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:42.306901 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:42.306982 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:42.315004 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:42.315099 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:43.309224 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:43.309397 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:43.309505 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:43.316189 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:43.316270 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:44.314275 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:44.314352 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:44.314391 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:44.323605 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:44.323662 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:45.306305 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:45.306511 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:45.306590 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:45.321765 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:45.321886 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:46.308193 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:46.308358 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:46.308449 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:46.319842 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:46.319944 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:47.310322 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:47.310472 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:47.310651 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:47.324665 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:47.324727 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:48.303288 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:48.303438 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:48.303784 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:48.311851 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:48.311943 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:49.306163 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:49.306476 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:49.306938 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:49.315330 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:49.315426 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:50.308450 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:50.308818 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:50.310031 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:50.318954 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:50.319167 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:51.307951 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:51.308426 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:51.308746 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:51.318044 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:51.318111 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:52.306674 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:52.306875 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:52.307225 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:52.316602 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:52.317051 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:53.306331 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:53.306875 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:53.307333 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:53.317973 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:53.318126 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:54.306089 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:54.306190 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:54.306238 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:54.317544 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:54.317760 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:55.306908 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:55.307430 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:55.307806 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:55.317445 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:55.318311 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:56.307288 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:56.308334 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:56.309478 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:56.323484 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:56.324367 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:57.310437 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:57.310601 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:57.310676 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:57.320590 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:57.320691 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:58.311024 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:58.311337 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:58.311725 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:58.320056 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:58.320452 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:53:59.312379 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:59.312502 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:59.312576 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:53:59.327038 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:53:59.327124 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:00.310854 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:00.310998 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:00.311061 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:00.321563 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:54:00.321660 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:01.317836 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:01.318012 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:01.318097 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:01.331259 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:54:01.331331 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:02.314836 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:02.314976 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:02.315053 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:02.325786 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:54:02.325847 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:03.309935 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:03.310136 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:03.310289 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:03.323675 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:54:03.323764 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:04.308777 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:04.309440 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:04.309792 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:04.319499 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:54:04.320028 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:05.308367 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:05.308497 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:05.308830 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:05.315915 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:54:05.316138 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:06.308157 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:06.308827 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:06.308960 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:06.318382 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 2
  I0512 14:54:06.318467 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 14:54:07.319499 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:07.319584 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:07.319629 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 14:54:07.329037 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 14:54:07.329099 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Getting /status @ 05/12/25 14:54:07.339
  I0512 14:54:07.351436 23 daemon_set.go:927] Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/12/25 14:54:07.351
  I0512 14:54:07.372186 23 daemon_set.go:947] updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/12/25 14:54:07.372
  I0512 14:54:07.377408 23 daemon_set.go:972] Observed &DaemonSet event: ADDED
  I0512 14:54:07.377687 23 daemon_set.go:972] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.377885 23 daemon_set.go:972] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.378365 23 daemon_set.go:972] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.378549 23 daemon_set.go:972] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.378735 23 daemon_set.go:972] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.378969 23 daemon_set.go:965] Found daemon set daemon-set in namespace daemonsets-3382 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  I0512 14:54:07.379255 23 daemon_set.go:976] Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/12/25 14:54:07.379
  STEP: watching for the daemon set status to be patched @ 05/12/25 14:54:07.396
  I0512 14:54:07.401847 23 daemon_set.go:1016] Observed &DaemonSet event: ADDED
  I0512 14:54:07.402360 23 daemon_set.go:1016] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.402955 23 daemon_set.go:1016] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.403260 23 daemon_set.go:1016] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.403479 23 daemon_set.go:1016] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.404057 23 daemon_set.go:1016] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.404576 23 daemon_set.go:1012] Observed daemon set daemon-set in namespace daemonsets-3382 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  I0512 14:54:07.405277 23 daemon_set.go:1016] Observed &DaemonSet event: MODIFIED
  I0512 14:54:07.405958 23 daemon_set.go:1009] Found daemon set daemon-set in namespace daemonsets-3382 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  I0512 14:54:07.406338 23 daemon_set.go:1020] Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/25 14:54:07.414
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3382, will wait for the garbage collector to delete the pods @ 05/12/25 14:54:07.414
  I0512 14:54:07.484722 23 resources.go:139] Deleting DaemonSet.extensions daemon-set took: 12.781584ms
  I0512 14:54:07.585642 23 resources.go:163] Terminating DaemonSet.extensions daemon-set pods took: 100.912466ms
  I0512 14:54:10.394883 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 14:54:10.395232 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  I0512 14:54:10.405342 23 daemon_set.go:135] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13748"},"items":null}

  I0512 14:54:10.414729 23 daemon_set.go:140] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13748"},"items":null}

  I0512 14:54:10.451789 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3382" for this suite. @ 05/12/25 14:54:10.46
• [30.313 seconds]
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/secrets.go:97
  STEP: Creating a kubernetes client @ 05/12/25 14:54:10.48
  I0512 14:54:10.480423 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 14:54:10.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 14:54:10.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 14:54:10.532
  STEP: creating secret secrets-4/secret-test-730f01e5-7651-4a19-a668-baf53916b30d @ 05/12/25 14:54:10.538
  STEP: Creating a pod to test consume secrets @ 05/12/25 14:54:10.549
  STEP: Saw pod success @ 05/12/25 14:54:16.608
  I0512 14:54:16.618035 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod pod-configmaps-a0827e77-c443-4051-a040-804eefa6cf72 container env-test: <nil>
  STEP: delete the pod @ 05/12/25 14:54:16.655
  I0512 14:54:16.690202 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4" for this suite. @ 05/12/25 14:54:16.7
• [6.233 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/service_accounts.go:163
  STEP: Creating a kubernetes client @ 05/12/25 14:54:16.716
  I0512 14:54:16.717192 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/25 14:54:16.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 14:54:16.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 14:54:16.769
  I0512 14:54:16.861630 23 service_accounts.go:253] created pod pod-service-account-defaultsa
  I0512 14:54:16.861830 23 service_accounts.go:267] pod pod-service-account-defaultsa service account token volume mount: true
  I0512 14:54:16.890979 23 service_accounts.go:253] created pod pod-service-account-mountsa
  I0512 14:54:16.891660 23 service_accounts.go:267] pod pod-service-account-mountsa service account token volume mount: true
  I0512 14:54:16.907213 23 service_accounts.go:253] created pod pod-service-account-nomountsa
  I0512 14:54:16.907310 23 service_accounts.go:267] pod pod-service-account-nomountsa service account token volume mount: false
  I0512 14:54:16.926416 23 service_accounts.go:253] created pod pod-service-account-defaultsa-mountspec
  I0512 14:54:16.927570 23 service_accounts.go:267] pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  I0512 14:54:16.953647 23 service_accounts.go:253] created pod pod-service-account-mountsa-mountspec
  I0512 14:54:16.953724 23 service_accounts.go:267] pod pod-service-account-mountsa-mountspec service account token volume mount: true
  I0512 14:54:16.971416 23 service_accounts.go:253] created pod pod-service-account-nomountsa-mountspec
  I0512 14:54:16.971972 23 service_accounts.go:267] pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  I0512 14:54:16.987650 23 service_accounts.go:253] created pod pod-service-account-defaultsa-nomountspec
  I0512 14:54:16.987937 23 service_accounts.go:267] pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  I0512 14:54:17.003884 23 service_accounts.go:253] created pod pod-service-account-mountsa-nomountspec
  I0512 14:54:17.003967 23 service_accounts.go:267] pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  I0512 14:54:17.050720 23 service_accounts.go:253] created pod pod-service-account-nomountsa-nomountspec
  I0512 14:54:17.051049 23 service_accounts.go:267] pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  I0512 14:54:17.051948 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6215" for this suite. @ 05/12/25 14:54:17.075
• [0.381 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for API chunking should support continue listing from the last key if the original version has been compacted away, though the list is inconsistent [Slow] [Conformance] [sig-api-machinery, Slow, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/chunking.go:144
  STEP: Creating a kubernetes client @ 05/12/25 14:54:17.098
  I0512 14:54:17.098465 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename chunking @ 05/12/25 14:54:17.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 14:54:17.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 14:54:17.182
  STEP: creating a large number of resources @ 05/12/25 14:54:17.193
  STEP: retrieving the first page @ 05/12/25 14:54:34.81
  I0512 14:54:34.864023 23 chunking.go:163] Retrieved 40/40 results with rv 14404 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ
  STEP: retrieving the second page until the token expires @ 05/12/25 14:54:34.864
  I0512 14:54:54.878227 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:55:14.877058 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:55:34.875223 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:55:54.887182 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:56:14.873061 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:56:34.872626 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:56:54.887838 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:57:14.877758 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:57:34.877433 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:57:54.874091 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:58:14.875309 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:58:34.877653 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:58:54.874635 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:59:14.876875 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:59:34.872248 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 14:59:54.881408 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 15:00:14.877182 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 15:00:34.876860 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 15:00:54.885750 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 15:01:14.876426 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 15:01:34.876246 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 15:01:54.877793 23 chunking.go:171] Token eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTQ0MDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMzlcdTAwMDAifQ has not expired yet
  I0512 15:02:14.874112 23 chunking.go:177] got error The provided continue parameter is too old to display a consistent list result. You can start a new list without the continue parameter, or use the continue token in this response to retrieve the remainder of the results. Continuing with the provided token results in an inconsistent list - objects that were created, modified, or deleted between the time the first chunk was returned and now may show up in the list.
  I0512 15:02:14.874195 23 chunking.go:186] Retrieved inconsistent continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6LTEsInN0YXJ0IjoidGVtcGxhdGUtMDAzOVx1MDAwMCJ9
  STEP: retrieving the second page again with the token received with the error message @ 05/12/25 15:02:14.874
  STEP: retrieving all remaining pages @ 05/12/25 15:02:14.886
  I0512 15:02:14.903167 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTY4NjEsInN0YXJ0IjoidGVtcGxhdGUtMDExOVx1MDAwMCJ9
  I0512 15:02:14.916684 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTY4NjEsInN0YXJ0IjoidGVtcGxhdGUtMDE1OVx1MDAwMCJ9
  I0512 15:02:14.932699 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTY4NjEsInN0YXJ0IjoidGVtcGxhdGUtMDE5OVx1MDAwMCJ9
  I0512 15:02:14.946260 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTY4NjEsInN0YXJ0IjoidGVtcGxhdGUtMDIzOVx1MDAwMCJ9
  I0512 15:02:14.958106 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTY4NjEsInN0YXJ0IjoidGVtcGxhdGUtMDI3OVx1MDAwMCJ9
  I0512 15:02:14.973501 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTY4NjEsInN0YXJ0IjoidGVtcGxhdGUtMDMxOVx1MDAwMCJ9
  I0512 15:02:14.985349 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MTY4NjEsInN0YXJ0IjoidGVtcGxhdGUtMDM1OVx1MDAwMCJ9
  I0512 15:02:14.998101 23 chunking.go:221] Retrieved 40/40 results with rv 16861 and continue 
  I0512 15:02:14.999149 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "chunking-3682" for this suite. @ 05/12/25 15:02:15.012
• [477.929 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1044
  STEP: Creating a kubernetes client @ 05/12/25 15:02:15.028
  I0512 15:02:15.028085 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 15:02:15.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:02:15.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:02:15.085
  STEP: create deployment with httpd image @ 05/12/25 15:02:15.091
  I0512 15:02:15.091982 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2154 create -f -'
  I0512 15:02:15.367292 23 builder.go:146] stderr: ""
  I0512 15:02:15.367373 23 builder.go:147] stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/12/25 15:02:15.367
  I0512 15:02:15.368146 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2154 diff -f -'
  I0512 15:02:20.528394 23 builder.go:135] rc: 1
  I0512 15:02:20.528501 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2154 delete -f -'
  I0512 15:02:20.674610 23 builder.go:146] stderr: ""
  I0512 15:02:20.674714 23 builder.go:147] stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  I0512 15:02:20.674904 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2154" for this suite. @ 05/12/25 15:02:20.688
• [5.676 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/12/25 15:02:20.704
  I0512 15:02:20.704790 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:02:20.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:02:20.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:02:20.754
  STEP: Creating secret with name secret-test-a44877ab-e613-4384-9135-9af81bdbd3f3 @ 05/12/25 15:02:20.762
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:02:20.79
  STEP: Saw pod success @ 05/12/25 15:02:24.862
  I0512 15:02:24.869028 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-secrets-74ef2c86-e18b-4f45-b8ed-578a712e8f1a container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:02:24.916
  I0512 15:02:24.974733 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8201" for this suite. @ 05/12/25 15:02:24.983
• [4.293 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:368
  STEP: Creating a kubernetes client @ 05/12/25 15:02:24.999
  I0512 15:02:25.000063 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:02:25.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:02:25.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:02:25.044
  STEP: Setting up server cert @ 05/12/25 15:02:25.138
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:02:25.816
  STEP: Deploying the webhook pod @ 05/12/25 15:02:25.839
  STEP: Wait for the deployment to be ready @ 05/12/25 15:02:25.873
  I0512 15:02:25.892481 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:02:27.911
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:02:27.937
  I0512 15:02:28.937465 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/12/25 15:02:28.951
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/25 15:02:28.951
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/12/25 15:02:28.983
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/12/25 15:02:30.012
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/25 15:02:30.012
  STEP: Having no error when timeout is longer than webhook latency @ 05/12/25 15:02:31.071
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/25 15:02:31.071
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/12/25 15:02:36.16
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/25 15:02:36.16
  I0512 15:02:41.446608 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1502" for this suite. @ 05/12/25 15:02:41.466
  STEP: Destroying namespace "webhook-markers-1243" for this suite. @ 05/12/25 15:02:41.488
• [16.515 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:96
  STEP: Creating a kubernetes client @ 05/12/25 15:02:41.515
  I0512 15:02:41.516133 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 15:02:41.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:02:41.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:02:41.604
  STEP: Creating a pod to test substitution in container's args @ 05/12/25 15:02:41.625
  STEP: Saw pod success @ 05/12/25 15:02:47.75
  I0512 15:02:47.761145 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod var-expansion-787d4cb1-6d47-416f-9627-6c48ab0101d3 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 15:02:47.791
  I0512 15:02:47.871350 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7364" for this suite. @ 05/12/25 15:02:47.891
• [6.424 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance] [sig-network, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/network/networking.go:108
  STEP: Creating a kubernetes client @ 05/12/25 15:02:47.941
  I0512 15:02:47.941126 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/25 15:02:47.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:02:48.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:02:48.022
  STEP: Performing setup for networking test in namespace pod-network-test-4627 @ 05/12/25 15:02:48.029
  STEP: creating a selector @ 05/12/25 15:02:48.029
  STEP: Creating the service pods in kubernetes @ 05/12/25 15:02:48.029
  I0512 15:02:48.029571 23 helper.go:48] Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/12/25 15:03:20.46
  I0512 15:03:22.572667 23 utils.go:803] Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  I0512 15:03:22.573115 23 utils.go:496] Going to poll 10.233.68.16 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:03:22.579873 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.68.16:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4627 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:22.580101 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:22.581627 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:22.582088 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4627/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.68.16%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  I0512 15:03:22.737911 23 utils.go:513] Found all 1 expected endpoints: [netserver-0]
  I0512 15:03:22.738037 23 utils.go:496] Going to poll 10.233.69.14 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:03:22.749647 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.69.14:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4627 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:22.749743 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:22.754381 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:22.754776 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4627/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.69.14%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  I0512 15:03:23.026017 23 utils.go:513] Found all 1 expected endpoints: [netserver-1]
  I0512 15:03:23.026423 23 utils.go:496] Going to poll 10.233.70.9 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:03:23.037131 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.70.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4627 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:23.037215 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:23.038755 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:23.038938 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4627/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.70.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  I0512 15:03:23.216318 23 utils.go:513] Found all 1 expected endpoints: [netserver-2]
  I0512 15:03:23.216406 23 utils.go:496] Going to poll 10.233.67.9 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:03:23.238098 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.67.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4627 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:23.238388 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:23.239847 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:23.239988 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4627/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.67.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  I0512 15:03:23.403095 23 utils.go:513] Found all 1 expected endpoints: [netserver-3]
  I0512 15:03:23.403360 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4627" for this suite. @ 05/12/25 15:03:23.414
• [35.487 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance] [sig-network, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/network/networking.go:96
  STEP: Creating a kubernetes client @ 05/12/25 15:03:23.429
  I0512 15:03:23.429535 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/25 15:03:23.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:03:23.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:03:23.528
  STEP: Performing setup for networking test in namespace pod-network-test-1847 @ 05/12/25 15:03:23.535
  STEP: creating a selector @ 05/12/25 15:03:23.535
  STEP: Creating the service pods in kubernetes @ 05/12/25 15:03:23.535
  I0512 15:03:23.535413 23 helper.go:48] Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/12/25 15:03:45.825
  I0512 15:03:47.868219 23 utils.go:803] Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  I0512 15:03:47.868678 23 networking.go:42] Breadth first check of 10.233.68.18 on host 10.62.16.75...
  I0512 15:03:47.881232 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.19:9080/dial?request=hostname&protocol=udp&host=10.233.68.18&port=8081&tries=1'] Namespace:pod-network-test-1847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:47.881755 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:47.883176 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:47.883586 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.19%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.68.18%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  I0512 15:03:48.041329 23 utils.go:356] Waiting for responses: map[]
  I0512 15:03:48.041446 23 utils.go:360] reached 10.233.68.18 after 0/1 tries
  I0512 15:03:48.041520 23 networking.go:42] Breadth first check of 10.233.69.15 on host 10.62.16.76...
  I0512 15:03:48.050175 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.19:9080/dial?request=hostname&protocol=udp&host=10.233.69.15&port=8081&tries=1'] Namespace:pod-network-test-1847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:48.050519 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:48.053860 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:48.054543 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.19%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.69.15%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  I0512 15:03:48.232284 23 utils.go:356] Waiting for responses: map[]
  I0512 15:03:48.232368 23 utils.go:360] reached 10.233.69.15 after 0/1 tries
  I0512 15:03:48.232399 23 networking.go:42] Breadth first check of 10.233.70.10 on host 10.62.16.77...
  I0512 15:03:48.243632 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.19:9080/dial?request=hostname&protocol=udp&host=10.233.70.10&port=8081&tries=1'] Namespace:pod-network-test-1847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:48.243702 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:48.245164 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:48.245669 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.19%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.70.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  I0512 15:03:48.390699 23 utils.go:356] Waiting for responses: map[]
  I0512 15:03:48.391552 23 utils.go:360] reached 10.233.70.10 after 0/1 tries
  I0512 15:03:48.392731 23 networking.go:42] Breadth first check of 10.233.67.10 on host 10.62.16.78...
  I0512 15:03:48.401789 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.19:9080/dial?request=hostname&protocol=udp&host=10.233.67.10&port=8081&tries=1'] Namespace:pod-network-test-1847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:48.402269 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:48.403994 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:48.404745 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.19%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.67.10%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  I0512 15:03:48.536774 23 utils.go:356] Waiting for responses: map[]
  I0512 15:03:48.537330 23 utils.go:360] reached 10.233.67.10 after 0/1 tries
  I0512 15:03:48.537740 23 networking.go:53] Going to retry 0 out of 4 pods....
  I0512 15:03:48.538963 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1847" for this suite. @ 05/12/25 15:03:48.56
• [25.147 seconds]
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/12/25 15:03:48.576
  I0512 15:03:48.576763 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 15:03:48.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:03:48.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:03:48.64
  I0512 15:03:48.647844 23 deployment.go:792] Creating deployment "test-recreate-deployment"
  I0512 15:03:48.663450 23 deployment.go:798] Waiting deployment "test-recreate-deployment" to be updated to revision 1
  I0512 15:03:48.684060 23 deployment.go:222] deployment "test-recreate-deployment" doesn't have the required revision set
  I0512 15:03:50.702863 23 deployment.go:802] Waiting deployment "test-recreate-deployment" to complete
  I0512 15:03:50.710891 23 deployment.go:807] Triggering a new rollout for deployment "test-recreate-deployment"
  I0512 15:03:50.734720 23 deployment.go:313] Updating deployment test-recreate-deployment
  I0512 15:03:50.734813 23 deployment.go:814] Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  I0512 15:03:50.967505 23 deployment.go:633] Deployment "test-recreate-deployment":
  (v1.Deployment) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=24) "test-recreate-deployment",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-8318",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "5e26fc17-3bb4-40ad-8a71-16b3b8e4ac87",
      ResourceVersion: (string) (len=5) "18035",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659028,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=1) {
        (string) (len=4) "name": (string) (len=12) "sample-pod-3"
      },
      Annotations: (map[string]string) (len=1) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "2"
      },
      OwnerReferences: ([]v1.OwnerReference) <nil>,
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=570) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 7d 7d 2c  |},"f:name":{}}},|
              00000030  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 70 72 6f  |"f:spec":{"f:pro|
              00000040  67 72 65 73 73 44 65 61  64 6c 69 6e 65 53 65 63  |gressDeadlineSec|
              00000050  6f 6e 64 73 22 3a 7b 7d  2c 22 66 3a 72 65 70 6c  |onds":{},"f:repl|
              00000060  69 63 61 73 22 3a 7b 7d  2c 22 66 3a 72 65 76 69  |icas":{},"f:revi|
              00000070  73 69 6f 6e 48 69 73 74  6f 72 79 4c 69 6d 69 74  |sionHistoryLimit|
              00000080  22 3a 7b 7d 2c 22 66 3a  73 65 6c 65 63 74 6f 72  |":{},"f:selector|
              00000090  22 3a 7b 7d 2c 22 66 3a  73 74 72 61 74 65 67 79  |":{},"f:strategy|
              000000a0  22 3a 7b 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |":{"f:type":{}},|
              000000b0  22 66 3a 74 65 6d 70 6c  61 74 65 22 3a 7b 22 66  |"f:template":{"f|
              000000c0  3a 6d 65 74 61 64 61 74  61 22 3a 7b 22 66 3a 6c  |:metadata":{"f:l|
              000000d0  61 62 65 6c 73 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |abels":{".":{},"|
              000000e0  66 3a 6e 61 6d 65 22 3a  7b 7d 7d 7d 2c 22 66 3a  |f:name":{}}},"f:|
              000000f0  73 70 65 63 22 3a 7b 22  66 3a 63 6f 6e 74 61 69  |spec":{"f:contai|
              00000100  6e 65 72 73 22 3a 7b 22  6b 3a 7b 5c 22 6e 61 6d  |ners":{"k:{\"nam|
              00000110  65 5c 22 3a 5c 22 68 74  74 70 64 5c 22 7d 22 3a  |e\":\"httpd\"}":|
              00000120  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |{".":{},"f:image|
              00000130  22 3a 7b 7d 2c 22 66 3a  69 6d 61 67 65 50 75 6c  |":{},"f:imagePul|
              00000140  6c 50 6f 6c 69 63 79 22  3a 7b 7d 2c 22 66 3a 6e  |lPolicy":{},"f:n|
              00000150  61 6d 65 22 3a 7b 7d 2c  22 66 3a 72 65 73 6f 75  |ame":{},"f:resou|
              00000160  72 63 65 73 22 3a 7b 7d  2c 22 66 3a 73 65 63 75  |rces":{},"f:secu|
              00000170  72 69 74 79 43 6f 6e 74  65 78 74 22 3a 7b 7d 2c  |rityContext":{},|
              00000180  22 66 3a 74 65 72 6d 69  6e 61 74 69 6f 6e 4d 65  |"f:terminationMe|
              00000190  73 73 61 67 65 50 61 74  68 22 3a 7b 7d 2c 22 66  |ssagePath":{},"f|
              000001a0  3a 74 65 72 6d 69 6e 61  74 69 6f 6e 4d 65 73 73  |:terminationMess|
              000001b0  61 67 65 50 6f 6c 69 63  79 22 3a 7b 7d 7d 7d 2c  |agePolicy":{}}},|
              000001c0  22 66 3a 64 6e 73 50 6f  6c 69 63 79 22 3a 7b 7d  |"f:dnsPolicy":{}|
              000001d0  2c 22 66 3a 72 65 73 74  61 72 74 50 6f 6c 69 63  |,"f:restartPolic|
              000001e0  79 22 3a 7b 7d 2c 22 66  3a 73 63 68 65 64 75 6c  |y":{},"f:schedul|
              000001f0  65 72 4e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 73 65  |erName":{},"f:se|
              00000200  63 75 72 69 74 79 43 6f  6e 74 65 78 74 22 3a 7b  |curityContext":{|
              00000210  7d 2c 22 66 3a 74 65 72  6d 69 6e 61 74 69 6f 6e  |},"f:termination|
              00000220  47 72 61 63 65 50 65 72  69 6f 64 53 65 63 6f 6e  |GracePeriodSecon|
              00000230  64 73 22 3a 7b 7d 7d 7d  7d 7d                    |ds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=495) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 72 65 76 69 73  69 6f 6e 22 3a 7b 7d 7d  |io/revision":{}}|
              00000050  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 22 66  |},"f:status":{"f|
              00000060  3a 63 6f 6e 64 69 74 69  6f 6e 73 22 3a 7b 22 2e  |:conditions":{".|
              00000070  22 3a 7b 7d 2c 22 6b 3a  7b 5c 22 74 79 70 65 5c  |":{},"k:{\"type\|
              00000080  22 3a 5c 22 41 76 61 69  6c 61 62 6c 65 5c 22 7d  |":\"Available\"}|
              00000090  22 3a 7b 22 2e 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |":{".":{},"f:las|
              000000a0  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              000000b0  3a 7b 7d 2c 22 66 3a 6c  61 73 74 55 70 64 61 74  |:{},"f:lastUpdat|
              000000c0  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6d 65 73  |eTime":{},"f:mes|
              000000d0  73 61 67 65 22 3a 7b 7d  2c 22 66 3a 72 65 61 73  |sage":{},"f:reas|
              000000e0  6f 6e 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |on":{},"f:status|
              000000f0  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000100  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000110  22 50 72 6f 67 72 65 73  73 69 6e 67 5c 22 7d 22  |"Progressing\"}"|
              00000120  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              00000130  54 72 61 6e 73 69 74 69  6f 6e 54 69 6d 65 22 3a  |TransitionTime":|
              00000140  7b 7d 2c 22 66 3a 6c 61  73 74 55 70 64 61 74 65  |{},"f:lastUpdate|
              00000150  54 69 6d 65 22 3a 7b 7d  2c 22 66 3a 6d 65 73 73  |Time":{},"f:mess|
              00000160  61 67 65 22 3a 7b 7d 2c  22 66 3a 72 65 61 73 6f  |age":{},"f:reaso|
              00000170  6e 22 3a 7b 7d 2c 22 66  3a 73 74 61 74 75 73 22  |n":{},"f:status"|
              00000180  3a 7b 7d 2c 22 66 3a 74  79 70 65 22 3a 7b 7d 7d  |:{},"f:type":{}}|
              00000190  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              000001a0  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              000001b0  65 70 6c 69 63 61 73 22  3a 7b 7d 2c 22 66 3a 75  |eplicas":{},"f:u|
              000001c0  6e 61 76 61 69 6c 61 62  6c 65 52 65 70 6c 69 63  |navailableReplic|
              000001d0  61 73 22 3a 7b 7d 2c 22  66 3a 75 70 64 61 74 65  |as":{},"f:update|
              000001e0  64 52 65 70 6c 69 63 61  73 22 3a 7b 7d 7d 7d     |dReplicas":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.DeploymentSpec) {
      Replicas: (*int32)(1),
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=1) {
          (string) (len=4) "name": (string) (len=12) "sample-pod-3"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=1) {
            (string) (len=4) "name": (string) (len=12) "sample-pod-3"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      },
      Strategy: (v1.DeploymentStrategy) {
        Type: (v1.DeploymentStrategyType) (len=8) "Recreate",
        RollingUpdate: (*v1.RollingUpdateDeployment)(<nil>)
      },
      MinReadySeconds: (int32) 0,
      RevisionHistoryLimit: (*int32)(10),
      Paused: (bool) false,
      ProgressDeadlineSeconds: (*int32)(600)
    },
    Status: (v1.DeploymentStatus) {
      ObservedGeneration: (int64) 2,
      Replicas: (int32) 1,
      UpdatedReplicas: (int32) 1,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      UnavailableReplicas: (int32) 1,
      Conditions: ([]v1.DeploymentCondition) (len=2) {
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=9) "Available",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=26) "MinimumReplicasUnavailable",
          Message: (string) (len=46) "Deployment does not have minimum availability."
        },
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=11) "Progressing",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659028,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=17) "ReplicaSetUpdated",
          Message: (string) (len=63) "ReplicaSet \"test-recreate-deployment-88d47c55d\" is progressing."
        }
      },
      CollisionCount: (*int32)(<nil>)
    }
  }


  I0512 15:03:50.981296 23 deployment.go:39] New ReplicaSet "test-recreate-deployment-88d47c55d" of Deployment "test-recreate-deployment":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=34) "test-recreate-deployment-88d47c55d",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-8318",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "4c67417c-8b8f-44fb-b2d4-99344fc57d2e",
      ResourceVersion: (string) (len=5) "18034",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659030,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=12) "sample-pod-3",
        (string) (len=17) "pod-template-hash": (string) (len=9) "88d47c55d"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "1",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "2",
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=24) "test-recreate-deployment",
          UID: (types.UID) (len=36) "5e26fc17-3bb4-40ad-8a71-16b3b8e4ac87",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=781) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 35 65 32 36 66 63  31 37 2d 33 62 62 34 2d  |\"5e26fc17-3bb4-|
              00000120  34 30 61 64 2d 38 61 37  31 2d 31 36 62 33 62 38  |40ad-8a71-16b3b8|
              00000130  65 34 61 63 38 37 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |e4ac87\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 72 65 70  |"f:spec":{"f:rep|
              00000150  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 73 65 6c  |licas":{},"f:sel|
              00000160  65 63 74 6f 72 22 3a 7b  7d 2c 22 66 3a 74 65 6d  |ector":{},"f:tem|
              00000170  70 6c 61 74 65 22 3a 7b  22 66 3a 6d 65 74 61 64  |plate":{"f:metad|
              00000180  61 74 61 22 3a 7b 22 66  3a 6c 61 62 65 6c 73 22  |ata":{"f:labels"|
              00000190  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6e 61 6d 65  |:{".":{},"f:name|
              000001a0  22 3a 7b 7d 2c 22 66 3a  70 6f 64 2d 74 65 6d 70  |":{},"f:pod-temp|
              000001b0  6c 61 74 65 2d 68 61 73  68 22 3a 7b 7d 7d 7d 2c  |late-hash":{}}},|
              000001c0  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 63 6f 6e  |"f:spec":{"f:con|
              000001d0  74 61 69 6e 65 72 73 22  3a 7b 22 6b 3a 7b 5c 22  |tainers":{"k:{\"|
              000001e0  6e 61 6d 65 5c 22 3a 5c  22 68 74 74 70 64 5c 22  |name\":\"httpd\"|
              000001f0  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000200  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000210  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000220  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              00000230  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              00000240  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000250  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000260  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              00000270  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000280  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              00000290  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              000002a0  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              000002b0  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              000002c0  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              000002d0  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              000002e0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              000002f0  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000300  63 6f 6e 64 73 22 3a 7b  7d 7d 7d 7d 7d           |conds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=84) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  66 75 6c 6c 79 4c 61 62  65 6c 65 64 52 65 70 6c  |fullyLabeledRepl|
              00000020  69 63 61 73 22 3a 7b 7d  2c 22 66 3a 6f 62 73 65  |icas":{},"f:obse|
              00000030  72 76 65 64 47 65 6e 65  72 61 74 69 6f 6e 22 3a  |rvedGeneration":|
              00000040  7b 7d 2c 22 66 3a 72 65  70 6c 69 63 61 73 22 3a  |{},"f:replicas":|
              00000050  7b 7d 7d 7d                                       |{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(1),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=12) "sample-pod-3",
          (string) (len=17) "pod-template-hash": (string) (len=9) "88d47c55d"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=12) "sample-pod-3",
            (string) (len=17) "pod-template-hash": (string) (len=9) "88d47c55d"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 1,
      FullyLabeledReplicas: (int32) 1,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 1,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 15:03:50.995465 23 deployment.go:44] All old ReplicaSets of Deployment "test-recreate-deployment":
  I0512 15:03:50.996454 23 deployment.go:47] (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=35) "test-recreate-deployment-7549bcf47c",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-8318",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "94c9ed96-6d71-4619-a765-86afd347963b",
      ResourceVersion: (string) (len=5) "18022",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659028,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "7549bcf47c",
        (string) (len=4) "name": (string) (len=12) "sample-pod-3"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "1",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=24) "test-recreate-deployment",
          UID: (types.UID) (len=36) "5e26fc17-3bb4-40ad-8a71-16b3b8e4ac87",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=783) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 35 65 32 36 66 63  31 37 2d 33 62 62 34 2d  |\"5e26fc17-3bb4-|
              00000120  34 30 61 64 2d 38 61 37  31 2d 31 36 62 33 62 38  |40ad-8a71-16b3b8|
              00000130  65 34 61 63 38 37 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |e4ac87\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 72 65 70  |"f:spec":{"f:rep|
              00000150  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 73 65 6c  |licas":{},"f:sel|
              00000160  65 63 74 6f 72 22 3a 7b  7d 2c 22 66 3a 74 65 6d  |ector":{},"f:tem|
              00000170  70 6c 61 74 65 22 3a 7b  22 66 3a 6d 65 74 61 64  |plate":{"f:metad|
              00000180  61 74 61 22 3a 7b 22 66  3a 6c 61 62 65 6c 73 22  |ata":{"f:labels"|
              00000190  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6e 61 6d 65  |:{".":{},"f:name|
              000001a0  22 3a 7b 7d 2c 22 66 3a  70 6f 64 2d 74 65 6d 70  |":{},"f:pod-temp|
              000001b0  6c 61 74 65 2d 68 61 73  68 22 3a 7b 7d 7d 7d 2c  |late-hash":{}}},|
              000001c0  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 63 6f 6e  |"f:spec":{"f:con|
              000001d0  74 61 69 6e 65 72 73 22  3a 7b 22 6b 3a 7b 5c 22  |tainers":{"k:{\"|
              000001e0  6e 61 6d 65 5c 22 3a 5c  22 61 67 6e 68 6f 73 74  |name\":\"agnhost|
              000001f0  5c 22 7d 22 3a 7b 22 2e  22 3a 7b 7d 2c 22 66 3a  |\"}":{".":{},"f:|
              00000200  69 6d 61 67 65 22 3a 7b  7d 2c 22 66 3a 69 6d 61  |image":{},"f:ima|
              00000210  67 65 50 75 6c 6c 50 6f  6c 69 63 79 22 3a 7b 7d  |gePullPolicy":{}|
              00000220  2c 22 66 3a 6e 61 6d 65  22 3a 7b 7d 2c 22 66 3a  |,"f:name":{},"f:|
              00000230  72 65 73 6f 75 72 63 65  73 22 3a 7b 7d 2c 22 66  |resources":{},"f|
              00000240  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              00000250  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000260  69 6f 6e 4d 65 73 73 61  67 65 50 61 74 68 22 3a  |ionMessagePath":|
              00000270  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000280  6e 4d 65 73 73 61 67 65  50 6f 6c 69 63 79 22 3a  |nMessagePolicy":|
              00000290  7b 7d 7d 7d 2c 22 66 3a  64 6e 73 50 6f 6c 69 63  |{}}},"f:dnsPolic|
              000002a0  79 22 3a 7b 7d 2c 22 66  3a 72 65 73 74 61 72 74  |y":{},"f:restart|
              000002b0  50 6f 6c 69 63 79 22 3a  7b 7d 2c 22 66 3a 73 63  |Policy":{},"f:sc|
              000002c0  68 65 64 75 6c 65 72 4e  61 6d 65 22 3a 7b 7d 2c  |hedulerName":{},|
              000002d0  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              000002e0  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              000002f0  61 74 69 6f 6e 47 72 61  63 65 50 65 72 69 6f 64  |ationGracePeriod|
              00000300  53 65 63 6f 6e 64 73 22  3a 7b 7d 7d 7d 7d 7d     |Seconds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=56) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  6f 62 73 65 72 76 65 64  47 65 6e 65 72 61 74 69  |observedGenerati|
              00000020  6f 6e 22 3a 7b 7d 2c 22  66 3a 72 65 70 6c 69 63  |on":{},"f:replic|
              00000030  61 73 22 3a 7b 7d 7d 7d                           |as":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(0),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=12) "sample-pod-3",
          (string) (len=17) "pod-template-hash": (string) (len=10) "7549bcf47c"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=12) "sample-pod-3",
            (string) (len=17) "pod-template-hash": (string) (len=10) "7549bcf47c"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=7) "agnhost",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 0,
      FullyLabeledReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 15:03:51.010022 23 deployment.go:67] Pod "test-recreate-deployment-88d47c55d-dk7pp" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=40) "test-recreate-deployment-88d47c55d-dk7pp",
      GenerateName: (string) (len=35) "test-recreate-deployment-88d47c55d-",
      Namespace: (string) (len=15) "deployment-8318",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "d86a6af1-e9e4-47a3-91b9-19f8eec6de71",
      ResourceVersion: (string) (len=5) "18033",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659030,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=12) "sample-pod-3",
        (string) (len=17) "pod-template-hash": (string) (len=9) "88d47c55d"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=34) "test-recreate-deployment-88d47c55d",
          UID: (types.UID) (len=36) "4c67417c-8b8f-44fb-b2d4-99344fc57d2e",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 34 63  36 37 34 31 37 63 2d 38  |d\":\"4c67417c-8|
              00000090  62 38 66 2d 34 34 66 62  2d 62 32 64 34 2d 39 39  |b8f-44fb-b2d4-99|
              000000a0  33 34 34 66 63 35 37 64  32 65 5c 22 7d 22 3a 7b  |344fc57d2e\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=624) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 6d 65 73 73 61 67  |me":{},"f:messag|
              00000080  65 22 3a 7b 7d 2c 22 66  3a 72 65 61 73 6f 6e 22  |e":{},"f:reason"|
              00000090  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              000000a0  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000000b0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 49 6e  |k:{\"type\":\"In|
              000000c0  69 74 69 61 6c 69 7a 65  64 5c 22 7d 22 3a 7b 22  |itialized\"}":{"|
              000000d0  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 50 72 6f  |.":{},"f:lastPro|
              000000e0  62 65 54 69 6d 65 22 3a  7b 7d 2c 22 66 3a 6c 61  |beTime":{},"f:la|
              000000f0  73 74 54 72 61 6e 73 69  74 69 6f 6e 54 69 6d 65  |stTransitionTime|
              00000100  22 3a 7b 7d 2c 22 66 3a  73 74 61 74 75 73 22 3a  |":{},"f:status":|
              00000110  7b 7d 2c 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |{},"f:type":{}},|
              00000120  22 6b 3a 7b 5c 22 74 79  70 65 5c 22 3a 5c 22 50  |"k:{\"type\":\"P|
              00000130  6f 64 52 65 61 64 79 54  6f 53 74 61 72 74 43 6f  |odReadyToStartCo|
              00000140  6e 74 61 69 6e 65 72 73  5c 22 7d 22 3a 7b 22 2e  |ntainers\"}":{".|
              00000150  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 50 72 6f 62  |":{},"f:lastProb|
              00000160  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |eTime":{},"f:las|
              00000170  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              00000180  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              00000190  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000001a0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 52 65  |k:{\"type\":\"Re|
              000001b0  61 64 79 5c 22 7d 22 3a  7b 22 2e 22 3a 7b 7d 2c  |ady\"}":{".":{},|
              000001c0  22 66 3a 6c 61 73 74 50  72 6f 62 65 54 69 6d 65  |"f:lastProbeTime|
              000001d0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000001e0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000001f0  66 3a 6d 65 73 73 61 67  65 22 3a 7b 7d 2c 22 66  |f:message":{},"f|
              00000200  3a 72 65 61 73 6f 6e 22  3a 7b 7d 2c 22 66 3a 73  |:reason":{},"f:s|
              00000210  74 61 74 75 73 22 3a 7b  7d 2c 22 66 3a 74 79 70  |tatus":{},"f:typ|
              00000220  65 22 3a 7b 7d 7d 7d 2c  22 66 3a 63 6f 6e 74 61  |e":{}}},"f:conta|
              00000230  69 6e 65 72 53 74 61 74  75 73 65 73 22 3a 7b 7d  |inerStatuses":{}|
              00000240  2c 22 66 3a 68 6f 73 74  49 50 22 3a 7b 7d 2c 22  |,"f:hostIP":{},"|
              00000250  66 3a 68 6f 73 74 49 50  73 22 3a 7b 7d 2c 22 66  |f:hostIPs":{},"f|
              00000260  3a 73 74 61 72 74 54 69  6d 65 22 3a 7b 7d 7d 7d  |:startTime":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-92fcc",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-92fcc",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659030,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659030,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)({
              Reason: (string) (len=17) "ContainerCreating",
              Message: (string) ""
            }),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) false,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) "",
          ContainerID: (string) "",
          Started: (*bool)(false),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-92fcc",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 15:03:51.018043 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8318" for this suite. @ 05/12/25 15:03:51.029
• [2.473 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:409
  STEP: Creating a kubernetes client @ 05/12/25 15:03:51.05
  I0512 15:03:51.050486 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 15:03:51.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:03:51.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:03:51.103
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/12/25 15:03:51.11
  I0512 15:03:51.131005 23 dns.go:421] Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1703  fa99ec70-e08a-408b-811f-58550840a4ad 18040 0 2025-05-12 15:03:51 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2025-05-12 15:03:51 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8pwj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},ClusterTrustBundle:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,ClusterTrustBundle:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,ClusterTrustBundle:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,Image:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.52,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8pwj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,AppArmorProfile:nil,SupplementalGroupsPolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/12/25 15:03:53.147
  I0512 15:03:53.147408 23 exec_util.go:59] ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1703 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:53.147463 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:53.149176 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:53.149357 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1703/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/12/25 15:03:53.311
  I0512 15:03:53.312094 23 exec_util.go:59] ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1703 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:03:53.312155 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:03:53.313281 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:03:53.313393 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1703/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  I0512 15:03:53.476319 23 dns.go:423] Deleting pod test-dns-nameservers...
  I0512 15:03:53.502712 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "dns-1703" for this suite. @ 05/12/25 15:03:53.513
• [2.483 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/12/25 15:03:53.535
  I0512 15:03:53.535608 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename watch @ 05/12/25 15:03:53.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:03:53.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:03:53.583
  STEP: getting a starting resourceVersion @ 05/12/25 15:03:53.59
  STEP: starting a background goroutine to produce watch events @ 05/12/25 15:03:53.598
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/12/25 15:03:53.598
  I0512 15:03:56.367440 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5500" for this suite. @ 05/12/25 15:03:56.407
• [2.934 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/12/25 15:03:56.47
  I0512 15:03:56.470804 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:03:56.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:03:56.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:03:56.561
  STEP: Creating secret with name s-test-opt-del-3cced2c6-8f41-4655-8725-fa376d651f52 @ 05/12/25 15:03:56.578
  STEP: Creating secret with name s-test-opt-upd-e22854cb-558c-4472-b224-e9f0d5947aca @ 05/12/25 15:03:56.6
  STEP: Creating the pod @ 05/12/25 15:03:56.624
  STEP: Deleting secret s-test-opt-del-3cced2c6-8f41-4655-8725-fa376d651f52 @ 05/12/25 15:04:00.724
  STEP: Updating secret s-test-opt-upd-e22854cb-558c-4472-b224-e9f0d5947aca @ 05/12/25 15:04:00.737
  STEP: Creating secret with name s-test-opt-create-893787e2-6c90-4a44-81cf-c0606644aad2 @ 05/12/25 15:04:00.745
  STEP: waiting to observe update in volume @ 05/12/25 15:04:00.754
  I0512 15:05:23.610286 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7041" for this suite. @ 05/12/25 15:05:23.636
• [87.191 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:209
  STEP: Creating a kubernetes client @ 05/12/25 15:05:23.662
  I0512 15:05:23.663012 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:05:23.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:05:23.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:05:23.727
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:05:23.735
  STEP: Saw pod success @ 05/12/25 15:05:27.793
  I0512 15:05:27.799937 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod downwardapi-volume-635e32d0-1986-495c-ad36-5f4f98420c31 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:05:27.836
  I0512 15:05:27.903890 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3040" for this suite. @ 05/12/25 15:05:27.912
• [4.265 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:154
  STEP: Creating a kubernetes client @ 05/12/25 15:05:27.928
  I0512 15:05:27.928919 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/25 15:05:27.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:05:27.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:05:27.981
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/25 15:05:28.012
  STEP: create the pod with lifecycle hook @ 05/12/25 15:05:32.076
  STEP: delete the pod with lifecycle hook @ 05/12/25 15:05:34.117
  STEP: check prestop hook @ 05/12/25 15:05:36.16
  I0512 15:05:36.174472 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7241" for this suite. @ 05/12/25 15:05:36.183
• [8.267 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/12/25 15:05:36.196
  I0512 15:05:36.196171 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:05:36.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:05:36.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:05:36.244
  STEP: creating the pod @ 05/12/25 15:05:36.25
  STEP: setting up watch @ 05/12/25 15:05:36.251
  STEP: submitting the pod to kubernetes @ 05/12/25 15:05:36.361
  STEP: verifying the pod is in kubernetes @ 05/12/25 15:05:36.382
  STEP: verifying pod creation was observed @ 05/12/25 15:05:36.398
  STEP: deleting the pod gracefully @ 05/12/25 15:05:38.427
  STEP: verifying pod deletion was observed @ 05/12/25 15:05:38.443
  I0512 15:05:40.091135 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3519" for this suite. @ 05/12/25 15:05:40.101
• [3.916 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0 [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1833
  STEP: Creating a kubernetes client @ 05/12/25 15:05:40.114
  I0512 15:05:40.114573 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 15:05:40.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:05:40.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:05:40.179
  STEP: starting the proxy server @ 05/12/25 15:05:40.186
  I0512 15:05:40.186866 23 util.go:585] Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6629 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/12/25 15:05:40.297
  I0512 15:05:40.325331 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  I0512 15:05:40.329249 23 kubectl.go:2229] kubectl proxy stderr: W0512 15:05:40.296389     142 proxy.go:177] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious

  I0512 15:05:40.329405 23 kubectl.go:2224] kubectl proxy stdout: Starting to serve on 127.0.0.1:36847

  STEP: Destroying namespace "kubectl-6629" for this suite. @ 05/12/25 15:05:40.334
• [0.233 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/12/25 15:05:40.348
  I0512 15:05:40.348054 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:05:40.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:05:40.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:05:40.41
  STEP: Creating secret with name s-test-opt-del-05bb2a6c-002c-4b7d-a244-54828bf12d77 @ 05/12/25 15:05:40.436
  STEP: Creating secret with name s-test-opt-upd-50037be0-2ce3-45a7-94b8-58bc7f72a890 @ 05/12/25 15:05:40.446
  STEP: Creating the pod @ 05/12/25 15:05:40.456
  STEP: Deleting secret s-test-opt-del-05bb2a6c-002c-4b7d-a244-54828bf12d77 @ 05/12/25 15:05:42.563
  STEP: Updating secret s-test-opt-upd-50037be0-2ce3-45a7-94b8-58bc7f72a890 @ 05/12/25 15:05:42.583
  STEP: Creating secret with name s-test-opt-create-df74c1e3-4832-4a06-9302-597b3008c4d6 @ 05/12/25 15:05:42.597
  STEP: waiting to observe update in volume @ 05/12/25 15:05:42.612
  I0512 15:07:03.520329 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6701" for this suite. @ 05/12/25 15:07:03.529
• [83.200 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity should support CSIStorageCapacities API operations [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/csistoragecapacity.go:50
  STEP: Creating a kubernetes client @ 05/12/25 15:07:03.548
  I0512 15:07:03.548581 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/12/25 15:07:03.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:07:03.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:07:03.639
  STEP: getting /apis @ 05/12/25 15:07:03.647
  STEP: getting /apis/storage.k8s.io @ 05/12/25 15:07:03.661
  STEP: getting /apis/storage.k8s.io/v1 @ 05/12/25 15:07:03.665
  STEP: creating @ 05/12/25 15:07:03.668
  STEP: watching @ 05/12/25 15:07:03.709
  I0512 15:07:03.709746 23 csistoragecapacity.go:143] starting watch
  STEP: getting @ 05/12/25 15:07:03.725
  STEP: listing in namespace @ 05/12/25 15:07:03.732
  STEP: listing across namespaces @ 05/12/25 15:07:03.739
  STEP: patching @ 05/12/25 15:07:03.748
  STEP: updating @ 05/12/25 15:07:03.763
  I0512 15:07:03.776512 23 csistoragecapacity.go:181] waiting for watch events with expected annotations in namespace
  I0512 15:07:03.776972 23 csistoragecapacity.go:181] waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/12/25 15:07:03.777
  STEP: deleting a collection @ 05/12/25 15:07:03.81
  I0512 15:07:03.842735 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-5324" for this suite. @ 05/12/25 15:07:03.857
• [0.345 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/12/25 15:07:03.894
  I0512 15:07:03.894356 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 15:07:03.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:07:03.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:07:03.942
  I0512 15:07:03.995619 23 resource.go:87] Pod name rollover-pod: Found 0 pods out of 1
  I0512 15:07:09.007394 23 resource.go:87] Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/25 15:07:09.008
  I0512 15:07:09.008638 23 deployment.go:911] Waiting for pods owned by replica set "test-rollover-controller" to become ready
  I0512 15:07:11.018813 23 deployment.go:921] Creating deployment "test-rollover-deployment"
  I0512 15:07:11.049128 23 deployment.go:934] Make sure deployment "test-rollover-deployment" performs scaling operations
  I0512 15:07:13.072405 23 deployment.go:939] Check revision of new replica set for deployment "test-rollover-deployment"
  I0512 15:07:13.089803 23 deployment.go:943] Ensure that both replica sets have 1 created replica
  I0512 15:07:13.103482 23 deployment.go:952] Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  I0512 15:07:13.129212 23 deployment.go:313] Updating deployment test-rollover-deployment
  I0512 15:07:13.129461 23 deployment.go:961] Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  I0512 15:07:15.148829 23 deployment.go:966] Wait for revision update of deployment "test-rollover-deployment" to 2
  I0512 15:07:15.168015 23 deployment.go:970] Make sure deployment "test-rollover-deployment" is complete
  I0512 15:07:15.270091 23 deployment.go:94] all replica sets need to contain the pod-template-hash label
  I0512 15:07:15.270359 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 14, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5f974d7468\" is progressing."}}, CollisionCount:(*int32)(nil)}
  I0512 15:07:17.288099 23 deployment.go:94] all replica sets need to contain the pod-template-hash label
  I0512 15:07:17.288379 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 14, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5f974d7468\" is progressing."}}, CollisionCount:(*int32)(nil)}
  I0512 15:07:19.294094 23 deployment.go:94] all replica sets need to contain the pod-template-hash label
  I0512 15:07:19.294200 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 14, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5f974d7468\" is progressing."}}, CollisionCount:(*int32)(nil)}
  I0512 15:07:21.288479 23 deployment.go:94] all replica sets need to contain the pod-template-hash label
  I0512 15:07:21.288620 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 14, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5f974d7468\" is progressing."}}, CollisionCount:(*int32)(nil)}
  I0512 15:07:23.543329 23 deployment.go:94] all replica sets need to contain the pod-template-hash label
  I0512 15:07:23.543495 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 7, 14, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 7, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5f974d7468\" is progressing."}}, CollisionCount:(*int32)(nil)}
  I0512 15:07:25.289366 23 deployment.go:94] 
  I0512 15:07:25.289435 23 deployment.go:974] Ensure that both old replica sets have no replicas
  I0512 15:07:25.308291 23 deployment.go:633] Deployment "test-rollover-deployment":
  (v1.Deployment) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=24) "test-rollover-deployment",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-5755",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "d9696ab5-f81c-4451-b30a-2a501505642f",
      ResourceVersion: (string) (len=5) "19536",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659231,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=1) {
        (string) (len=4) "name": (string) (len=12) "rollover-pod"
      },
      Annotations: (map[string]string) (len=1) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "2"
      },
      OwnerReferences: ([]v1.OwnerReference) <nil>,
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659233,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=660) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 7d 7d 2c  |},"f:name":{}}},|
              00000030  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 6d 69 6e  |"f:spec":{"f:min|
              00000040  52 65 61 64 79 53 65 63  6f 6e 64 73 22 3a 7b 7d  |ReadySeconds":{}|
              00000050  2c 22 66 3a 70 72 6f 67  72 65 73 73 44 65 61 64  |,"f:progressDead|
              00000060  6c 69 6e 65 53 65 63 6f  6e 64 73 22 3a 7b 7d 2c  |lineSeconds":{},|
              00000070  22 66 3a 72 65 70 6c 69  63 61 73 22 3a 7b 7d 2c  |"f:replicas":{},|
              00000080  22 66 3a 72 65 76 69 73  69 6f 6e 48 69 73 74 6f  |"f:revisionHisto|
              00000090  72 79 4c 69 6d 69 74 22  3a 7b 7d 2c 22 66 3a 73  |ryLimit":{},"f:s|
              000000a0  65 6c 65 63 74 6f 72 22  3a 7b 7d 2c 22 66 3a 73  |elector":{},"f:s|
              000000b0  74 72 61 74 65 67 79 22  3a 7b 22 66 3a 72 6f 6c  |trategy":{"f:rol|
              000000c0  6c 69 6e 67 55 70 64 61  74 65 22 3a 7b 22 2e 22  |lingUpdate":{"."|
              000000d0  3a 7b 7d 2c 22 66 3a 6d  61 78 53 75 72 67 65 22  |:{},"f:maxSurge"|
              000000e0  3a 7b 7d 2c 22 66 3a 6d  61 78 55 6e 61 76 61 69  |:{},"f:maxUnavai|
              000000f0  6c 61 62 6c 65 22 3a 7b  7d 7d 2c 22 66 3a 74 79  |lable":{}},"f:ty|
              00000100  70 65 22 3a 7b 7d 7d 2c  22 66 3a 74 65 6d 70 6c  |pe":{}},"f:templ|
              00000110  61 74 65 22 3a 7b 22 66  3a 6d 65 74 61 64 61 74  |ate":{"f:metadat|
              00000120  61 22 3a 7b 22 66 3a 6c  61 62 65 6c 73 22 3a 7b  |a":{"f:labels":{|
              00000130  22 2e 22 3a 7b 7d 2c 22  66 3a 6e 61 6d 65 22 3a  |".":{},"f:name":|
              00000140  7b 7d 7d 7d 2c 22 66 3a  73 70 65 63 22 3a 7b 22  |{}}},"f:spec":{"|
              00000150  66 3a 63 6f 6e 74 61 69  6e 65 72 73 22 3a 7b 22  |f:containers":{"|
              00000160  6b 3a 7b 5c 22 6e 61 6d  65 5c 22 3a 5c 22 61 67  |k:{\"name\":\"ag|
              00000170  6e 68 6f 73 74 5c 22 7d  22 3a 7b 22 2e 22 3a 7b  |nhost\"}":{".":{|
              00000180  7d 2c 22 66 3a 69 6d 61  67 65 22 3a 7b 7d 2c 22  |},"f:image":{},"|
              00000190  66 3a 69 6d 61 67 65 50  75 6c 6c 50 6f 6c 69 63  |f:imagePullPolic|
              000001a0  79 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |y":{},"f:name":{|
              000001b0  7d 2c 22 66 3a 72 65 73  6f 75 72 63 65 73 22 3a  |},"f:resources":|
              000001c0  7b 7d 2c 22 66 3a 73 65  63 75 72 69 74 79 43 6f  |{},"f:securityCo|
              000001d0  6e 74 65 78 74 22 3a 7b  7d 2c 22 66 3a 74 65 72  |ntext":{},"f:ter|
              000001e0  6d 69 6e 61 74 69 6f 6e  4d 65 73 73 61 67 65 50  |minationMessageP|
              000001f0  61 74 68 22 3a 7b 7d 2c  22 66 3a 74 65 72 6d 69  |ath":{},"f:termi|
              00000200  6e 61 74 69 6f 6e 4d 65  73 73 61 67 65 50 6f 6c  |nationMessagePol|
              00000210  69 63 79 22 3a 7b 7d 7d  7d 2c 22 66 3a 64 6e 73  |icy":{}}},"f:dns|
              00000220  50 6f 6c 69 63 79 22 3a  7b 7d 2c 22 66 3a 72 65  |Policy":{},"f:re|
              00000230  73 74 61 72 74 50 6f 6c  69 63 79 22 3a 7b 7d 2c  |startPolicy":{},|
              00000240  22 66 3a 73 63 68 65 64  75 6c 65 72 4e 61 6d 65  |"f:schedulerName|
              00000250  22 3a 7b 7d 2c 22 66 3a  73 65 63 75 72 69 74 79  |":{},"f:security|
              00000260  43 6f 6e 74 65 78 74 22  3a 7b 7d 2c 22 66 3a 74  |Context":{},"f:t|
              00000270  65 72 6d 69 6e 61 74 69  6f 6e 47 72 61 63 65 50  |erminationGraceP|
              00000280  65 72 69 6f 64 53 65 63  6f 6e 64 73 22 3a 7b 7d  |eriodSeconds":{}|
              00000290  7d 7d 7d 7d                                       |}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659244,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=514) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 72 65 76 69 73  69 6f 6e 22 3a 7b 7d 7d  |io/revision":{}}|
              00000050  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 22 66  |},"f:status":{"f|
              00000060  3a 61 76 61 69 6c 61 62  6c 65 52 65 70 6c 69 63  |:availableReplic|
              00000070  61 73 22 3a 7b 7d 2c 22  66 3a 63 6f 6e 64 69 74  |as":{},"f:condit|
              00000080  69 6f 6e 73 22 3a 7b 22  2e 22 3a 7b 7d 2c 22 6b  |ions":{".":{},"k|
              00000090  3a 7b 5c 22 74 79 70 65  5c 22 3a 5c 22 41 76 61  |:{\"type\":\"Ava|
              000000a0  69 6c 61 62 6c 65 5c 22  7d 22 3a 7b 22 2e 22 3a  |ilable\"}":{".":|
              000000b0  7b 7d 2c 22 66 3a 6c 61  73 74 54 72 61 6e 73 69  |{},"f:lastTransi|
              000000c0  74 69 6f 6e 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |tionTime":{},"f:|
              000000d0  6c 61 73 74 55 70 64 61  74 65 54 69 6d 65 22 3a  |lastUpdateTime":|
              000000e0  7b 7d 2c 22 66 3a 6d 65  73 73 61 67 65 22 3a 7b  |{},"f:message":{|
              000000f0  7d 2c 22 66 3a 72 65 61  73 6f 6e 22 3a 7b 7d 2c  |},"f:reason":{},|
              00000100  22 66 3a 73 74 61 74 75  73 22 3a 7b 7d 2c 22 66  |"f:status":{},"f|
              00000110  3a 74 79 70 65 22 3a 7b  7d 7d 2c 22 6b 3a 7b 5c  |:type":{}},"k:{\|
              00000120  22 74 79 70 65 5c 22 3a  5c 22 50 72 6f 67 72 65  |"type\":\"Progre|
              00000130  73 73 69 6e 67 5c 22 7d  22 3a 7b 22 2e 22 3a 7b  |ssing\"}":{".":{|
              00000140  7d 2c 22 66 3a 6c 61 73  74 54 72 61 6e 73 69 74  |},"f:lastTransit|
              00000150  69 6f 6e 54 69 6d 65 22  3a 7b 7d 2c 22 66 3a 6c  |ionTime":{},"f:l|
              00000160  61 73 74 55 70 64 61 74  65 54 69 6d 65 22 3a 7b  |astUpdateTime":{|
              00000170  7d 2c 22 66 3a 6d 65 73  73 61 67 65 22 3a 7b 7d  |},"f:message":{}|
              00000180  2c 22 66 3a 72 65 61 73  6f 6e 22 3a 7b 7d 2c 22  |,"f:reason":{},"|
              00000190  66 3a 73 74 61 74 75 73  22 3a 7b 7d 2c 22 66 3a  |f:status":{},"f:|
              000001a0  74 79 70 65 22 3a 7b 7d  7d 7d 2c 22 66 3a 6f 62  |type":{}}},"f:ob|
              000001b0  73 65 72 76 65 64 47 65  6e 65 72 61 74 69 6f 6e  |servedGeneration|
              000001c0  22 3a 7b 7d 2c 22 66 3a  72 65 61 64 79 52 65 70  |":{},"f:readyRep|
              000001d0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 72 65 70  |licas":{},"f:rep|
              000001e0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 75 70 64  |licas":{},"f:upd|
              000001f0  61 74 65 64 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |atedReplicas":{}|
              00000200  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.DeploymentSpec) {
      Replicas: (*int32)(1),
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=1) {
          (string) (len=4) "name": (string) (len=12) "rollover-pod"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=1) {
            (string) (len=4) "name": (string) (len=12) "rollover-pod"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=7) "agnhost",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      },
      Strategy: (v1.DeploymentStrategy) {
        Type: (v1.DeploymentStrategyType) (len=13) "RollingUpdate",
        RollingUpdate: (*v1.RollingUpdateDeployment)({
          MaxUnavailable: (*intstr.IntOrString)({
            Type: (intstr.Type) 0,
            IntVal: (int32) 0,
            StrVal: (string) ""
          }),
          MaxSurge: (*intstr.IntOrString)({
            Type: (intstr.Type) 0,
            IntVal: (int32) 1,
            StrVal: (string) ""
          })
        })
      },
      MinReadySeconds: (int32) 10,
      RevisionHistoryLimit: (*int32)(10),
      Paused: (bool) false,
      ProgressDeadlineSeconds: (*int32)(600)
    },
    Status: (v1.DeploymentStatus) {
      ObservedGeneration: (int64) 2,
      Replicas: (int32) 1,
      UpdatedReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      UnavailableReplicas: (int32) 0,
      Conditions: ([]v1.DeploymentCondition) (len=2) {
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=9) "Available",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659231,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659231,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=24) "MinimumReplicasAvailable",
          Message: (string) (len=36) "Deployment has minimum availability."
        },
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=11) "Progressing",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659244,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659231,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=22) "NewReplicaSetAvailable",
          Message: (string) (len=77) "ReplicaSet \"test-rollover-deployment-5f974d7468\" has successfully progressed."
        }
      },
      CollisionCount: (*int32)(<nil>)
    }
  }


  I0512 15:07:25.388584 23 deployment.go:39] New ReplicaSet "test-rollover-deployment-5f974d7468" of Deployment "test-rollover-deployment":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=35) "test-rollover-deployment-5f974d7468",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-5755",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "6aa5fd27-7901-4ca1-9fc8-415befb6bae3",
      ResourceVersion: (string) (len=5) "19523",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659233,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=12) "rollover-pod",
        (string) (len=17) "pod-template-hash": (string) (len=10) "5f974d7468"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "2"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=24) "test-rollover-deployment",
          UID: (types.UID) (len=36) "d9696ab5-f81c-4451-b30a-2a501505642f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659233,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=806) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 64 39 36 39 36 61  62 35 2d 66 38 31 63 2d  |\"d9696ab5-f81c-|
              00000120  34 34 35 31 2d 62 33 30  61 2d 32 61 35 30 31 35  |4451-b30a-2a5015|
              00000130  30 35 36 34 32 66 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |05642f\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 6d 69 6e  |"f:spec":{"f:min|
              00000150  52 65 61 64 79 53 65 63  6f 6e 64 73 22 3a 7b 7d  |ReadySeconds":{}|
              00000160  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000170  2c 22 66 3a 73 65 6c 65  63 74 6f 72 22 3a 7b 7d  |,"f:selector":{}|
              00000180  2c 22 66 3a 74 65 6d 70  6c 61 74 65 22 3a 7b 22  |,"f:template":{"|
              00000190  66 3a 6d 65 74 61 64 61  74 61 22 3a 7b 22 66 3a  |f:metadata":{"f:|
              000001a0  6c 61 62 65 6c 73 22 3a  7b 22 2e 22 3a 7b 7d 2c  |labels":{".":{},|
              000001b0  22 66 3a 6e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 70  |"f:name":{},"f:p|
              000001c0  6f 64 2d 74 65 6d 70 6c  61 74 65 2d 68 61 73 68  |od-template-hash|
              000001d0  22 3a 7b 7d 7d 7d 2c 22  66 3a 73 70 65 63 22 3a  |":{}}},"f:spec":|
              000001e0  7b 22 66 3a 63 6f 6e 74  61 69 6e 65 72 73 22 3a  |{"f:containers":|
              000001f0  7b 22 6b 3a 7b 5c 22 6e  61 6d 65 5c 22 3a 5c 22  |{"k:{\"name\":\"|
              00000200  61 67 6e 68 6f 73 74 5c  22 7d 22 3a 7b 22 2e 22  |agnhost\"}":{"."|
              00000210  3a 7b 7d 2c 22 66 3a 69  6d 61 67 65 22 3a 7b 7d  |:{},"f:image":{}|
              00000220  2c 22 66 3a 69 6d 61 67  65 50 75 6c 6c 50 6f 6c  |,"f:imagePullPol|
              00000230  69 63 79 22 3a 7b 7d 2c  22 66 3a 6e 61 6d 65 22  |icy":{},"f:name"|
              00000240  3a 7b 7d 2c 22 66 3a 72  65 73 6f 75 72 63 65 73  |:{},"f:resources|
              00000250  22 3a 7b 7d 2c 22 66 3a  73 65 63 75 72 69 74 79  |":{},"f:security|
              00000260  43 6f 6e 74 65 78 74 22  3a 7b 7d 2c 22 66 3a 74  |Context":{},"f:t|
              00000270  65 72 6d 69 6e 61 74 69  6f 6e 4d 65 73 73 61 67  |erminationMessag|
              00000280  65 50 61 74 68 22 3a 7b  7d 2c 22 66 3a 74 65 72  |ePath":{},"f:ter|
              00000290  6d 69 6e 61 74 69 6f 6e  4d 65 73 73 61 67 65 50  |minationMessageP|
              000002a0  6f 6c 69 63 79 22 3a 7b  7d 7d 7d 2c 22 66 3a 64  |olicy":{}}},"f:d|
              000002b0  6e 73 50 6f 6c 69 63 79  22 3a 7b 7d 2c 22 66 3a  |nsPolicy":{},"f:|
              000002c0  72 65 73 74 61 72 74 50  6f 6c 69 63 79 22 3a 7b  |restartPolicy":{|
              000002d0  7d 2c 22 66 3a 73 63 68  65 64 75 6c 65 72 4e 61  |},"f:schedulerNa|
              000002e0  6d 65 22 3a 7b 7d 2c 22  66 3a 73 65 63 75 72 69  |me":{},"f:securi|
              000002f0  74 79 43 6f 6e 74 65 78  74 22 3a 7b 7d 2c 22 66  |tyContext":{},"f|
              00000300  3a 74 65 72 6d 69 6e 61  74 69 6f 6e 47 72 61 63  |:terminationGrac|
              00000310  65 50 65 72 69 6f 64 53  65 63 6f 6e 64 73 22 3a  |ePeriodSeconds":|
              00000320  7b 7d 7d 7d 7d 7d                                 |{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659244,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=130) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  61 76 61 69 6c 61 62 6c  65 52 65 70 6c 69 63 61  |availableReplica|
              00000020  73 22 3a 7b 7d 2c 22 66  3a 66 75 6c 6c 79 4c 61  |s":{},"f:fullyLa|
              00000030  62 65 6c 65 64 52 65 70  6c 69 63 61 73 22 3a 7b  |beledReplicas":{|
              00000040  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              00000050  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              00000060  65 61 64 79 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |eadyReplicas":{}|
              00000070  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000080  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(1),
      MinReadySeconds: (int32) 10,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=12) "rollover-pod",
          (string) (len=17) "pod-template-hash": (string) (len=10) "5f974d7468"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=12) "rollover-pod",
            (string) (len=17) "pod-template-hash": (string) (len=10) "5f974d7468"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=7) "agnhost",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 1,
      FullyLabeledReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 15:07:25.401765 23 deployment.go:44] All old ReplicaSets of Deployment "test-rollover-deployment":
  I0512 15:07:25.402666 23 deployment.go:47] (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=24) "test-rollover-controller",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-5755",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "13f57cf9-9aeb-4b42-b4cc-1c543c3b0141",
      ResourceVersion: (string) (len=5) "19534",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659223,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=3) "pod": (string) (len=5) "httpd",
        (string) (len=4) "name": (string) (len=12) "rollover-pod"
      },
      Annotations: (map[string]string) (len=2) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=24) "test-rollover-deployment",
          UID: (types.UID) (len=36) "d9696ab5-f81c-4451-b30a-2a501505642f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=3) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659223,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=467) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              00000030  3a 70 6f 64 22 3a 7b 7d  7d 7d 2c 22 66 3a 73 70  |:pod":{}}},"f:sp|
              00000040  65 63 22 3a 7b 22 66 3a  73 65 6c 65 63 74 6f 72  |ec":{"f:selector|
              00000050  22 3a 7b 7d 2c 22 66 3a  74 65 6d 70 6c 61 74 65  |":{},"f:template|
              00000060  22 3a 7b 22 66 3a 6d 65  74 61 64 61 74 61 22 3a  |":{"f:metadata":|
              00000070  7b 22 66 3a 6c 61 62 65  6c 73 22 3a 7b 22 2e 22  |{"f:labels":{"."|
              00000080  3a 7b 7d 2c 22 66 3a 6e  61 6d 65 22 3a 7b 7d 2c  |:{},"f:name":{},|
              00000090  22 66 3a 70 6f 64 22 3a  7b 7d 7d 7d 2c 22 66 3a  |"f:pod":{}}},"f:|
              000000a0  73 70 65 63 22 3a 7b 22  66 3a 63 6f 6e 74 61 69  |spec":{"f:contai|
              000000b0  6e 65 72 73 22 3a 7b 22  6b 3a 7b 5c 22 6e 61 6d  |ners":{"k:{\"nam|
              000000c0  65 5c 22 3a 5c 22 68 74  74 70 64 5c 22 7d 22 3a  |e\":\"httpd\"}":|
              000000d0  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |{".":{},"f:image|
              000000e0  22 3a 7b 7d 2c 22 66 3a  69 6d 61 67 65 50 75 6c  |":{},"f:imagePul|
              000000f0  6c 50 6f 6c 69 63 79 22  3a 7b 7d 2c 22 66 3a 6e  |lPolicy":{},"f:n|
              00000100  61 6d 65 22 3a 7b 7d 2c  22 66 3a 72 65 73 6f 75  |ame":{},"f:resou|
              00000110  72 63 65 73 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |rces":{},"f:term|
              00000120  69 6e 61 74 69 6f 6e 4d  65 73 73 61 67 65 50 61  |inationMessagePa|
              00000130  74 68 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |th":{},"f:termin|
              00000140  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 6f 6c 69  |ationMessagePoli|
              00000150  63 79 22 3a 7b 7d 7d 7d  2c 22 66 3a 64 6e 73 50  |cy":{}}},"f:dnsP|
              00000160  6f 6c 69 63 79 22 3a 7b  7d 2c 22 66 3a 72 65 73  |olicy":{},"f:res|
              00000170  74 61 72 74 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |tartPolicy":{},"|
              00000180  66 3a 73 63 68 65 64 75  6c 65 72 4e 61 6d 65 22  |f:schedulerName"|
              00000190  3a 7b 7d 2c 22 66 3a 73  65 63 75 72 69 74 79 43  |:{},"f:securityC|
              000001a0  6f 6e 74 65 78 74 22 3a  7b 7d 2c 22 66 3a 74 65  |ontext":{},"f:te|
              000001b0  72 6d 69 6e 61 74 69 6f  6e 47 72 61 63 65 50 65  |rminationGracePe|
              000001c0  72 69 6f 64 53 65 63 6f  6e 64 73 22 3a 7b 7d 7d  |riodSeconds":{}}|
              000001d0  7d 7d 7d                                          |}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659244,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=249) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 7d 2c 22 66  3a 6f 77 6e 65 72 52 65  |":{}},"f:ownerRe|
              00000090  66 65 72 65 6e 63 65 73  22 3a 7b 22 2e 22 3a 7b  |ferences":{".":{|
              000000a0  7d 2c 22 6b 3a 7b 5c 22  75 69 64 5c 22 3a 5c 22  |},"k:{\"uid\":\"|
              000000b0  64 39 36 39 36 61 62 35  2d 66 38 31 63 2d 34 34  |d9696ab5-f81c-44|
              000000c0  35 31 2d 62 33 30 61 2d  32 61 35 30 31 35 30 35  |51-b30a-2a501505|
              000000d0  36 34 32 66 5c 22 7d 22  3a 7b 7d 7d 7d 2c 22 66  |642f\"}":{}}},"f|
              000000e0  3a 73 70 65 63 22 3a 7b  22 66 3a 72 65 70 6c 69  |:spec":{"f:repli|
              000000f0  63 61 73 22 3a 7b 7d 7d  7d                       |cas":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659244,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=56) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  6f 62 73 65 72 76 65 64  47 65 6e 65 72 61 74 69  |observedGenerati|
              00000020  6f 6e 22 3a 7b 7d 2c 22  66 3a 72 65 70 6c 69 63  |on":{},"f:replic|
              00000030  61 73 22 3a 7b 7d 7d 7d                           |as":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(0),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=12) "rollover-pod",
          (string) (len=3) "pod": (string) (len=5) "httpd"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=12) "rollover-pod",
            (string) (len=3) "pod": (string) (len=5) "httpd"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)(<nil>),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 0,
      FullyLabeledReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 15:07:25.423302 23 deployment.go:47] (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=35) "test-rollover-deployment-55f4dbffff",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-5755",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "a039beff-326e-4286-9486-6ac7aab758c7",
      ResourceVersion: (string) (len=5) "19359",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659231,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=12) "rollover-pod",
        (string) (len=17) "pod-template-hash": (string) (len=10) "55f4dbffff"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=24) "test-rollover-deployment",
          UID: (types.UID) (len=36) "d9696ab5-f81c-4451-b30a-2a501505642f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659233,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=810) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 64 39 36 39 36 61  62 35 2d 66 38 31 63 2d  |\"d9696ab5-f81c-|
              00000120  34 34 35 31 2d 62 33 30  61 2d 32 61 35 30 31 35  |4451-b30a-2a5015|
              00000130  30 35 36 34 32 66 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |05642f\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 6d 69 6e  |"f:spec":{"f:min|
              00000150  52 65 61 64 79 53 65 63  6f 6e 64 73 22 3a 7b 7d  |ReadySeconds":{}|
              00000160  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000170  2c 22 66 3a 73 65 6c 65  63 74 6f 72 22 3a 7b 7d  |,"f:selector":{}|
              00000180  2c 22 66 3a 74 65 6d 70  6c 61 74 65 22 3a 7b 22  |,"f:template":{"|
              00000190  66 3a 6d 65 74 61 64 61  74 61 22 3a 7b 22 66 3a  |f:metadata":{"f:|
              000001a0  6c 61 62 65 6c 73 22 3a  7b 22 2e 22 3a 7b 7d 2c  |labels":{".":{},|
              000001b0  22 66 3a 6e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 70  |"f:name":{},"f:p|
              000001c0  6f 64 2d 74 65 6d 70 6c  61 74 65 2d 68 61 73 68  |od-template-hash|
              000001d0  22 3a 7b 7d 7d 7d 2c 22  66 3a 73 70 65 63 22 3a  |":{}}},"f:spec":|
              000001e0  7b 22 66 3a 63 6f 6e 74  61 69 6e 65 72 73 22 3a  |{"f:containers":|
              000001f0  7b 22 6b 3a 7b 5c 22 6e  61 6d 65 5c 22 3a 5c 22  |{"k:{\"name\":\"|
              00000200  72 65 64 69 73 2d 73 6c  61 76 65 5c 22 7d 22 3a  |redis-slave\"}":|
              00000210  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |{".":{},"f:image|
              00000220  22 3a 7b 7d 2c 22 66 3a  69 6d 61 67 65 50 75 6c  |":{},"f:imagePul|
              00000230  6c 50 6f 6c 69 63 79 22  3a 7b 7d 2c 22 66 3a 6e  |lPolicy":{},"f:n|
              00000240  61 6d 65 22 3a 7b 7d 2c  22 66 3a 72 65 73 6f 75  |ame":{},"f:resou|
              00000250  72 63 65 73 22 3a 7b 7d  2c 22 66 3a 73 65 63 75  |rces":{},"f:secu|
              00000260  72 69 74 79 43 6f 6e 74  65 78 74 22 3a 7b 7d 2c  |rityContext":{},|
              00000270  22 66 3a 74 65 72 6d 69  6e 61 74 69 6f 6e 4d 65  |"f:terminationMe|
              00000280  73 73 61 67 65 50 61 74  68 22 3a 7b 7d 2c 22 66  |ssagePath":{},"f|
              00000290  3a 74 65 72 6d 69 6e 61  74 69 6f 6e 4d 65 73 73  |:terminationMess|
              000002a0  61 67 65 50 6f 6c 69 63  79 22 3a 7b 7d 7d 7d 2c  |agePolicy":{}}},|
              000002b0  22 66 3a 64 6e 73 50 6f  6c 69 63 79 22 3a 7b 7d  |"f:dnsPolicy":{}|
              000002c0  2c 22 66 3a 72 65 73 74  61 72 74 50 6f 6c 69 63  |,"f:restartPolic|
              000002d0  79 22 3a 7b 7d 2c 22 66  3a 73 63 68 65 64 75 6c  |y":{},"f:schedul|
              000002e0  65 72 4e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 73 65  |erName":{},"f:se|
              000002f0  63 75 72 69 74 79 43 6f  6e 74 65 78 74 22 3a 7b  |curityContext":{|
              00000300  7d 2c 22 66 3a 74 65 72  6d 69 6e 61 74 69 6f 6e  |},"f:termination|
              00000310  47 72 61 63 65 50 65 72  69 6f 64 53 65 63 6f 6e  |GracePeriodSecon|
              00000320  64 73 22 3a 7b 7d 7d 7d  7d 7d                    |ds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659233,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=56) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  6f 62 73 65 72 76 65 64  47 65 6e 65 72 61 74 69  |observedGenerati|
              00000020  6f 6e 22 3a 7b 7d 2c 22  66 3a 72 65 70 6c 69 63  |on":{},"f:replic|
              00000030  61 73 22 3a 7b 7d 7d 7d                           |as":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(0),
      MinReadySeconds: (int32) 10,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=12) "rollover-pod",
          (string) (len=17) "pod-template-hash": (string) (len=10) "55f4dbffff"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=12) "rollover-pod",
            (string) (len=17) "pod-template-hash": (string) (len=10) "55f4dbffff"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=11) "redis-slave",
              Image: (string) (len=47) "gcr.io/google_samples/gb-redisslave:nonexistent",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 0,
      FullyLabeledReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 15:07:25.443814 23 deployment.go:67] Pod "test-rollover-deployment-5f974d7468-7wxmj" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=41) "test-rollover-deployment-5f974d7468-7wxmj",
      GenerateName: (string) (len=36) "test-rollover-deployment-5f974d7468-",
      Namespace: (string) (len=15) "deployment-5755",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "fa3a4b66-28af-4e62-8c20-1090233d6672",
      ResourceVersion: (string) (len=5) "19370",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659233,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=12) "rollover-pod",
        (string) (len=17) "pod-template-hash": (string) (len=10) "5f974d7468"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=35) "test-rollover-deployment-5f974d7468",
          UID: (types.UID) (len=36) "6aa5fd27-7901-4ca1-9fc8-415befb6bae3",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659233,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=539) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 36 61  61 35 66 64 32 37 2d 37  |d\":\"6aa5fd27-7|
              00000090  39 30 31 2d 34 63 61 31  2d 39 66 63 38 2d 34 31  |901-4ca1-9fc8-41|
              000000a0  35 62 65 66 62 36 62 61  65 33 5c 22 7d 22 3a 7b  |5befb6bae3\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 61 67 6e  |:{\"name\":\"agn|
              000000e0  68 6f 73 74 5c 22 7d 22  3a 7b 22 2e 22 3a 7b 7d  |host\"}":{".":{}|
              000000f0  2c 22 66 3a 69 6d 61 67  65 22 3a 7b 7d 2c 22 66  |,"f:image":{},"f|
              00000100  3a 69 6d 61 67 65 50 75  6c 6c 50 6f 6c 69 63 79  |:imagePullPolicy|
              00000110  22 3a 7b 7d 2c 22 66 3a  6e 61 6d 65 22 3a 7b 7d  |":{},"f:name":{}|
              00000120  2c 22 66 3a 72 65 73 6f  75 72 63 65 73 22 3a 7b  |,"f:resources":{|
              00000130  7d 2c 22 66 3a 73 65 63  75 72 69 74 79 43 6f 6e  |},"f:securityCon|
              00000140  74 65 78 74 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |text":{},"f:term|
              00000150  69 6e 61 74 69 6f 6e 4d  65 73 73 61 67 65 50 61  |inationMessagePa|
              00000160  74 68 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |th":{},"f:termin|
              00000170  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 6f 6c 69  |ationMessagePoli|
              00000180  63 79 22 3a 7b 7d 7d 7d  2c 22 66 3a 64 6e 73 50  |cy":{}}},"f:dnsP|
              00000190  6f 6c 69 63 79 22 3a 7b  7d 2c 22 66 3a 65 6e 61  |olicy":{},"f:ena|
              000001a0  62 6c 65 53 65 72 76 69  63 65 4c 69 6e 6b 73 22  |bleServiceLinks"|
              000001b0  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              000001c0  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              000001d0  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              000001e0  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              000001f0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000200  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000210  63 6f 6e 64 73 22 3a 7b  7d 7d 7d                 |conds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659234,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=661) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 38 2e  32 35 5c 22 7d 22 3a 7b  |.233.68.25\"}":{|
              00000270  22 2e 22 3a 7b 7d 2c 22  66 3a 69 70 22 3a 7b 7d  |".":{},"f:ip":{}|
              00000280  7d 7d 2c 22 66 3a 73 74  61 72 74 54 69 6d 65 22  |}},"f:startTime"|
              00000290  3a 7b 7d 7d 7d                                    |:{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-7jsvg",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=7) "agnhost",
          Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-7jsvg",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659234,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659233,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659234,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659234,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882659233,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) (len=12) "10.233.68.25",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=12) "10.233.68.25"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882659233,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=7) "agnhost",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882659234,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
          ImageID: (string) (len=111) "registry.k8s.io/e2e-test-images/agnhost@sha256:b173c7d0ffe3d805d49f4dfe48375169b7b8d2e1feb81783efd61eb9d08042e6",
          ContainerID: (string) (len=77) "containerd://0f9e706ba7f1fee8d4663b486c879174e6bff4b6df484b077c750cf9a9308ff5",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-7jsvg",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 15:07:25.446898 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5755" for this suite. @ 05/12/25 15:07:25.459
• [21.578 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:263
  STEP: Creating a kubernetes client @ 05/12/25 15:07:25.474
  I0512 15:07:25.474575 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:07:25.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:07:25.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:07:25.527
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:07:25.533
  STEP: Saw pod success @ 05/12/25 15:07:29.584
  I0512 15:07:29.591542 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-29ba679f-e68e-4785-9c66-59af595596e0 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:07:29.646
  I0512 15:07:29.674904 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-203" for this suite. @ 05/12/25 15:07:29.69
• [4.228 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:331
  STEP: Creating a kubernetes client @ 05/12/25 15:07:29.703
  I0512 15:07:29.703557 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 15:07:29.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:07:29.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:07:29.833
  STEP: Creating a test externalName service @ 05/12/25 15:07:29.839
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-744.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-744.svc.cluster.local; sleep 1; done
   @ 05/12/25 15:07:29.85
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-744.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-744.svc.cluster.local; sleep 1; done
   @ 05/12/25 15:07:29.85
  STEP: creating a pod to probe DNS @ 05/12/25 15:07:29.85
  STEP: submitting the pod to kubernetes @ 05/12/25 15:07:29.851
  STEP: retrieving the pod @ 05/12/25 15:08:04.117
  STEP: looking for the results for each expected name from probers @ 05/12/25 15:08:04.124
  I0512 15:08:04.145208 23 dns_common.go:552] DNS probes using dns-test-a7c79a75-6c05-4d8e-9336-1518f0686357 succeeded

  STEP: changing the externalName to bar.example.com @ 05/12/25 15:08:04.145
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-744.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-744.svc.cluster.local; sleep 1; done
   @ 05/12/25 15:08:04.171
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-744.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-744.svc.cluster.local; sleep 1; done
   @ 05/12/25 15:08:04.172
  STEP: creating a second pod to probe DNS @ 05/12/25 15:08:04.173
  STEP: submitting the pod to kubernetes @ 05/12/25 15:08:04.173
  STEP: retrieving the pod @ 05/12/25 15:08:38.387
  STEP: looking for the results for each expected name from probers @ 05/12/25 15:08:38.401
  I0512 15:08:38.419383 23 dns_common.go:552] DNS probes using dns-test-94206922-4927-48f5-92b3-ce58e378c77d succeeded

  STEP: changing the service to type=ClusterIP @ 05/12/25 15:08:38.419
  W0512 15:08:38.457335      23 warnings.go:70] spec.externalName is ignored when spec.type is not "ExternalName"
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-744.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-744.svc.cluster.local; sleep 1; done
   @ 05/12/25 15:08:38.458
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-744.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-744.svc.cluster.local; sleep 1; done
   @ 05/12/25 15:08:38.458
  STEP: creating a third pod to probe DNS @ 05/12/25 15:08:38.458
  STEP: submitting the pod to kubernetes @ 05/12/25 15:08:38.467
  STEP: retrieving the pod @ 05/12/25 15:08:40.535
  STEP: looking for the results for each expected name from probers @ 05/12/25 15:08:40.545
  I0512 15:08:40.578241 23 dns_common.go:552] DNS probes using dns-test-038f901c-2ac6-4eba-b21f-b03b8fc9e247 succeeded

  STEP: deleting the pod @ 05/12/25 15:08:40.578
  STEP: deleting the pod @ 05/12/25 15:08:40.634
  STEP: deleting the pod @ 05/12/25 15:08:40.667
  STEP: deleting the test externalName service @ 05/12/25 15:08:40.699
  I0512 15:08:40.728125 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "dns-744" for this suite. @ 05/12/25 15:08:40.743
• [71.060 seconds]
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/logs.go:167
  STEP: Creating a kubernetes client @ 05/12/25 15:08:40.764
  I0512 15:08:40.764904 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl-logs @ 05/12/25 15:08:40.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:08:40.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:08:40.828
  STEP: creating a pod @ 05/12/25 15:08:40.842
  I0512 15:08:40.842494 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.52 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  I0512 15:08:41.042281 23 builder.go:146] stderr: ""
  I0512 15:08:41.042333 23 builder.go:147] stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/12/25 15:08:41.042
  I0512 15:08:41.042458 23 resource.go:413] Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  I0512 15:08:43.075151 23 resource.go:435] Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/12/25 15:08:43.075
  I0512 15:08:43.075324 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 logs logs-generator logs-generator'
  I0512 15:08:43.253922 23 builder.go:146] stderr: ""
  I0512 15:08:43.254019 23 builder.go:147] stdout: "I0512 15:08:41.954267       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/kfd4 282\nI0512 15:08:42.154677       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/4qnz 442\nI0512 15:08:42.355240       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/h7k 289\nI0512 15:08:42.554378       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rbg 516\nI0512 15:08:42.754835       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/tf4n 317\nI0512 15:08:42.954505       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/hwkv 263\nI0512 15:08:43.156690       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/nz6k 341\n"
  STEP: limiting log lines @ 05/12/25 15:08:43.254
  I0512 15:08:43.254162 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 logs logs-generator logs-generator --tail=1'
  I0512 15:08:43.459684 23 builder.go:146] stderr: ""
  I0512 15:08:43.459759 23 builder.go:147] stdout: "I0512 15:08:43.354258       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/q5p6 268\n"
  I0512 15:08:43.459787 23 logs.go:180] got output "I0512 15:08:43.354258       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/q5p6 268\n"
  STEP: limiting log bytes @ 05/12/25 15:08:43.459
  I0512 15:08:43.459932 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 logs logs-generator logs-generator --limit-bytes=1'
  I0512 15:08:43.625324 23 builder.go:146] stderr: ""
  I0512 15:08:43.625379 23 builder.go:147] stdout: "I"
  I0512 15:08:43.625395 23 logs.go:186] got output "I"
  STEP: exposing timestamps @ 05/12/25 15:08:43.625
  I0512 15:08:43.625507 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 logs logs-generator logs-generator --tail=1 --timestamps'
  I0512 15:08:43.800886 23 builder.go:146] stderr: ""
  I0512 15:08:43.800960 23 builder.go:147] stdout: "2025-05-12T15:08:43.755271111Z I0512 15:08:43.755070       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/wtp 311\n"
  I0512 15:08:43.800990 23 logs.go:192] got output "2025-05-12T15:08:43.755271111Z I0512 15:08:43.755070       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/wtp 311\n"
  STEP: restricting to a time range @ 05/12/25 15:08:43.801
  I0512 15:08:46.301514 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 logs logs-generator logs-generator --since=1s'
  I0512 15:08:46.482639 23 builder.go:146] stderr: ""
  I0512 15:08:46.482797 23 builder.go:147] stdout: "I0512 15:08:45.555272       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/nmh 462\nI0512 15:08:45.754779       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/8jmr 292\nI0512 15:08:45.955270       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/44q 545\nI0512 15:08:46.154972       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/sn7r 463\nI0512 15:08:46.354278       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/f7h 559\n"
  I0512 15:08:46.482960 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 logs logs-generator logs-generator --since=24h'
  I0512 15:08:46.680134 23 builder.go:146] stderr: ""
  I0512 15:08:46.680246 23 builder.go:147] stdout: "I0512 15:08:41.954267       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/kfd4 282\nI0512 15:08:42.154677       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/4qnz 442\nI0512 15:08:42.355240       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/h7k 289\nI0512 15:08:42.554378       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rbg 516\nI0512 15:08:42.754835       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/tf4n 317\nI0512 15:08:42.954505       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/hwkv 263\nI0512 15:08:43.156690       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/nz6k 341\nI0512 15:08:43.354258       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/q5p6 268\nI0512 15:08:43.554496       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/sd2 348\nI0512 15:08:43.755070       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/wtp 311\nI0512 15:08:43.954496       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/zcfh 222\nI0512 15:08:44.154950       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/97q 545\nI0512 15:08:44.354273       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/tgm7 358\nI0512 15:08:44.554967       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/vm4 519\nI0512 15:08:44.754272       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/cbk 599\nI0512 15:08:44.954757       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/sdvx 233\nI0512 15:08:45.155282       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/d8b 287\nI0512 15:08:45.354734       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/b7st 387\nI0512 15:08:45.555272       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/nmh 462\nI0512 15:08:45.754779       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/8jmr 292\nI0512 15:08:45.955270       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/44q 545\nI0512 15:08:46.154972       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/sn7r 463\nI0512 15:08:46.354278       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/f7h 559\nI0512 15:08:46.554895       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/vh5 407\n"
  I0512 15:08:46.680419 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-logs-1716 delete pod logs-generator'
  I0512 15:08:47.678524 23 builder.go:146] stderr: ""
  I0512 15:08:47.678633 23 builder.go:147] stdout: "pod \"logs-generator\" deleted\n"
  I0512 15:08:47.678959 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-1716" for this suite. @ 05/12/25 15:08:47.696
• [6.948 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:393
  STEP: Creating a kubernetes client @ 05/12/25 15:08:47.715
  I0512 15:08:47.715559 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 15:08:47.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:08:47.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:08:47.772
  STEP: creating all guestbook components @ 05/12/25 15:08:47.779
  I0512 15:08:47.779455 23 kubectl.go:399] apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  I0512 15:08:47.779955 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 create -f -'
  I0512 15:08:48.154803 23 builder.go:146] stderr: ""
  I0512 15:08:48.154896 23 builder.go:147] stdout: "service/agnhost-replica created\n"
  I0512 15:08:48.154989 23 kubectl.go:399] apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  I0512 15:08:48.155164 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 create -f -'
  I0512 15:08:48.481661 23 builder.go:146] stderr: ""
  I0512 15:08:48.481729 23 builder.go:147] stdout: "service/agnhost-primary created\n"
  I0512 15:08:48.481805 23 kubectl.go:399] apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  I0512 15:08:48.481922 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 create -f -'
  I0512 15:08:48.804474 23 builder.go:146] stderr: ""
  I0512 15:08:48.804574 23 builder.go:147] stdout: "service/frontend created\n"
  I0512 15:08:48.804687 23 kubectl.go:399] apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.52
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  I0512 15:08:48.804883 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 create -f -'
  I0512 15:08:49.084739 23 builder.go:146] stderr: ""
  I0512 15:08:49.084835 23 builder.go:147] stdout: "deployment.apps/frontend created\n"
  I0512 15:08:49.084947 23 kubectl.go:399] apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.52
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  I0512 15:08:49.085424 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 create -f -'
  I0512 15:08:49.337750 23 builder.go:146] stderr: ""
  I0512 15:08:49.337828 23 builder.go:147] stdout: "deployment.apps/agnhost-primary created\n"
  I0512 15:08:49.337939 23 kubectl.go:399] apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.52
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  I0512 15:08:49.338245 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 create -f -'
  I0512 15:08:49.602961 23 builder.go:146] stderr: ""
  I0512 15:08:49.603024 23 builder.go:147] stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/12/25 15:08:49.603
  I0512 15:08:49.603116 23 kubectl.go:2272] Waiting for all frontend pods to be Running.
  I0512 15:08:54.655070 23 kubectl.go:2276] Waiting for frontend to serve content.
  I0512 15:08:54.677896 23 kubectl.go:2281] Trying to add a new entry to the guestbook.
  I0512 15:08:54.704201 23 kubectl.go:2286] Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/12/25 15:08:54.725
  I0512 15:08:54.725559 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 delete --grace-period=0 --force -f -'
  I0512 15:08:54.999033 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:08:54.999108 23 builder.go:147] stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/25 15:08:54.999
  I0512 15:08:54.999503 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 delete --grace-period=0 --force -f -'
  I0512 15:08:55.275628 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:08:55.275855 23 builder.go:147] stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/25 15:08:55.277
  I0512 15:08:55.278190 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 delete --grace-period=0 --force -f -'
  I0512 15:08:55.480736 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:08:55.480829 23 builder.go:147] stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/25 15:08:55.481
  I0512 15:08:55.481287 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 delete --grace-period=0 --force -f -'
  I0512 15:08:55.635919 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:08:55.636311 23 builder.go:147] stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/25 15:08:55.636
  I0512 15:08:55.636835 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 delete --grace-period=0 --force -f -'
  I0512 15:08:55.908842 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:08:55.908991 23 builder.go:147] stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/25 15:08:55.909
  I0512 15:08:55.909273 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-2102 delete --grace-period=0 --force -f -'
  I0512 15:08:56.077422 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:08:56.077474 23 builder.go:147] stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  I0512 15:08:56.078521 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2102" for this suite. @ 05/12/25 15:08:56.092
• [8.399 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Environment:NotInUserNS] [Conformance] [sig-node, NodeConformance, Environment:NotInUserNS, Conformance]
k8s.io/kubernetes/test/e2e/common/node/sysctl.go:79
  STEP: Creating a kubernetes client @ 05/12/25 15:08:56.114
  I0512 15:08:56.114727 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sysctl @ 05/12/25 15:08:56.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:08:56.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:08:56.199
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/12/25 15:08:56.203
  STEP: Watching for error events or started pod @ 05/12/25 15:08:56.223
  STEP: Waiting for pod completion @ 05/12/25 15:08:58.232
  STEP: Checking that the pod succeeded @ 05/12/25 15:09:00.257
  STEP: Getting logs from the pod @ 05/12/25 15:09:00.257
  STEP: Checking that the sysctl is actually updated @ 05/12/25 15:09:00.299
  I0512 15:09:00.299859 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3527" for this suite. @ 05/12/25 15:09:00.313
• [4.214 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/12/25 15:09:00.331
  I0512 15:09:00.331374 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename subjectreview @ 05/12/25 15:09:00.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:00.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:00.415
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-4996" @ 05/12/25 15:09:00.423
  I0512 15:09:00.435323 23 subjectreviews.go:66] saUsername: "system:serviceaccount:subjectreview-4996:e2e"
  I0512 15:09:00.435406 23 subjectreviews.go:69] saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-4996"}
  I0512 15:09:00.435433 23 subjectreviews.go:71] saUID: "b7ed2239-7e7f-48c4-ba06-bd90f1f8dd1b"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-4996:e2e" @ 05/12/25 15:09:00.435
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-4996:e2e" @ 05/12/25 15:09:00.436
  I0512 15:09:00.440206 23 subjectreviews.go:102] sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-4996:e2e" api 'list' configmaps in "subjectreview-4996" namespace @ 05/12/25 15:09:00.441
  I0512 15:09:00.445769 23 subjectreviews.go:121] SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-4996:e2e" @ 05/12/25 15:09:00.445
  I0512 15:09:00.450676 23 subjectreviews.go:144] lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  I0512 15:09:00.450746 23 subjectreviews.go:150] LocalSubjectAccessReview has been verified
  I0512 15:09:00.450931 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-4996" for this suite. @ 05/12/25 15:09:00.465
• [0.149 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:120
  STEP: Creating a kubernetes client @ 05/12/25 15:09:00.48
  I0512 15:09:00.480203 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:09:00.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:00.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:00.527
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/12/25 15:09:00.538
  STEP: Saw pod success @ 05/12/25 15:09:04.599
  I0512 15:09:04.607614 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-f2639d9e-ca9a-468b-a07f-3af76fca74c5 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:09:04.62
  I0512 15:09:04.672417 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7126" for this suite. @ 05/12/25 15:09:04.684
• [4.227 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:77
  STEP: Creating a kubernetes client @ 05/12/25 15:09:04.708
  I0512 15:09:04.708330 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 15:09:04.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:04.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:04.779
  STEP: Creating a pod to test substitution in container's command @ 05/12/25 15:09:04.786
  STEP: Saw pod success @ 05/12/25 15:09:08.85
  I0512 15:09:08.880479 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod var-expansion-b2cddb8e-c20f-4c4b-9b90-4deb1318a768 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 15:09:08.903
  I0512 15:09:08.985111 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5287" for this suite. @ 05/12/25 15:09:09
• [4.303 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/controller_revision.go:126
  STEP: Creating a kubernetes client @ 05/12/25 15:09:09.011
  I0512 15:09:09.011606 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename controllerrevisions @ 05/12/25 15:09:09.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:09.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:09.05
  STEP: Creating DaemonSet "e2e-dvpnf-daemon-set" @ 05/12/25 15:09:09.145
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/25 15:09:09.159
  I0512 15:09:09.241292 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:09.241819 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:09.242304 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:09.275977 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset e2e-dvpnf-daemon-set: 0
  I0512 15:09:09.276076 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 15:09:10.171009 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:10.171237 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:10.171490 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:10.179788 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset e2e-dvpnf-daemon-set: 0
  I0512 15:09:10.179988 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 15:09:11.171922 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:11.172056 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:11.172174 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:09:11.182719 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset e2e-dvpnf-daemon-set: 4
  I0512 15:09:11.182819 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset e2e-dvpnf-daemon-set
  STEP: Confirm DaemonSet "e2e-dvpnf-daemon-set" successfully created with "daemonset-name=e2e-dvpnf-daemon-set" label @ 05/12/25 15:09:11.189
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-dvpnf-daemon-set" @ 05/12/25 15:09:11.206
  I0512 15:09:11.214405 23 controller_revision.go:162] Located ControllerRevision: "e2e-dvpnf-daemon-set-5b7b4bb8fd"
  STEP: Patching ControllerRevision "e2e-dvpnf-daemon-set-5b7b4bb8fd" @ 05/12/25 15:09:11.222
  I0512 15:09:11.237850 23 controller_revision.go:173] e2e-dvpnf-daemon-set-5b7b4bb8fd has been patched
  STEP: Create a new ControllerRevision @ 05/12/25 15:09:11.237
  I0512 15:09:11.246031 23 controller_revision.go:191] Created ControllerRevision: e2e-dvpnf-daemon-set-78957686fb
  STEP: Confirm that there are two ControllerRevisions @ 05/12/25 15:09:11.246
  I0512 15:09:11.246189 23 controller_revision.go:254] Requesting list of ControllerRevisions to confirm quantity
  I0512 15:09:11.251595 23 controller_revision.go:265] Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-dvpnf-daemon-set-5b7b4bb8fd" @ 05/12/25 15:09:11.252
  STEP: Confirm that there is only one ControllerRevision @ 05/12/25 15:09:11.26
  I0512 15:09:11.260275 23 controller_revision.go:254] Requesting list of ControllerRevisions to confirm quantity
  I0512 15:09:11.265110 23 controller_revision.go:265] Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-dvpnf-daemon-set-78957686fb" @ 05/12/25 15:09:11.269
  I0512 15:09:11.288870 23 controller_revision.go:220] e2e-dvpnf-daemon-set-78957686fb has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/12/25 15:09:11.289
  W0512 15:09:11.297014      23 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/12/25 15:09:11.297
  I0512 15:09:11.298307 23 controller_revision.go:254] Requesting list of ControllerRevisions to confirm quantity
  I0512 15:09:12.298947 23 controller_revision.go:254] Requesting list of ControllerRevisions to confirm quantity
  I0512 15:09:12.304678 23 controller_revision.go:265] Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-dvpnf-daemon-set-78957686fb=updated" @ 05/12/25 15:09:12.305
  STEP: Confirm that there is only one ControllerRevision @ 05/12/25 15:09:12.314
  I0512 15:09:12.315254 23 controller_revision.go:254] Requesting list of ControllerRevisions to confirm quantity
  I0512 15:09:12.320297 23 controller_revision.go:265] Found 1 ControllerRevisions
  I0512 15:09:12.327070 23 controller_revision.go:246] ControllerRevision "e2e-dvpnf-daemon-set-6c78bf797" has revision 3
  STEP: Deleting DaemonSet "e2e-dvpnf-daemon-set" @ 05/12/25 15:09:12.331
  STEP: deleting DaemonSet.extensions e2e-dvpnf-daemon-set in namespace controllerrevisions-4926, will wait for the garbage collector to delete the pods @ 05/12/25 15:09:12.332
  I0512 15:09:12.401234 23 resources.go:139] Deleting DaemonSet.extensions e2e-dvpnf-daemon-set took: 13.107014ms
  I0512 15:09:12.504656 23 resources.go:163] Terminating DaemonSet.extensions e2e-dvpnf-daemon-set pods took: 103.41843ms
  I0512 15:09:14.915963 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset e2e-dvpnf-daemon-set: 0
  I0512 15:09:14.916070 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset e2e-dvpnf-daemon-set
  I0512 15:09:14.923323 23 controller_revision.go:73] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20806"},"items":null}

  I0512 15:09:14.928601 23 controller_revision.go:78] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20806"},"items":null}

  I0512 15:09:14.979715 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-4926" for this suite. @ 05/12/25 15:09:14.994
• [6.006 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/downwardapi.go:269
  STEP: Creating a kubernetes client @ 05/12/25 15:09:15.02
  I0512 15:09:15.020447 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 15:09:15.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:15.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:15.074
  STEP: Creating a pod to test downward api env vars @ 05/12/25 15:09:15.079
  STEP: Saw pod success @ 05/12/25 15:09:19.14
  I0512 15:09:19.153007 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod downward-api-e628b629-6029-4b7b-8e9f-ecb14c42289f container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 15:09:19.169
  I0512 15:09:19.205531 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8885" for this suite. @ 05/12/25 15:09:19.217
• [4.212 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:1396
  STEP: Creating a kubernetes client @ 05/12/25 15:09:19.232
  I0512 15:09:19.232982 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:09:19.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:19.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:19.283
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-6982 @ 05/12/25 15:09:19.29
  STEP: changing the ExternalName service to type=ClusterIP @ 05/12/25 15:09:19.301
  STEP: creating replication controller externalname-service in namespace services-6982 @ 05/12/25 15:09:19.357
  I0512 15:09:19.390433      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6982, replica count: 2
  I0512 15:09:22.441473      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 15:09:22.441929 23 resource.go:361] Creating new exec pod
  I0512 15:09:25.498269 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6982 exec execpodpv4tw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  I0512 15:09:25.793841 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  I0512 15:09:25.793955 23 builder.go:147] stdout: ""
  I0512 15:09:26.497982 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6982 exec execpodpv4tw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  I0512 15:09:26.795756 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  I0512 15:09:26.795847 23 builder.go:147] stdout: ""
  I0512 15:09:27.498607 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6982 exec execpodpv4tw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  I0512 15:09:27.805076 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  I0512 15:09:27.805174 23 builder.go:147] stdout: "externalname-service-kqvwt"
  I0512 15:09:27.805323 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6982 exec execpodpv4tw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.45.221 80'
  I0512 15:09:28.097614 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.45.221 80\nConnection to 10.233.45.221 80 port [tcp/http] succeeded!\n"
  I0512 15:09:28.097729 23 builder.go:147] stdout: "externalname-service-znb6l"
  I0512 15:09:28.097912 23 service.go:1405] Cleaning up the ExternalName to ClusterIP test service
  I0512 15:09:28.157461 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6982" for this suite. @ 05/12/25 15:09:28.175
• [8.967 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/replica_set.go:155
  STEP: Creating a kubernetes client @ 05/12/25 15:09:28.2
  I0512 15:09:28.200327 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replicaset @ 05/12/25 15:09:28.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:28.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:28.264
  I0512 15:09:28.330218 23 resource.go:87] Pod name sample-pod: Found 0 pods out of 1
  I0512 15:09:33.345313 23 resource.go:87] Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/25 15:09:33.345
  STEP: Scaling up "test-rs" replicaset @ 05/12/25 15:09:33.345
  I0512 15:09:33.373179 23 replicaset.go:44] Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/12/25 15:09:33.373
  I0512 15:09:33.408149 23 replica_set.go:542] observed ReplicaSet test-rs in namespace replicaset-2664 with ReadyReplicas 1, AvailableReplicas 1
  I0512 15:09:33.459090 23 replica_set.go:542] observed ReplicaSet test-rs in namespace replicaset-2664 with ReadyReplicas 1, AvailableReplicas 1
  I0512 15:09:33.512688 23 replica_set.go:542] observed ReplicaSet test-rs in namespace replicaset-2664 with ReadyReplicas 1, AvailableReplicas 1
  I0512 15:09:33.537222 23 replica_set.go:542] observed ReplicaSet test-rs in namespace replicaset-2664 with ReadyReplicas 1, AvailableReplicas 1
  I0512 15:09:34.935142 23 replica_set.go:542] observed ReplicaSet test-rs in namespace replicaset-2664 with ReadyReplicas 2, AvailableReplicas 2
  I0512 15:09:35.473643 23 replica_set.go:545] observed Replicaset test-rs in namespace replicaset-2664 with ReadyReplicas 3 found true
  I0512 15:09:35.474516 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2664" for this suite. @ 05/12/25 15:09:35.488
• [7.302 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/12/25 15:09:35.503
  I0512 15:09:35.503891 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename tables @ 05/12/25 15:09:35.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:35.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:35.58
  I0512 15:09:35.594061 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-8338" for this suite. @ 05/12/25 15:09:35.606
• [0.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:75
  STEP: Creating a kubernetes client @ 05/12/25 15:09:35.632
  I0512 15:09:35.632898 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:09:35.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:35.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:35.691
  STEP: Creating configMap with name configmap-test-volume-e7c32175-f3a4-4a02-aa1c-d408c826941f @ 05/12/25 15:09:35.698
  STEP: Creating a pod to test consume configMaps @ 05/12/25 15:09:35.712
  STEP: Saw pod success @ 05/12/25 15:09:39.768
  I0512 15:09:39.777643 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod pod-configmaps-1d0651aa-0b9e-41e9-9b09-49ba8be73d9d container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:09:39.791
  I0512 15:09:39.821365 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4503" for this suite. @ 05/12/25 15:09:39.833
• [4.218 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:454
  STEP: Creating a kubernetes client @ 05/12/25 15:09:39.851
  I0512 15:09:39.851282 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 15:09:39.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:39.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:39.94
  STEP: Counting existing ResourceQuota @ 05/12/25 15:09:39.949
  STEP: Creating a ResourceQuota @ 05/12/25 15:09:44.964
  STEP: Ensuring resource quota status is calculated @ 05/12/25 15:09:44.983
  STEP: Creating a ReplicaSet @ 05/12/25 15:09:46.99
  STEP: Ensuring resource quota status captures replicaset creation @ 05/12/25 15:09:47.015
  STEP: Deleting a ReplicaSet @ 05/12/25 15:09:49.027
  STEP: Ensuring resource quota status released usage @ 05/12/25 15:09:49.043
  I0512 15:09:51.051054 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2372" for this suite. @ 05/12/25 15:09:51.063
• [11.227 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtimeclass.go:131
  STEP: Creating a kubernetes client @ 05/12/25 15:09:51.078
  I0512 15:09:51.078789 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/25 15:09:51.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:51.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:51.136
  I0512 15:09:53.233280 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1549" for this suite. @ 05/12/25 15:09:53.248
• [2.203 seconds]
------------------------------
SS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/podtemplates.go:177
  STEP: Creating a kubernetes client @ 05/12/25 15:09:53.284
  I0512 15:09:53.284579 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename podtemplate @ 05/12/25 15:09:53.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:53.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:53.357
  STEP: Create a pod template @ 05/12/25 15:09:53.364
  STEP: Replace a pod template @ 05/12/25 15:09:53.374
  I0512 15:09:53.401340 23 podtemplates.go:210] Found updated podtemplate annotation: "true"

  I0512 15:09:53.401772 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-559" for this suite. @ 05/12/25 15:09:53.426
• [0.160 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should reject validating webhook configurations with invalid match conditions [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:814
  STEP: Creating a kubernetes client @ 05/12/25 15:09:53.444
  I0512 15:09:53.444843 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:09:53.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:53.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:53.498
  STEP: Setting up server cert @ 05/12/25 15:09:53.589
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:09:54.501
  STEP: Deploying the webhook pod @ 05/12/25 15:09:54.517
  STEP: Wait for the deployment to be ready @ 05/12/25 15:09:54.536
  I0512 15:09:54.554382 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:09:56.587
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:09:56.613
  I0512 15:09:57.614171 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: creating a validating webhook with match conditions @ 05/12/25 15:09:57.628
  I0512 15:09:57.778906 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6884" for this suite. @ 05/12/25 15:09:57.799
  STEP: Destroying namespace "webhook-markers-4035" for this suite. @ 05/12/25 15:09:57.822
• [4.414 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:223
  STEP: Creating a kubernetes client @ 05/12/25 15:09:57.859
  I0512 15:09:57.859327 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:09:57.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:09:57.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:09:57.925
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:09:57.932
  STEP: Saw pod success @ 05/12/25 15:10:02.002
  I0512 15:10:02.010900 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod downwardapi-volume-e02a76f0-a27a-40ee-a81a-fe9aa892e6df container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:10:02.025
  I0512 15:10:02.052955 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9122" for this suite. @ 05/12/25 15:10:02.067
• [4.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:196
  STEP: Creating a kubernetes client @ 05/12/25 15:10:02.086
  I0512 15:10:02.086714 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:10:02.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:02.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:02.153
  STEP: Setting up server cert @ 05/12/25 15:10:02.247
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:10:03.876
  STEP: Deploying the webhook pod @ 05/12/25 15:10:03.887
  STEP: Wait for the deployment to be ready @ 05/12/25 15:10:03.909
  I0512 15:10:03.936982 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:10:05.982
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:10:06.027
  I0512 15:10:07.027481 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/12/25 15:10:07.043
  STEP: create a pod that should be denied by the webhook @ 05/12/25 15:10:07.078
  STEP: create a pod that causes the webhook to hang @ 05/12/25 15:10:07.119
  STEP: create a configmap that should be denied by the webhook @ 05/12/25 15:10:17.143
  STEP: create a configmap that should be admitted by the webhook @ 05/12/25 15:10:17.211
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/12/25 15:10:17.238
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/12/25 15:10:17.259
  STEP: create a namespace that bypass the webhook @ 05/12/25 15:10:17.275
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/12/25 15:10:17.327
  I0512 15:10:17.553386 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3926" for this suite. @ 05/12/25 15:10:17.569
  STEP: Destroying namespace "webhook-markers-5715" for this suite. @ 05/12/25 15:10:17.588
  STEP: Destroying namespace "exempted-namespace-4060" for this suite. @ 05/12/25 15:10:17.606
• [15.535 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AggregatedDiscovery should support aggregated discovery interface [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/aggregated_discovery.go:259
  STEP: Creating a kubernetes client @ 05/12/25 15:10:17.623
  I0512 15:10:17.623608 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename aggregateddiscovery @ 05/12/25 15:10:17.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:17.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:17.711
  I0512 15:10:17.736293 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregateddiscovery-4379" for this suite. @ 05/12/25 15:10:17.751
• [0.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/12/25 15:10:17.781
  I0512 15:10:17.781836 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/25 15:10:17.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:17.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:17.855
  I0512 15:10:19.929595 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3743" for this suite. @ 05/12/25 15:10:19.941
• [2.177 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:116
  STEP: Creating a kubernetes client @ 05/12/25 15:10:19.959
  I0512 15:10:19.959300 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:10:19.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:20.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:20.008
  STEP: Setting up server cert @ 05/12/25 15:10:20.105
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:10:21.357
  STEP: Deploying the webhook pod @ 05/12/25 15:10:21.374
  STEP: Wait for the deployment to be ready @ 05/12/25 15:10:21.401
  I0512 15:10:21.422067 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:10:23.457
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:10:23.492
  I0512 15:10:24.492163 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/12/25 15:10:24.512
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/12/25 15:10:24.515
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/12/25 15:10:24.515
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/12/25 15:10:24.515
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/12/25 15:10:24.517
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/12/25 15:10:24.517
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/12/25 15:10:24.519
  I0512 15:10:24.598767 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-501" for this suite. @ 05/12/25 15:10:24.61
  STEP: Destroying namespace "webhook-markers-1179" for this suite. @ 05/12/25 15:10:24.631
• [4.689 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/12/25 15:10:24.649
  I0512 15:10:24.649770 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:10:24.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:24.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:24.758
  STEP: Creating secret with name secret-test-map-faec95e5-bcce-4d5d-8bff-abadee4b39bc @ 05/12/25 15:10:24.769
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:10:24.781
  STEP: Saw pod success @ 05/12/25 15:10:28.834
  I0512 15:10:28.844803 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod pod-secrets-ea707f90-15ed-42a5-9df1-02b7d5c8f19e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:10:28.89
  I0512 15:10:28.925001 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1688" for this suite. @ 05/12/25 15:10:28.943
• [4.326 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:70
  STEP: Creating a kubernetes client @ 05/12/25 15:10:28.979
  I0512 15:10:28.979458 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:10:28.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:29.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:29.038
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:10:29.044
  STEP: Saw pod success @ 05/12/25 15:10:33.132
  I0512 15:10:33.140831 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-f32b52fa-8c66-4dfa-a150-d4a392bb61d6 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:10:33.156
  I0512 15:10:33.202559 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3183" for this suite. @ 05/12/25 15:10:33.217
• [4.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:3393
  STEP: Creating a kubernetes client @ 05/12/25 15:10:33.24
  I0512 15:10:33.240715 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:10:33.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:33.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:33.303
  STEP: creating a Service @ 05/12/25 15:10:33.316
  STEP: watching for the Service to be added @ 05/12/25 15:10:33.349
  I0512 15:10:33.354680 23 service.go:3445] Found Service test-service-fgz5r in namespace services-3658 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 32298}]
  I0512 15:10:33.354804 23 service.go:3452] Service test-service-fgz5r created
  STEP: Getting /status @ 05/12/25 15:10:33.354
  I0512 15:10:33.370853 23 service.go:3463] Service test-service-fgz5r has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/12/25 15:10:33.37
  STEP: watching for the Service to be patched @ 05/12/25 15:10:33.39
  I0512 15:10:33.395474 23 service.go:3486] observed Service test-service-fgz5r in namespace services-3658 with annotations: map[] & LoadBalancer: {[]}
  I0512 15:10:33.396407 23 service.go:3489] Found Service test-service-fgz5r in namespace services-3658 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  0xc003c82db0 []}]}
  I0512 15:10:33.397118 23 service.go:3496] Service test-service-fgz5r has service status patched
  STEP: updating the ServiceStatus @ 05/12/25 15:10:33.397
  I0512 15:10:33.431946 23 service.go:3516] updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/12/25 15:10:33.433
  I0512 15:10:33.440315 23 service.go:3527] Observed Service test-service-fgz5r in namespace services-3658 with annotations: map[] & Conditions: []
  I0512 15:10:33.440448 23 service.go:3538] Observed Service test-service-fgz5r in namespace services-3658 with annotations: map[patchedstatus:true] & Conditions: []
  I0512 15:10:33.440600 23 service.go:3534] Found Service test-service-fgz5r in namespace services-3658 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  I0512 15:10:33.440733 23 service.go:3545] Service test-service-fgz5r has service status updated
  STEP: patching the service @ 05/12/25 15:10:33.44
  STEP: watching for the Service to be patched @ 05/12/25 15:10:33.488
  I0512 15:10:33.496378 23 service.go:3568] observed Service test-service-fgz5r in namespace services-3658 with labels: map[test-service-static:true]
  I0512 15:10:33.496492 23 service.go:3568] observed Service test-service-fgz5r in namespace services-3658 with labels: map[test-service-static:true]
  I0512 15:10:33.497266 23 service.go:3568] observed Service test-service-fgz5r in namespace services-3658 with labels: map[test-service-static:true]
  I0512 15:10:33.497419 23 service.go:3571] Found Service test-service-fgz5r in namespace services-3658 with labels: map[test-service:patched test-service-static:true]
  I0512 15:10:33.497630 23 service.go:3578] Service test-service-fgz5r patched
  STEP: deleting the service @ 05/12/25 15:10:33.498
  STEP: watching for the Service to be deleted @ 05/12/25 15:10:33.552
  I0512 15:10:33.558522 23 service.go:3602] Observed event: ADDED
  I0512 15:10:33.558647 23 service.go:3602] Observed event: MODIFIED
  I0512 15:10:33.558700 23 service.go:3602] Observed event: MODIFIED
  I0512 15:10:33.560095 23 service.go:3602] Observed event: MODIFIED
  I0512 15:10:33.560208 23 service.go:3598] Found Service test-service-fgz5r in namespace services-3658 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  I0512 15:10:33.560584 23 service.go:3607] Service test-service-fgz5r deleted
  I0512 15:10:33.561550 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3658" for this suite. @ 05/12/25 15:10:33.581
• [0.360 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/field_validation.go:62
  STEP: Creating a kubernetes client @ 05/12/25 15:10:33.604
  I0512 15:10:33.604972 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename field-validation @ 05/12/25 15:10:33.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:33.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:33.727
  STEP: apply creating a deployment @ 05/12/25 15:10:33.738
  I0512 15:10:33.795883 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-819" for this suite. @ 05/12/25 15:10:33.815
• [0.239 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/12/25 15:10:33.845
  I0512 15:10:33.846208 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-watch @ 05/12/25 15:10:33.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:10:33.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:10:33.92
  I0512 15:10:33.928068 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Creating first CR  @ 05/12/25 15:10:41.534
  I0512 15:10:41.551836 23 watch.go:431] Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2025-05-12T15:10:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2025-05-12T15:10:41Z]] name:name1 resourceVersion:21857 uid:8593e74b-9419-4f0d-8bf2-ba4178b99d1e] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 05/12/25 15:10:51.553
  I0512 15:10:51.570658 23 watch.go:431] Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2025-05-12T15:10:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2025-05-12T15:10:51Z]] name:name2 resourceVersion:21901 uid:e7a89dfb-9f61-4227-9ef2-2f9254d758ba] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 05/12/25 15:11:01.572
  I0512 15:11:01.587674 23 watch.go:431] Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2025-05-12T15:10:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2025-05-12T15:11:01Z]] name:name1 resourceVersion:21950 uid:8593e74b-9419-4f0d-8bf2-ba4178b99d1e] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 05/12/25 15:11:11.588
  I0512 15:11:11.603660 23 watch.go:431] Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2025-05-12T15:10:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2025-05-12T15:11:11Z]] name:name2 resourceVersion:21992 uid:e7a89dfb-9f61-4227-9ef2-2f9254d758ba] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 05/12/25 15:11:21.605
  I0512 15:11:21.624052 23 watch.go:431] Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2025-05-12T15:10:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2025-05-12T15:11:01Z]] name:name1 resourceVersion:22036 uid:8593e74b-9419-4f0d-8bf2-ba4178b99d1e] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 05/12/25 15:11:31.624
  I0512 15:11:31.650151 23 watch.go:431] Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2025-05-12T15:10:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2025-05-12T15:11:11Z]] name:name2 resourceVersion:22080 uid:e7a89dfb-9f61-4227-9ef2-2f9254d758ba] num:map[num1:9223372036854775807 num2:1000000]]}
  I0512 15:11:42.171927 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-4991" for this suite. @ 05/12/25 15:11:42.182
• [68.355 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:137
  STEP: Creating a kubernetes client @ 05/12/25 15:11:42.202
  I0512 15:11:42.202228 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/25 15:11:42.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:11:42.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:11:42.285
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/25 15:11:42.309
  STEP: create the pod with lifecycle hook @ 05/12/25 15:11:44.39
  STEP: check poststart hook @ 05/12/25 15:11:46.434
  STEP: delete the pod with lifecycle hook @ 05/12/25 15:11:46.448
  I0512 15:11:50.500060 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8765" for this suite. @ 05/12/25 15:11:50.511
• [8.330 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:335
  STEP: Creating a kubernetes client @ 05/12/25 15:11:50.534
  I0512 15:11:50.534680 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 15:11:50.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:11:50.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:11:50.584
  STEP: Creating service test in namespace statefulset-3546 @ 05/12/25 15:11:50.591
  STEP: Creating a new StatefulSet @ 05/12/25 15:11:50.604
  I0512 15:11:50.628881 23 wait.go:40] Found 0 stateful pods, waiting for 3
  I0512 15:12:00.632312 23 wait.go:50] Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:00.633398 23 wait.go:50] Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:00.634366 23 wait.go:50] Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/12/25 15:12:00.733
  I0512 15:12:00.754656 23 statefulset.go:2507] Updating stateful set ss2
  STEP: Creating a new revision @ 05/12/25 15:12:00.754
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/12/25 15:12:10.775
  STEP: Performing a canary update @ 05/12/25 15:12:10.775
  I0512 15:12:10.799736 23 statefulset.go:2507] Updating stateful set ss2
  I0512 15:12:10.824980 23 wait.go:74] Waiting for Pod statefulset-3546/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/12/25 15:12:20.829
  I0512 15:12:20.949449 23 wait.go:40] Found 1 stateful pods, waiting for 3
  I0512 15:12:30.938666 23 wait.go:50] Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:30.938743 23 wait.go:50] Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:30.938766 23 wait.go:50] Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  I0512 15:12:40.945884 23 wait.go:50] Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:40.946524 23 wait.go:50] Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:40.947078 23 wait.go:50] Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  I0512 15:12:50.940000 23 wait.go:50] Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:50.940095 23 wait.go:50] Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:12:50.940138 23 wait.go:50] Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/12/25 15:12:50.952
  I0512 15:12:50.971768 23 statefulset.go:2507] Updating stateful set ss2
  I0512 15:12:51.004348 23 wait.go:74] Waiting for Pod statefulset-3546/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  I0512 15:13:01.023723 23 statefulset.go:2507] Updating stateful set ss2
  I0512 15:13:01.106565 23 wait.go:56] Waiting for StatefulSet statefulset-3546/ss2 to complete update
  I0512 15:13:01.106674 23 wait.go:63] Waiting for Pod statefulset-3546/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  I0512 15:13:11.040465 23 wait.go:56] Waiting for StatefulSet statefulset-3546/ss2 to complete update
  I0512 15:13:11.040592 23 wait.go:63] Waiting for Pod statefulset-3546/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  I0512 15:13:21.036897 23 wait.go:56] Waiting for StatefulSet statefulset-3546/ss2 to complete update
  I0512 15:13:31.043416 23 wait.go:56] Waiting for StatefulSet statefulset-3546/ss2 to complete update
  I0512 15:13:41.046278 23 statefulset.go:138] Deleting all statefulset in ns statefulset-3546
  I0512 15:13:41.055526 23 rest.go:150] Scaling statefulset ss2 to 0
  I0512 15:13:51.091560 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 15:13:51.109713 23 rest.go:88] Deleting statefulset ss2
  I0512 15:13:51.138499 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3546" for this suite. @ 05/12/25 15:13:51.149
• [120.631 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:200
  STEP: Creating a kubernetes client @ 05/12/25 15:13:51.165
  I0512 15:13:51.165950 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:13:51.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:13:51.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:13:51.223
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/12/25 15:13:51.228
  STEP: Saw pod success @ 05/12/25 15:13:55.275
  I0512 15:13:55.285016 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod pod-f9437620-e03a-49bf-b45c-b2b1750cc0e1 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:13:55.3
  I0512 15:13:55.363649 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1828" for this suite. @ 05/12/25 15:13:55.375
• [4.227 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/12/25 15:13:55.396
  I0512 15:13:55.397114 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename containers @ 05/12/25 15:13:55.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:13:55.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:13:55.451
  STEP: Creating a pod to test override arguments @ 05/12/25 15:13:55.462
  STEP: Saw pod success @ 05/12/25 15:13:59.533
  I0512 15:13:59.561290 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod client-containers-d944dc81-b09e-494b-a7c7-0208978a5a89 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:13:59.577
  I0512 15:13:59.619065 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3051" for this suite. @ 05/12/25 15:13:59.634
• [4.257 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/12/25 15:13:59.654
  I0512 15:13:59.654760 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-runtime @ 05/12/25 15:13:59.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:13:59.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:13:59.723
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/12/25 15:13:59.76
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/12/25 15:14:18.009
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/12/25 15:14:18.018
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/12/25 15:14:18.037
  STEP: Container 'terminate-cmd-rpa': should be possible to delete @ 05/12/25 15:14:18.037
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/12/25 15:14:18.089
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/12/25 15:14:21.145
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/12/25 15:14:23.193
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/12/25 15:14:23.214
  STEP: Container 'terminate-cmd-rpof': should be possible to delete @ 05/12/25 15:14:23.214
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/12/25 15:14:23.286
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/12/25 15:14:24.329
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/12/25 15:14:26.362
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/12/25 15:14:26.376
  STEP: Container 'terminate-cmd-rpn': should be possible to delete @ 05/12/25 15:14:26.377
  I0512 15:14:26.441769 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4179" for this suite. @ 05/12/25 15:14:26.455
• [26.819 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:1513
  STEP: Creating a kubernetes client @ 05/12/25 15:14:26.477
  I0512 15:14:26.477583 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:14:26.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:14:26.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:14:26.526
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-3913 @ 05/12/25 15:14:26.532
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/12/25 15:14:26.557
  STEP: creating service externalsvc in namespace services-3913 @ 05/12/25 15:14:26.558
  STEP: creating replication controller externalsvc in namespace services-3913 @ 05/12/25 15:14:26.601
  I0512 15:14:26.613711      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3913, replica count: 2
  I0512 15:14:29.665843      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/12/25 15:14:29.676
  I0512 15:14:29.726657 23 resource.go:361] Creating new exec pod
  I0512 15:14:31.803683 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3913 exec execpod8kkf4 -- /bin/sh -x -c nslookup nodeport-service.services-3913.svc.cluster.local'
  I0512 15:14:32.149372 23 builder.go:146] stderr: "+ nslookup nodeport-service.services-3913.svc.cluster.local\n"
  I0512 15:14:32.149486 23 builder.go:147] stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-3913.svc.cluster.local\tcanonical name = externalsvc.services-3913.svc.cluster.local.\nName:\texternalsvc.services-3913.svc.cluster.local\nAddress: 10.233.22.228\n\n"
  STEP: deleting ReplicationController externalsvc in namespace services-3913, will wait for the garbage collector to delete the pods @ 05/12/25 15:14:32.149
  I0512 15:14:32.228295 23 resources.go:139] Deleting ReplicationController externalsvc took: 17.637736ms
  I0512 15:14:32.329408 23 resources.go:163] Terminating ReplicationController externalsvc pods took: 101.100023ms
  I0512 15:14:35.504198 23 service.go:1524] Cleaning up the NodePort to ExternalName test service
  I0512 15:14:35.548023 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3913" for this suite. @ 05/12/25 15:14:35.573
• [9.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] PersistentVolumes CSI Conformance should apply changes to a pv/pvc status [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/persistent_volumes.go:668
  STEP: Creating a kubernetes client @ 05/12/25 15:14:35.606
  I0512 15:14:35.606873 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pv @ 05/12/25 15:14:35.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:14:35.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:14:35.673
  STEP: Creating initial PV and PVC @ 05/12/25 15:14:35.683
  I0512 15:14:35.683696 23 pv.go:394] Creating a PV followed by a PVC
  STEP: Listing all PVs with the labelSelector: "e2e-pv-pool=pv-7560" @ 05/12/25 15:14:35.73
  STEP: Listing PVCs in namespace "pv-7560" @ 05/12/25 15:14:35.744
  STEP: Reading "pvc-mktc4" Status @ 05/12/25 15:14:35.773
  STEP: Reading "pv-7560-njbfj" Status @ 05/12/25 15:14:35.788
  STEP: Patching "pvc-mktc4" Status @ 05/12/25 15:14:35.81
  STEP: Patching "pv-7560-njbfj" Status @ 05/12/25 15:14:35.837
  STEP: Updating "pvc-mktc4" Status @ 05/12/25 15:14:35.861
  STEP: Updating "pv-7560-njbfj" Status @ 05/12/25 15:14:35.901
  I0512 15:14:35.935147 23 persistent_volumes.go:406] AfterEach: deleting 1 PVCs and 1 PVs...
  I0512 15:14:35.935242 23 pv.go:205] Deleting PersistentVolumeClaim "pvc-mktc4"
  I0512 15:14:35.978009 23 pv.go:193] Deleting PersistentVolume "pv-7560-njbfj"
  I0512 15:14:35.993019 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pv-7560" for this suite. @ 05/12/25 15:14:36.009
• [0.421 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:235
  STEP: Creating a kubernetes client @ 05/12/25 15:14:36.027
  I0512 15:14:36.027636 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 15:14:36.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:14:36.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:14:36.084
  STEP: Counting existing ResourceQuota @ 05/12/25 15:14:36.099
  STEP: Creating a ResourceQuota @ 05/12/25 15:14:41.113
  STEP: Ensuring resource quota status is calculated @ 05/12/25 15:14:41.125
  STEP: Creating a Pod that fits quota @ 05/12/25 15:14:43.136
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/12/25 15:14:43.177
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/12/25 15:14:45.187
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/12/25 15:14:45.193
  STEP: Ensuring a pod cannot update its resource requirements @ 05/12/25 15:14:45.2
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/12/25 15:14:45.212
  STEP: Deleting the pod @ 05/12/25 15:14:47.22
  STEP: Ensuring resource quota status released the pod usage @ 05/12/25 15:14:47.246
  I0512 15:14:49.264665 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3914" for this suite. @ 05/12/25 15:14:49.283
• [13.272 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate everything except 'skip-me' configmaps [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:863
  STEP: Creating a kubernetes client @ 05/12/25 15:14:49.3
  I0512 15:14:49.300197 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:14:49.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:14:49.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:14:49.377
  STEP: Setting up server cert @ 05/12/25 15:14:49.47
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:14:50.065
  STEP: Deploying the webhook pod @ 05/12/25 15:14:50.082
  STEP: Wait for the deployment to be ready @ 05/12/25 15:14:50.123
  I0512 15:14:50.147361 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:14:52.184
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:14:52.211
  I0512 15:14:53.212051 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: creating a mutating webhook with match conditions @ 05/12/25 15:14:53.232
  STEP: create the configmap with a random name @ 05/12/25 15:14:53.289
  STEP: verify the configmap is mutated @ 05/12/25 15:14:53.339
  STEP: create the configmap with 'skip-me' name @ 05/12/25 15:14:53.339
  I0512 15:14:53.491261 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-810" for this suite. @ 05/12/25 15:14:53.611
  STEP: Destroying namespace "webhook-markers-8182" for this suite. @ 05/12/25 15:14:53.633
• [4.351 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:277
  STEP: Creating a kubernetes client @ 05/12/25 15:14:53.658
  I0512 15:14:53.658966 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 15:14:53.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:14:53.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:14:53.728
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/12/25 15:14:53.747
  I0512 15:14:53.750266 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:15:02.269934 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:15:20.085418 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2583" for this suite. @ 05/12/25 15:15:20.111
• [26.469 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/12/25 15:15:20.129
  I0512 15:15:20.129673 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-pred @ 05/12/25 15:15:20.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:15:20.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:15:20.198
  I0512 15:15:20.210688 23 helper.go:122] Waiting up to 1m0s for all (but 0) nodes to be ready
  I0512 15:15:20.237284 23 util.go:393] Waiting for terminating namespaces to be deleted...
  I0512 15:15:20.249292 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-0 before test
  I0512 15:15:20.261807 23 predicates.go:957] harbor-core-569d44cfd7-46qkt from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.261893 23 predicates.go:959] 	Container core ready: true, restart count 0
  I0512 15:15:20.261972 23 predicates.go:957] harbor-database-0 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.262000 23 predicates.go:959] 	Container database ready: true, restart count 0
  I0512 15:15:20.262055 23 predicates.go:957] kube-flannel-m6mdj from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.262164 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:15:20.262191 23 predicates.go:957] kube-proxy-ksc6h from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.262247 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:15:20.262274 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-0 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.262290 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:15:20.262338 23 predicates.go:957] nodelocaldns-fp5rs from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.262358 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:15:20.262376 23 predicates.go:957] vsphere-csi-node-zmnvq from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.262419 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:15:20.262439 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:15:20.262456 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:15:20.262500 23 predicates.go:957] busybox1 from smoke-tests-gpkyttqvwmrflgnlgfxgvaqkcwcgdbcf started at 2025-05-12 15:14:27 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.262521 23 predicates.go:959] 	Container busybox1 ready: true, restart count 0
  I0512 15:15:20.262543 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-9znn4 from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:15:20.262559 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:15:20.262608 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:15:20.262628 23 predicates.go:957] vault-1 from vault started at 2025-05-12 14:25:29 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.262645 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:15:20.262692 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:15:20.262709 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:15:20.262730 23 predicates.go:957] vault-configurer-59545bd678-lmkzg from vault started at 2025-05-12 14:24:44 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.262775 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:15:20.262795 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-1 before test
  I0512 15:15:20.278326 23 predicates.go:957] harbor-redis-0 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.279572 23 predicates.go:959] 	Container redis ready: true, restart count 0
  I0512 15:15:20.281228 23 predicates.go:957] kube-flannel-8bzqg from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.282599 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:15:20.283733 23 predicates.go:957] kube-proxy-g82r5 from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.285224 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:15:20.286453 23 predicates.go:957] metrics-server-c5b7b4dc-5596r from kube-system started at 2025-05-12 14:15:20 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.287622 23 predicates.go:959] 	Container metrics-server ready: true, restart count 0
  I0512 15:15:20.288874 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-1 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.289956 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:15:20.290888 23 predicates.go:957] nodelocaldns-v22ld from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.291508 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:15:20.292411 23 predicates.go:957] vsphere-csi-node-4z66x from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.293087 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:15:20.293724 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:15:20.294219 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:15:20.294836 23 predicates.go:957] busybox2 from smoke-tests-gpkyttqvwmrflgnlgfxgvaqkcwcgdbcf started at 2025-05-12 15:14:28 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.295569 23 predicates.go:959] 	Container busybox2 ready: true, restart count 0
  I0512 15:15:20.296361 23 predicates.go:957] sonobuoy from sonobuoy started at 2025-05-12 14:52:15 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.297371 23 predicates.go:959] 	Container kube-sonobuoy ready: true, restart count 0
  I0512 15:15:20.298342 23 predicates.go:957] sonobuoy-e2e-job-0883294174af4fef from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:15:20.299124 23 predicates.go:959] 	Container e2e ready: true, restart count 0
  I0512 15:15:20.300049 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:15:20.300825 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-b5spt from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:15:20.301581 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:15:20.302335 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:15:20.303121 23 predicates.go:957] vault-0 from vault started at 2025-05-12 14:24:45 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.304635 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:15:20.304688 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:15:20.304712 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:15:20.304733 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-2 before test
  I0512 15:15:20.316407 23 predicates.go:957] harbor-registry-5584d97487-9cst2 from harbor started at 2025-05-12 14:20:33 +0000 UTC (2 container statuses recorded)
  I0512 15:15:20.316485 23 predicates.go:959] 	Container registry ready: true, restart count 0
  I0512 15:15:20.316507 23 predicates.go:959] 	Container registryctl ready: true, restart count 0
  I0512 15:15:20.316574 23 predicates.go:957] kube-flannel-sfqr5 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.316594 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:15:20.316613 23 predicates.go:957] kube-proxy-95b2r from kube-system started at 2025-05-12 14:12:58 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.316630 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:15:20.316647 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-2 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.316665 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:15:20.316683 23 predicates.go:957] nodelocaldns-5lcd4 from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.316698 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:15:20.316715 23 predicates.go:957] vsphere-csi-node-sqm2v from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.316731 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:15:20.316749 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:15:20.316765 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:15:20.316784 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-qbt8k from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:15:20.316800 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:15:20.316815 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:15:20.316833 23 predicates.go:957] vault-2 from vault started at 2025-05-12 14:26:08 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.316848 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:15:20.316863 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:15:20.316879 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:15:20.316896 23 predicates.go:957] velero-5f4c979ccf-4c5mx from velero started at 2025-05-12 14:22:48 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.316911 23 predicates.go:959] 	Container velero ready: true, restart count 0
  I0512 15:15:20.316928 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-3 before test
  I0512 15:15:20.361867 23 predicates.go:957] harbor-jobservice-79c749f9bc-qm554 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.362666 23 predicates.go:959] 	Container jobservice ready: true, restart count 2
  I0512 15:15:20.363446 23 predicates.go:957] harbor-portal-757685fc6b-7smbd from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.364005 23 predicates.go:959] 	Container portal ready: true, restart count 0
  I0512 15:15:20.365001 23 predicates.go:957] kube-flannel-pglr4 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.365858 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:15:20.366843 23 predicates.go:957] kube-proxy-5nnvw from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.367665 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:15:20.368503 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-3 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.369369 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:15:20.370616 23 predicates.go:957] nodelocaldns-v58mx from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.371499 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:15:20.372713 23 predicates.go:957] vsphere-csi-node-blmnp from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.373752 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:15:20.374503 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:15:20.374963 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:15:20.375427 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-rbcss from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:15:20.375766 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:15:20.376401 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:15:20.376834 23 predicates.go:957] vault-3 from vault started at 2025-05-12 14:27:02 +0000 UTC (3 container statuses recorded)
  I0512 15:15:20.377202 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:15:20.377579 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:15:20.377936 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:15:20.378746 23 predicates.go:957] vault-operator-56c68d678-2gr95 from vault started at 2025-05-12 14:24:33 +0000 UTC (1 container statuses recorded)
  I0512 15:15:20.379430 23 predicates.go:959] 	Container vault-operator ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/12/25 15:15:20.38
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/12/25 15:15:22.448
  STEP: Trying to apply a random label on the found node. @ 05/12/25 15:15:22.513
  STEP: verifying the node has the label kubernetes.io/e2e-247155a3-e2df-4146-a777-1e4daf70de9e 42 @ 05/12/25 15:15:22.552
  STEP: Trying to relaunch the pod, now with labels. @ 05/12/25 15:15:22.561
  STEP: removing the label kubernetes.io/e2e-247155a3-e2df-4146-a777-1e4daf70de9e off the node opscontrol-jaku1-worker-0 @ 05/12/25 15:15:24.606
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-247155a3-e2df-4146-a777-1e4daf70de9e @ 05/12/25 15:15:24.651
  I0512 15:15:24.667222 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8622" for this suite. @ 05/12/25 15:15:24.678
• [4.570 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:218
  STEP: Creating a kubernetes client @ 05/12/25 15:15:24.701
  I0512 15:15:24.701714 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:15:24.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:15:24.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:15:24.767
  STEP: Setting up server cert @ 05/12/25 15:15:24.865
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:15:25.919
  STEP: Deploying the webhook pod @ 05/12/25 15:15:25.947
  STEP: Wait for the deployment to be ready @ 05/12/25 15:15:25.973
  I0512 15:15:25.994902 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:15:28.02
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:15:28.037
  I0512 15:15:29.037797 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  I0512 15:15:29.073091 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/12/25 15:15:34.612
  STEP: Creating a custom resource that should be denied by the webhook @ 05/12/25 15:15:34.648
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/12/25 15:15:36.702
  STEP: Updating the custom resource with disallowed data should be denied @ 05/12/25 15:15:36.719
  STEP: Deleting the custom resource should be denied @ 05/12/25 15:15:36.745
  STEP: Remove the offending key and value from the custom resource data @ 05/12/25 15:15:36.761
  STEP: Deleting the updated custom resource should be successful @ 05/12/25 15:15:36.784
  I0512 15:15:37.561900 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4739" for this suite. @ 05/12/25 15:15:37.57
  STEP: Destroying namespace "webhook-markers-7453" for this suite. @ 05/12/25 15:15:37.584
• [12.897 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:170
  STEP: Creating a kubernetes client @ 05/12/25 15:15:37.599
  I0512 15:15:37.599436 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/25 15:15:37.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:15:37.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:15:37.647
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/25 15:15:37.67
  STEP: create the pod with lifecycle hook @ 05/12/25 15:15:39.725
  STEP: check poststart hook @ 05/12/25 15:15:41.766
  STEP: delete the pod with lifecycle hook @ 05/12/25 15:15:41.797
  I0512 15:15:43.831222 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7266" for this suite. @ 05/12/25 15:15:43.844
• [6.271 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/job.go:788
  STEP: Creating a kubernetes client @ 05/12/25 15:15:43.87
  I0512 15:15:43.870798 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename job @ 05/12/25 15:15:43.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:15:43.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:15:43.937
  STEP: Creating a job @ 05/12/25 15:15:43.946
  STEP: Ensuring job reaches completions @ 05/12/25 15:15:43.971
  I0512 15:15:55.994208 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7211" for this suite. @ 05/12/25 15:15:56.006
• [12.152 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 05/12/25 15:15:56.022
  I0512 15:15:56.022977 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename daemonsets @ 05/12/25 15:15:56.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:15:56.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:15:56.078
  I0512 15:15:56.176175 23 daemon_set.go:447] Create a RollingUpdate DaemonSet
  I0512 15:15:56.193217 23 daemon_set.go:454] Check that daemon pods launch on every node of the cluster
  I0512 15:15:56.223446 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:56.223644 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:56.223897 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:56.234825 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:15:56.234909 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 15:15:57.218283 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:57.218426 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:57.218486 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:57.233729 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:15:57.233993 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 15:15:58.217610 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:58.218002 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:58.218278 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:15:58.227693 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:15:58.227909 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  I0512 15:15:58.228012 23 daemon_set.go:458] Update the DaemonSet to trigger a rollout
  I0512 15:15:58.248888 23 daemon_set.go:102] Updating DaemonSet daemon-set
  I0512 15:16:00.288425 23 daemon_set.go:493] Roll back the DaemonSet before rollout is complete
  I0512 15:16:00.305199 23 daemon_set.go:102] Updating DaemonSet daemon-set
  I0512 15:16:00.305288 23 daemon_set.go:499] Make sure DaemonSet rollback is complete
  I0512 15:16:00.388781 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:00.388867 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:00.388936 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:01.334942 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:01.335325 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:01.335593 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:02.343623 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:02.343739 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:02.343816 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:03.337769 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:03.337939 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:03.338160 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:04.341388 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:04.341678 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:04.341847 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:05.326234 23 daemon_set.go:1198] Pod daemon-set-mmsfd is not available
  I0512 15:16:05.338546 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:05.338806 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:16:05.339000 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/25 15:16:05.357
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7330, will wait for the garbage collector to delete the pods @ 05/12/25 15:16:05.357
  I0512 15:16:05.441263 23 resources.go:139] Deleting DaemonSet.extensions daemon-set took: 25.587249ms
  I0512 15:16:05.542328 23 resources.go:163] Terminating DaemonSet.extensions daemon-set pods took: 101.060252ms
  I0512 15:16:08.150016 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:16:08.150136 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  I0512 15:16:08.158601 23 daemon_set.go:135] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24851"},"items":null}

  I0512 15:16:08.167639 23 daemon_set.go:140] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24851"},"items":null}

  I0512 15:16:08.216946 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7330" for this suite. @ 05/12/25 15:16:08.226
• [12.218 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance] [sig-api-machinery, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/namespace.go:305
  STEP: Creating a kubernetes client @ 05/12/25 15:16:08.241
  I0512 15:16:08.241083 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename namespaces @ 05/12/25 15:16:08.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:08.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:08.293
  STEP: Read namespace status @ 05/12/25 15:16:08.299
  I0512 15:16:08.305508 23 namespace.go:318] Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/12/25 15:16:08.305
  I0512 15:16:08.320438 23 namespace.go:338] Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/12/25 15:16:08.321
  I0512 15:16:08.351894 23 namespace.go:363] Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  I0512 15:16:08.352632 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1878" for this suite. @ 05/12/25 15:16:08.385
• [0.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/12/25 15:16:08.4
  I0512 15:16:08.401024 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:16:08.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:08.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:08.445
  STEP: Creating projection with secret that has name projected-secret-test-f1256112-1a9f-4835-ba51-53e4f313decf @ 05/12/25 15:16:08.452
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:16:08.464
  STEP: Saw pod success @ 05/12/25 15:16:12.512
  I0512 15:16:12.521448 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod pod-projected-secrets-a5dcb926-e3fc-4d5b-b807-cee45b65aa30 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:16:12.565
  I0512 15:16:12.602618 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4137" for this suite. @ 05/12/25 15:16:12.617
• [4.236 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/12/25 15:16:12.638
  I0512 15:16:12.638931 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:16:12.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:12.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:12.714
  STEP: Creating projection with secret that has name projected-secret-test-75197fe4-64ee-412d-aa8b-ceea25776935 @ 05/12/25 15:16:12.722
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:16:12.746
  STEP: Saw pod success @ 05/12/25 15:16:16.806
  I0512 15:16:16.817771 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod pod-projected-secrets-73d98519-0bcb-4c73-88c6-e6095cd5372a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:16:16.832
  I0512 15:16:16.869847 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1947" for this suite. @ 05/12/25 15:16:16.881
• [4.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/12/25 15:16:16.908
  I0512 15:16:16.908573 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-pred @ 05/12/25 15:16:16.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:16.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:16.981
  I0512 15:16:16.989551 23 helper.go:122] Waiting up to 1m0s for all (but 0) nodes to be ready
  I0512 15:16:17.011594 23 util.go:393] Waiting for terminating namespaces to be deleted...
  I0512 15:16:17.018570 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-0 before test
  I0512 15:16:17.033381 23 predicates.go:957] harbor-core-569d44cfd7-46qkt from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.033456 23 predicates.go:959] 	Container core ready: true, restart count 0
  I0512 15:16:17.033485 23 predicates.go:957] harbor-database-0 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.033502 23 predicates.go:959] 	Container database ready: true, restart count 0
  I0512 15:16:17.033521 23 predicates.go:957] kube-flannel-m6mdj from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.033549 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:16:17.033567 23 predicates.go:957] kube-proxy-ksc6h from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.033582 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:16:17.033601 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-0 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.033618 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:16:17.033636 23 predicates.go:957] nodelocaldns-fp5rs from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.033651 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:16:17.033669 23 predicates.go:957] vsphere-csi-node-zmnvq from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.033684 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:16:17.033706 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:16:17.033724 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:16:17.033741 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-9znn4 from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:16:17.033758 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:16:17.033774 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:16:17.033791 23 predicates.go:957] vault-1 from vault started at 2025-05-12 14:25:29 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.033811 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:16:17.033838 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:16:17.033871 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:16:17.033909 23 predicates.go:957] vault-configurer-59545bd678-lmkzg from vault started at 2025-05-12 14:24:44 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.033938 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:16:17.033968 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-1 before test
  I0512 15:16:17.045588 23 predicates.go:957] harbor-redis-0 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.045682 23 predicates.go:959] 	Container redis ready: true, restart count 0
  I0512 15:16:17.045711 23 predicates.go:957] kube-flannel-8bzqg from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.045727 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:16:17.045745 23 predicates.go:957] kube-proxy-g82r5 from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.045762 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:16:17.045792 23 predicates.go:957] metrics-server-c5b7b4dc-5596r from kube-system started at 2025-05-12 14:15:20 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.045808 23 predicates.go:959] 	Container metrics-server ready: true, restart count 0
  I0512 15:16:17.045828 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-1 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.045845 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:16:17.045862 23 predicates.go:957] nodelocaldns-v22ld from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.045887 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:16:17.045919 23 predicates.go:957] vsphere-csi-node-4z66x from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.045942 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:16:17.045961 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:16:17.045976 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:16:17.045994 23 predicates.go:957] sonobuoy from sonobuoy started at 2025-05-12 14:52:15 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.046009 23 predicates.go:959] 	Container kube-sonobuoy ready: true, restart count 0
  I0512 15:16:17.046027 23 predicates.go:957] sonobuoy-e2e-job-0883294174af4fef from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:16:17.046043 23 predicates.go:959] 	Container e2e ready: true, restart count 0
  I0512 15:16:17.046058 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:16:17.046076 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-b5spt from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:16:17.046091 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:16:17.046106 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:16:17.046123 23 predicates.go:957] vault-0 from vault started at 2025-05-12 14:24:45 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.046138 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:16:17.046157 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:16:17.046187 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:16:17.046218 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-2 before test
  I0512 15:16:17.056624 23 predicates.go:957] harbor-registry-5584d97487-9cst2 from harbor started at 2025-05-12 14:20:33 +0000 UTC (2 container statuses recorded)
  I0512 15:16:17.056692 23 predicates.go:959] 	Container registry ready: true, restart count 0
  I0512 15:16:17.056717 23 predicates.go:959] 	Container registryctl ready: true, restart count 0
  I0512 15:16:17.056751 23 predicates.go:957] kube-flannel-sfqr5 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.056768 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:16:17.056787 23 predicates.go:957] kube-proxy-95b2r from kube-system started at 2025-05-12 14:12:58 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.056804 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:16:17.056822 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-2 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.056838 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:16:17.056858 23 predicates.go:957] nodelocaldns-5lcd4 from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.056875 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:16:17.056893 23 predicates.go:957] vsphere-csi-node-sqm2v from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.056914 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:16:17.056931 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:16:17.056946 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:16:17.056964 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-qbt8k from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:16:17.056979 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:16:17.056994 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:16:17.057012 23 predicates.go:957] vault-2 from vault started at 2025-05-12 14:26:08 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.057027 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:16:17.057043 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:16:17.057058 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:16:17.057074 23 predicates.go:957] velero-5f4c979ccf-4c5mx from velero started at 2025-05-12 14:22:48 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.057089 23 predicates.go:959] 	Container velero ready: true, restart count 0
  I0512 15:16:17.057106 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-3 before test
  I0512 15:16:17.069167 23 predicates.go:957] harbor-jobservice-79c749f9bc-qm554 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.069409 23 predicates.go:959] 	Container jobservice ready: true, restart count 2
  I0512 15:16:17.069693 23 predicates.go:957] harbor-portal-757685fc6b-7smbd from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.070069 23 predicates.go:959] 	Container portal ready: true, restart count 0
  I0512 15:16:17.070561 23 predicates.go:957] kube-flannel-pglr4 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.070968 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:16:17.071305 23 predicates.go:957] kube-proxy-5nnvw from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.071529 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:16:17.071904 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-3 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.072090 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:16:17.072358 23 predicates.go:957] nodelocaldns-v58mx from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.072776 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:16:17.073152 23 predicates.go:957] vsphere-csi-node-blmnp from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.073481 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:16:17.073673 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:16:17.074053 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:16:17.074287 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-rbcss from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:16:17.074522 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:16:17.074714 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:16:17.075071 23 predicates.go:957] vault-3 from vault started at 2025-05-12 14:27:02 +0000 UTC (3 container statuses recorded)
  I0512 15:16:17.075297 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:16:17.075485 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:16:17.075706 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:16:17.075937 23 predicates.go:957] vault-operator-56c68d678-2gr95 from vault started at 2025-05-12 14:24:33 +0000 UTC (1 container statuses recorded)
  I0512 15:16:17.076185 23 predicates.go:959] 	Container vault-operator ready: true, restart count 0
  STEP: verifying the node has the label node opscontrol-jaku1-worker-0 @ 05/12/25 15:16:17.15
  STEP: verifying the node has the label node opscontrol-jaku1-worker-1 @ 05/12/25 15:16:17.188
  STEP: verifying the node has the label node opscontrol-jaku1-worker-2 @ 05/12/25 15:16:17.26
  STEP: verifying the node has the label node opscontrol-jaku1-worker-3 @ 05/12/25 15:16:17.325
  I0512 15:16:17.394351 23 predicates.go:372] Pod harbor-core-569d44cfd7-46qkt requesting resource cpu=0m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.395031 23 predicates.go:372] Pod harbor-database-0 requesting resource cpu=0m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.395331 23 predicates.go:372] Pod harbor-jobservice-79c749f9bc-qm554 requesting resource cpu=0m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.395457 23 predicates.go:372] Pod harbor-portal-757685fc6b-7smbd requesting resource cpu=0m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.395641 23 predicates.go:372] Pod harbor-redis-0 requesting resource cpu=0m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.395780 23 predicates.go:372] Pod harbor-registry-5584d97487-9cst2 requesting resource cpu=0m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.395904 23 predicates.go:372] Pod kube-flannel-8bzqg requesting resource cpu=150m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.396051 23 predicates.go:372] Pod kube-flannel-m6mdj requesting resource cpu=150m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.396073 23 predicates.go:372] Pod kube-flannel-pglr4 requesting resource cpu=150m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.396332 23 predicates.go:372] Pod kube-flannel-sfqr5 requesting resource cpu=150m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.396857 23 predicates.go:372] Pod kube-proxy-5nnvw requesting resource cpu=0m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.396884 23 predicates.go:372] Pod kube-proxy-95b2r requesting resource cpu=0m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.396907 23 predicates.go:372] Pod kube-proxy-g82r5 requesting resource cpu=0m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.396925 23 predicates.go:372] Pod kube-proxy-ksc6h requesting resource cpu=0m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.396945 23 predicates.go:372] Pod metrics-server-c5b7b4dc-5596r requesting resource cpu=100m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.396963 23 predicates.go:372] Pod nginx-proxy-opscontrol-jaku1-worker-0 requesting resource cpu=25m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.396981 23 predicates.go:372] Pod nginx-proxy-opscontrol-jaku1-worker-1 requesting resource cpu=25m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.396998 23 predicates.go:372] Pod nginx-proxy-opscontrol-jaku1-worker-2 requesting resource cpu=25m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.397016 23 predicates.go:372] Pod nginx-proxy-opscontrol-jaku1-worker-3 requesting resource cpu=25m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.397034 23 predicates.go:372] Pod nodelocaldns-5lcd4 requesting resource cpu=100m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.397062 23 predicates.go:372] Pod nodelocaldns-fp5rs requesting resource cpu=100m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.397083 23 predicates.go:372] Pod nodelocaldns-v22ld requesting resource cpu=100m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.397100 23 predicates.go:372] Pod nodelocaldns-v58mx requesting resource cpu=100m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.397122 23 predicates.go:372] Pod vsphere-csi-node-4z66x requesting resource cpu=0m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.397140 23 predicates.go:372] Pod vsphere-csi-node-blmnp requesting resource cpu=0m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.397159 23 predicates.go:372] Pod vsphere-csi-node-sqm2v requesting resource cpu=0m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.397176 23 predicates.go:372] Pod vsphere-csi-node-zmnvq requesting resource cpu=0m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.397195 23 predicates.go:372] Pod sonobuoy requesting resource cpu=0m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.397213 23 predicates.go:372] Pod sonobuoy-e2e-job-0883294174af4fef requesting resource cpu=0m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.397229 23 predicates.go:372] Pod sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-9znn4 requesting resource cpu=0m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.397246 23 predicates.go:372] Pod sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-b5spt requesting resource cpu=0m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.397264 23 predicates.go:372] Pod sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-qbt8k requesting resource cpu=0m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.397284 23 predicates.go:372] Pod sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-rbcss requesting resource cpu=0m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.397304 23 predicates.go:372] Pod vault-0 requesting resource cpu=2150m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.397336 23 predicates.go:372] Pod vault-1 requesting resource cpu=2150m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.397356 23 predicates.go:372] Pod vault-2 requesting resource cpu=2150m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.397374 23 predicates.go:372] Pod vault-3 requesting resource cpu=2150m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.397394 23 predicates.go:372] Pod vault-configurer-59545bd678-lmkzg requesting resource cpu=100m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.397412 23 predicates.go:372] Pod vault-operator-56c68d678-2gr95 requesting resource cpu=500m on Node opscontrol-jaku1-worker-3
  I0512 15:16:17.397433 23 predicates.go:372] Pod velero-5f4c979ccf-4c5mx requesting resource cpu=500m on Node opscontrol-jaku1-worker-2
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/12/25 15:16:17.397
  I0512 15:16:17.397540 23 predicates.go:382] Creating a pod which consumes cpu=612m on Node opscontrol-jaku1-worker-0
  I0512 15:16:17.439298 23 predicates.go:382] Creating a pod which consumes cpu=612m on Node opscontrol-jaku1-worker-1
  I0512 15:16:17.461214 23 predicates.go:382] Creating a pod which consumes cpu=332m on Node opscontrol-jaku1-worker-2
  I0512 15:16:17.475192 23 predicates.go:382] Creating a pod which consumes cpu=332m on Node opscontrol-jaku1-worker-3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/12/25 15:16:19.565
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2e625f8a-a656-491c-94d0-88171af9ff39.183ed0858ee64c47], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6793/filler-pod-2e625f8a-a656-491c-94d0-88171af9ff39 to opscontrol-jaku1-worker-2] @ 05/12/25 15:16:19.578
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2e625f8a-a656-491c-94d0-88171af9ff39.183ed085c0361a6a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.10" already present on machine] @ 05/12/25 15:16:19.578
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2e625f8a-a656-491c-94d0-88171af9ff39.183ed085c15ca4d9], Reason = [Created], Message = [Created container filler-pod-2e625f8a-a656-491c-94d0-88171af9ff39] @ 05/12/25 15:16:19.579
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2e625f8a-a656-491c-94d0-88171af9ff39.183ed085d2afd4b0], Reason = [Started], Message = [Started container filler-pod-2e625f8a-a656-491c-94d0-88171af9ff39] @ 05/12/25 15:16:19.579
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c5634d7c-f40d-4d08-9907-b8211a76777f.183ed0858f72571b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6793/filler-pod-c5634d7c-f40d-4d08-9907-b8211a76777f to opscontrol-jaku1-worker-3] @ 05/12/25 15:16:19.579
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c5634d7c-f40d-4d08-9907-b8211a76777f.183ed085c33bc8a0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.10" already present on machine] @ 05/12/25 15:16:19.579
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c5634d7c-f40d-4d08-9907-b8211a76777f.183ed085c45f2140], Reason = [Created], Message = [Created container filler-pod-c5634d7c-f40d-4d08-9907-b8211a76777f] @ 05/12/25 15:16:19.58
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c5634d7c-f40d-4d08-9907-b8211a76777f.183ed085d4684de5], Reason = [Started], Message = [Started container filler-pod-c5634d7c-f40d-4d08-9907-b8211a76777f] @ 05/12/25 15:16:19.58
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d489a163-dd02-4d2b-96fa-4e1fa9b91de0.183ed0858d21c532], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6793/filler-pod-d489a163-dd02-4d2b-96fa-4e1fa9b91de0 to opscontrol-jaku1-worker-1] @ 05/12/25 15:16:19.58
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d489a163-dd02-4d2b-96fa-4e1fa9b91de0.183ed085b981202b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.10" already present on machine] @ 05/12/25 15:16:19.581
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d489a163-dd02-4d2b-96fa-4e1fa9b91de0.183ed085bac495e8], Reason = [Created], Message = [Created container filler-pod-d489a163-dd02-4d2b-96fa-4e1fa9b91de0] @ 05/12/25 15:16:19.581
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d489a163-dd02-4d2b-96fa-4e1fa9b91de0.183ed085c9cc70dc], Reason = [Started], Message = [Started container filler-pod-d489a163-dd02-4d2b-96fa-4e1fa9b91de0] @ 05/12/25 15:16:19.581
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fb4dd9b4-4f4d-438a-9585-3f7c2058034a.183ed0858ab25f06], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6793/filler-pod-fb4dd9b4-4f4d-438a-9585-3f7c2058034a to opscontrol-jaku1-worker-0] @ 05/12/25 15:16:19.581
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fb4dd9b4-4f4d-438a-9585-3f7c2058034a.183ed085b5f16d6a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.10" already present on machine] @ 05/12/25 15:16:19.582
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fb4dd9b4-4f4d-438a-9585-3f7c2058034a.183ed085b78f891a], Reason = [Created], Message = [Created container filler-pod-fb4dd9b4-4f4d-438a-9585-3f7c2058034a] @ 05/12/25 15:16:19.582
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fb4dd9b4-4f4d-438a-9585-3f7c2058034a.183ed085c8ad6f6e], Reason = [Started], Message = [Started container filler-pod-fb4dd9b4-4f4d-438a-9585-3f7c2058034a] @ 05/12/25 15:16:19.582
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.183ed0860acfdf12], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 Insufficient cpu. preemption: 0/7 nodes are available: 3 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod.] @ 05/12/25 15:16:19.605
  STEP: removing the label node off the node opscontrol-jaku1-worker-2 @ 05/12/25 15:16:20.603
  STEP: verifying the node doesn't have the label node @ 05/12/25 15:16:20.64
  STEP: removing the label node off the node opscontrol-jaku1-worker-3 @ 05/12/25 15:16:20.653
  STEP: verifying the node doesn't have the label node @ 05/12/25 15:16:20.693
  STEP: removing the label node off the node opscontrol-jaku1-worker-0 @ 05/12/25 15:16:20.705
  STEP: verifying the node doesn't have the label node @ 05/12/25 15:16:20.751
  STEP: removing the label node off the node opscontrol-jaku1-worker-1 @ 05/12/25 15:16:20.768
  STEP: verifying the node doesn't have the label node @ 05/12/25 15:16:20.819
  I0512 15:16:20.841936 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6793" for this suite. @ 05/12/25 15:16:20.866
• [3.984 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:104
  STEP: Creating a kubernetes client @ 05/12/25 15:16:20.896
  I0512 15:16:20.896601 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 15:16:20.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:20.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:20.966
  STEP: Counting existing ResourceQuota @ 05/12/25 15:16:20.974
  STEP: Creating a ResourceQuota @ 05/12/25 15:16:25.994
  STEP: Ensuring resource quota status is calculated @ 05/12/25 15:16:26.009
  STEP: Creating a Service @ 05/12/25 15:16:28.017
  STEP: Creating a NodePort Service @ 05/12/25 15:16:28.05
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/12/25 15:16:28.12
  STEP: Ensuring resource quota status captures service creation @ 05/12/25 15:16:28.18
  STEP: Deleting Services @ 05/12/25 15:16:30.193
  STEP: Ensuring resource quota status released usage @ 05/12/25 15:16:30.324
  I0512 15:16:32.332066 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3821" for this suite. @ 05/12/25 15:16:32.344
• [11.470 seconds]
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/podtemplates.go:54
  STEP: Creating a kubernetes client @ 05/12/25 15:16:32.366
  I0512 15:16:32.366597 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename podtemplate @ 05/12/25 15:16:32.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:32.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:32.421
  I0512 15:16:32.498908 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8420" for this suite. @ 05/12/25 15:16:32.508
• [0.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:392
  STEP: Creating a kubernetes client @ 05/12/25 15:16:32.524
  I0512 15:16:32.525072 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 15:16:32.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:32.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:32.58
  STEP: set up a multi version CRD @ 05/12/25 15:16:32.586
  I0512 15:16:32.587691 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: rename a version @ 05/12/25 15:16:44.588
  STEP: check the new version name is served @ 05/12/25 15:16:44.624
  STEP: check the old version name is removed @ 05/12/25 15:16:47.126
  STEP: check the other version is not changed @ 05/12/25 15:16:48.215
  I0512 15:16:54.120990 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2392" for this suite. @ 05/12/25 15:16:54.146
• [21.635 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:100
  STEP: Creating a kubernetes client @ 05/12/25 15:16:54.161
  I0512 15:16:54.161361 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:16:54.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:54.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:54.229
  STEP: Creating configMap with name configmap-test-volume-map-63fca00c-7860-4bc1-a39f-1083012326c9 @ 05/12/25 15:16:54.237
  STEP: Creating a pod to test consume configMaps @ 05/12/25 15:16:54.257
  STEP: Saw pod success @ 05/12/25 15:16:58.31
  I0512 15:16:58.320481 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-0 pod pod-configmaps-de0804a7-067c-4420-bd57-b6672567cbe2 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:16:58.339
  I0512 15:16:58.375228 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3975" for this suite. @ 05/12/25 15:16:58.387
• [4.239 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:193
  STEP: Creating a kubernetes client @ 05/12/25 15:16:58.4
  I0512 15:16:58.400944 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 15:16:58.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:16:58.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:16:58.451
  STEP: Creating a test headless service @ 05/12/25 15:16:58.457
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5551 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5551;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5551 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5551;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5551.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5551.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5551.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5551.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5551.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5551.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5551.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5551.svc;check="$$(dig +notcp +noall +answer +search 147.52.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.52.147_udp@PTR;check="$$(dig +tcp +noall +answer +search 147.52.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.52.147_tcp@PTR;sleep 1; done
   @ 05/12/25 15:16:58.53
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5551 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5551;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5551 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5551;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5551.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5551.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5551.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5551.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5551.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5551.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5551.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5551.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5551.svc;check="$$(dig +notcp +noall +answer +search 147.52.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.52.147_udp@PTR;check="$$(dig +tcp +noall +answer +search 147.52.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.52.147_tcp@PTR;sleep 1; done
   @ 05/12/25 15:16:58.53
  STEP: creating a pod to probe DNS @ 05/12/25 15:16:58.53
  STEP: submitting the pod to kubernetes @ 05/12/25 15:16:58.531
  STEP: retrieving the pod @ 05/12/25 15:17:00.6
  STEP: looking for the results for each expected name from probers @ 05/12/25 15:17:00.612
  I0512 15:17:00.628335 23 dns_common.go:478] Unable to read wheezy_udp@dns-test-service from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.637607 23 dns_common.go:478] Unable to read wheezy_tcp@dns-test-service from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.647918 23 dns_common.go:478] Unable to read wheezy_udp@dns-test-service.dns-5551 from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.656602 23 dns_common.go:478] Unable to read wheezy_tcp@dns-test-service.dns-5551 from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.665461 23 dns_common.go:478] Unable to read wheezy_udp@dns-test-service.dns-5551.svc from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.673242 23 dns_common.go:478] Unable to read wheezy_tcp@dns-test-service.dns-5551.svc from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.738637 23 dns_common.go:478] Unable to read jessie_udp@dns-test-service from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.746544 23 dns_common.go:478] Unable to read jessie_tcp@dns-test-service from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.753660 23 dns_common.go:478] Unable to read jessie_udp@dns-test-service.dns-5551 from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.759704 23 dns_common.go:478] Unable to read jessie_tcp@dns-test-service.dns-5551 from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.767219 23 dns_common.go:478] Unable to read jessie_udp@dns-test-service.dns-5551.svc from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.775965 23 dns_common.go:478] Unable to read jessie_tcp@dns-test-service.dns-5551.svc from pod dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f: the server could not find the requested resource (get pods dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f)
  I0512 15:17:00.818052 23 dns_common.go:489] Lookups using dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5551 wheezy_tcp@dns-test-service.dns-5551 wheezy_udp@dns-test-service.dns-5551.svc wheezy_tcp@dns-test-service.dns-5551.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5551 jessie_tcp@dns-test-service.dns-5551 jessie_udp@dns-test-service.dns-5551.svc jessie_tcp@dns-test-service.dns-5551.svc]

  I0512 15:17:00.830642 23 dns_common.go:495] Pod client logs for webserver: 
  I0512 15:17:00.839571 23 dns_common.go:495] Pod client logs for querier: 
  I0512 15:17:00.853254 23 dns_common.go:495] Pod client logs for jessie-querier: 
  I0512 15:17:05.916814 23 dns_common.go:527] DNS probes using dns-5551/dns-test-5dcb01e5-ed8f-4ba8-b1e0-df114825836f succeeded

  STEP: deleting the pod @ 05/12/25 15:17:05.918
  STEP: deleting the test service @ 05/12/25 15:17:05.956
  STEP: deleting the test headless service @ 05/12/25 15:17:06.028
  I0512 15:17:06.065298 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "dns-5551" for this suite. @ 05/12/25 15:17:06.095
• [7.732 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/field_validation.go:472
  STEP: Creating a kubernetes client @ 05/12/25 15:17:06.132
  I0512 15:17:06.132999 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename field-validation @ 05/12/25 15:17:06.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:17:06.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:17:06.336
  I0512 15:17:06.345144 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  W0512 15:17:13.958584      23 warnings.go:70] unknown field "alpha"
  W0512 15:17:13.958670      23 warnings.go:70] unknown field "beta"
  W0512 15:17:13.958690      23 warnings.go:70] unknown field "delta"
  W0512 15:17:13.958707      23 warnings.go:70] unknown field "epsilon"
  W0512 15:17:13.958795      23 warnings.go:70] unknown field "gamma"
  I0512 15:17:14.569305 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7352" for this suite. @ 05/12/25 15:17:14.581
• [8.469 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/12/25 15:17:14.602
  I0512 15:17:14.602739 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:17:14.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:17:14.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:17:14.65
  STEP: creating the pod @ 05/12/25 15:17:14.657
  STEP: submitting the pod to kubernetes @ 05/12/25 15:17:14.657
  W0512 15:17:14.679678      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/12/25 15:17:16.707
  STEP: updating the pod @ 05/12/25 15:17:16.715
  I0512 15:17:17.245778 23 pod_client.go:173] Successfully updated pod "pod-update-activedeadlineseconds-1aa0f8b6-8f18-498e-8a83-50590301e7b3"
  I0512 15:17:21.273394 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1100" for this suite. @ 05/12/25 15:17:21.289
• [6.704 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance] [sig-node, Serial, Disruptive, Conformance]
k8s.io/kubernetes/test/e2e/node/taints.go:444
  STEP: Creating a kubernetes client @ 05/12/25 15:17:21.308
  I0512 15:17:21.308631 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/12/25 15:17:21.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:17:21.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:17:21.366
  I0512 15:17:21.376244 23 wait.go:50] Waiting up to 1m0s for all nodes to be ready
  I0512 15:18:21.377856 23 util.go:393] Waiting for terminating namespaces to be deleted...
  I0512 15:18:21.386738 23 taints.go:144] Starting informer...
  STEP: Starting pods... @ 05/12/25 15:18:21.387
  I0512 15:18:21.632716 23 taints.go:463] Pod1 is running on opscontrol-jaku1-worker-1. Tainting Node
  I0512 15:18:23.884953 23 taints.go:471] Pod2 is running on opscontrol-jaku1-worker-1. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/12/25 15:18:23.885
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/25 15:18:23.958
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/12/25 15:18:23.97
  I0512 15:18:30.359116 23 taints.go:492] Noticed Pod "taint-eviction-b1" gets evicted.
  I0512 15:18:49.769596 23 taints.go:492] Noticed Pod "taint-eviction-b2" gets evicted.
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/25 15:18:49.813
  I0512 15:18:49.823845 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-multiple-pods-3200" for this suite. @ 05/12/25 15:18:49.838
• [88.545 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/12/25 15:18:49.859
  I0512 15:18:49.859933 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:18:49.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:18:49.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:18:49.906
  STEP: creating the pod @ 05/12/25 15:18:49.912
  STEP: submitting the pod to kubernetes @ 05/12/25 15:18:49.912
  STEP: verifying QOS class is set on the pod @ 05/12/25 15:18:49.929
  I0512 15:18:49.937573 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-800" for this suite. @ 05/12/25 15:18:49.961
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/daemon_set.go:851
  STEP: Creating a kubernetes client @ 05/12/25 15:18:49.995
  I0512 15:18:49.995375 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename daemonsets @ 05/12/25 15:18:49.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:18:50.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:18:50.043
  STEP: Creating simple DaemonSet "daemon-set" @ 05/12/25 15:18:50.126
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/25 15:18:50.142
  I0512 15:18:50.173888 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:50.174241 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:50.174438 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:50.197768 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:18:50.197885 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 15:18:51.153304 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:51.153411 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:51.153567 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:51.160998 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:18:51.161256 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 15:18:52.155399 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:52.155744 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:52.156346 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:18:52.165699 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:18:52.166003 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/12/25 15:18:52.173
  STEP: DeleteCollection of the DaemonSets @ 05/12/25 15:18:52.181
  STEP: Verify that ReplicaSets have been deleted @ 05/12/25 15:18:52.196
  I0512 15:18:52.216457 23 daemon_set.go:135] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26163"},"items":null}

  I0512 15:18:52.262219 23 daemon_set.go:140] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26164"},"items":[{"metadata":{"name":"daemon-set-dn5xq","generateName":"daemon-set-","namespace":"daemonsets-5652","uid":"7f6a30c9-ece5-4d4c-8929-733b0eeb70ce","resourceVersion":"26157","creationTimestamp":"2025-05-12T15:18:50Z","labels":{"controller-revision-hash":"6fc6fb49db","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c1223726-078a-4de1-9c53-7500dbcc91dd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1223726-078a-4de1-9c53-7500dbcc91dd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PodReadyToStartContainers\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:hostIPs":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.70.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6l87s","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6l87s","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"opscontrol-jaku1-worker-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["opscontrol-jaku1-worker-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"}],"hostIP":"10.62.16.77","hostIPs":[{"ip":"10.62.16.77"}],"podIP":"10.233.70.21","podIPs":[{"ip":"10.233.70.21"}],"startTime":"2025-05-12T15:18:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2025-05-12T15:18:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6ddee373955d9f0793df90fbb253df44699c8acbcf0bafeb6b09c2c77ed87025","started":true,"volumeMounts":[{"name":"kube-api-access-6l87s","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"recursiveReadOnly":"Disabled"}]}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jqts8","generateName":"daemon-set-","namespace":"daemonsets-5652","uid":"9227b81c-b4e6-4b18-95c8-57bd592e1dbe","resourceVersion":"26164","creationTimestamp":"2025-05-12T15:18:50Z","deletionTimestamp":"2025-05-12T15:19:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6fc6fb49db","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c1223726-078a-4de1-9c53-7500dbcc91dd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1223726-078a-4de1-9c53-7500dbcc91dd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PodReadyToStartContainers\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:hostIPs":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-k227r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-k227r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"opscontrol-jaku1-worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["opscontrol-jaku1-worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"}],"hostIP":"10.62.16.76","hostIPs":[{"ip":"10.62.16.76"}],"podIP":"10.233.69.47","podIPs":[{"ip":"10.233.69.47"}],"startTime":"2025-05-12T15:18:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2025-05-12T15:18:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://45e28cf9895f512744c01bc9152fa9f0c63acd541d685adb842f8b2ad7358373","started":true,"volumeMounts":[{"name":"kube-api-access-k227r","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"recursiveReadOnly":"Disabled"}]}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mr8bx","generateName":"daemon-set-","namespace":"daemonsets-5652","uid":"a6154fc9-e768-470d-993d-9ebbab35e115","resourceVersion":"26159","creationTimestamp":"2025-05-12T15:18:50Z","labels":{"controller-revision-hash":"6fc6fb49db","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c1223726-078a-4de1-9c53-7500dbcc91dd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1223726-078a-4de1-9c53-7500dbcc91dd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PodReadyToStartContainers\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:hostIPs":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zt4h7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zt4h7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"opscontrol-jaku1-worker-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["opscontrol-jaku1-worker-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"}],"hostIP":"10.62.16.78","hostIPs":[{"ip":"10.62.16.78"}],"podIP":"10.233.67.16","podIPs":[{"ip":"10.233.67.16"}],"startTime":"2025-05-12T15:18:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2025-05-12T15:18:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d207bfc3d5ff35ddd56ba38bc475e7aaeb4fcf27051c149a51efc9c5d340e20c","started":true,"volumeMounts":[{"name":"kube-api-access-zt4h7","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"recursiveReadOnly":"Disabled"}]}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xbmrx","generateName":"daemon-set-","namespace":"daemonsets-5652","uid":"91ce0062-f9e6-4a90-a7b5-9caa1f1ab8b2","resourceVersion":"26150","creationTimestamp":"2025-05-12T15:18:50Z","labels":{"controller-revision-hash":"6fc6fb49db","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c1223726-078a-4de1-9c53-7500dbcc91dd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1223726-078a-4de1-9c53-7500dbcc91dd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2025-05-12T15:18:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PodReadyToStartContainers\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:hostIPs":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q2w4x","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q2w4x","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"opscontrol-jaku1-worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["opscontrol-jaku1-worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2025-05-12T15:18:50Z"}],"hostIP":"10.62.16.75","hostIPs":[{"ip":"10.62.16.75"}],"podIP":"10.233.68.69","podIPs":[{"ip":"10.233.68.69"}],"startTime":"2025-05-12T15:18:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2025-05-12T15:18:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://bb107168ca051b0a0e1460d213cef2ed321cbcbe9cc160220e51b31b5ee0109a","started":true,"volumeMounts":[{"name":"kube-api-access-q2w4x","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"recursiveReadOnly":"Disabled"}]}],"qosClass":"BestEffort"}}]}

  I0512 15:18:52.333352 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5652" for this suite. @ 05/12/25 15:18:52.384
• [2.415 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AggregatedDiscovery should support raw aggregated discovery request for CRDs [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/aggregated_discovery.go:194
  STEP: Creating a kubernetes client @ 05/12/25 15:18:52.411
  I0512 15:18:52.411778 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename aggregateddiscovery @ 05/12/25 15:18:52.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:18:52.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:18:52.466
  I0512 15:18:52.474040 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:19:00.597871 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregateddiscovery-6643" for this suite. @ 05/12/25 15:19:00.607
• [8.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/endpointslice.go:208
  STEP: Creating a kubernetes client @ 05/12/25 15:19:00.623
  I0512 15:19:00.623411 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename endpointslice @ 05/12/25 15:19:00.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:19:00.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:19:00.691
  STEP: referencing a single matching pod @ 05/12/25 15:19:06.858
  STEP: referencing matching pods with named port @ 05/12/25 15:19:06.876
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/12/25 15:19:06.894
  STEP: recreating EndpointSlices after they've been deleted @ 05/12/25 15:19:06.914
  I0512 15:19:07.012146 23 endpointslice.go:938] EndpointSlice for Service endpointslice-8650/example-named-port not found
  I0512 15:19:09.019214 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8650" for this suite. @ 05/12/25 15:19:09.031
• [8.422 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/preemption.go:626
  STEP: Creating a kubernetes client @ 05/12/25 15:19:09.046
  I0512 15:19:09.046647 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/25 15:19:09.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:19:09.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:19:09.126
  I0512 15:19:09.159854 23 wait.go:50] Waiting up to 1m0s for all nodes to be ready
  I0512 15:20:09.170029 23 util.go:393] Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/12/25 15:20:09.178
  I0512 15:20:09.178691 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/12/25 15:20:09.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:20:09.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:20:09.236
  STEP: Finding an available node @ 05/12/25 15:20:09.241
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/12/25 15:20:09.241
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/12/25 15:20:11.285
  I0512 15:20:11.316959 23 preemption.go:585] found a healthy node: opscontrol-jaku1-worker-1
  I0512 15:20:17.498057 23 preemption.go:708] pods created so far: [1 1 1]
  I0512 15:20:17.498151 23 preemption.go:709] length of pods created so far: 3
  I0512 15:20:19.527981 23 preemption.go:726] pods created so far: [2 2 1]
  I0512 15:20:26.721975 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2779" for this suite. @ 05/12/25 15:20:26.732
  I0512 15:20:26.745120 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7516" for this suite. @ 05/12/25 15:20:26.833
• [77.803 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/preemption.go:127
  STEP: Creating a kubernetes client @ 05/12/25 15:20:26.849
  I0512 15:20:26.849241 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/25 15:20:26.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:20:26.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:20:26.9
  I0512 15:20:26.942425 23 wait.go:50] Waiting up to 1m0s for all nodes to be ready
  I0512 15:21:26.954679 23 util.go:393] Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/12/25 15:21:26.964
  I0512 15:21:27.022990 23 preemption.go:175] Created pod: pod0-0-sched-preemption-low-priority
  I0512 15:21:27.035316 23 preemption.go:175] Created pod: pod0-1-sched-preemption-medium-priority
  I0512 15:21:27.106239 23 preemption.go:175] Created pod: pod1-0-sched-preemption-medium-priority
  I0512 15:21:27.143361 23 preemption.go:175] Created pod: pod1-1-sched-preemption-medium-priority
  I0512 15:21:27.211224 23 preemption.go:175] Created pod: pod2-0-sched-preemption-medium-priority
  I0512 15:21:27.232124 23 preemption.go:175] Created pod: pod2-1-sched-preemption-medium-priority
  I0512 15:21:27.312740 23 preemption.go:175] Created pod: pod3-0-sched-preemption-medium-priority
  I0512 15:21:27.359617 23 preemption.go:175] Created pod: pod3-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/12/25 15:21:27.359
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/12/25 15:21:29.459
  I0512 15:21:33.814929 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-5719" for this suite. @ 05/12/25 15:21:33.831
• [67.004 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:230
  STEP: Creating a kubernetes client @ 05/12/25 15:21:33.854
  I0512 15:21:33.854105 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:21:33.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:21:33.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:21:33.936
  STEP: Creating Pod @ 05/12/25 15:21:33.948
  STEP: Reading file content from the nginx-container @ 05/12/25 15:21:36.008
  I0512 15:21:36.008926 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6896 PodName:pod-sharedvolume-7fe17fd6-b7fb-4256-99dc-2c4b1862bd97 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:21:36.008990 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:21:36.012169 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:21:36.012587 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-6896/pods/pod-sharedvolume-7fe17fd6-b7fb-4256-99dc-2c4b1862bd97/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  I0512 15:21:36.141527 23 exec_util.go:111] Exec stderr: ""
  I0512 15:21:36.141917 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6896" for this suite. @ 05/12/25 15:21:36.152
• [2.312 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance] [sig-node, Slow, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:190
  STEP: Creating a kubernetes client @ 05/12/25 15:21:36.173
  I0512 15:21:36.173969 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 15:21:36.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:21:36.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:21:36.234
  I0512 15:21:38.302022 23 delete.go:62] Deleting pod "var-expansion-6aeb4653-e3a1-48ad-8471-36e94fc703b7" in namespace "var-expansion-6820"
  I0512 15:21:38.328488 23 delete.go:70] Wait up to 5m0s for pod "var-expansion-6aeb4653-e3a1-48ad-8471-36e94fc703b7" to be fully deleted
  I0512 15:21:40.360424 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6820" for this suite. @ 05/12/25 15:21:40.371
• [4.225 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/12/25 15:21:40.399
  I0512 15:21:40.399427 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:21:40.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:21:40.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:21:40.485
  STEP: creating pod @ 05/12/25 15:21:40.499
  I0512 15:21:42.565953 23 pods.go:83] Pod pod-hostip-bb050257-6c33-4d5c-81f6-ee29140090c9 has hostIP: 10.62.16.76
  I0512 15:21:42.566464 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8932" for this suite. @ 05/12/25 15:21:42.589
• [2.205 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/12/25 15:21:42.605
  I0512 15:21:42.605611 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:21:42.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:21:42.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:21:42.701
  I0512 15:21:42.714562 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: creating the pod @ 05/12/25 15:21:42.716
  STEP: submitting the pod to kubernetes @ 05/12/25 15:21:42.716
  I0512 15:21:44.977034 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9911" for this suite. @ 05/12/25 15:21:44.988
• [2.396 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/subpath.go:91
  STEP: Creating a kubernetes client @ 05/12/25 15:21:45.005
  I0512 15:21:45.005789 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename subpath @ 05/12/25 15:21:45.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:21:45.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:21:45.062
  STEP: Setting up data @ 05/12/25 15:21:45.071
  STEP: Creating pod pod-subpath-test-downwardapi-wff7 @ 05/12/25 15:21:45.106
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/25 15:21:45.107
  STEP: Saw pod success @ 05/12/25 15:22:09.279
  I0512 15:22:09.290259 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-subpath-test-downwardapi-wff7 container test-container-subpath-downwardapi-wff7: <nil>
  STEP: delete the pod @ 05/12/25 15:22:09.329
  STEP: Deleting pod pod-subpath-test-downwardapi-wff7 @ 05/12/25 15:22:09.377
  I0512 15:22:09.377934 23 delete.go:62] Deleting pod "pod-subpath-test-downwardapi-wff7" in namespace "subpath-3654"
  I0512 15:22:09.385875 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3654" for this suite. @ 05/12/25 15:22:09.394
• [24.403 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/job.go:907
  STEP: Creating a kubernetes client @ 05/12/25 15:22:09.413
  I0512 15:22:09.413574 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename job @ 05/12/25 15:22:09.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:09.465
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:09.473
  STEP: Creating a job @ 05/12/25 15:22:09.481
  STEP: Ensuring active pods == parallelism @ 05/12/25 15:22:09.5
  STEP: Orphaning one of the Job's Pods @ 05/12/25 15:22:11.512
  I0512 15:22:12.051715 23 pod_client.go:173] Successfully updated pod "adopt-release-dm74d"
  STEP: Checking that the Job readopts the Pod @ 05/12/25 15:22:12.051
  STEP: Removing the labels from the Job's Pod @ 05/12/25 15:22:14.074
  I0512 15:22:14.598758 23 pod_client.go:173] Successfully updated pod "adopt-release-dm74d"
  STEP: Checking that the Job releases the Pod @ 05/12/25 15:22:14.598
  I0512 15:22:16.619107 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5593" for this suite. @ 05/12/25 15:22:16.629
• [7.229 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:215
  STEP: Creating a kubernetes client @ 05/12/25 15:22:16.642
  I0512 15:22:16.642746 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/25 15:22:16.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:16.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:16.692
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/25 15:22:16.731
  STEP: create the pod with lifecycle hook @ 05/12/25 15:22:18.784
  STEP: delete the pod with lifecycle hook @ 05/12/25 15:22:20.866
  STEP: check prestop hook @ 05/12/25 15:22:22.908
  I0512 15:22:22.977730 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2899" for this suite. @ 05/12/25 15:22:22.998
• [6.373 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/12/25 15:22:23.016
  I0512 15:22:23.016771 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename containers @ 05/12/25 15:22:23.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:23.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:23.068
  STEP: Creating a pod to test override command @ 05/12/25 15:22:23.079
  STEP: Saw pod success @ 05/12/25 15:22:27.147
  I0512 15:22:27.154170 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod client-containers-5f4672b5-aafb-4420-b531-76941d1e8f35 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:22:27.179
  I0512 15:22:27.231280 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7122" for this suite. @ 05/12/25 15:22:27.241
• [4.241 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/12/25 15:22:27.257
  I0512 15:22:27.257182 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:22:27.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:27.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:27.314
  I0512 15:22:27.319553 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: creating the pod @ 05/12/25 15:22:27.321
  STEP: submitting the pod to kubernetes @ 05/12/25 15:22:27.321
  I0512 15:22:29.443013 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2299" for this suite. @ 05/12/25 15:22:29.458
• [2.213 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] StorageClasses CSI Conformance should run through the lifecycle of a StorageClass [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/storageclass.go:53
  STEP: Creating a kubernetes client @ 05/12/25 15:22:29.471
  I0512 15:22:29.471213 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename csi-storageclass @ 05/12/25 15:22:29.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:29.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:29.533
  STEP: Creating a StorageClass @ 05/12/25 15:22:29.541
  STEP: Get StorageClass "e2e-jdlr5" @ 05/12/25 15:22:29.551
  STEP: Patching the StorageClass "e2e-jdlr5" @ 05/12/25 15:22:29.559
  STEP: Delete StorageClass "e2e-jdlr5" @ 05/12/25 15:22:29.573
  STEP: Confirm deletion of StorageClass "e2e-jdlr5" @ 05/12/25 15:22:29.594
  STEP: Create a replacement StorageClass @ 05/12/25 15:22:29.603
  STEP: Updating StorageClass "e2e-v2-hzd7c" @ 05/12/25 15:22:29.615
  STEP: Listing all StorageClass with the labelSelector: "e2e-v2-hzd7c=updated" @ 05/12/25 15:22:29.635
  STEP: Deleting StorageClass "e2e-v2-hzd7c" via DeleteCollection @ 05/12/25 15:22:29.641
  STEP: Confirm deletion of StorageClass "e2e-v2-hzd7c" @ 05/12/25 15:22:29.663
  I0512 15:22:29.674132 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csi-storageclass-2498" for this suite. @ 05/12/25 15:22:29.693
• [0.248 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/endpointslice.go:69
  STEP: Creating a kubernetes client @ 05/12/25 15:22:29.719
  I0512 15:22:29.719964 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename endpointslice @ 05/12/25 15:22:29.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:29.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:29.772
  I0512 15:22:29.803763 23 endpointslice.go:1045] Endpoints addresses: [10.62.16.69 10.62.16.70 10.62.16.71] , ports: [6443]
  I0512 15:22:29.804046 23 endpointslice.go:1075] EndpointSlices addresses: [10.62.16.69 10.62.16.70 10.62.16.71] , ports: [6443]
  I0512 15:22:29.804343 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3789" for this suite. @ 05/12/25 15:22:29.813
• [0.126 seconds]
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/service_accounts.go:810
  STEP: Creating a kubernetes client @ 05/12/25 15:22:29.846
  I0512 15:22:29.846809 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/25 15:22:29.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:29.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:29.901
  STEP: Creating ServiceAccount "e2e-sa-7jnt8"  @ 05/12/25 15:22:29.908
  I0512 15:22:29.921281 23 service_accounts.go:825] AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-7jnt8"  @ 05/12/25 15:22:29.921
  I0512 15:22:29.943311 23 service_accounts.go:839] AutomountServiceAccountToken: true
  I0512 15:22:29.943613 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-965" for this suite. @ 05/12/25 15:22:29.955
• [0.133 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:246
  STEP: Creating a kubernetes client @ 05/12/25 15:22:29.98
  I0512 15:22:29.980445 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 15:22:29.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:30.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:30.036
  STEP: Creating a test headless service @ 05/12/25 15:22:30.043
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8305.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8305.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/12/25 15:22:30.06
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8305.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8305.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/12/25 15:22:30.06
  STEP: creating a pod to probe DNS @ 05/12/25 15:22:30.06
  STEP: submitting the pod to kubernetes @ 05/12/25 15:22:30.06
  STEP: retrieving the pod @ 05/12/25 15:22:32.118
  STEP: looking for the results for each expected name from probers @ 05/12/25 15:22:32.127
  I0512 15:22:32.178248 23 dns_common.go:527] DNS probes using dns-8305/dns-test-d0b663d9-cb8a-4d0f-a0a3-37081d769073 succeeded

  STEP: deleting the pod @ 05/12/25 15:22:32.178
  STEP: deleting the test headless service @ 05/12/25 15:22:32.232
  I0512 15:22:32.269549 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "dns-8305" for this suite. @ 05/12/25 15:22:32.281
• [2.316 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/rc.go:95
  STEP: Creating a kubernetes client @ 05/12/25 15:22:32.297
  I0512 15:22:32.297270 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replication-controller @ 05/12/25 15:22:32.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:32.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:32.374
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/12/25 15:22:32.389
  STEP: When a replication controller with a matching selector is created @ 05/12/25 15:22:34.448
  STEP: Then the orphan pod is adopted @ 05/12/25 15:22:34.464
  I0512 15:22:35.485500 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7122" for this suite. @ 05/12/25 15:22:35.501
• [3.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] API priority and fairness should support PriorityLevelConfiguration API operations [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/flowcontrol.go:514
  STEP: Creating a kubernetes client @ 05/12/25 15:22:35.531
  I0512 15:22:35.531196 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename apf @ 05/12/25 15:22:35.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:35.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:35.585
  STEP: getting /apis @ 05/12/25 15:22:35.594
  STEP: getting /apis/flowcontrol.apiserver.k8s.io @ 05/12/25 15:22:35.606
  STEP: getting /apis/flowcontrol.apiserver.k8s.io/v1 @ 05/12/25 15:22:35.611
  STEP: creating @ 05/12/25 15:22:35.617
  STEP: getting @ 05/12/25 15:22:35.67
  STEP: listing @ 05/12/25 15:22:35.682
  STEP: watching @ 05/12/25 15:22:35.69
  I0512 15:22:35.691365 23 flowcontrol.go:620] starting watch
  STEP: patching @ 05/12/25 15:22:35.694
  STEP: updating @ 05/12/25 15:22:35.718
  I0512 15:22:35.769635 23 flowcontrol.go:648] waiting for watch events with expected annotations
  STEP: getting /status @ 05/12/25 15:22:35.771
  STEP: patching /status @ 05/12/25 15:22:35.796
  STEP: updating /status @ 05/12/25 15:22:35.845
  STEP: deleting @ 05/12/25 15:22:35.87
  STEP: deleting a collection @ 05/12/25 15:22:35.906
  I0512 15:22:36.003877 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "apf-6661" for this suite. @ 05/12/25 15:22:36.022
• [0.517 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1858
  STEP: Creating a kubernetes client @ 05/12/25 15:22:36.049
  I0512 15:22:36.049170 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 15:22:36.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:36.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:36.133
  STEP: Starting the proxy @ 05/12/25 15:22:36.141
  I0512 15:22:36.142088 23 util.go:585] Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5100 proxy --unix-socket=/tmp/kubectl-proxy-unix2034001721/test'
  STEP: retrieving proxy /api/ output @ 05/12/25 15:22:36.267
  I0512 15:22:36.269096 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5100" for this suite. @ 05/12/25 15:22:36.29
• [0.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance] [sig-node, Slow, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:229
  STEP: Creating a kubernetes client @ 05/12/25 15:22:36.308
  I0512 15:22:36.308741 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 15:22:36.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:22:36.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:22:36.389
  STEP: creating the pod with failed condition @ 05/12/25 15:22:36.394
  STEP: updating the pod @ 05/12/25 15:24:36.412
  I0512 15:24:36.943844 23 pod_client.go:173] Successfully updated pod "var-expansion-84f521b6-fc6b-4af1-b432-71eba21ea328"
  STEP: waiting for pod running @ 05/12/25 15:24:36.944
  STEP: deleting the pod gracefully @ 05/12/25 15:24:38.982
  I0512 15:24:38.982795 23 delete.go:62] Deleting pod "var-expansion-84f521b6-fc6b-4af1-b432-71eba21ea328" in namespace "var-expansion-658"
  I0512 15:24:39.003972 23 delete.go:70] Wait up to 5m0s for pod "var-expansion-84f521b6-fc6b-4af1-b432-71eba21ea328" to be fully deleted
  I0512 15:25:11.163943 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-658" for this suite. @ 05/12/25 15:25:11.175
• [154.881 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/secrets.go:156
  STEP: Creating a kubernetes client @ 05/12/25 15:25:11.19
  I0512 15:25:11.190701 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:25:11.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:25:11.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:25:11.242
  STEP: creating a secret @ 05/12/25 15:25:11.249
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/12/25 15:25:11.261
  STEP: patching the secret @ 05/12/25 15:25:11.268
  STEP: deleting the secret using a LabelSelector @ 05/12/25 15:25:11.294
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/12/25 15:25:11.317
  I0512 15:25:11.326362 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4048" for this suite. @ 05/12/25 15:25:11.339
• [0.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:505
  STEP: Creating a kubernetes client @ 05/12/25 15:25:11.359
  I0512 15:25:11.359233 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:25:11.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:25:11.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:25:11.409
  I0512 15:25:11.492356 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1502" for this suite. @ 05/12/25 15:25:11.501
• [0.155 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/12/25 15:25:11.514
  I0512 15:25:11.514697 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename proxy @ 05/12/25 15:25:11.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:25:11.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:25:11.563
  I0512 15:25:11.570772 23 proxy.go:293] Creating pod...
  I0512 15:25:13.611832 23 proxy.go:317] Creating service...
  I0512 15:25:13.644227 23 proxy.go:354] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/pods/agnhost/proxy/some/path/with/DELETE
  I0512 15:25:13.662881 23 proxy.go:530] http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  I0512 15:25:13.663343 23 proxy.go:354] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/pods/agnhost/proxy/some/path/with/GET
  I0512 15:25:13.676663 23 proxy.go:530] http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  I0512 15:25:13.677080 23 proxy.go:354] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/pods/agnhost/proxy/some/path/with/HEAD
  I0512 15:25:13.691651 23 proxy.go:517] http.Client request:HEAD | StatusCode:200
  I0512 15:25:13.692051 23 proxy.go:354] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/pods/agnhost/proxy/some/path/with/OPTIONS
  I0512 15:25:13.707460 23 proxy.go:530] http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  I0512 15:25:13.707851 23 proxy.go:354] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/pods/agnhost/proxy/some/path/with/PATCH
  I0512 15:25:13.719303 23 proxy.go:530] http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  I0512 15:25:13.719795 23 proxy.go:354] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/pods/agnhost/proxy/some/path/with/POST
  I0512 15:25:13.734768 23 proxy.go:530] http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  I0512 15:25:13.735355 23 proxy.go:354] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/pods/agnhost/proxy/some/path/with/PUT
  I0512 15:25:13.749175 23 proxy.go:530] http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  I0512 15:25:13.749782 23 proxy.go:365] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/services/test-service/proxy/some/path/with/DELETE
  I0512 15:25:13.768816 23 proxy.go:530] http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  I0512 15:25:13.769397 23 proxy.go:365] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/services/test-service/proxy/some/path/with/GET
  I0512 15:25:13.788213 23 proxy.go:530] http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  I0512 15:25:13.788872 23 proxy.go:365] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/services/test-service/proxy/some/path/with/HEAD
  I0512 15:25:13.813846 23 proxy.go:517] http.Client request:HEAD | StatusCode:200
  I0512 15:25:13.814342 23 proxy.go:365] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/services/test-service/proxy/some/path/with/OPTIONS
  I0512 15:25:13.845407 23 proxy.go:530] http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  I0512 15:25:13.846045 23 proxy.go:365] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/services/test-service/proxy/some/path/with/PATCH
  I0512 15:25:13.872453 23 proxy.go:530] http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  I0512 15:25:13.872872 23 proxy.go:365] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/services/test-service/proxy/some/path/with/POST
  I0512 15:25:13.891033 23 proxy.go:530] http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  I0512 15:25:13.891169 23 proxy.go:365] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5255/services/test-service/proxy/some/path/with/PUT
  I0512 15:25:13.908983 23 proxy.go:530] http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  I0512 15:25:13.909732 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-5255" for this suite. @ 05/12/25 15:25:13.929
• [2.446 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/replica_set.go:166
  STEP: Creating a kubernetes client @ 05/12/25 15:25:13.963
  I0512 15:25:13.963104 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replicaset @ 05/12/25 15:25:13.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:25:14.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:25:14.052
  STEP: Create a ReplicaSet @ 05/12/25 15:25:14.068
  STEP: Verify that the required pods have come up @ 05/12/25 15:25:14.1
  I0512 15:25:14.174915 23 resource.go:87] Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/12/25 15:25:14.175
  I0512 15:25:16.272003 23 replica_set.go:583] Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/12/25 15:25:16.272
  STEP: DeleteCollection of the ReplicaSets @ 05/12/25 15:25:16.287
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/12/25 15:25:16.311
  I0512 15:25:16.328286 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9856" for this suite. @ 05/12/25 15:25:16.344
• [2.399 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:247
  STEP: Creating a kubernetes client @ 05/12/25 15:25:16.362
  I0512 15:25:16.362884 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:25:16.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:25:16.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:25:16.464
  STEP: Setting up server cert @ 05/12/25 15:25:16.558
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:25:18.001
  STEP: Deploying the webhook pod @ 05/12/25 15:25:18.019
  STEP: Wait for the deployment to be ready @ 05/12/25 15:25:18.047
  I0512 15:25:18.070763 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:25:20.093
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:25:20.114
  I0512 15:25:21.114931 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/12/25 15:25:21.136
  STEP: create a configmap that should be updated by the webhook @ 05/12/25 15:25:21.162
  I0512 15:25:21.330981 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9823" for this suite. @ 05/12/25 15:25:21.348
  STEP: Destroying namespace "webhook-markers-9231" for this suite. @ 05/12/25 15:25:21.37
• [5.026 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:108
  STEP: Creating a kubernetes client @ 05/12/25 15:25:21.393
  I0512 15:25:21.394492 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 15:25:21.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:25:21.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:25:21.454
  I0512 15:26:21.504292 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6238" for this suite. @ 05/12/25 15:26:21.515
• [60.141 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/disruption.go:144
  STEP: Creating a kubernetes client @ 05/12/25 15:26:21.536
  I0512 15:26:21.536101 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename disruption @ 05/12/25 15:26:21.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:21.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:21.643
  STEP: Waiting for the pdb to be processed @ 05/12/25 15:26:21.665
  STEP: Waiting for all pods to be running @ 05/12/25 15:26:23.747
  I0512 15:26:23.776806 23 disruption.go:691] running pods: 0 < 3
  I0512 15:26:25.762141 23 disruption.go:691] running pods: 1 < 3
  I0512 15:26:27.764472 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7712" for this suite. @ 05/12/25 15:26:27.776
• [6.256 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:48
  STEP: Creating a kubernetes client @ 05/12/25 15:26:27.792
  I0512 15:26:27.792751 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:26:27.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:27.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:27.866
  STEP: Creating configMap with name projected-configmap-test-volume-27d0c1b5-0d7b-4573-8959-2b555c9fca78 @ 05/12/25 15:26:27.877
  STEP: Creating a pod to test consume configMaps @ 05/12/25 15:26:27.889
  STEP: Saw pod success @ 05/12/25 15:26:31.964
  I0512 15:26:31.972064 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-configmaps-2b047081-bddc-4bbc-bfa1-5e324093a3ab container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:26:32.005
  I0512 15:26:32.045183 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-529" for this suite. @ 05/12/25 15:26:32.059
• [4.294 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:235
  STEP: Creating a kubernetes client @ 05/12/25 15:26:32.093
  I0512 15:26:32.094139 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:26:32.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:32.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:32.151
  STEP: Setting up server cert @ 05/12/25 15:26:32.247
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:26:33.678
  STEP: Deploying the webhook pod @ 05/12/25 15:26:33.704
  STEP: Wait for the deployment to be ready @ 05/12/25 15:26:33.747
  I0512 15:26:33.778788 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:26:35.812
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:26:35.849
  I0512 15:26:36.849756 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/12/25 15:26:36.867
  STEP: create a namespace for the webhook @ 05/12/25 15:26:36.897
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/12/25 15:26:36.929
  I0512 15:26:37.165795 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-847" for this suite. @ 05/12/25 15:26:37.182
  STEP: Destroying namespace "webhook-markers-644" for this suite. @ 05/12/25 15:26:37.239
  STEP: Destroying namespace "fail-closed-namespace-5076" for this suite. @ 05/12/25 15:26:37.257
• [5.189 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/secrets.go:142
  STEP: Creating a kubernetes client @ 05/12/25 15:26:37.287
  I0512 15:26:37.287371 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:26:37.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:37.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:37.369
  STEP: Creating projection with secret that has name secret-emptykey-test-c04688e9-4442-4e02-b095-db533d4abfef @ 05/12/25 15:26:37.381
  I0512 15:26:37.388275 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9582" for this suite. @ 05/12/25 15:26:37.403
• [0.137 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:2157
  STEP: Creating a kubernetes client @ 05/12/25 15:26:37.426
  I0512 15:26:37.426506 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:26:37.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:37.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:37.486
  STEP: creating service in namespace services-7512 @ 05/12/25 15:26:37.496
  STEP: creating service affinity-clusterip in namespace services-7512 @ 05/12/25 15:26:37.497
  STEP: creating replication controller affinity-clusterip in namespace services-7512 @ 05/12/25 15:26:37.52
  I0512 15:26:37.537017      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7512, replica count: 3
  I0512 15:26:40.589483      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 15:26:40.605856 23 resource.go:361] Creating new exec pod
  I0512 15:26:43.662215 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7512 exec execpod-affinityxld8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  I0512 15:26:44.074672 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  I0512 15:26:44.074793 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:26:44.075127 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7512 exec execpod-affinityxld8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.99 80'
  I0512 15:26:44.424961 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.99 80\nConnection to 10.233.23.99 80 port [tcp/http] succeeded!\n"
  I0512 15:26:44.425057 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:26:44.425507 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7512 exec execpod-affinityxld8b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.23.99:80/ ; done'
  I0512 15:26:44.919961 23 builder.go:146] stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.99:80/\n"
  I0512 15:26:44.920070 23 builder.go:147] stdout: "\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz\naffinity-clusterip-h5plz"
  I0512 15:26:44.920104 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920133 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920187 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920221 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920254 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920276 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920295 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920314 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920913 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.920949 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.921022 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.921041 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.921108 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.921130 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.921221 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.921259 23 service.go:242] Received response from host: affinity-clusterip-h5plz
  I0512 15:26:44.921433 23 service.go:4061] Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-7512, will wait for the garbage collector to delete the pods @ 05/12/25 15:26:44.96
  I0512 15:26:45.050837 23 resources.go:139] Deleting ReplicationController affinity-clusterip took: 16.620434ms
  I0512 15:26:45.152009 23 resources.go:163] Terminating ReplicationController affinity-clusterip pods took: 101.165242ms
  I0512 15:26:48.441812 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7512" for this suite. @ 05/12/25 15:26:48.457
• [11.056 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/12/25 15:26:48.489
  I0512 15:26:48.490066 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:26:48.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:48.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:48.568
  STEP: Creating secret with name projected-secret-test-38bb60cd-a833-49ce-a18c-94bba00c9f01 @ 05/12/25 15:26:48.577
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:26:48.59
  STEP: Saw pod success @ 05/12/25 15:26:52.647
  I0512 15:26:52.656285 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-secrets-efe94d97-726b-42eb-af47-8aad163787b8 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:26:52.677
  I0512 15:26:52.742841 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2491" for this suite. @ 05/12/25 15:26:52.757
• [4.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:90
  STEP: Creating a kubernetes client @ 05/12/25 15:26:52.771
  I0512 15:26:52.771686 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:26:52.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:52.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:52.827
  STEP: Creating configMap with name configmap-test-volume-map-5f10e725-f472-401e-99b5-702291385732 @ 05/12/25 15:26:52.838
  STEP: Creating a pod to test consume configMaps @ 05/12/25 15:26:52.853
  STEP: Saw pod success @ 05/12/25 15:26:56.907
  I0512 15:26:56.917034 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-configmaps-244509fc-b2da-495c-a638-6f25140794e1 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:26:56.932
  I0512 15:26:56.986647 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6247" for this suite. @ 05/12/25 15:26:57.003
• [4.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates pod disruption condition is added to the preempted pod [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/preemption.go:329
  STEP: Creating a kubernetes client @ 05/12/25 15:26:57.022
  I0512 15:26:57.023052 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/25 15:26:57.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:26:57.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:26:57.071
  I0512 15:26:57.108505 23 wait.go:50] Waiting up to 1m0s for all nodes to be ready
  I0512 15:27:57.121612 23 util.go:393] Waiting for terminating namespaces to be deleted...
  STEP: Select a node to run the lower and higher priority pods @ 05/12/25 15:27:57.13
  STEP: Create a low priority pod that consumes 1/1 of node resources @ 05/12/25 15:27:57.166
  I0512 15:27:57.212036 23 preemption.go:367] Created pod: victim-pod
  STEP: Wait for the victim pod to be scheduled @ 05/12/25 15:27:57.212
  STEP: Create a high priority pod to trigger preemption of the lower priority pod @ 05/12/25 15:27:59.23
  I0512 15:27:59.242019 23 preemption.go:385] Created pod: preemptor-pod
  STEP: Waiting for the victim pod to be terminating @ 05/12/25 15:27:59.242
  STEP: Verifying the pod has the pod disruption condition @ 05/12/25 15:28:01.264
  I0512 15:28:01.277202 23 pod_client.go:378] Removing pod's "victim-pod" finalizer: "example.com/test-finalizer"
  I0512 15:28:01.830921 23 pod_client.go:173] Successfully updated pod "victim-pod"
  I0512 15:28:02.027007 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1368" for this suite. @ 05/12/25 15:28:02.043
• [65.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/server_version.go:41
  STEP: Creating a kubernetes client @ 05/12/25 15:28:02.059
  I0512 15:28:02.059215 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename server-version @ 05/12/25 15:28:02.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:28:02.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:28:02.118
  STEP: Request ServerVersion @ 05/12/25 15:28:02.125
  STEP: Confirm major version @ 05/12/25 15:28:02.127
  I0512 15:28:02.127674 23 server_version.go:52] Major version: 1
  STEP: Confirm minor version @ 05/12/25 15:28:02.127
  I0512 15:28:02.127827 23 server_version.go:58] cleanMinorVersion: 31
  I0512 15:28:02.127875 23 server_version.go:62] Minor version: 31
  I0512 15:28:02.128101 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-3443" for this suite. @ 05/12/25 15:28:02.14
• [0.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/subpath.go:59
  STEP: Creating a kubernetes client @ 05/12/25 15:28:02.154
  I0512 15:28:02.154269 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename subpath @ 05/12/25 15:28:02.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:28:02.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:28:02.204
  STEP: Setting up data @ 05/12/25 15:28:02.21
  STEP: Creating pod pod-subpath-test-secret-ftll @ 05/12/25 15:28:02.231
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/25 15:28:02.231
  STEP: Saw pod success @ 05/12/25 15:28:26.385
  I0512 15:28:26.393759 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-subpath-test-secret-ftll container test-container-subpath-secret-ftll: <nil>
  STEP: delete the pod @ 05/12/25 15:28:26.408
  STEP: Deleting pod pod-subpath-test-secret-ftll @ 05/12/25 15:28:26.452
  I0512 15:28:26.452772 23 delete.go:62] Deleting pod "pod-subpath-test-secret-ftll" in namespace "subpath-2397"
  I0512 15:28:26.461099 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2397" for this suite. @ 05/12/25 15:28:26.475
• [24.340 seconds]
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/sysctl.go:125
  STEP: Creating a kubernetes client @ 05/12/25 15:28:26.494
  I0512 15:28:26.494760 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sysctl @ 05/12/25 15:28:26.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:28:26.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:28:26.55
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/12/25 15:28:26.556
  I0512 15:28:26.569571 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7624" for this suite. @ 05/12/25 15:28:26.579
• [0.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:220
  STEP: Creating a kubernetes client @ 05/12/25 15:28:26.594
  I0512 15:28:26.594428 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:28:26.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:28:26.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:28:26.648
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/12/25 15:28:26.656
  STEP: Saw pod success @ 05/12/25 15:28:30.712
  I0512 15:28:30.719284 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-7b003ad4-9de9-491a-94f4-bf7593023bf8 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:28:30.736
  I0512 15:28:30.772958 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1152" for this suite. @ 05/12/25 15:28:30.784
• [4.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/chunking.go:83
  STEP: Creating a kubernetes client @ 05/12/25 15:28:30.8
  I0512 15:28:30.800450 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename chunking @ 05/12/25 15:28:30.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:28:30.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:28:30.853
  STEP: creating a large number of resources @ 05/12/25 15:28:30.859
  STEP: retrieving those results in paged fashion several times @ 05/12/25 15:28:48.516
  I0512 15:28:48.565182 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMTZcdTAwMDAifQ
  I0512 15:28:48.614446 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDAzM1x1MDAwMCJ9
  I0512 15:28:48.668283 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDA1MFx1MDAwMCJ9
  I0512 15:28:48.718914 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDA2N1x1MDAwMCJ9
  I0512 15:28:48.767024 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDA4NFx1MDAwMCJ9
  I0512 15:28:48.816134 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDEwMVx1MDAwMCJ9
  I0512 15:28:48.865306 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDExOFx1MDAwMCJ9
  I0512 15:28:48.913840 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDEzNVx1MDAwMCJ9
  I0512 15:28:48.965609 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDE1Mlx1MDAwMCJ9
  I0512 15:28:49.013755 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDE2OVx1MDAwMCJ9
  I0512 15:28:49.066066 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDE4Nlx1MDAwMCJ9
  I0512 15:28:49.119154 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDIwM1x1MDAwMCJ9
  I0512 15:28:49.169067 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDIyMFx1MDAwMCJ9
  I0512 15:28:49.216818 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDIzN1x1MDAwMCJ9
  I0512 15:28:49.265911 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDI1NFx1MDAwMCJ9
  I0512 15:28:49.315090 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDI3MVx1MDAwMCJ9
  I0512 15:28:49.367604 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDI4OFx1MDAwMCJ9
  I0512 15:28:49.417140 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDMwNVx1MDAwMCJ9
  I0512 15:28:49.466991 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDMyMlx1MDAwMCJ9
  I0512 15:28:49.514545 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDMzOVx1MDAwMCJ9
  I0512 15:28:49.563912 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDM1Nlx1MDAwMCJ9
  I0512 15:28:49.614422 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDM3M1x1MDAwMCJ9
  I0512 15:28:49.666652 23 chunking.go:98] Retrieved 17/17 results with rv 30837 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4MzcsInN0YXJ0IjoidGVtcGxhdGUtMDM5MFx1MDAwMCJ9
  I0512 15:28:49.724492 23 chunking.go:98] Retrieved 9/17 results with rv 30837 and continue 
  I0512 15:28:49.767132 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMTZcdTAwMDAifQ
  I0512 15:28:49.818321 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDAzM1x1MDAwMCJ9
  I0512 15:28:49.874522 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDA1MFx1MDAwMCJ9
  I0512 15:28:49.917544 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDA2N1x1MDAwMCJ9
  I0512 15:28:49.970276 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDA4NFx1MDAwMCJ9
  I0512 15:28:50.025952 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDEwMVx1MDAwMCJ9
  I0512 15:28:50.067835 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDExOFx1MDAwMCJ9
  I0512 15:28:50.118887 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDEzNVx1MDAwMCJ9
  I0512 15:28:50.168306 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDE1Mlx1MDAwMCJ9
  I0512 15:28:50.216457 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDE2OVx1MDAwMCJ9
  I0512 15:28:50.265549 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDE4Nlx1MDAwMCJ9
  I0512 15:28:50.313834 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDIwM1x1MDAwMCJ9
  I0512 15:28:50.363142 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDIyMFx1MDAwMCJ9
  I0512 15:28:50.411669 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDIzN1x1MDAwMCJ9
  I0512 15:28:50.463390 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDI1NFx1MDAwMCJ9
  I0512 15:28:50.513033 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDI3MVx1MDAwMCJ9
  I0512 15:28:50.561452 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDI4OFx1MDAwMCJ9
  I0512 15:28:50.612719 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDMwNVx1MDAwMCJ9
  I0512 15:28:50.663206 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDMyMlx1MDAwMCJ9
  I0512 15:28:50.713665 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDMzOVx1MDAwMCJ9
  I0512 15:28:50.780183 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDM1Nlx1MDAwMCJ9
  I0512 15:28:50.815571 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDM3M1x1MDAwMCJ9
  I0512 15:28:50.864293 23 chunking.go:98] Retrieved 17/17 results with rv 30844 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDQsInN0YXJ0IjoidGVtcGxhdGUtMDM5MFx1MDAwMCJ9
  I0512 15:28:50.913191 23 chunking.go:98] Retrieved 9/17 results with rv 30844 and continue 
  I0512 15:28:50.970463 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoiL3RlbXBsYXRlLTAwMTZcdTAwMDAifQ
  I0512 15:28:51.013813 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDAzM1x1MDAwMCJ9
  I0512 15:28:51.064170 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDA1MFx1MDAwMCJ9
  I0512 15:28:51.114771 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDA2N1x1MDAwMCJ9
  I0512 15:28:51.164481 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDA4NFx1MDAwMCJ9
  I0512 15:28:51.216430 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDEwMVx1MDAwMCJ9
  I0512 15:28:51.268075 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDExOFx1MDAwMCJ9
  I0512 15:28:51.314665 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDEzNVx1MDAwMCJ9
  I0512 15:28:51.366149 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDE1Mlx1MDAwMCJ9
  I0512 15:28:51.412674 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDE2OVx1MDAwMCJ9
  I0512 15:28:51.463419 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDE4Nlx1MDAwMCJ9
  I0512 15:28:51.514545 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDIwM1x1MDAwMCJ9
  I0512 15:28:51.564029 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDIyMFx1MDAwMCJ9
  I0512 15:28:51.616605 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDIzN1x1MDAwMCJ9
  I0512 15:28:51.664211 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDI1NFx1MDAwMCJ9
  I0512 15:28:51.714234 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDI3MVx1MDAwMCJ9
  I0512 15:28:51.767110 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDI4OFx1MDAwMCJ9
  I0512 15:28:51.814890 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDMwNVx1MDAwMCJ9
  I0512 15:28:51.867256 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDMyMlx1MDAwMCJ9
  I0512 15:28:51.916990 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDMzOVx1MDAwMCJ9
  I0512 15:28:51.965185 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDM1Nlx1MDAwMCJ9
  I0512 15:28:52.012974 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDM3M1x1MDAwMCJ9
  I0512 15:28:52.064443 23 chunking.go:98] Retrieved 17/17 results with rv 30848 and continue eyJ2IjoibWV0YS5rOHMuaW8vdjEiLCJydiI6MzA4NDgsInN0YXJ0IjoidGVtcGxhdGUtMDM5MFx1MDAwMCJ9
  I0512 15:28:52.114213 23 chunking.go:98] Retrieved 9/17 results with rv 30848 and continue 
  STEP: retrieving those results all at once @ 05/12/25 15:28:52.114
  I0512 15:28:52.174662 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "chunking-3440" for this suite. @ 05/12/25 15:28:52.215
• [21.471 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/configmap.go:95
  STEP: Creating a kubernetes client @ 05/12/25 15:28:52.278
  I0512 15:28:52.278387 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:28:52.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:28:52.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:28:52.33
  STEP: Creating configMap configmap-2801/configmap-test-476cadc8-c0a5-4ab3-a409-6809bfeeb0f2 @ 05/12/25 15:28:52.338
  STEP: Creating a pod to test consume configMaps @ 05/12/25 15:28:52.348
  STEP: Saw pod success @ 05/12/25 15:28:56.422
  I0512 15:28:56.431522 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-configmaps-a881dd0c-0acb-4892-ab07-c584d674f9bc container env-test: <nil>
  STEP: delete the pod @ 05/12/25 15:28:56.448
  I0512 15:28:56.488300 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2801" for this suite. @ 05/12/25 15:28:56.502
• [4.239 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to create and update validating webhook configurations with match conditions [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:712
  STEP: Creating a kubernetes client @ 05/12/25 15:28:56.518
  I0512 15:28:56.518816 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:28:56.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:28:56.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:28:56.571
  STEP: Setting up server cert @ 05/12/25 15:28:56.671
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:28:57.91
  STEP: Deploying the webhook pod @ 05/12/25 15:28:57.933
  STEP: Wait for the deployment to be ready @ 05/12/25 15:28:57.959
  I0512 15:28:57.978076 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/25 15:29:00.027
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:29:00.053
  I0512 15:29:01.054381 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: creating a validating webhook with match conditions @ 05/12/25 15:29:01.083
  STEP: verifying the validating webhook match conditions @ 05/12/25 15:29:01.114
  STEP: updating the validating webhook match conditions @ 05/12/25 15:29:01.122
  STEP: verifying the validating webhook match conditions @ 05/12/25 15:29:01.143
  I0512 15:29:01.243731 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9062" for this suite. @ 05/12/25 15:29:01.254
  STEP: Destroying namespace "webhook-markers-3511" for this suite. @ 05/12/25 15:29:01.284
• [4.793 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance] [sig-node, Slow, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:156
  STEP: Creating a kubernetes client @ 05/12/25 15:29:01.312
  I0512 15:29:01.312406 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 15:29:01.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:29:01.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:29:01.405
  I0512 15:29:03.468311 23 delete.go:62] Deleting pod "var-expansion-6010466e-a1bb-4e3b-a8fb-6ade257331cf" in namespace "var-expansion-3404"
  I0512 15:29:03.481384 23 delete.go:70] Wait up to 5m0s for pod "var-expansion-6010466e-a1bb-4e3b-a8fb-6ade257331cf" to be fully deleted
  I0512 15:29:05.500495 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3404" for this suite. @ 05/12/25 15:29:05.511
• [4.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/rc.go:113
  STEP: Creating a kubernetes client @ 05/12/25 15:29:05.528
  I0512 15:29:05.528182 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replication-controller @ 05/12/25 15:29:05.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:29:05.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:29:05.581
  STEP: creating a ReplicationController @ 05/12/25 15:29:05.599
  STEP: waiting for RC to be added @ 05/12/25 15:29:05.617
  STEP: waiting for available Replicas @ 05/12/25 15:29:05.617
  STEP: patching ReplicationController @ 05/12/25 15:29:06.741
  STEP: waiting for RC to be modified @ 05/12/25 15:29:06.76
  STEP: patching ReplicationController status @ 05/12/25 15:29:06.76
  STEP: waiting for RC to be modified @ 05/12/25 15:29:06.776
  STEP: waiting for available Replicas @ 05/12/25 15:29:06.776
  STEP: fetching ReplicationController status @ 05/12/25 15:29:06.786
  STEP: patching ReplicationController scale @ 05/12/25 15:29:06.795
  STEP: waiting for RC to be modified @ 05/12/25 15:29:06.81
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/12/25 15:29:06.812
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/12/25 15:29:10.967
  STEP: updating ReplicationController status @ 05/12/25 15:29:10.973
  STEP: waiting for RC to be modified @ 05/12/25 15:29:10.983
  STEP: listing all ReplicationControllers @ 05/12/25 15:29:10.983
  STEP: checking that ReplicationController has expected values @ 05/12/25 15:29:10.989
  STEP: deleting ReplicationControllers by collection @ 05/12/25 15:29:10.989
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/12/25 15:29:11.008
  I0512 15:29:11.113177 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 15:29:11.113618      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-5971" for this suite. @ 05/12/25 15:29:11.121
• [5.605 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:3184
  STEP: Creating a kubernetes client @ 05/12/25 15:29:11.134
  I0512 15:29:11.134184 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:29:11.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:29:11.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:29:11.176
  STEP: fetching services @ 05/12/25 15:29:11.185
  I0512 15:29:11.194514 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9918" for this suite. @ 05/12/25 15:29:11.226
• [0.109 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:251
  STEP: Creating a kubernetes client @ 05/12/25 15:29:11.244
  I0512 15:29:11.244331 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:29:11.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:29:11.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:29:11.285
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:29:11.29
  E0512 15:29:12.113588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:13.114283      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:14.114947      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:15.115033      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:29:15.342
  I0512 15:29:15.350049 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-0b79ccdf-9f7e-451d-bf3f-97e4af4774b1 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:29:15.367
  I0512 15:29:15.402909 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3854" for this suite. @ 05/12/25 15:29:15.412
• [4.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API should support creating IngressClass API operations [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/ingressclass.go:268
  STEP: Creating a kubernetes client @ 05/12/25 15:29:15.427
  I0512 15:29:15.427056 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename ingressclass @ 05/12/25 15:29:15.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:29:15.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:29:15.473
  STEP: getting /apis @ 05/12/25 15:29:15.48
  STEP: getting /apis/networking.k8s.io @ 05/12/25 15:29:15.49
  STEP: getting /apis/networking.k8s.iov1 @ 05/12/25 15:29:15.492
  STEP: creating @ 05/12/25 15:29:15.495
  STEP: getting @ 05/12/25 15:29:15.525
  STEP: listing @ 05/12/25 15:29:15.536
  STEP: watching @ 05/12/25 15:29:15.554
  I0512 15:29:15.554194 23 ingressclass.go:348] starting watch
  STEP: patching @ 05/12/25 15:29:15.558
  STEP: updating @ 05/12/25 15:29:15.575
  I0512 15:29:15.586824 23 ingressclass.go:364] waiting for watch events with expected annotations
  I0512 15:29:15.587415 23 ingressclass.go:377] saw patched and updated annotations
  STEP: deleting @ 05/12/25 15:29:15.588
  STEP: deleting a collection @ 05/12/25 15:29:15.624
  I0512 15:29:15.659964 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-8525" for this suite. @ 05/12/25 15:29:15.676
• [0.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/12/25 15:29:15.695
  I0512 15:29:15.695070 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename cronjob @ 05/12/25 15:29:15.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:29:15.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:29:15.752
  STEP: Creating a cronjob @ 05/12/25 15:29:15.757
  STEP: Ensuring more than one job is running at a time @ 05/12/25 15:29:15.767
  E0512 15:29:16.115883      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:17.116074      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:18.116206      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:19.116400      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:20.116743      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:21.117260      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:22.117491      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:23.117754      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:24.118869      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:25.119910      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:26.120397      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:27.121095      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:28.121402      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:29.122305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:30.122484      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:31.122984      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:32.123720      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:33.124128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:34.124234      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:35.125334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:36.125568      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:37.126134      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:38.127064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:39.128109      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:40.128172      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:41.128442      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:42.128898      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:43.129350      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:44.129983      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:45.130237      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:46.130731      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:47.131330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:48.131756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:49.131861      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:50.132994      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:51.133801      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:52.134527      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:53.135499      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:54.136426      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:55.137117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:56.137357      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:57.137962      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:58.138894      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:29:59.139152      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:00.139391      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:01.139840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:02.140454      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:03.140713      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:04.141603      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:05.141992      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:06.142844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:07.143049      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:08.144165      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:09.144406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:10.144837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:11.144917      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:12.145158      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:13.145666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:14.146435      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:15.146995      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:16.147738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:17.147932      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:18.148676      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:19.148930      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:20.149147      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:21.149515      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:22.150823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:23.150714      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:24.151701      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:25.152219      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:26.152452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:27.152955      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:28.153613      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:29.154037      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:30.154247      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:31.154489      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:32.155634      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:33.155898      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:34.156879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:35.157135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:36.158297      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:37.158597      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:38.158678      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:39.159107      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:40.160135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:41.160495      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:42.161468      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:43.162744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:44.162733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:45.163288      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:46.164238      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:47.164779      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:48.165802      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:49.166089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:50.166228      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:51.166434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:52.167330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:53.167885      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:54.169122      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:55.169474      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:56.169910      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:57.170939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:58.171854      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:30:59.172261      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:00.173047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:01.173268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/12/25 15:31:01.774
  STEP: Removing cronjob @ 05/12/25 15:31:01.78
  I0512 15:31:01.792655 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9714" for this suite. @ 05/12/25 15:31:01.8
• [106.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:58
  STEP: Creating a kubernetes client @ 05/12/25 15:31:01.816
  I0512 15:31:01.816978 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:31:01.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:31:01.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:31:01.993
  STEP: Creating configMap with name configmap-test-volume-cc6cfdd6-b74c-49fb-8d44-f612660ada28 @ 05/12/25 15:31:02.001
  STEP: Creating a pod to test consume configMaps @ 05/12/25 15:31:02.01
  E0512 15:31:02.174156      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:03.174706      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:04.174577      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:05.174749      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:31:06.054
  I0512 15:31:06.063328 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-configmaps-606e4d7c-8bc7-47cd-bb52-376d782a3522 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:31:06.105
  I0512 15:31:06.138191 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1527" for this suite. @ 05/12/25 15:31:06.15
• [4.346 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:72
  STEP: Creating a kubernetes client @ 05/12/25 15:31:06.163
  I0512 15:31:06.163884 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 15:31:06.166
  E0512 15:31:06.175232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:31:06.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:31:06.214
  E0512 15:31:07.175411      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:08.175963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:09.176717      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:10.176911      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:11.177490      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:12.177652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:13.177890      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:14.178325      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:15.179440      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:16.180386      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:17.180831      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:18.181199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:19.181956      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:20.182471      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:21.182511      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:22.182734      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:23.183973      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:24.188208      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:25.188675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:26.188821      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:27.189138      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:28.190141      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:28.370166 23 container_probe.go:92] Container started at 2025-05-12 15:31:07 +0000 UTC, pod became ready at 2025-05-12 15:31:26 +0000 UTC
  I0512 15:31:28.371051 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-7197" for this suite. @ 05/12/25 15:31:28.386
• [22.240 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should patch a pod status [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/12/25 15:31:28.404
  I0512 15:31:28.404110 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:31:28.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:31:28.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:31:28.454
  STEP: Create a pod @ 05/12/25 15:31:28.462
  E0512 15:31:29.195052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:30.195448      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/12/25 15:31:30.509
  I0512 15:31:30.530880 23 pods.go:1124] Status Message: "Patched by e2e test" and Reason: "E2E"
  I0512 15:31:30.532441 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9550" for this suite. @ 05/12/25 15:31:30.553
• [2.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 05/12/25 15:31:30.589
  I0512 15:31:30.589997 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename daemonsets @ 05/12/25 15:31:30.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:31:30.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:31:30.653
  STEP: Creating simple DaemonSet "daemon-set" @ 05/12/25 15:31:30.721
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/25 15:31:30.734
  I0512 15:31:30.789963 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:30.790666 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:30.791171 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:30.801618 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:31:30.801692 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  E0512 15:31:31.195557      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:31.749920 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:31.750369 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:31.750715 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:31.764597 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:31:31.765573 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  E0512 15:31:32.195894      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:32.770920 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:32.771252 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:32.772021 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:32.792932 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:31:32.793068 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:31:33.195856      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:33.745375 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:33.746119 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:33.746585 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:33.756879 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:31:33.756972 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/12/25 15:31:33.767
  I0512 15:31:33.846736 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:33.846835 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:33.846882 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:33.860226 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:31:33.860832 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:31:34.195982      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:34.807333 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:34.807457 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:34.807506 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:34.819026 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:31:34.819131 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:31:35.196255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:35.806419 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:35.806576 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:35.806818 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:35.838237 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:31:35.838349 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:31:36.196863      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:36.802465 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:36.802558 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:36.802618 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:36.813634 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:31:36.813702 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:31:37.197868      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:37.804661 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:37.807037 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:37.807146 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:31:37.820059 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:31:37.820174 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/25 15:31:37.83
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9674, will wait for the garbage collector to delete the pods @ 05/12/25 15:31:37.83
  I0512 15:31:37.905934 23 resources.go:139] Deleting DaemonSet.extensions daemon-set took: 15.380995ms
  I0512 15:31:38.007496 23 resources.go:163] Terminating DaemonSet.extensions daemon-set pods took: 101.558046ms
  E0512 15:31:38.198354      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:39.198398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:39.515126 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:31:39.515212 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  I0512 15:31:39.523457 23 daemon_set.go:135] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32417"},"items":null}

  I0512 15:31:39.531691 23 daemon_set.go:140] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32417"},"items":null}

  I0512 15:31:39.578721 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9674" for this suite. @ 05/12/25 15:31:39.646
• [9.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] [sig-apps, Slow, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:644
  STEP: Creating a kubernetes client @ 05/12/25 15:31:39.664
  I0512 15:31:39.664709 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 15:31:39.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:31:39.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:31:39.711
  STEP: Creating service test in namespace statefulset-5176 @ 05/12/25 15:31:39.717
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/12/25 15:31:39.729
  STEP: Creating stateful set ss in namespace statefulset-5176 @ 05/12/25 15:31:39.738
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5176 @ 05/12/25 15:31:39.755
  I0512 15:31:39.793097 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0512 15:31:40.199651      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:41.199714      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:42.200498      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:43.201177      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:44.201353      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:45.202007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:46.202657      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:47.203178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:48.203958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:49.204968      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:31:49.770049 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/12/25 15:31:49.77
  I0512 15:31:49.778938 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 15:31:50.115565 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:31:50.115676 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:31:50.115726 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:31:50.127965 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0512 15:31:50.205416      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:51.205587      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:52.205924      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:53.206818      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:54.206949      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:55.207141      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:56.207389      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:57.207708      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:58.207869      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:31:59.208604      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:00.131913 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:32:00.132036 23 wait.go:101] Waiting for statefulset status.readyReplicas updated to 0
  E0512 15:32:00.209427      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:00.230640 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 9.99999896s
  E0512 15:32:01.209464      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:01.240319 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 8.93419243s
  E0512 15:32:02.209982      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:02.251973 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 7.924599233s
  E0512 15:32:03.210229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:03.262117 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 6.913450584s
  E0512 15:32:04.210552      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:04.274668 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 5.902800319s
  E0512 15:32:05.210789      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:05.283993 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 4.890801916s
  E0512 15:32:06.211093      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:06.293944 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 3.88143912s
  E0512 15:32:07.211649      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:07.305348 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 2.871863742s
  E0512 15:32:08.212195      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:08.314764 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 1.860449918s
  E0512 15:32:09.212627      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:09.329124 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 1 for another 850.915627ms
  E0512 15:32:10.212843      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5176 @ 05/12/25 15:32:10.33
  I0512 15:32:10.341285 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  I0512 15:32:10.628309 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  I0512 15:32:10.628430 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 15:32:10.628483 23 rest.go:241] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  I0512 15:32:10.637862 23 wait.go:40] Found 1 stateful pods, waiting for 3
  E0512 15:32:11.214011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:12.215064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:13.215346      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:14.215567      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:15.216644      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:16.217581      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:17.217914      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:18.218564      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:19.219196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:20.219287      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:20.638049 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:32:20.638142 23 wait.go:50] Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:32:20.638169 23 wait.go:50] Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/12/25 15:32:20.638
  STEP: Scale down will halt with unhealthy stateful pod @ 05/12/25 15:32:20.638
  I0512 15:32:20.654075 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 15:32:20.986081 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:32:20.986184 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:32:20.986424 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:32:20.986609 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0512 15:32:21.220450      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:21.294027 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:32:21.294114 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:32:21.294144 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:32:21.294246 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 15:32:21.582247 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:32:21.582328 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:32:21.582356 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:32:21.582382 23 wait.go:101] Waiting for statefulset status.readyReplicas updated to 0
  I0512 15:32:21.588862 23 wait.go:114] Waiting for statefulset status.readyReplicas to become 0, currently 2
  E0512 15:32:22.220666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:23.221626      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:24.221905      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:25.221918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:26.222154      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:27.222489      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:28.222655      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:29.223012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:30.223845      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:31.224231      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:31.597521 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:32:31.597665 23 wait.go:50] Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:32:31.597884 23 wait.go:50] Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:32:31.700007 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 9.999999324s
  E0512 15:32:32.224637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:32.715006 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 8.916578901s
  E0512 15:32:33.224866      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:33.725226 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 7.901490168s
  E0512 15:32:34.225554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:34.735613 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 6.890764849s
  E0512 15:32:35.225706      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:35.761720 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 5.880305691s
  E0512 15:32:36.225939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:36.771097 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 4.854211952s
  E0512 15:32:37.225981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:37.783092 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 3.845603271s
  E0512 15:32:38.227034      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:38.793407 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 2.833553002s
  E0512 15:32:39.227249      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:39.803082 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 1.822598756s
  E0512 15:32:40.227378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:40.813063 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 812.647745ms
  E0512 15:32:41.228440      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5176 @ 05/12/25 15:32:41.813
  I0512 15:32:41.822861 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  I0512 15:32:42.098674 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  I0512 15:32:42.098836 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 15:32:42.098884 23 rest.go:241] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  I0512 15:32:42.099038 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0512 15:32:42.229358      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:42.386319 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  I0512 15:32:42.386404 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 15:32:42.386452 23 rest.go:241] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  I0512 15:32:42.386871 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-5176 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  I0512 15:32:42.645144 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  I0512 15:32:42.645230 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 15:32:42.645283 23 rest.go:241] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  I0512 15:32:42.645321 23 rest.go:150] Scaling statefulset ss to 0
  E0512 15:32:43.229532      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:44.230175      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:45.231378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:46.231857      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:47.232096      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:48.232280      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:49.232684      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:50.232870      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:51.233425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:52.233786      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/12/25 15:32:52.695
  I0512 15:32:52.696204 23 statefulset.go:138] Deleting all statefulset in ns statefulset-5176
  I0512 15:32:52.705303 23 rest.go:150] Scaling statefulset ss to 0
  I0512 15:32:52.724262 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 15:32:52.733238 23 rest.go:88] Deleting statefulset ss
  I0512 15:32:52.778473 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5176" for this suite. @ 05/12/25 15:32:52.803
• [73.168 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:119
  STEP: Creating a kubernetes client @ 05/12/25 15:32:52.833
  I0512 15:32:52.833882 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 15:32:52.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:32:52.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:32:52.885
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2270.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2270.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/12/25 15:32:52.892
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2270.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2270.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/12/25 15:32:52.892
  STEP: creating a pod to probe /etc/hosts @ 05/12/25 15:32:52.892
  STEP: submitting the pod to kubernetes @ 05/12/25 15:32:52.892
  E0512 15:32:53.234578      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:54.235190      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/25 15:32:54.93
  STEP: looking for the results for each expected name from probers @ 05/12/25 15:32:54.937
  I0512 15:32:54.972951 23 dns_common.go:527] DNS probes using dns-2270/dns-test-555a53e0-c7f4-4b9c-8e43-dac67f90fed4 succeeded

  STEP: deleting the pod @ 05/12/25 15:32:54.974
  I0512 15:32:55.008803 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "dns-2270" for this suite. @ 05/12/25 15:32:55.02
• [2.209 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:551
  STEP: Creating a kubernetes client @ 05/12/25 15:32:55.042
  I0512 15:32:55.043083 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 15:32:55.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:32:55.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:32:55.095
  STEP: Creating pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123 @ 05/12/25 15:32:55.103
  E0512 15:32:55.235935      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:56.236689      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 15:32:57.156
  I0512 15:32:57.164220 23 container_probe.go:1749] Initial restart count of pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea is 0
  I0512 15:32:57.174688 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:32:57.236882      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:32:58.237211      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:32:59.183051 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:32:59.238009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:00.238142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:01.191081 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:01.238218      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:02.238511      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:03.204967 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:03.239106      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:04.239654      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:05.214346 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:05.240783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:06.241618      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:07.229036 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:07.241574      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:08.241830      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:09.238758 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:09.242803      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:10.243351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:11.244478      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:11.247095 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:12.245251      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:13.245835      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:13.259846 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:14.246417      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:15.246447      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:15.271700 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:16.246547      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:17.247356      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:17.282963 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:18.247865      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:19.248474      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:19.299109 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:20.249306      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:21.249614      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:21.307855 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:22.249771      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:23.250267      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:23.323312 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:24.250604      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:25.251675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:25.335167 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:26.252234      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:27.253452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:27.345738 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:28.253653      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:29.254345      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:29.354624 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:30.255047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:31.254988      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:31.364694 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:32.255662      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:33.256207      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:33.374410 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:34.256311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:35.256737      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:35.385370 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:36.256913      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:37.257380      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:37.393356 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:38.258048      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:39.258604      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:39.405900 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:40.259253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:41.259571      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:41.417864 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:42.259745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:43.260061      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:43.430552 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:44.260694      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:45.260677      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:45.445118 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:46.260915      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:47.261543      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:47.460050 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:48.261878      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:49.261880      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:49.474272 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:50.261987      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:51.262287      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:51.485001 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:52.263294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:53.263497      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:53.496430 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:54.263919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:55.265032      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:55.506591 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:56.265277      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:57.265876      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:57.515017 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:33:58.266958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:33:59.267346      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:33:59.522652 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:34:00.267925      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:01.268465      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:34:01.533839 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:34:02.268670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:03.268814      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:34:03.543678 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:34:04.270226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:05.270165      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:34:05.552623 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:34:06.270170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:07.270318      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:34:07.562002 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:34:08.270529      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:09.270869      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:34:09.571296 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  E0512 15:34:10.271066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:11.271473      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:34:11.581830 23 container_probe.go:1759] Get pod test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea in namespace container-probe-6123
  I0512 15:34:11.581933 23 container_probe.go:1763] Restart count of pod container-probe-6123/test-grpc-34ad13aa-fa0c-4c84-8435-bd22a2bb66ea is now 1 (1m14.417628352s elapsed)
  STEP: deleting the pod @ 05/12/25 15:34:11.582
  I0512 15:34:11.626621 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6123" for this suite. @ 05/12/25 15:34:11.639
• [76.616 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:90
  STEP: Creating a kubernetes client @ 05/12/25 15:34:11.659
  I0512 15:34:11.659471 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:34:11.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:34:11.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:34:11.715
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/12/25 15:34:11.721
  E0512 15:34:12.271403      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:13.273421      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:14.273008      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:15.273058      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:34:15.777
  I0512 15:34:15.785500 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-cf1162a3-ef30-400c-99fc-f05338438418 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:34:15.834
  I0512 15:34:15.900709 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4327" for this suite. @ 05/12/25 15:34:15.912
• [4.271 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:195
  STEP: Creating a kubernetes client @ 05/12/25 15:34:15.931
  I0512 15:34:15.931918 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:34:15.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:34:15.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:34:15.984
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:34:15.991
  E0512 15:34:16.273310      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:17.273925      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:18.274104      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:19.274670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:34:20.057
  I0512 15:34:20.066233 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-802f4ffa-b965-468c-9e9b-610c088c97b3 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:34:20.079
  I0512 15:34:20.128683 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-106" for this suite. @ 05/12/25 15:34:20.141
• [4.228 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/subpath.go:69
  STEP: Creating a kubernetes client @ 05/12/25 15:34:20.16
  I0512 15:34:20.160204 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename subpath @ 05/12/25 15:34:20.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:34:20.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:34:20.22
  STEP: Setting up data @ 05/12/25 15:34:20.228
  STEP: Creating pod pod-subpath-test-configmap-9rf8 @ 05/12/25 15:34:20.258
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/25 15:34:20.258
  E0512 15:34:20.274746      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:21.275332      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:22.275508      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:23.276012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:24.280950      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:25.281111      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:26.282142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:27.282362      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:28.282992      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:29.283910      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:30.284236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:31.284475      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:32.285066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:33.285913      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:34.286591      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:35.287184      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:36.287843      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:37.288368      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:38.288846      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:39.289028      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:40.289402      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:41.289534      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:42.289696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:43.290171      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:44.290319      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:34:44.445
  I0512 15:34:44.454683 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-subpath-test-configmap-9rf8 container test-container-subpath-configmap-9rf8: <nil>
  STEP: delete the pod @ 05/12/25 15:34:44.47
  STEP: Deleting pod pod-subpath-test-configmap-9rf8 @ 05/12/25 15:34:44.513
  I0512 15:34:44.514027 23 delete.go:62] Deleting pod "pod-subpath-test-configmap-9rf8" in namespace "subpath-2701"
  I0512 15:34:44.525143 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2701" for this suite. @ 05/12/25 15:34:44.536
• [24.392 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance] [sig-api-machinery, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/namespace.go:274
  STEP: Creating a kubernetes client @ 05/12/25 15:34:44.552
  I0512 15:34:44.552399 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename namespaces @ 05/12/25 15:34:44.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:34:44.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:34:44.601
  STEP: creating a Namespace @ 05/12/25 15:34:44.607
  STEP: patching the Namespace @ 05/12/25 15:34:44.704
  STEP: get the Namespace and ensuring it has the label @ 05/12/25 15:34:44.719
  I0512 15:34:44.726095 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6185" for this suite. @ 05/12/25 15:34:44.74
  STEP: Destroying namespace "nspatchtest-74b04a1a-cee0-4d49-8375-5f0b0e67f62b-3972" for this suite. @ 05/12/25 15:34:44.757
• [0.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:746
  STEP: Creating a kubernetes client @ 05/12/25 15:34:44.784
  I0512 15:34:44.784805 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 15:34:44.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:34:44.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:34:44.845
  STEP: Creating a ResourceQuota with terminating scope @ 05/12/25 15:34:44.853
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/25 15:34:44.887
  E0512 15:34:45.291025      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:46.291773      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 05/12/25 15:34:46.896
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/25 15:34:46.907
  E0512 15:34:47.292912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:48.293501      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 05/12/25 15:34:48.914
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/12/25 15:34:48.963
  E0512 15:34:49.294181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:50.294963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/12/25 15:34:50.975
  E0512 15:34:51.296041      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:52.296616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/12/25 15:34:52.985
  STEP: Ensuring resource quota status released the pod usage @ 05/12/25 15:34:53.011
  E0512 15:34:53.297087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:54.297554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 05/12/25 15:34:55.023
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/12/25 15:34:55.05
  E0512 15:34:55.298565      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:56.299071      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/12/25 15:34:57.058
  E0512 15:34:57.299772      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:34:58.300043      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/12/25 15:34:59.068
  STEP: Ensuring resource quota status released the pod usage @ 05/12/25 15:34:59.105
  E0512 15:34:59.300782      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:00.301040      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:35:01.122974 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9501" for this suite. @ 05/12/25 15:35:01.135
• [16.379 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:110
  STEP: Creating a kubernetes client @ 05/12/25 15:35:01.163
  I0512 15:35:01.163865 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:35:01.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:35:01.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:35:01.218
  STEP: Creating configMap with name projected-configmap-test-volume-map-2b626a7f-507e-49b8-9eb6-0a20465d4c7f @ 05/12/25 15:35:01.23
  STEP: Creating a pod to test consume configMaps @ 05/12/25 15:35:01.241
  E0512 15:35:01.301133      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:02.301717      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:03.302011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:04.302696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:35:05.301
  E0512 15:35:05.302755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:35:05.307950 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-configmaps-9e842842-627b-4221-a016-cd83caf8f42a container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 15:35:05.325
  I0512 15:35:05.365962 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5181" for this suite. @ 05/12/25 15:35:05.379
• [4.231 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance] [sig-api-machinery, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/namespace.go:372
  STEP: Creating a kubernetes client @ 05/12/25 15:35:05.395
  I0512 15:35:05.395488 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename namespaces @ 05/12/25 15:35:05.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:35:05.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:35:05.443
  STEP: Updating Namespace "namespaces-717" @ 05/12/25 15:35:05.45
  I0512 15:35:05.474533 23 namespace.go:389] Namespace "namespaces-717" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"c7668d51-6bbe-406a-be04-bad37eb1c076", "kubernetes.io/metadata.name":"namespaces-717", "namespaces-717":"updated", "pod-security.kubernetes.io/audit":"baseline", "pod-security.kubernetes.io/enforce":"baseline", "pod-security.kubernetes.io/warn":"baseline"}
  I0512 15:35:05.474980 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-717" for this suite. @ 05/12/25 15:35:05.484
• [0.100 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:199
  STEP: Creating a kubernetes client @ 05/12/25 15:35:05.496
  I0512 15:35:05.496491 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/25 15:35:05.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:35:05.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:35:05.545
  STEP: fetching the /apis discovery document @ 05/12/25 15:35:05.551
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/12/25 15:35:05.553
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/12/25 15:35:05.553
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/12/25 15:35:05.553
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/12/25 15:35:05.555
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/12/25 15:35:05.555
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/12/25 15:35:05.557
  I0512 15:35:05.557894 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2367" for this suite. @ 05/12/25 15:35:05.587
• [0.105 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:190
  STEP: Creating a kubernetes client @ 05/12/25 15:35:05.601
  I0512 15:35:05.601985 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:35:05.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:35:05.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:35:05.651
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/12/25 15:35:05.656
  E0512 15:35:06.303287      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:07.303433      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:08.304303      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:09.304589      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:35:09.714
  I0512 15:35:09.722362 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-271609b8-4823-4481-b4f9-eb22c260bf5d container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:35:09.738
  I0512 15:35:09.773824 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8339" for this suite. @ 05/12/25 15:35:09.782
• [4.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/12/25 15:35:09.799
  I0512 15:35:09.799612 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename prestop @ 05/12/25 15:35:09.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:35:09.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:35:09.85
  STEP: Creating server pod server in namespace prestop-694 @ 05/12/25 15:35:09.868
  STEP: Waiting for pods to come up. @ 05/12/25 15:35:09.895
  E0512 15:35:10.304652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:11.304986      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-694 @ 05/12/25 15:35:11.947
  E0512 15:35:12.305308      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:13.305788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 05/12/25 15:35:13.973
  E0512 15:35:14.306883      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:15.307135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:16.307419      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:17.307770      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:18.307958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:35:19.016156 23 pre_stop.go:140] Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  STEP: Deleting the server pod @ 05/12/25 15:35:19.019
  I0512 15:35:19.048130 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "prestop-694" for this suite. @ 05/12/25 15:35:19.063
• [9.279 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:241
  STEP: Creating a kubernetes client @ 05/12/25 15:35:19.081
  I0512 15:35:19.081949 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:35:19.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:35:19.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:35:19.138
  STEP: Creating configMap with name cm-test-opt-del-15bfba7b-7b29-4995-9be4-485e89502d96 @ 05/12/25 15:35:19.164
  STEP: Creating configMap with name cm-test-opt-upd-141fe51e-2e0d-441e-8c99-69385933aec7 @ 05/12/25 15:35:19.175
  STEP: Creating the pod @ 05/12/25 15:35:19.184
  E0512 15:35:19.308910      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:20.309053      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:21.309538      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:22.310271      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:23.310845      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-15bfba7b-7b29-4995-9be4-485e89502d96 @ 05/12/25 15:35:23.32
  STEP: Updating configmap cm-test-opt-upd-141fe51e-2e0d-441e-8c99-69385933aec7 @ 05/12/25 15:35:23.333
  STEP: Creating configMap with name cm-test-opt-create-be9f90b6-611f-4405-adaf-c14d0f52efcd @ 05/12/25 15:35:23.347
  STEP: waiting to observe update in volume @ 05/12/25 15:35:23.357
  E0512 15:35:24.311405      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:25.312292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:26.313018      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:27.320181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:28.320667      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:29.321287      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:30.321233      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:31.322385      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:32.322521      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:33.323081      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:34.323393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:35.324898      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:36.324934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:37.326928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:38.327101      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:39.328246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:40.328899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:41.329039      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:42.330307      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:43.331365      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:44.331912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:45.332745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:46.333268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:47.333516      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:48.334285      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:49.334432      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:50.335159      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:51.335860      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:52.336945      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:53.337501      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:54.337616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:55.338560      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:56.338745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:57.339334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:58.340088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:35:59.340332      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:00.340428      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:01.340879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:02.341493      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:03.341714      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:04.341948      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:05.342203      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:06.342685      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:07.343052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:08.344013      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:09.344772      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:10.344858      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:11.345326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:12.346231      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:13.346494      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:14.346613      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:15.347750      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:16.348075      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:17.348483      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:18.348946      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:19.349116      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:20.349970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:21.350239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:22.350602      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:23.351107      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:24.351362      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:25.351534      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:26.352769      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:27.353315      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:28.353523      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:29.354256      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:30.354583      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:31.355030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:32.356036      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:33.356451      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:34.357337      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:35.358253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:36.359003      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:37.359380      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:38.360082      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:39.360220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:40.360623      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:41.361174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:42.361276      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:43.361573      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:44.361560      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:45.361831      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:46.362117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:47.362406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:48.363332      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:36:49.363876      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:50.293240 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1012" for this suite. @ 05/12/25 15:36:50.306
• [91.254 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 05/12/25 15:36:50.339
  I0512 15:36:50.339422 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename daemonsets @ 05/12/25 15:36:50.342
  E0512 15:36:50.364015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:36:50.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:36:50.394
  I0512 15:36:50.454840 23 daemon_set.go:388] Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/25 15:36:50.465
  I0512 15:36:50.543150 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:50.543278 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:50.543336 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:50.558770 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:36:50.558851 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  E0512 15:36:51.365334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:51.476844 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:51.476933 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:51.476972 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:51.484467 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:36:51.484630 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  E0512 15:36:52.366020      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:52.479519 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:52.479665 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:52.479856 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:52.490015 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:36:52.490080 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/12/25 15:36:52.524
  STEP: Check that daemon pods images are updated. @ 05/12/25 15:36:52.583
  I0512 15:36:52.599627 23 daemon_set.go:1193] Wrong image for pod: daemon-set-6cx2r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:52.600437 23 daemon_set.go:1193] Wrong image for pod: daemon-set-mjbqf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:52.600911 23 daemon_set.go:1193] Wrong image for pod: daemon-set-pwvms. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:52.680751 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:52.681145 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:52.681542 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:36:53.366963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:53.595296 23 daemon_set.go:1193] Wrong image for pod: daemon-set-6cx2r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:53.595931 23 daemon_set.go:1193] Wrong image for pod: daemon-set-mjbqf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:53.596387 23 daemon_set.go:1193] Wrong image for pod: daemon-set-pwvms. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:53.596887 23 daemon_set.go:1198] Pod daemon-set-r2sj9 is not available
  I0512 15:36:53.608273 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:53.608882 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:53.609189 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:36:54.367196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:54.598046 23 daemon_set.go:1193] Wrong image for pod: daemon-set-6cx2r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:54.598140 23 daemon_set.go:1193] Wrong image for pod: daemon-set-mjbqf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:54.598163 23 daemon_set.go:1193] Wrong image for pod: daemon-set-pwvms. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:54.598699 23 daemon_set.go:1198] Pod daemon-set-r2sj9 is not available
  I0512 15:36:54.611626 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:54.612011 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:54.612365 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:36:55.367883      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:55.594841 23 daemon_set.go:1198] Pod daemon-set-hr2kc is not available
  I0512 15:36:55.595441 23 daemon_set.go:1193] Wrong image for pod: daemon-set-mjbqf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:55.595865 23 daemon_set.go:1193] Wrong image for pod: daemon-set-pwvms. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:55.606850 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:55.607419 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:55.607869 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:36:56.368100      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:56.594063 23 daemon_set.go:1198] Pod daemon-set-hr2kc is not available
  I0512 15:36:56.594404 23 daemon_set.go:1193] Wrong image for pod: daemon-set-mjbqf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:56.594623 23 daemon_set.go:1193] Wrong image for pod: daemon-set-pwvms. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:56.604993 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:56.605300 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:56.605793 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:36:57.368735      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:57.593133 23 daemon_set.go:1198] Pod daemon-set-4zzdl is not available
  I0512 15:36:57.593234 23 daemon_set.go:1193] Wrong image for pod: daemon-set-pwvms. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:57.603825 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:57.604276 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:57.604586 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:36:58.369126      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:58.593065 23 daemon_set.go:1198] Pod daemon-set-4zzdl is not available
  I0512 15:36:58.593608 23 daemon_set.go:1193] Wrong image for pod: daemon-set-pwvms. Expected: registry.k8s.io/e2e-test-images/agnhost:2.52, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  I0512 15:36:58.604091 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:58.604671 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:58.605181 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:36:59.369777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:36:59.595541 23 daemon_set.go:1198] Pod daemon-set-xmfqp is not available
  I0512 15:36:59.606450 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:59.606786 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:59.607201 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/12/25 15:36:59.607
  I0512 15:36:59.707719 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:59.707888 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:59.707974 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:36:59.722923 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:36:59.723016 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:37:00.370129      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:00.623820 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:37:00.624435 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:37:00.625042 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:37:00.634125 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:37:00.634465 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:37:01.370623      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:01.620580 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:37:01.620693 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:37:01.620745 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:37:01.631337 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:37:01.631663 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/25 15:37:01.685
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1040, will wait for the garbage collector to delete the pods @ 05/12/25 15:37:01.685
  I0512 15:37:01.764471 23 resources.go:139] Deleting DaemonSet.extensions daemon-set took: 16.843646ms
  I0512 15:37:01.865026 23 resources.go:163] Terminating DaemonSet.extensions daemon-set pods took: 100.54264ms
  E0512 15:37:02.371537      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:03.372259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:04.171045 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:04.171345 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  I0512 15:37:04.177296 23 daemon_set.go:135] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34525"},"items":null}

  I0512 15:37:04.182211 23 daemon_set.go:140] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34525"},"items":null}

  I0512 15:37:04.220197 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1040" for this suite. @ 05/12/25 15:37:04.237
• [13.908 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:70
  STEP: Creating a kubernetes client @ 05/12/25 15:37:04.247
  I0512 15:37:04.247852 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 15:37:04.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:04.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:04.287
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:37:04.292
  E0512 15:37:04.372574      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:05.373036      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:06.374054      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:07.374832      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:37:08.34
  I0512 15:37:08.350102 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-f5f3c6b2-6c65-4ee5-adfa-6569978193ec container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:37:08.365
  E0512 15:37:08.374488      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:08.399101 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1963" for this suite. @ 05/12/25 15:37:08.408
• [4.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:170
  STEP: Creating a kubernetes client @ 05/12/25 15:37:08.425
  I0512 15:37:08.425621 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:37:08.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:08.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:08.474
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/12/25 15:37:08.482
  E0512 15:37:09.376205      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:10.376290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:11.376377      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:12.376616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:37:12.532
  I0512 15:37:12.540830 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-cb929cc3-a950-42ab-a8a7-6d2c6a8a3a81 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:37:12.557
  I0512 15:37:12.591832 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1304" for this suite. @ 05/12/25 15:37:12.603
• [4.196 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/12/25 15:37:12.622
  I0512 15:37:12.622830 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/25 15:37:12.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:12.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:12.692
  E0512 15:37:13.377290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:14.377918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:15.378143      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:16.378489      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:16.754435 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1374" for this suite. @ 05/12/25 15:37:16.773
• [4.173 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ValidatingAdmissionPolicy [Privileged:ClusterAdmin] should support ValidatingAdmissionPolicy API operations [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/validatingadmissionpolicy.go:406
  STEP: Creating a kubernetes client @ 05/12/25 15:37:16.799
  I0512 15:37:16.799290 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename validating-admission-policy @ 05/12/25 15:37:16.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:16.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:16.851
  STEP: getting /apis @ 05/12/25 15:37:16.88
  STEP: getting /apis/admissionregistration.k8s.io @ 05/12/25 15:37:16.895
  STEP: getting /apis/admissionregistration.k8s.io/v1 @ 05/12/25 15:37:16.898
  STEP: creating @ 05/12/25 15:37:16.9
  STEP: getting @ 05/12/25 15:37:16.941
  STEP: listing @ 05/12/25 15:37:16.959
  STEP: watching @ 05/12/25 15:37:16.981
  I0512 15:37:16.981948 23 validatingadmissionpolicy.go:523] starting watch
  STEP: patching @ 05/12/25 15:37:16.985
  STEP: updating @ 05/12/25 15:37:16.998
  I0512 15:37:17.022377 23 validatingadmissionpolicy.go:552] waiting for watch events with expected annotations
  STEP: getting /status @ 05/12/25 15:37:17.022
  STEP: patching /status @ 05/12/25 15:37:17.038
  STEP: updating /status @ 05/12/25 15:37:17.055
  STEP: deleting @ 05/12/25 15:37:17.081
  STEP: deleting a collection @ 05/12/25 15:37:17.125
  I0512 15:37:17.177910 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "validating-admission-policy-8317" for this suite. @ 05/12/25 15:37:17.191
• [0.439 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/garbage_collector.go:322
  STEP: Creating a kubernetes client @ 05/12/25 15:37:17.241
  I0512 15:37:17.241748 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename gc @ 05/12/25 15:37:17.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:17.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:17.296
  STEP: create the rc @ 05/12/25 15:37:17.305
  W0512 15:37:17.321673      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0512 15:37:17.379742      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:18.379666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:19.379921      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:20.380783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:21.380811      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/12/25 15:37:22.355
  STEP: wait for all pods to be garbage collected @ 05/12/25 15:37:22.371
  E0512 15:37:22.382110      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:23.382136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:24.382917      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:25.383425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:26.383806      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:27.384654      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/25 15:37:27.395
  I0512 15:37:27.677514 23 garbage_collector.go:265] For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  I0512 15:37:27.678117 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-604" for this suite. @ 05/12/25 15:37:27.689
• [10.465 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 05/12/25 15:37:27.715
  I0512 15:37:27.715132 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename daemonsets @ 05/12/25 15:37:27.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:27.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:27.767
  I0512 15:37:27.837015 23 daemon_set.go:208] Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/12/25 15:37:27.85
  I0512 15:37:27.872263 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:27.872407 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/12/25 15:37:27.872
  I0512 15:37:27.970370 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:27.970447 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:37:28.385744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:28.960171 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:28.960257 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:37:29.385782      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:29.949981 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 1
  I0512 15:37:29.950377 23 fixtures.go:135] Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/12/25 15:37:29.958
  I0512 15:37:30.015134 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:30.015213 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/12/25 15:37:30.015
  I0512 15:37:30.059149 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:30.059660 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:37:30.385957      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:31.057731 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:31.057835 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:37:31.386183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:32.058067 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:32.058141 23 fixtures.go:130] Node opscontrol-jaku1-worker-3 is running 0 daemon pod, expected 1
  E0512 15:37:32.386539      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:33.057513 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 1
  I0512 15:37:33.057598 23 fixtures.go:135] Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/25 15:37:33.077
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7150, will wait for the garbage collector to delete the pods @ 05/12/25 15:37:33.078
  I0512 15:37:33.151402 23 resources.go:139] Deleting DaemonSet.extensions daemon-set took: 15.849403ms
  I0512 15:37:33.252798 23 resources.go:163] Terminating DaemonSet.extensions daemon-set pods took: 101.352561ms
  E0512 15:37:33.387018      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:34.368492 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:37:34.368655 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  E0512 15:37:34.387912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:34.392112 23 daemon_set.go:135] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34897"},"items":null}

  I0512 15:37:34.397823 23 daemon_set.go:140] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34897"},"items":null}

  I0512 15:37:34.482542 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7150" for this suite. @ 05/12/25 15:37:34.499
• [6.803 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:337
  STEP: Creating a kubernetes client @ 05/12/25 15:37:34.519
  I0512 15:37:34.519991 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 15:37:34.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:34.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:34.58
  STEP: creating a replication controller @ 05/12/25 15:37:34.589
  I0512 15:37:34.589202 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 create -f -'
  I0512 15:37:34.910502 23 builder.go:146] stderr: ""
  I0512 15:37:34.910638 23 builder.go:147] stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/25 15:37:34.91
  I0512 15:37:34.911073 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 15:37:35.147483 23 builder.go:146] stderr: ""
  I0512 15:37:35.148122 23 builder.go:147] stdout: "update-demo-nautilus-x9g2r update-demo-nautilus-z5k9k "
  I0512 15:37:35.148294 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods update-demo-nautilus-x9g2r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 15:37:35.317368 23 builder.go:146] stderr: ""
  I0512 15:37:35.317445 23 builder.go:147] stdout: ""
  I0512 15:37:35.317477 23 kubectl.go:2502] update-demo-nautilus-x9g2r is created but not running
  E0512 15:37:35.389028      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:36.389170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:37.390048      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:38.390330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:39.390844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:40.317780 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0512 15:37:40.391457      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:40.494752 23 builder.go:146] stderr: ""
  I0512 15:37:40.494827 23 builder.go:147] stdout: "update-demo-nautilus-x9g2r update-demo-nautilus-z5k9k "
  I0512 15:37:40.494941 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods update-demo-nautilus-x9g2r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 15:37:40.647891 23 builder.go:146] stderr: ""
  I0512 15:37:40.647998 23 builder.go:147] stdout: ""
  I0512 15:37:40.648041 23 kubectl.go:2502] update-demo-nautilus-x9g2r is created but not running
  E0512 15:37:41.391829      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:42.392149      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:43.392357      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:44.392745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:45.392865      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:45.648981 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 15:37:45.855976 23 builder.go:146] stderr: ""
  I0512 15:37:45.856092 23 builder.go:147] stdout: "update-demo-nautilus-x9g2r update-demo-nautilus-z5k9k "
  I0512 15:37:45.856239 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods update-demo-nautilus-x9g2r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 15:37:46.070425 23 builder.go:146] stderr: ""
  I0512 15:37:46.070542 23 builder.go:147] stdout: ""
  I0512 15:37:46.070582 23 kubectl.go:2502] update-demo-nautilus-x9g2r is created but not running
  E0512 15:37:46.393939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:47.394475      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:48.395187      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:49.395626      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:50.396560      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:51.071815 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 15:37:51.250011 23 builder.go:146] stderr: ""
  I0512 15:37:51.250086 23 builder.go:147] stdout: "update-demo-nautilus-x9g2r update-demo-nautilus-z5k9k "
  I0512 15:37:51.250179 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods update-demo-nautilus-x9g2r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 15:37:51.381274 23 builder.go:146] stderr: ""
  I0512 15:37:51.381345 23 builder.go:147] stdout: "true"
  I0512 15:37:51.381436 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods update-demo-nautilus-x9g2r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0512 15:37:51.397443      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:51.527093 23 builder.go:146] stderr: ""
  I0512 15:37:51.527225 23 builder.go:147] stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  I0512 15:37:51.527258 23 kubectl.go:2393] validating pod update-demo-nautilus-x9g2r
  I0512 15:37:51.535980 23 kubectl.go:2413] got data: {
    "image": "nautilus.jpg"
  }

  I0512 15:37:51.536108 23 kubectl.go:2418] Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  I0512 15:37:51.536136 23 kubectl.go:2520] update-demo-nautilus-x9g2r is verified up and running
  I0512 15:37:51.536216 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods update-demo-nautilus-z5k9k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 15:37:51.680918 23 builder.go:146] stderr: ""
  I0512 15:37:51.681003 23 builder.go:147] stdout: "true"
  I0512 15:37:51.681287 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods update-demo-nautilus-z5k9k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  I0512 15:37:51.833888 23 builder.go:146] stderr: ""
  I0512 15:37:51.833970 23 builder.go:147] stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  I0512 15:37:51.834001 23 kubectl.go:2393] validating pod update-demo-nautilus-z5k9k
  I0512 15:37:51.845539 23 kubectl.go:2413] got data: {
    "image": "nautilus.jpg"
  }

  I0512 15:37:51.845659 23 kubectl.go:2418] Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  I0512 15:37:51.845690 23 kubectl.go:2520] update-demo-nautilus-z5k9k is verified up and running
  STEP: using delete to clean up resources @ 05/12/25 15:37:51.845
  I0512 15:37:51.845841 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 delete --grace-period=0 --force -f -'
  I0512 15:37:52.026429 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:37:52.026534 23 builder.go:147] stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  I0512 15:37:52.026690 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get rc,svc -l name=update-demo --no-headers'
  I0512 15:37:52.234077 23 builder.go:146] stderr: "No resources found in kubectl-6452 namespace.\n"
  I0512 15:37:52.234189 23 builder.go:147] stdout: ""
  I0512 15:37:52.234348 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6452 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  E0512 15:37:52.398556      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:52.411852 23 builder.go:146] stderr: ""
  I0512 15:37:52.411981 23 builder.go:147] stdout: ""
  I0512 15:37:52.412310 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6452" for this suite. @ 05/12/25 15:37:52.427
• [17.923 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:821
  STEP: Creating a kubernetes client @ 05/12/25 15:37:52.443
  I0512 15:37:52.443594 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:37:52.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:37:52.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:37:52.503
  STEP: creating service multi-endpoint-test in namespace services-2725 @ 05/12/25 15:37:52.51
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2725 to expose endpoints map[] @ 05/12/25 15:37:52.537
  I0512 15:37:52.547611 23 service.go:4267] Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0512 15:37:53.399038      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:37:53.566560 23 service.go:4299] successfully validated that service multi-endpoint-test in namespace services-2725 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2725 @ 05/12/25 15:37:53.567
  E0512 15:37:54.400052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:55.400097      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2725 to expose endpoints map[pod1:[100]] @ 05/12/25 15:37:55.614
  I0512 15:37:55.641539 23 service.go:4299] successfully validated that service multi-endpoint-test in namespace services-2725 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-2725 @ 05/12/25 15:37:55.641
  E0512 15:37:56.401236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:57.401549      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2725 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/12/25 15:37:57.69
  I0512 15:37:57.722986 23 service.go:4299] successfully validated that service multi-endpoint-test in namespace services-2725 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/12/25 15:37:57.723
  I0512 15:37:57.723284 23 resource.go:361] Creating new exec pod
  E0512 15:37:58.402419      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:37:59.402695      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:00.403011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:00.757136 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-2725 exec execpods44qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  I0512 15:38:01.065345 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  I0512 15:38:01.065442 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:38:01.065590 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-2725 exec execpods44qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.173 80'
  I0512 15:38:01.343489 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.173 80\nConnection to 10.233.25.173 80 port [tcp/http] succeeded!\n"
  I0512 15:38:01.343614 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:38:01.343812 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-2725 exec execpods44qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  E0512 15:38:01.403797      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:01.618223 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  I0512 15:38:01.618297 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:38:01.618733 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-2725 exec execpods44qh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.173 81'
  I0512 15:38:01.896897 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.173 81\nConnection to 10.233.25.173 81 port [tcp/*] succeeded!\n"
  I0512 15:38:01.897014 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2725 @ 05/12/25 15:38:01.897
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2725 to expose endpoints map[pod2:[101]] @ 05/12/25 15:38:01.932
  I0512 15:38:01.985027 23 service.go:4299] successfully validated that service multi-endpoint-test in namespace services-2725 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-2725 @ 05/12/25 15:38:01.985
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2725 to expose endpoints map[] @ 05/12/25 15:38:02.027
  I0512 15:38:02.053578 23 service.go:4299] successfully validated that service multi-endpoint-test in namespace services-2725 exposes endpoints map[]
  I0512 15:38:02.108260 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2725" for this suite. @ 05/12/25 15:38:02.131
• [9.730 seconds]
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/12/25 15:38:02.174
  I0512 15:38:02.174496 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/12/25 15:38:02.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:02.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:02.257
  E0512 15:38:02.404784      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:03.405848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Cleaning up the secret @ 05/12/25 15:38:04.357
  STEP: Cleaning up the configmap @ 05/12/25 15:38:04.375
  STEP: Cleaning up the pod @ 05/12/25 15:38:04.388
  E0512 15:38:04.406927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:04.416082 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-wrapper-8229" for this suite. @ 05/12/25 15:38:04.427
• [2.265 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/service_accounts.go:742
  STEP: Creating a kubernetes client @ 05/12/25 15:38:04.445
  I0512 15:38:04.446221 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/25 15:38:04.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:04.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:04.486
  I0512 15:38:04.499067 23 service_accounts.go:754] Got root ca configmap in namespace "svcaccounts-9443"
  I0512 15:38:04.509080 23 service_accounts.go:757] Deleted root ca configmap in namespace "svcaccounts-9443"
  STEP: waiting for a new root ca configmap created @ 05/12/25 15:38:05.01
  I0512 15:38:05.018954 23 service_accounts.go:771] Recreated root ca configmap in namespace "svcaccounts-9443"
  I0512 15:38:05.029949 23 service_accounts.go:782] Updated root ca configmap in namespace "svcaccounts-9443"
  E0512 15:38:05.406609      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 05/12/25 15:38:05.53
  I0512 15:38:05.542250 23 service_accounts.go:800] Reconciled root ca configmap in namespace "svcaccounts-9443"
  I0512 15:38:05.542537 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9443" for this suite. @ 05/12/25 15:38:05.555
• [1.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:2179
  STEP: Creating a kubernetes client @ 05/12/25 15:38:05.57
  I0512 15:38:05.570915 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:38:05.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:05.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:05.623
  STEP: creating service in namespace services-617 @ 05/12/25 15:38:05.631
  STEP: creating service affinity-clusterip-transition in namespace services-617 @ 05/12/25 15:38:05.631
  STEP: creating replication controller affinity-clusterip-transition in namespace services-617 @ 05/12/25 15:38:05.653
  I0512 15:38:05.668469      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-617, replica count: 3
  E0512 15:38:06.408680      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:07.409285      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:08.409481      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:08.720157      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 15:38:08.739746 23 resource.go:361] Creating new exec pod
  E0512 15:38:09.410051      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:10.410222      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:11.410392      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:11.784606 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-617 exec execpod-affinity6zgj7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  I0512 15:38:12.045010 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  I0512 15:38:12.045096 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:38:12.045378 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-617 exec execpod-affinity6zgj7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.222 80'
  I0512 15:38:12.316832 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.222 80\nConnection to 10.233.52.222 80 port [tcp/http] succeeded!\n"
  I0512 15:38:12.316938 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:38:12.336759 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-617 exec execpod-affinity6zgj7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.52.222:80/ ; done'
  E0512 15:38:12.411622      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:12.784415 23 builder.go:146] stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n"
  I0512 15:38:12.784530 23 builder.go:147] stdout: "\naffinity-clusterip-transition-vhvlz\naffinity-clusterip-transition-4swjf\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-vhvlz\naffinity-clusterip-transition-4swjf\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-vhvlz\naffinity-clusterip-transition-4swjf\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-vhvlz\naffinity-clusterip-transition-4swjf\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-vhvlz\naffinity-clusterip-transition-4swjf\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-vhvlz"
  I0512 15:38:12.784731 23 service.go:242] Received response from host: affinity-clusterip-transition-vhvlz
  I0512 15:38:12.784761 23 service.go:242] Received response from host: affinity-clusterip-transition-4swjf
  I0512 15:38:12.784782 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:12.784801 23 service.go:242] Received response from host: affinity-clusterip-transition-vhvlz
  I0512 15:38:12.784820 23 service.go:242] Received response from host: affinity-clusterip-transition-4swjf
  I0512 15:38:12.784996 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:12.785099 23 service.go:242] Received response from host: affinity-clusterip-transition-vhvlz
  I0512 15:38:12.785241 23 service.go:242] Received response from host: affinity-clusterip-transition-4swjf
  I0512 15:38:12.785267 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:12.785429 23 service.go:242] Received response from host: affinity-clusterip-transition-vhvlz
  I0512 15:38:12.785467 23 service.go:242] Received response from host: affinity-clusterip-transition-4swjf
  I0512 15:38:12.785486 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:12.785504 23 service.go:242] Received response from host: affinity-clusterip-transition-vhvlz
  I0512 15:38:12.785657 23 service.go:242] Received response from host: affinity-clusterip-transition-4swjf
  I0512 15:38:12.785680 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:12.785698 23 service.go:242] Received response from host: affinity-clusterip-transition-vhvlz
  I0512 15:38:12.807215 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-617 exec execpod-affinity6zgj7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.52.222:80/ ; done'
  I0512 15:38:13.264577 23 builder.go:146] stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.52.222:80/\n"
  I0512 15:38:13.264869 23 builder.go:147] stdout: "\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv\naffinity-clusterip-transition-pxcwv"
  I0512 15:38:13.264914 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.264939 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.264959 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.264977 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.264997 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265015 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265034 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265052 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265072 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265091 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265113 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265131 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265149 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265176 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265194 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265212 23 service.go:242] Received response from host: affinity-clusterip-transition-pxcwv
  I0512 15:38:13.265314 23 service.go:4061] Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-617, will wait for the garbage collector to delete the pods @ 05/12/25 15:38:13.292
  I0512 15:38:13.368255 23 resources.go:139] Deleting ReplicationController affinity-clusterip-transition took: 18.664867ms
  E0512 15:38:13.411678      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:13.469343 23 resources.go:163] Terminating ReplicationController affinity-clusterip-transition pods took: 101.106454ms
  E0512 15:38:14.412618      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:15.413168      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:16.409552 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 15:38:16.413666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-617" for this suite. @ 05/12/25 15:38:16.421
• [10.863 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/12/25 15:38:16.435
  I0512 15:38:16.435055 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/12/25 15:38:16.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:16.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:16.484
  STEP: mirroring a new custom Endpoint @ 05/12/25 15:38:16.505
  I0512 15:38:16.522544 23 endpointslicemirroring.go:96] Waiting for at least 1 EndpointSlice to exist, got 0
  E0512 15:38:17.413833      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:18.414113      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 05/12/25 15:38:18.532
  STEP: mirroring deletion of a custom Endpoint @ 05/12/25 15:38:18.56
  I0512 15:38:18.595845 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-2880" for this suite. @ 05/12/25 15:38:18.608
• [2.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:878
  STEP: Creating a kubernetes client @ 05/12/25 15:38:18.632
  I0512 15:38:18.632201 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 15:38:18.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:18.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:18.683
  STEP: validating api versions @ 05/12/25 15:38:18.692
  I0512 15:38:18.693827 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7285 api-versions'
  I0512 15:38:18.879384 23 builder.go:146] stderr: ""
  I0512 15:38:18.879532 23 builder.go:147] stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncns.vmware.com/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\ntraefik.containo.us/v1alpha1\ntraefik.io/v1alpha1\nv1\nvault.banzaicloud.com/v1alpha1\nvelero.io/v1\nvelero.io/v2alpha1\n"
  I0512 15:38:18.879836 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7285" for this suite. @ 05/12/25 15:38:18.89
• [0.273 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:251
  STEP: Creating a kubernetes client @ 05/12/25 15:38:18.905
  I0512 15:38:18.905567 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 15:38:18.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:18.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:18.951
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:38:18.959
  E0512 15:38:19.414775      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:20.415393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:21.416600      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:22.416970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:38:23.012
  I0512 15:38:23.023603 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-7b6e5a6d-6f61-47ba-bb6a-fc06af730996 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:38:23.046
  I0512 15:38:23.092842 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2605" for this suite. @ 05/12/25 15:38:23.106
• [4.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/replica_set.go:132
  STEP: Creating a kubernetes client @ 05/12/25 15:38:23.126
  I0512 15:38:23.126500 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replicaset @ 05/12/25 15:38:23.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:23.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:23.188
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/12/25 15:38:23.201
  E0512 15:38:23.417768      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:24.418723      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/12/25 15:38:25.267
  STEP: Then the orphan pod is adopted @ 05/12/25 15:38:25.282
  E0512 15:38:25.419257      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 05/12/25 15:38:26.297
  I0512 15:38:26.307936 23 resource.go:87] Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/12/25 15:38:26.346
  E0512 15:38:26.419723      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:27.367395 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9115" for this suite. @ 05/12/25 15:38:27.38
• [4.272 seconds]
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/garbage_collector.go:639
  STEP: Creating a kubernetes client @ 05/12/25 15:38:27.399
  I0512 15:38:27.399395 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename gc @ 05/12/25 15:38:27.401
  E0512 15:38:27.419857      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:27.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:27.474
  STEP: create the rc @ 05/12/25 15:38:27.49
  W0512 15:38:27.500630      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0512 15:38:28.420843      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:29.421916      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:30.422844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:31.423896      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:32.424553      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:33.424804      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/12/25 15:38:33.516
  STEP: wait for the rc to be deleted @ 05/12/25 15:38:33.541
  E0512 15:38:34.425487      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:34.566184 23 garbage_collector.go:670] 80 pods remaining
  I0512 15:38:34.566450 23 garbage_collector.go:677] 80 pods has nil DeletionTimestamp
  I0512 15:38:34.566577 23 garbage_collector.go:678] 
  E0512 15:38:35.426096      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:35.570166 23 garbage_collector.go:670] 71 pods remaining
  I0512 15:38:35.570572 23 garbage_collector.go:677] 71 pods has nil DeletionTimestamp
  I0512 15:38:35.570814 23 garbage_collector.go:678] 
  E0512 15:38:36.427047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:36.578868 23 garbage_collector.go:670] 60 pods remaining
  I0512 15:38:36.578917 23 garbage_collector.go:677] 59 pods has nil DeletionTimestamp
  I0512 15:38:36.578928 23 garbage_collector.go:678] 
  E0512 15:38:37.427915      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:37.555959 23 garbage_collector.go:670] 40 pods remaining
  I0512 15:38:37.556133 23 garbage_collector.go:677] 40 pods has nil DeletionTimestamp
  I0512 15:38:37.556237 23 garbage_collector.go:678] 
  E0512 15:38:38.428171      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:38.584101 23 garbage_collector.go:670] 31 pods remaining
  I0512 15:38:38.584177 23 garbage_collector.go:677] 30 pods has nil DeletionTimestamp
  I0512 15:38:38.584198 23 garbage_collector.go:678] 
  E0512 15:38:39.428451      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:39.553967 23 garbage_collector.go:670] 20 pods remaining
  I0512 15:38:39.554015 23 garbage_collector.go:677] 20 pods has nil DeletionTimestamp
  I0512 15:38:39.554028 23 garbage_collector.go:678] 
  E0512 15:38:40.428847      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/25 15:38:40.558
  I0512 15:38:40.803512 23 garbage_collector.go:265] For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  I0512 15:38:40.803830 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2438" for this suite. @ 05/12/25 15:38:40.814
• [13.439 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance] [sig-apps, Slow, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:754
  STEP: Creating a kubernetes client @ 05/12/25 15:38:40.838
  I0512 15:38:40.838878 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 15:38:40.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:38:40.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:38:40.918
  STEP: Creating service test in namespace statefulset-8412 @ 05/12/25 15:38:40.926
  STEP: Creating stateful set ss in namespace statefulset-8412 @ 05/12/25 15:38:40.971
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8412 @ 05/12/25 15:38:40.982
  I0512 15:38:40.990917 23 wait.go:40] Found 0 stateful pods, waiting for 1
  E0512 15:38:41.429775      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:42.430525      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:43.431259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:44.431689      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:45.432436      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:46.432785      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:47.433278      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:48.433642      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:49.434071      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:50.434822      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:38:50.994985 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/12/25 15:38:50.995
  I0512 15:38:51.001041 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-8412 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 15:38:51.243483 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:38:51.243623 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:38:51.243687 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:38:51.255075 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0512 15:38:51.435426      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:52.435645      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:53.436203      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:54.436740      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:55.437441      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:56.437803      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:57.437990      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:58.438119      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:38:59.438405      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:00.438501      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:01.255362 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:39:01.255522 23 wait.go:101] Waiting for statefulset status.readyReplicas updated to 0
  I0512 15:39:01.310875 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 9.999999616s
  E0512 15:39:01.438536      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:02.331515 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 8.97200589s
  E0512 15:39:02.439243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:03.351018 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 7.95137772s
  E0512 15:39:03.440133      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:04.358980 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 6.93178375s
  E0512 15:39:04.440685      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:05.369888 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 5.923952498s
  E0512 15:39:05.441582      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:06.382634 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 4.912139949s
  E0512 15:39:06.442497      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:07.391092 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 3.900180251s
  E0512 15:39:07.443591      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:08.407082 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 2.891842393s
  E0512 15:39:08.444776      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:09.417631 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 1.875730251s
  E0512 15:39:09.445533      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:10.430518 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 3 for another 865.059767ms
  E0512 15:39:10.445690      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8412 @ 05/12/25 15:39:11.43
  I0512 15:39:11.442062 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-8412 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0512 15:39:11.445670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:11.725324 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  I0512 15:39:11.725473 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 15:39:11.725506 23 rest.go:241] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  I0512 15:39:11.725856 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-8412 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  I0512 15:39:12.006108 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  I0512 15:39:12.006447 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 15:39:12.006489 23 rest.go:241] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  I0512 15:39:12.006589 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-8412 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  I0512 15:39:12.305375 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  I0512 15:39:12.305503 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 15:39:12.305536 23 rest.go:241] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  I0512 15:39:12.319010 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0512 15:39:12.446295      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:13.446869      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:14.447033      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:15.447435      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:16.448198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:17.448420      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:18.448644      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:19.448902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:20.449493      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:21.449979      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:22.320034 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:39:22.320129 23 wait.go:50] Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  I0512 15:39:22.320155 23 wait.go:50] Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/12/25 15:39:22.32
  I0512 15:39:22.339032 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-8412 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0512 15:39:22.450898      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:22.660266 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:39:22.660771 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:39:22.660945 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:39:22.661348 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-8412 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 15:39:22.960678 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:39:22.960756 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:39:22.960794 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:39:22.960884 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-8412 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 15:39:23.262838 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 15:39:23.263278 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 15:39:23.263393 23 rest.go:241] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  I0512 15:39:23.263433 23 wait.go:101] Waiting for statefulset status.readyReplicas updated to 0
  I0512 15:39:23.275009 23 wait.go:114] Waiting for statefulset status.readyReplicas to become 0, currently 3
  E0512 15:39:23.451366      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:24.452009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:25.452768      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:26.452980      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:27.453840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:28.454818      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:29.455269      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:30.455702      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:31.456787      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:32.457823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:33.283716 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:39:33.283807 23 wait.go:50] Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:39:33.283832 23 wait.go:50] Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  I0512 15:39:33.343488 23 resource.go:168] POD   NODE                       PHASE    GRACE  CONDITIONS
  I0512 15:39:33.343706 23 resource.go:175] ss-0  opscontrol-jaku1-worker-1  Running         [{PodReadyToStartContainers True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:38:42 +0000 UTC  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:38:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:38:41 +0000 UTC  }]
  I0512 15:39:33.343768 23 resource.go:175] ss-1  opscontrol-jaku1-worker-2  Running         [{PodReadyToStartContainers True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:03 +0000 UTC  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:01 +0000 UTC  }]
  I0512 15:39:33.343822 23 resource.go:175] ss-2  opscontrol-jaku1-worker-0  Running  30s    [{PodReadyToStartContainers True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:02 +0000 UTC  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:01 +0000 UTC  }]
  I0512 15:39:33.343847 23 resource.go:178] 
  I0512 15:39:33.343881 23 statefulset.go:2413] StatefulSet ss has not reached scale 0, at 3
  E0512 15:39:33.458565      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:34.379532 23 resource.go:168] POD   NODE                       PHASE      GRACE  CONDITIONS
  I0512 15:39:34.379726 23 resource.go:175] ss-0  opscontrol-jaku1-worker-1  Succeeded  30s    [{PodReadyToStartContainers False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:33 +0000 UTC  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:38:41 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:38:41 +0000 UTC  }]
  I0512 15:39:34.379942 23 resource.go:175] ss-2  opscontrol-jaku1-worker-0  Succeeded  30s    [{PodReadyToStartContainers False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:33 +0000 UTC  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:01 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:23 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:39:01 +0000 UTC  }]
  I0512 15:39:34.379977 23 resource.go:178] 
  I0512 15:39:34.380010 23 statefulset.go:2413] StatefulSet ss has not reached scale 0, at 2
  E0512 15:39:34.458729      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:35.389576 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 7.923274347s
  E0512 15:39:35.459198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:36.399654 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 6.913461769s
  E0512 15:39:36.459200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:37.407219 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 5.90348117s
  E0512 15:39:37.459762      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:38.416439 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 4.895964658s
  E0512 15:39:38.460284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:39.428897 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 3.88668412s
  E0512 15:39:39.461258      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:40.438403 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 2.874050018s
  E0512 15:39:40.461832      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:41.447801 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 1.864528467s
  E0512 15:39:41.462198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:42.459282 23 statefulset.go:2418] Verifying statefulset ss doesn't scale past 0 for another 854.94442ms
  E0512 15:39:42.462702      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8412 @ 05/12/25 15:39:43.459
  E0512 15:39:43.463237      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:43.471464 23 rest.go:150] Scaling statefulset ss to 0
  I0512 15:39:43.484876 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 15:39:43.489985 23 statefulset.go:138] Deleting all statefulset in ns statefulset-8412
  I0512 15:39:43.496301 23 rest.go:150] Scaling statefulset ss to 0
  I0512 15:39:43.572404 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 15:39:43.579796 23 rest.go:88] Deleting statefulset ss
  I0512 15:39:43.613959 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8412" for this suite. @ 05/12/25 15:39:43.623
• [62.800 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance] [sig-scheduling, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/limit_range.go:62
  STEP: Creating a kubernetes client @ 05/12/25 15:39:43.639
  I0512 15:39:43.639683 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename limitrange @ 05/12/25 15:39:43.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:39:43.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:39:43.681
  STEP: Creating a LimitRange @ 05/12/25 15:39:43.686
  STEP: Setting up watch @ 05/12/25 15:39:43.687
  STEP: Submitting a LimitRange @ 05/12/25 15:39:43.795
  STEP: Verifying LimitRange creation was observed @ 05/12/25 15:39:43.809
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/12/25 15:39:43.809
  I0512 15:39:43.817044 23 limit_range.go:355] Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  I0512 15:39:43.817179 23 limit_range.go:360] Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/12/25 15:39:43.817
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/12/25 15:39:43.828
  I0512 15:39:43.842803 23 limit_range.go:355] Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  I0512 15:39:43.842902 23 limit_range.go:360] Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/12/25 15:39:43.842
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/12/25 15:39:43.859
  I0512 15:39:43.868005 23 limit_range.go:355] Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  I0512 15:39:43.868143 23 limit_range.go:360] Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/12/25 15:39:43.868
  STEP: Failing to create a Pod with more than max resources @ 05/12/25 15:39:43.874
  STEP: Updating a LimitRange @ 05/12/25 15:39:43.883
  STEP: Verifying LimitRange updating is effective @ 05/12/25 15:39:43.896
  E0512 15:39:44.464434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:45.464450      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 05/12/25 15:39:45.905
  STEP: Failing to create a Pod with more than max resources @ 05/12/25 15:39:45.92
  STEP: Deleting a LimitRange @ 05/12/25 15:39:45.936
  STEP: Verifying the LimitRange was deleted @ 05/12/25 15:39:45.956
  E0512 15:39:46.464840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:47.465074      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:48.465220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:49.465488      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:50.465787      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:50.965186 23 limit_range.go:211] limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/12/25 15:39:50.965
  I0512 15:39:50.977472 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-8148" for this suite. @ 05/12/25 15:39:50.992
• [7.362 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/12/25 15:39:51.001
  I0512 15:39:51.002110 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename watch @ 05/12/25 15:39:51.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:39:51.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:39:51.029
  STEP: creating a watch on configmaps @ 05/12/25 15:39:51.033
  STEP: creating a new configmap @ 05/12/25 15:39:51.035
  STEP: modifying the configmap once @ 05/12/25 15:39:51.041
  STEP: closing the watch once it receives two notifications @ 05/12/25 15:39:51.053
  I0512 15:39:51.053535 23 watch.go:431] Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8745  7199c3fe-b6b0-41ce-983a-0e232b56b1fb 37897 0 2025-05-12 15:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2025-05-12 15:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 15:39:51.053782 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8745  7199c3fe-b6b0-41ce-983a-0e232b56b1fb 37898 0 2025-05-12 15:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2025-05-12 15:39:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/12/25 15:39:51.053
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/12/25 15:39:51.065
  STEP: deleting the configmap @ 05/12/25 15:39:51.067
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/12/25 15:39:51.075
  I0512 15:39:51.075773 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8745  7199c3fe-b6b0-41ce-983a-0e232b56b1fb 37899 0 2025-05-12 15:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2025-05-12 15:39:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 15:39:51.076099 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8745  7199c3fe-b6b0-41ce-983a-0e232b56b1fb 37900 0 2025-05-12 15:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2025-05-12 15:39:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 15:39:51.076231 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8745" for this suite. @ 05/12/25 15:39:51.093
• [0.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
  STEP: Creating a kubernetes client @ 05/12/25 15:39:51.106
  I0512 15:39:51.106308 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 15:39:51.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:39:51.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:39:51.138
  STEP: creating the pod @ 05/12/25 15:39:51.143
  I0512 15:39:51.143319 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 create -f -'
  E0512 15:39:51.466331      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:51.493345 23 builder.go:146] stderr: ""
  I0512 15:39:51.493424 23 builder.go:147] stdout: "pod/pause created\n"
  E0512 15:39:52.466437      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:53.466790      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/12/25 15:39:53.511
  I0512 15:39:53.511275 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 label pods pause testing-label=testing-label-value'
  I0512 15:39:53.720564 23 builder.go:146] stderr: ""
  I0512 15:39:53.720647 23 builder.go:147] stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/12/25 15:39:53.72
  I0512 15:39:53.720933 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 get pod pause -L testing-label'
  I0512 15:39:53.919434 23 builder.go:146] stderr: ""
  I0512 15:39:53.919576 23 builder.go:147] stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/12/25 15:39:53.919
  I0512 15:39:53.919778 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 label pods pause testing-label-'
  I0512 15:39:54.101045 23 builder.go:146] stderr: ""
  I0512 15:39:54.101132 23 builder.go:147] stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/12/25 15:39:54.101
  I0512 15:39:54.101285 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 get pod pause -L testing-label'
  I0512 15:39:54.266228 23 builder.go:146] stderr: ""
  I0512 15:39:54.266342 23 builder.go:147] stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 05/12/25 15:39:54.266
  I0512 15:39:54.266839 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 delete --grace-period=0 --force -f -'
  I0512 15:39:54.436782 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 15:39:54.436859 23 builder.go:147] stdout: "pod \"pause\" force deleted\n"
  I0512 15:39:54.436962 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 get rc,svc -l name=pause --no-headers'
  E0512 15:39:54.467906      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:39:54.639654 23 builder.go:146] stderr: "No resources found in kubectl-5060 namespace.\n"
  I0512 15:39:54.639939 23 builder.go:147] stdout: ""
  I0512 15:39:54.640123 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5060 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  I0512 15:39:54.799858 23 builder.go:146] stderr: ""
  I0512 15:39:54.799952 23 builder.go:147] stdout: ""
  I0512 15:39:54.800450 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5060" for this suite. @ 05/12/25 15:39:54.814
• [3.726 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:495
  STEP: Creating a kubernetes client @ 05/12/25 15:39:54.833
  I0512 15:39:54.833714 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:39:54.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:39:54.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:39:54.92
  STEP: Setting up server cert @ 05/12/25 15:39:55.011
  E0512 15:39:55.468742      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:56.469666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:57.470654      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:39:57.724
  STEP: Deploying the webhook pod @ 05/12/25 15:39:57.75
  STEP: Wait for the deployment to be ready @ 05/12/25 15:39:57.78
  I0512 15:39:57.798543 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 15:39:58.472240      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:39:59.472354      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 15:39:59.833
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:39:59.89
  E0512 15:40:00.473423      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:00.891603 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/12/25 15:40:00.909
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/12/25 15:40:00.946
  STEP: Creating a configMap that should not be mutated @ 05/12/25 15:40:00.958
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/12/25 15:40:00.977
  STEP: Creating a configMap that should be mutated @ 05/12/25 15:40:00.987
  I0512 15:40:01.125475 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1535" for this suite. @ 05/12/25 15:40:01.139
  STEP: Destroying namespace "webhook-markers-7942" for this suite. @ 05/12/25 15:40:01.16
• [6.350 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/replica_set.go:112
  STEP: Creating a kubernetes client @ 05/12/25 15:40:01.197
  I0512 15:40:01.198429 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replicaset @ 05/12/25 15:40:01.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:40:01.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:40:01.251
  I0512 15:40:01.256582 23 replica_set.go:191] Creating ReplicaSet my-hostname-basic-81070329-b966-4ca3-be3f-9765d5d282e4
  I0512 15:40:01.273253 23 resource.go:87] Pod name my-hostname-basic-81070329-b966-4ca3-be3f-9765d5d282e4: Found 0 pods out of 1
  E0512 15:40:01.473550      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:02.473902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:03.473919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:04.474040      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:05.474259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:06.284828 23 resource.go:87] Pod name my-hostname-basic-81070329-b966-4ca3-be3f-9765d5d282e4: Found 1 pods out of 1
  I0512 15:40:06.284914 23 replica_set.go:204] Ensuring a pod for ReplicaSet "my-hostname-basic-81070329-b966-4ca3-be3f-9765d5d282e4" is running
  I0512 15:40:06.291374 23 replica_set.go:220] Pod "my-hostname-basic-81070329-b966-4ca3-be3f-9765d5d282e4-2wrt9" is running (conditions: [{Type:PodReadyToStartContainers Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 15:40:02 +0000 UTC Reason: Message:} {Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 15:40:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 15:40:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 15:40:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 15:40:01 +0000 UTC Reason: Message:}])
  I0512 15:40:06.291439 23 replica_set.go:228] Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/12/25 15:40:06.291
  I0512 15:40:06.314029 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-48" for this suite. @ 05/12/25 15:40:06.335
• [5.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/configmap.go:140
  STEP: Creating a kubernetes client @ 05/12/25 15:40:06.351
  I0512 15:40:06.351595 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:40:06.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:40:06.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:40:06.395
  STEP: Creating configMap that has name configmap-test-emptyKey-9ce8a602-b139-4e81-bf1d-b138f5fc58c9 @ 05/12/25 15:40:06.401
  I0512 15:40:06.405808 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1282" for this suite. @ 05/12/25 15:40:06.433
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:209
  STEP: Creating a kubernetes client @ 05/12/25 15:40:06.451
  I0512 15:40:06.451820 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 15:40:06.453
  E0512 15:40:06.474432      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:40:06.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:40:06.503
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:40:06.509
  E0512 15:40:07.481819      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:08.481104      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:09.481978      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:10.483087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:40:10.562
  I0512 15:40:10.571029 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-37ae5a14-f6e7-4ec4-8694-48572ef62695 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:40:10.604
  I0512 15:40:10.638411 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8947" for this suite. @ 05/12/25 15:40:10.649
• [4.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/12/25 15:40:10.665
  I0512 15:40:10.665304 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/25 15:40:10.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:40:10.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:40:10.717
  E0512 15:40:11.483117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:12.483833      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:12.801363 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5962" for this suite. @ 05/12/25 15:40:12.818
• [2.184 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:152
  STEP: Creating a kubernetes client @ 05/12/25 15:40:12.85
  I0512 15:40:12.850343 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 15:40:12.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:40:12.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:40:12.923
  STEP: Creating pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033 @ 05/12/25 15:40:12.93
  E0512 15:40:13.484338      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:14.485004      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 15:40:14.995
  I0512 15:40:15.002562 23 container_probe.go:1749] Initial restart count of pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f is 0
  I0512 15:40:15.010953 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:15.485751      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:16.486389      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:17.027900 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:17.486872      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:18.487251      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:19.037676 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:19.487963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:20.488933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:21.044927 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:21.489741      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:22.490367      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:23.054005 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:23.491402      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:24.491965      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:25.062487 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:25.492253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:26.492842      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:27.069963 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:27.493884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:28.494232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:29.084434 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:29.494327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:30.494487      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:31.096552 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:31.495201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:32.495816      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:33.107809 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:33.496039      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:34.496448      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:35.118804 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:35.497120      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:36.498006      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:37.132263 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:37.498773      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:38.499463      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:39.141734 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:39.500055      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:40.500990      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:41.153031 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:41.501090      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:42.501554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:43.162416 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:43.501692      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:44.502183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:45.173475 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:45.503024      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:46.503524      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:47.183356 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:47.503719      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:48.504145      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:49.193057 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:49.504510      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:50.505037      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:51.203056 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:51.505351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:52.505853      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:53.210036 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:53.506401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:54.506659      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:55.219762 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:55.507213      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:56.507643      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:57.228741 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:57.507857      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:40:58.508090      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:40:59.238327 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:40:59.508393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:00.508704      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:01.252510 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:01.508805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:02.509239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:03.262182 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:03.509592      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:04.510599      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:05.274402 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:05.510690      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:06.511092      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:07.285075 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:07.511279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:08.511499      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:09.300131 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:09.511899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:10.512778      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:11.311024 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:11.513139      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:12.513456      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:13.323000 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:13.513781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:14.514036      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:15.333695 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:15.514531      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:16.514668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:17.345913 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:17.515661      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:18.515927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:19.358365 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:19.517045      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:20.517218      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:21.365661 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:21.518270      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:22.523573      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:23.375815 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:23.522284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:24.522708      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:25.390828 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:25.523225      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:26.523848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:27.397368 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:27.524635      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:28.525067      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:29.406379 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:29.526038      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:30.526422      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:31.415440 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:31.526878      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:32.527318      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:33.425911 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:33.527563      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:34.527743      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:35.435583 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:35.528467      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:36.528864      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:37.446321 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:37.528760      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:38.529247      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:39.454965 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:39.530121      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:40.530879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:41.467648 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:41.531869      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:42.532722      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:43.478543 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:43.532859      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:44.533126      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:45.497444 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:45.533773      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:46.534597      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:47.506651 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:47.534628      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:48.535038      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:49.517622 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:49.535955      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:50.536485      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:51.529771 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:51.537114      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:52.537407      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:53.538196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:53.543299 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:54.538515      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:55.540450      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:55.567020 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:56.539255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:57.539319      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:57.574620 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:41:58.539593      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:41:59.539804      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:41:59.582909 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:00.539949      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:01.540096      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:01.593008 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:02.540397      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:03.540637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:03.607365 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:04.541565      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:05.542333      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:05.618461 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:06.542480      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:07.542899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:07.627357 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:08.543532      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:09.543917      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:09.634292 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:10.544136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:11.544602      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:11.644055 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:12.544953      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:13.545528      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:13.663398 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:14.545477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:15.546228      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:15.676116 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:16.546015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:17.546523      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:17.685994 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:18.546935      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:19.548197      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:19.695196 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:20.548305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:21.548504      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:21.704911 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:22.548738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:23.549284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:23.720624 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:24.549435      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:25.549759      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:25.729386 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:26.550127      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:27.550745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:27.739394 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:28.550993      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:29.552254      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:29.749214 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:30.552827      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:31.553627      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:31.761863 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:32.554040      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:33.554536      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:33.772769 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:34.555075      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:35.555638      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:35.793186 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:36.555744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:37.556304      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:37.802454 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:38.556588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:39.556752      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:39.810781 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:40.556969      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:41.557172      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:41.821676 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:42.557562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:43.557847      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:43.838454 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:44.558886      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:45.559102      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:45.853955 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:46.559504      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:47.560036      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:47.872387 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:48.560075      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:49.561006      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:49.891104 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:50.561370      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:51.561335      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:51.902607 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:52.561566      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:53.561978      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:53.912952 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:54.563003      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:55.563135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:55.922141 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:56.563307      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:57.563467      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:57.929965 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:42:58.564696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:42:59.565926      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:42:59.938391 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:00.567804      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:01.567407      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:01.953383 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:02.567562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:03.568032      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:03.961116 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:04.568718      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:05.569603      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:05.971604 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:06.569762      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:07.570201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:07.980103 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:08.570326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:09.571305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:09.989712 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:10.572616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:11.573255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:11.999196 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:12.573431      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:13.573902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:14.011894 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:14.574460      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:15.575401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:16.022623 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:16.576340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:17.576756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:18.031112 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:18.576984      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:19.577922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:20.040802 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:20.578645      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:21.579099      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:22.047926 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:22.579261      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:23.580187      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:24.062265 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:24.581089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:25.581665      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:26.073752 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:26.582535      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:27.582981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:28.082050 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:28.583546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:29.584509      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:30.089856 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:30.585683      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:31.586063      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:32.101179 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:32.586196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:33.586761      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:34.112816 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:34.587636      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:35.588199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:36.122671 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:36.589161      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:37.589666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:38.131868 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:38.590554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:39.591487      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:40.141095 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:40.591598      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:41.592406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:42.150775 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:42.593150      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:43.593663      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:44.160330 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:44.594154      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:45.594716      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:46.171372 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:46.595817      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:47.596292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:48.181708 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:48.597011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:49.597927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:50.191827 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:50.598761      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:51.599270      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:52.201578 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:52.599576      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:53.600127      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:54.217694 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:54.600449      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:55.600686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:56.225912 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:56.601965      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:57.602246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:43:58.234720 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:43:58.603243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:43:59.603927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:00.245459 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:00.604931      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:01.605095      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:02.255105 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:02.605439      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:03.605954      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:04.263774 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:04.606823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:05.607314      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:06.279676 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:06.608284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:07.608811      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:08.290308 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:08.609017      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:09.609925      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:10.299320 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:10.610897      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:11.611849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:12.308626 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:12.612954      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:13.613479      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:14.320732 23 container_probe.go:1759] Get pod busybox-5f032fc0-a820-4f9c-8250-fbe34b20129f in namespace container-probe-1033
  E0512 15:44:14.613905      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:15.614223      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting the pod @ 05/12/25 15:44:16.322
  I0512 15:44:16.367457 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1033" for this suite. @ 05/12/25 15:44:16.382
• [243.557 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ValidatingAdmissionPolicy [Privileged:ClusterAdmin] should support ValidatingAdmissionPolicyBinding API operations [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/validatingadmissionpolicy.go:673
  STEP: Creating a kubernetes client @ 05/12/25 15:44:16.41
  I0512 15:44:16.410513 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename validating-admission-policy @ 05/12/25 15:44:16.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:44:16.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:44:16.466
  STEP: getting /apis @ 05/12/25 15:44:16.49
  STEP: getting /apis/admissionregistration.k8s.io @ 05/12/25 15:44:16.505
  STEP: getting /apis/admissionregistration.k8s.io/v1 @ 05/12/25 15:44:16.509
  STEP: creating @ 05/12/25 15:44:16.513
  STEP: getting @ 05/12/25 15:44:16.545
  STEP: listing @ 05/12/25 15:44:16.552
  STEP: watching @ 05/12/25 15:44:16.561
  I0512 15:44:16.561512 23 validatingadmissionpolicy.go:768] starting watch
  STEP: patching @ 05/12/25 15:44:16.564
  STEP: updating @ 05/12/25 15:44:16.576
  I0512 15:44:16.595354 23 validatingadmissionpolicy.go:796] waiting for watch events with expected annotations
  STEP: deleting @ 05/12/25 15:44:16.595
  E0512 15:44:16.615008      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting a collection @ 05/12/25 15:44:16.628
  I0512 15:44:16.672416 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "validating-admission-policy-1812" for this suite. @ 05/12/25 15:44:16.682
• [0.289 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:398
  STEP: Creating a kubernetes client @ 05/12/25 15:44:16.699
  I0512 15:44:16.699863 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 15:44:16.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:44:16.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:44:16.747
  STEP: Counting existing ResourceQuota @ 05/12/25 15:44:16.754
  E0512 15:44:17.615054      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:18.615219      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:19.615748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:20.616616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:21.616819      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/25 15:44:21.768
  STEP: Ensuring resource quota status is calculated @ 05/12/25 15:44:21.782
  E0512 15:44:22.617147      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:23.617741      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/12/25 15:44:23.797
  STEP: Ensuring resource quota status captures replication controller creation @ 05/12/25 15:44:23.872
  E0512 15:44:24.618050      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:25.618751      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/12/25 15:44:25.884
  STEP: Ensuring resource quota status released usage @ 05/12/25 15:44:25.901
  E0512 15:44:26.619505      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:27.620213      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:27.911489 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1097" for this suite. @ 05/12/25 15:44:27.925
• [11.238 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/12/25 15:44:27.941
  I0512 15:44:27.942011 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:44:27.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:44:27.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:44:27.99
  STEP: Creating secret with name secret-test-ba0d0d49-be4e-4735-bdc1-0dfe48f159cf @ 05/12/25 15:44:28
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:44:28.011
  E0512 15:44:28.620719      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:29.620836      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:30.621090      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:31.621710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:44:32.075
  I0512 15:44:32.082736 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-secrets-72784ded-a4e0-41ca-a328-092558f495a9 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:44:32.116
  I0512 15:44:32.152133 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9361" for this suite. @ 05/12/25 15:44:32.163
• [4.237 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:760
  STEP: Creating a kubernetes client @ 05/12/25 15:44:32.179
  I0512 15:44:32.179484 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:44:32.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:44:32.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:44:32.224
  STEP: creating service endpoint-test2 in namespace services-8383 @ 05/12/25 15:44:32.231
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8383 to expose endpoints map[] @ 05/12/25 15:44:32.255
  I0512 15:44:32.264893 23 service.go:4267] Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0512 15:44:32.622402      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:33.302754 23 service.go:4299] successfully validated that service endpoint-test2 in namespace services-8383 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8383 @ 05/12/25 15:44:33.302
  E0512 15:44:33.623312      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:34.623892      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8383 to expose endpoints map[pod1:[80]] @ 05/12/25 15:44:35.373
  I0512 15:44:35.402265 23 service.go:4299] successfully validated that service endpoint-test2 in namespace services-8383 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/12/25 15:44:35.402
  I0512 15:44:35.402860 23 resource.go:361] Creating new exec pod
  E0512 15:44:35.624045      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:36.624658      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:37.625216      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:38.441972 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-8383 exec execpodklpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0512 15:44:38.626091      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:38.772290 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  I0512 15:44:38.772389 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:44:38.772579 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-8383 exec execpodklpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.6.171 80'
  I0512 15:44:39.094490 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.6.171 80\nConnection to 10.233.6.171 80 port [tcp/http] succeeded!\n"
  I0512 15:44:39.094593 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-8383 @ 05/12/25 15:44:39.094
  E0512 15:44:39.626801      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:40.627950      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8383 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/12/25 15:44:41.135
  I0512 15:44:41.173623 23 service.go:4299] successfully validated that service endpoint-test2 in namespace services-8383 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/12/25 15:44:41.173
  E0512 15:44:41.628750      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:42.174845 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-8383 exec execpodklpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  I0512 15:44:42.467490 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  I0512 15:44:42.467680 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:44:42.467824 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-8383 exec execpodklpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.6.171 80'
  E0512 15:44:42.629924      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:42.775367 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.6.171 80\nConnection to 10.233.6.171 80 port [tcp/http] succeeded!\n"
  I0512 15:44:42.775461 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8383 @ 05/12/25 15:44:42.775
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8383 to expose endpoints map[pod2:[80]] @ 05/12/25 15:44:42.811
  I0512 15:44:42.873395 23 service.go:4299] successfully validated that service endpoint-test2 in namespace services-8383 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/12/25 15:44:42.873
  E0512 15:44:43.630841      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:43.874573 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-8383 exec execpodklpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  I0512 15:44:44.138121 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  I0512 15:44:44.138216 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:44:44.138364 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-8383 exec execpodklpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.6.171 80'
  I0512 15:44:44.447071 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.6.171 80\nConnection to 10.233.6.171 80 port [tcp/http] succeeded!\n"
  I0512 15:44:44.447179 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-8383 @ 05/12/25 15:44:44.447
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8383 to expose endpoints map[] @ 05/12/25 15:44:44.489
  I0512 15:44:44.517234 23 service.go:4299] successfully validated that service endpoint-test2 in namespace services-8383 exposes endpoints map[]
  I0512 15:44:44.574938 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8383" for this suite. @ 05/12/25 15:44:44.593
  E0512 15:44:44.631939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
• [12.457 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/12/25 15:44:44.637
  I0512 15:44:44.637630 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/12/25 15:44:44.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:44:44.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:44:44.706
  STEP: Setting up the test @ 05/12/25 15:44:44.714
  STEP: Creating hostNetwork=false pod @ 05/12/25 15:44:44.714
  E0512 15:44:45.632170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:46.633023      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 05/12/25 15:44:46.776
  E0512 15:44:47.633912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:48.634242      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Running the test @ 05/12/25 15:44:48.812
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/12/25 15:44:48.812
  I0512 15:44:48.812346 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:48.812408 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:48.813669 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:48.813809 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  I0512 15:44:48.965303 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:48.965769 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:48.965827 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:48.967291 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:48.967494 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  I0512 15:44:49.100201 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:49.100295 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:49.100325 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:49.102002 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:49.102197 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  I0512 15:44:49.233496 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:49.233592 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:49.233617 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:49.234811 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:49.234937 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  I0512 15:44:49.361964 23 exec_util.go:111] Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/12/25 15:44:49.362
  I0512 15:44:49.363271 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:49.363788 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:49.365685 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:49.366098 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  I0512 15:44:49.474861 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:49.474981 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:49.475027 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:49.476437 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:49.476641 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  I0512 15:44:49.593288 23 exec_util.go:111] Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/12/25 15:44:49.593
  I0512 15:44:49.593479 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:49.593551 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:49.594902 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:49.595092 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E0512 15:44:49.634843      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:49.735402 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:49.735592 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:49.735712 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:49.737622 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:49.737920 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  I0512 15:44:49.859759 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:49.860116 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:49.860391 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:49.861956 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:49.862414 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  I0512 15:44:50.014092 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:50.014228 23 exec_util.go:59] ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6350 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:44:50.014306 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:44:50.016231 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:44:50.016368 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6350/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  I0512 15:44:50.202899 23 exec_util.go:111] Exec stderr: ""
  I0512 15:44:50.204182 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-6350" for this suite. @ 05/12/25 15:44:50.224
• [5.614 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:643
  STEP: Creating a kubernetes client @ 05/12/25 15:44:50.254
  I0512 15:44:50.255052 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:44:50.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:44:50.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:44:50.33
  STEP: Setting up server cert @ 05/12/25 15:44:50.422
  E0512 15:44:50.635531      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:44:51.501
  STEP: Deploying the webhook pod @ 05/12/25 15:44:51.514
  STEP: Wait for the deployment to be ready @ 05/12/25 15:44:51.531
  I0512 15:44:51.547784 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 15:44:51.636100      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:52.637077      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:53.571879 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 15, 44, 51, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 44, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 15, 44, 51, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 15, 44, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5b9c4f9645\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 15:44:53.638294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:54.639228      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 15:44:55.582
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:44:55.607
  E0512 15:44:55.641278      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:44:56.608472 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0512 15:44:56.641150      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Listing all of the created validation webhooks @ 05/12/25 15:44:56.779
  STEP: Creating a configMap that should be mutated @ 05/12/25 15:44:56.828
  STEP: Deleting the collection of validation webhooks @ 05/12/25 15:44:56.945
  STEP: Creating a configMap that should not be mutated @ 05/12/25 15:44:57.083
  I0512 15:44:57.279751 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-55" for this suite. @ 05/12/25 15:44:57.303
  STEP: Destroying namespace "webhook-markers-2173" for this suite. @ 05/12/25 15:44:57.342
• [7.111 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance] [sig-apps, Slow, Conformance]
k8s.io/kubernetes/test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/12/25 15:44:57.365
  I0512 15:44:57.365836 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename cronjob @ 05/12/25 15:44:57.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:44:57.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:44:57.437
  STEP: Creating a ForbidConcurrent cronjob @ 05/12/25 15:44:57.457
  STEP: Ensuring a job is scheduled @ 05/12/25 15:44:57.477
  E0512 15:44:57.642356      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:58.643358      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:44:59.643751      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:00.644334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/12/25 15:45:01.487
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/12/25 15:45:01.496
  STEP: Ensuring no more jobs are scheduled @ 05/12/25 15:45:01.505
  STEP: Removing cronjob @ 05/12/25 15:45:01.511
  I0512 15:45:01.533418 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7475" for this suite. @ 05/12/25 15:45:01.553
• [4.206 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:310
  STEP: Creating a kubernetes client @ 05/12/25 15:45:01.572
  I0512 15:45:01.572327 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 15:45:01.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:45:01.638
  E0512 15:45:01.645397      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:45:01.648
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/12/25 15:45:01.659
  I0512 15:45:01.662050 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 15:45:02.645721      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:03.646545      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:04.647504      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:05.647939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:06.648008      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:07.649399      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:08.650387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:09.650552      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:10.651174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:11.652069      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:12.652696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:13.659466      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:14.660052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:15.660928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:16.661433      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:17.662456      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/12/25 15:45:18.309
  I0512 15:45:18.311020 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 15:45:18.662635      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:19.663240      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:20.663870      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:21.664767      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:22.665396      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:23.666323      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:24.666952      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:25.667247      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:26.668323      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:45:27.136032 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 15:45:27.668581      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:28.669445      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:29.670633      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:30.670724      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:31.671170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:32.672313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:33.672782      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:34.673334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:35.673341      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:36.673805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:37.674710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:38.675656      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:39.676335      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:40.677596      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:41.678632      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:42.680961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:43.681327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:44.681918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:45:44.829614 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3233" for this suite. @ 05/12/25 15:45:44.85
• [43.301 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/12/25 15:45:44.877
  I0512 15:45:44.877400 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:45:44.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:45:44.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:45:44.931
  STEP: Create set of pods @ 05/12/25 15:45:44.943
  I0512 15:45:44.972238 23 pods.go:871] created test-pod-1
  I0512 15:45:44.995477 23 pods.go:871] created test-pod-2
  I0512 15:45:45.019348 23 pods.go:871] created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/12/25 15:45:45.019
  E0512 15:45:45.682349      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:46.682944      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 05/12/25 15:45:47.106
  I0512 15:45:47.119848 23 pods.go:1140] Pod quantity 3 is different from expected quantity 0
  E0512 15:45:47.683299      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:45:48.122002 23 pods.go:1140] Pod quantity 3 is different from expected quantity 0
  E0512 15:45:48.683923      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:45:49.117537 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3152" for this suite. @ 05/12/25 15:45:49.13
• [4.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/service_accounts.go:80
  STEP: Creating a kubernetes client @ 05/12/25 15:45:49.149
  I0512 15:45:49.149273 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/25 15:45:49.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:45:49.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:45:49.201
  E0512 15:45:49.684789      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:50.685666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/12/25 15:45:51.26
  I0512 15:45:51.260839 23 kubectl_utils.go:203] Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6534 pod-service-account-d3c7162d-c0db-4c14-a457-e6986d83576f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/12/25 15:45:51.564
  I0512 15:45:51.564769 23 kubectl_utils.go:203] Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6534 pod-service-account-d3c7162d-c0db-4c14-a457-e6986d83576f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  E0512 15:45:51.685814      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/12/25 15:45:51.846
  I0512 15:45:51.846897 23 kubectl_utils.go:203] Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6534 pod-service-account-d3c7162d-c0db-4c14-a457-e6986d83576f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  I0512 15:45:52.131781 23 service_accounts.go:114] Got root ca configmap in namespace "svcaccounts-6534"
  I0512 15:45:52.136506 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6534" for this suite. @ 05/12/25 15:45:52.147
• [3.018 seconds]
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/init_container.go:459
  STEP: Creating a kubernetes client @ 05/12/25 15:45:52.166
  I0512 15:45:52.166623 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename init-container @ 05/12/25 15:45:52.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:45:52.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:45:52.217
  STEP: creating the pod @ 05/12/25 15:45:52.221
  I0512 15:45:52.221937 23 init_container.go:499] PodSpec: initContainers in spec.initContainers
  E0512 15:45:52.686166      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:53.686605      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:54.687110      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:55.688200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:45:55.727271 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6002" for this suite. @ 05/12/25 15:45:55.745
• [3.596 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/12/25 15:45:55.763
  I0512 15:45:55.763644 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-pred @ 05/12/25 15:45:55.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:45:55.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:45:55.826
  I0512 15:45:55.847349 23 helper.go:122] Waiting up to 1m0s for all (but 0) nodes to be ready
  I0512 15:45:55.882107 23 util.go:393] Waiting for terminating namespaces to be deleted...
  I0512 15:45:55.905994 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-0 before test
  I0512 15:45:55.921876 23 predicates.go:957] harbor-core-569d44cfd7-46qkt from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922004 23 predicates.go:959] 	Container core ready: true, restart count 1
  I0512 15:45:55.922043 23 predicates.go:957] harbor-database-0 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922076 23 predicates.go:959] 	Container database ready: true, restart count 0
  I0512 15:45:55.922108 23 predicates.go:957] harbor-redis-0 from harbor started at 2025-05-12 15:18:25 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922136 23 predicates.go:959] 	Container redis ready: true, restart count 0
  I0512 15:45:55.922165 23 predicates.go:957] kube-flannel-m6mdj from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922194 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:45:55.922226 23 predicates.go:957] kube-proxy-ksc6h from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922251 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:45:55.922282 23 predicates.go:957] metrics-server-c5b7b4dc-sb7sz from kube-system started at 2025-05-12 15:18:24 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922312 23 predicates.go:959] 	Container metrics-server ready: true, restart count 0
  I0512 15:45:55.922344 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-0 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922378 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:45:55.922418 23 predicates.go:957] nodelocaldns-fp5rs from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922444 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:45:55.922475 23 predicates.go:957] vsphere-csi-node-zmnvq from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.922505 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:45:55.922546 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:45:55.922577 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:45:55.922610 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-9znn4 from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:45:55.922639 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:45:55.922667 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:45:55.922698 23 predicates.go:957] vault-1 from vault started at 2025-05-12 14:25:29 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.922731 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:45:55.922758 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:45:55.922784 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:45:55.922820 23 predicates.go:957] vault-configurer-59545bd678-lmkzg from vault started at 2025-05-12 14:24:44 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.922849 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:45:55.922880 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-1 before test
  I0512 15:45:55.944199 23 predicates.go:957] pod-init-9b6b8071-57a3-42a8-bbfb-7f13247f8571 from init-container-6002 started at 2025-05-12 15:45:52 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.944613 23 predicates.go:959] 	Container run1 ready: false, restart count 0
  I0512 15:45:55.944815 23 predicates.go:957] kube-flannel-8bzqg from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.945097 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:45:55.945467 23 predicates.go:957] kube-proxy-g82r5 from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.945717 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:45:55.946364 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-1 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.947135 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:45:55.947758 23 predicates.go:957] nodelocaldns-v22ld from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.948441 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:45:55.948839 23 predicates.go:957] vsphere-csi-node-4z66x from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.949116 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:45:55.949692 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:45:55.950135 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:45:55.950596 23 predicates.go:957] sonobuoy from sonobuoy started at 2025-05-12 14:52:15 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.950910 23 predicates.go:959] 	Container kube-sonobuoy ready: true, restart count 0
  I0512 15:45:55.951134 23 predicates.go:957] sonobuoy-e2e-job-0883294174af4fef from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:45:55.951434 23 predicates.go:959] 	Container e2e ready: true, restart count 0
  I0512 15:45:55.951683 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:45:55.952054 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-b5spt from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:45:55.952492 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:45:55.953156 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:45:55.953452 23 predicates.go:957] pod-service-account-d3c7162d-c0db-4c14-a457-e6986d83576f from svcaccounts-6534 started at 2025-05-12 15:45:49 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.953604 23 predicates.go:959] 	Container test ready: true, restart count 0
  I0512 15:45:55.953750 23 predicates.go:957] vault-0 from vault started at 2025-05-12 15:18:55 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.953874 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:45:55.953988 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:45:55.954120 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:45:55.954264 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-2 before test
  I0512 15:45:55.978791 23 predicates.go:957] harbor-registry-5584d97487-9cst2 from harbor started at 2025-05-12 14:20:33 +0000 UTC (2 container statuses recorded)
  I0512 15:45:55.978873 23 predicates.go:959] 	Container registry ready: true, restart count 0
  I0512 15:45:55.978896 23 predicates.go:959] 	Container registryctl ready: true, restart count 0
  I0512 15:45:55.978918 23 predicates.go:957] kube-flannel-sfqr5 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.978934 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:45:55.978968 23 predicates.go:957] kube-proxy-95b2r from kube-system started at 2025-05-12 14:12:58 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.978983 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:45:55.979001 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-2 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.979017 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:45:55.979037 23 predicates.go:957] nodelocaldns-5lcd4 from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.979054 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:45:55.979072 23 predicates.go:957] vsphere-csi-node-sqm2v from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.979087 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:45:55.979102 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:45:55.979573 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:45:55.979616 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-qbt8k from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:45:55.979635 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:45:55.979651 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:45:55.979669 23 predicates.go:957] vault-2 from vault started at 2025-05-12 14:26:08 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.979684 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:45:55.979700 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:45:55.979715 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:45:55.979733 23 predicates.go:957] velero-5f4c979ccf-4c5mx from velero started at 2025-05-12 14:22:48 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.979748 23 predicates.go:959] 	Container velero ready: true, restart count 0
  I0512 15:45:55.979766 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-3 before test
  I0512 15:45:55.990427 23 predicates.go:957] harbor-jobservice-79c749f9bc-qm554 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.990506 23 predicates.go:959] 	Container jobservice ready: true, restart count 2
  I0512 15:45:55.990534 23 predicates.go:957] harbor-portal-757685fc6b-7smbd from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.990552 23 predicates.go:959] 	Container portal ready: true, restart count 0
  I0512 15:45:55.990570 23 predicates.go:957] kube-flannel-pglr4 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.990586 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 15:45:55.990604 23 predicates.go:957] kube-proxy-5nnvw from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.990619 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 15:45:55.990636 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-3 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.990655 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 15:45:55.990673 23 predicates.go:957] nodelocaldns-v58mx from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.990691 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 15:45:55.990712 23 predicates.go:957] vsphere-csi-node-blmnp from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.990728 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 15:45:55.990743 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 15:45:55.990775 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 15:45:55.990794 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-rbcss from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 15:45:55.990809 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 15:45:55.990825 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 15:45:55.990842 23 predicates.go:957] vault-3 from vault started at 2025-05-12 14:27:02 +0000 UTC (3 container statuses recorded)
  I0512 15:45:55.990857 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 15:45:55.990872 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 15:45:55.990887 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 15:45:55.990907 23 predicates.go:957] vault-operator-56c68d678-2gr95 from vault started at 2025-05-12 14:24:33 +0000 UTC (1 container statuses recorded)
  I0512 15:45:55.990922 23 predicates.go:959] 	Container vault-operator ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/12/25 15:45:55.991
  E0512 15:45:56.689022      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:57.689185      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/12/25 15:45:58.044
  STEP: Trying to apply a random label on the found node. @ 05/12/25 15:45:58.111
  STEP: verifying the node has the label kubernetes.io/e2e-e1838030-d98d-4a78-bae2-2396121c7abf 95 @ 05/12/25 15:45:58.152
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/12/25 15:45:58.169
  E0512 15:45:58.689524      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:45:59.690026      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.62.16.76 on the node which pod4 resides and expect not scheduled @ 05/12/25 15:46:00.222
  E0512 15:46:00.690333      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:01.690661      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:02.691159      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:03.691807      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:04.692659      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:05.693385      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:06.694361      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:07.695411      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:08.696327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:09.697148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:10.698073      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:11.698199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:12.698696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:13.699625      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:14.700606      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:15.701082      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:16.701719      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:17.702257      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:18.703089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:19.704019      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:20.705104      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:21.705629      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:22.705797      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:23.706451      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:24.706901      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:25.707383      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:26.708491      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:27.708979      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:28.710054      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:29.710886      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:30.711612      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:31.712344      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:32.712667      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:33.712780      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:34.713222      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:35.713664      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:36.714699      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:37.714930      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:38.715012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:39.715799      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:40.716035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:41.716559      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:42.718464      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:43.718153      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:44.719070      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:45.719329      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:46.719477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:47.719731      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:48.720508      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:49.721177      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:50.721840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:51.721908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:52.722452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:53.722515      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:54.723494      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:55.723743      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:56.723860      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:57.724020      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:58.724884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:46:59.725868      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:00.725903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:01.726416      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:02.727210      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:03.727425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:04.728686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:05.728887      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:06.729793      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:07.730304      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:08.731178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:09.732184      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:10.732229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:11.732690      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:12.732760      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:13.733325      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:14.733562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:15.733791      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:16.734880      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:17.734846      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:18.735794      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:19.735848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:20.736158      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:21.736168      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:22.737129      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:23.736951      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:24.737894      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:25.738352      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:26.738682      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:27.739176      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:28.739686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:29.740142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:30.740879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:31.741087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:32.741877      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:33.742053      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:34.742543      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:35.742848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:36.743231      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:37.744327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:38.744884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:39.745594      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:40.746074      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:41.746685      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:42.747033      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:43.747227      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:44.748216      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:45.748797      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:46.750052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:47.750370      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:48.750581      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:49.751168      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:50.751378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:51.751823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:52.752344      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:53.752875      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:54.753974      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:55.754809      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:56.754831      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:57.755093      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:58.755658      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:47:59.756078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:00.756452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:01.756645      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:02.756964      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:03.756919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:04.757839      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:05.758334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:06.758695      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:07.758939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:08.759227      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:09.759896      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:10.760206      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:11.761315      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:12.761541      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:13.762499      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:14.763350      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:15.763525      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:16.763912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:17.764110      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:18.764433      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:19.764694      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:20.765575      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:21.766028      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:22.766519      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:23.767086      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:24.768260      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:25.768997      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:26.769407      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:27.769969      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:28.770568      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:29.771185      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:30.771712      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:31.772652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:32.772840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:33.773206      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:34.773957      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:35.775515      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:36.776594      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:37.777025      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:38.777509      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:39.778333      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:40.778719      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:41.779028      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:42.779709      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:43.779789      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:44.779926      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:45.780955      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:46.781969      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:47.782429      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:48.782841      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:49.783081      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:50.783315      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:51.783548      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:52.784423      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:53.784837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:54.785474      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:55.785632      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:56.785732      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:57.786826      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:58.787401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:48:59.787626      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:00.788136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:01.788835      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:02.789342      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:03.790413      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:04.791237      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:05.791895      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:06.792269      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:07.792585      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:08.793440      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:09.793520      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:10.793656      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:11.794562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:12.794883      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:13.795404      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:14.795420      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:15.796087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:16.797029      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:17.797613      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:18.797796      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:19.797994      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:20.798601      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:21.798858      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:22.799508      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:23.799821      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:24.800947      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:25.801204      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:26.801730      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:27.801962      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:28.802225      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:29.802292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:30.802965      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:31.803063      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:32.804174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:33.805073      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:34.805036      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:35.805113      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:36.805349      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:37.806017      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:38.806246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:39.807112      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:40.807406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:41.807554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:42.807657      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:43.808718      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:44.809317      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:45.809383      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:46.809614      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:47.810628      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:48.811541      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:49.811651      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:50.812123      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:51.812243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:52.812780      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:53.813989      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:54.813994      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:55.815080      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:56.815237      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:57.816078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:58.816748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:49:59.817522      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:00.817695      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:01.818179      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:02.818371      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:03.819919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:04.820971      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:05.822072      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:06.822261      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:07.822489      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:08.823454      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:09.823770      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:10.824902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:11.825545      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:12.826147      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:13.826466      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:14.827415      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:15.828496      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:16.828792      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:17.829011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:18.830030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:19.831232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:20.831357      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:21.831676      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:22.831808      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:23.832778      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:24.833978      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:25.834066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:26.834221      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:27.834956      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:28.835295      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:29.835519      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:30.835811      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:31.836147      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:32.836691      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:33.837047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:34.837250      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:35.837715      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:36.838177      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:37.838901      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:38.839378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:39.840550      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:40.841243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:41.841887      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:42.842264      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:43.843186      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:44.844048      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:45.844173      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:46.845186      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:47.846246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:48.846875      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:49.847448      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:50.847961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:51.848273      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:52.848783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:53.848868      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:54.849826      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:55.850423      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:56.850928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:57.851664      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:58.853304      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:50:59.853538      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-e1838030-d98d-4a78-bae2-2396121c7abf off the node opscontrol-jaku1-worker-1 @ 05/12/25 15:51:00.245
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-e1838030-d98d-4a78-bae2-2396121c7abf @ 05/12/25 15:51:00.292
  I0512 15:51:00.313741 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-607" for this suite. @ 05/12/25 15:51:00.334
• [304.604 seconds]
------------------------------
S
------------------------------
[sig-node] Lease lease API should be available [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/lease.go:73
  STEP: Creating a kubernetes client @ 05/12/25 15:51:00.367
  I0512 15:51:00.367391 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename lease-test @ 05/12/25 15:51:00.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:00.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:00.461
  I0512 15:51:00.682733 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-3220" for this suite. @ 05/12/25 15:51:00.695
• [0.347 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:180
  STEP: Creating a kubernetes client @ 05/12/25 15:51:00.714
  I0512 15:51:00.714663 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:51:00.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:00.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:00.758
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/12/25 15:51:00.764
  E0512 15:51:00.854010      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:01.854116      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:02.855351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:03.855260      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:51:04.822
  I0512 15:51:04.831565 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-59577d95-0fa1-4d1c-892d-32348ef85f72 container test-container: <nil>
  E0512 15:51:04.855700      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete the pod @ 05/12/25 15:51:04.87
  I0512 15:51:04.911625 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9937" for this suite. @ 05/12/25 15:51:04.921
• [4.231 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/12/25 15:51:04.946
  I0512 15:51:04.946434 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename certificates @ 05/12/25 15:51:04.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:04.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:04.995
  E0512 15:51:05.855851      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: getting /apis @ 05/12/25 15:51:06.466
  STEP: getting /apis/certificates.k8s.io @ 05/12/25 15:51:06.476
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/12/25 15:51:06.478
  STEP: creating @ 05/12/25 15:51:06.48
  STEP: getting @ 05/12/25 15:51:06.516
  STEP: listing @ 05/12/25 15:51:06.524
  STEP: watching @ 05/12/25 15:51:06.53
  I0512 15:51:06.530613 23 certificates.go:316] starting watch
  STEP: patching @ 05/12/25 15:51:06.533
  STEP: updating @ 05/12/25 15:51:06.546
  I0512 15:51:06.562189 23 certificates.go:332] waiting for watch events with expected annotations
  I0512 15:51:06.563243 23 certificates.go:345] saw patched and updated annotations
  STEP: getting /approval @ 05/12/25 15:51:06.564
  STEP: patching /approval @ 05/12/25 15:51:06.57
  STEP: updating /approval @ 05/12/25 15:51:06.587
  STEP: getting /status @ 05/12/25 15:51:06.604
  STEP: patching /status @ 05/12/25 15:51:06.611
  STEP: updating /status @ 05/12/25 15:51:06.624
  STEP: deleting @ 05/12/25 15:51:06.643
  STEP: deleting a collection @ 05/12/25 15:51:06.669
  I0512 15:51:06.699819 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-745" for this suite. @ 05/12/25 15:51:06.707
• [1.777 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:48
  STEP: Creating a kubernetes client @ 05/12/25 15:51:06.725
  I0512 15:51:06.725247 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 15:51:06.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:06.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:06.774
  STEP: Creating a pod to test env composition @ 05/12/25 15:51:06.781
  E0512 15:51:06.856699      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:07.857417      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:08.857940      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:09.858767      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:51:10.833
  I0512 15:51:10.839794 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod var-expansion-14b67949-0065-4867-b6db-286aabd34041 container dapi-container: <nil>
  E0512 15:51:10.866635      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete the pod @ 05/12/25 15:51:10.873
  I0512 15:51:10.909137 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7893" for this suite. @ 05/12/25 15:51:10.921
• [4.212 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:55
  STEP: Creating a kubernetes client @ 05/12/25 15:51:10.938
  I0512 15:51:10.938281 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 15:51:10.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:10.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:10.99
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:51:10.998
  E0512 15:51:11.866777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:12.866921      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:13.867173      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:14.868031      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:51:15.058
  I0512 15:51:15.065642 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-752dd403-8cfa-4348-89f8-9368aab56330 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:51:15.082
  I0512 15:51:15.127553 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9767" for this suite. @ 05/12/25 15:51:15.142
• [4.224 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/12/25 15:51:15.161
  I0512 15:51:15.162022 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:51:15.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:15.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:15.212
  I0512 15:51:15.334414 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2753" for this suite. @ 05/12/25 15:51:15.346
• [0.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:150
  STEP: Creating a kubernetes client @ 05/12/25 15:51:15.362
  I0512 15:51:15.362358 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:51:15.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:15.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:15.415
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/12/25 15:51:15.423
  E0512 15:51:15.869141      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:16.869295      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:17.869885      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:18.870313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:51:19.475
  I0512 15:51:19.482193 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-fb32ba4f-bd34-46cd-8198-23144e1f2c48 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:51:19.5
  I0512 15:51:19.537334 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-929" for this suite. @ 05/12/25 15:51:19.551
• [4.204 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AggregatedDiscovery should support aggregated discovery interface for CRDs [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/aggregated_discovery.go:303
  STEP: Creating a kubernetes client @ 05/12/25 15:51:19.568
  I0512 15:51:19.568643 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename aggregateddiscovery @ 05/12/25 15:51:19.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:19.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:19.626
  I0512 15:51:19.633290 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 15:51:19.871280      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:20.871961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:21.872148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:22.873173      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:23.873534      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:24.873663      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:25.874733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:26.875399      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:51:27.779291 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregateddiscovery-5801" for this suite. @ 05/12/25 15:51:27.793
• [8.241 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/12/25 15:51:27.81
  I0512 15:51:27.810768 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-runtime @ 05/12/25 15:51:27.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:27.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:27.862
  STEP: create the container @ 05/12/25 15:51:27.871
  E0512 15:51:27.876328      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  W0512 15:51:27.900280      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/12/25 15:51:27.9
  E0512 15:51:28.876668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:29.876972      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:30.876962      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:31.877401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/12/25 15:51:31.949
  STEP: the container should be terminated @ 05/12/25 15:51:31.956
  STEP: the termination message should be set @ 05/12/25 15:51:31.956
  I0512 15:51:31.956512 23 runtime.go:167] Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/12/25 15:51:31.956
  I0512 15:51:31.989336 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2906" for this suite. @ 05/12/25 15:51:32
• [4.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/init_container.go:335
  STEP: Creating a kubernetes client @ 05/12/25 15:51:32.039
  I0512 15:51:32.039424 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename init-container @ 05/12/25 15:51:32.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:51:32.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:51:32.094
  STEP: creating the pod @ 05/12/25 15:51:32.101
  I0512 15:51:32.101615 23 init_container.go:374] PodSpec: initContainers in spec.initContainers
  E0512 15:51:32.880952      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:33.882116      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:34.882248      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:35.882650      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:36.882877      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:37.883331      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:38.883843      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:39.883979      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:40.884697      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:41.885197      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:42.885616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:43.886684      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:44.887375      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:45.887228      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:46.887476      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:47.887806      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:48.888125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:49.888947      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:50.889291      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:51.889732      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:52.890128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:53.890651      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:54.890908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:55.891438      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:56.892117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:57.892496      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:58.892808      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:51:59.893691      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:00.894077      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:01.894556      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:02.895053      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:03.895436      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:04.896595      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:05.896734      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:06.897122      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:07.897533      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:08.897748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:09.898848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:10.899221      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:11.899597      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:12.900011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:13.900658      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:14.901553      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:15.902004      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:16.902411      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:17.133068 23 init_container.go:432] init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-926f59c9-4018-4c2e-90de-98a4095d5407", GenerateName:"", Namespace:"init-container-3634", SelfLink:"", UID:"7386f7bb-d409-484b-8ee1-55495d810436", ResourceVersion:"41997", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 15, 51, 32, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"101586554"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 15, 51, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004c1e1e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 15, 52, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004c1e240), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-7gfxg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0010c7da0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil), Image:(*v1.ImageVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.36.1-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), RestartPolicy:(*v1.ContainerRestartPolicy)(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7gfxg", ReadOnly:true, RecursiveReadOnly:(*v1.RecursiveReadOnlyMode)(nil), MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.36.1-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), RestartPolicy:(*v1.ContainerRestartPolicy)(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7gfxg", ReadOnly:true, RecursiveReadOnly:(*v1.RecursiveReadOnlyMode)(nil), MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.10", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), RestartPolicy:(*v1.ContainerRestartPolicy)(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7gfxg", ReadOnly:true, RecursiveReadOnly:(*v1.RecursiveReadOnlyMode)(nil), MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027b69a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"opscontrol-jaku1-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003d52900), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027b6a20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027b6a40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027b6a48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027b6a4c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0034b25a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"PodReadyToStartContainers", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2025, time.May, 12, 15, 51, 33, 0, time.Local), Reason:"", Message:""}, v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2025, time.May, 12, 15, 51, 32, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2025, time.May, 12, 15, 51, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2025, time.May, 12, 15, 51, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2025, time.May, 12, 15, 51, 32, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.62.16.76", HostIPs:[]v1.HostIP{v1.HostIP{IP:"10.62.16.76"}}, PodIP:"10.233.69.174", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.69.174"}}, StartTime:time.Date(2025, time.May, 12, 15, 51, 32, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000040310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000404d0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.36.1-1", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9", ContainerID:"containerd://26d388430fcf80a7fe84432829ff8f4126186a909c6584763cc3faecc344b9dd", Started:(*bool)(0xc0027b6b1a), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil), VolumeMounts:[]v1.VolumeMountStatus{v1.VolumeMountStatus{Name:"kube-api-access-7gfxg", MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", ReadOnly:true, RecursiveReadOnly:(*v1.RecursiveReadOnlyMode)(0xc0034b25d0)}}, User:(*v1.ContainerUser)(nil), AllocatedResourcesStatus:[]v1.ResourceStatus(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0010c7e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.36.1-1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0027b6b2d), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil), VolumeMounts:[]v1.VolumeMountStatus{v1.VolumeMountStatus{Name:"kube-api-access-7gfxg", MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", ReadOnly:true, RecursiveReadOnly:(*v1.RecursiveReadOnlyMode)(0xc0034b25e0)}}, User:(*v1.ContainerUser)(nil), AllocatedResourcesStatus:[]v1.ResourceStatus(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0010c7de0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.10", ImageID:"", ContainerID:"", Started:(*bool)(0xc0027b6acf), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil), VolumeMounts:[]v1.VolumeMountStatus{v1.VolumeMountStatus{Name:"kube-api-access-7gfxg", MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", ReadOnly:true, RecursiveReadOnly:(*v1.RecursiveReadOnlyMode)(0xc0034b25b0)}}, User:(*v1.ContainerUser)(nil), AllocatedResourcesStatus:[]v1.ResourceStatus(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:"", ResourceClaimStatuses:[]v1.PodResourceClaimStatus(nil)}}
  I0512 15:52:17.133473 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3634" for this suite. @ 05/12/25 15:52:17.147
• [45.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [sig-api-machinery, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/namespace.go:254
  STEP: Creating a kubernetes client @ 05/12/25 15:52:17.165
  I0512 15:52:17.165911 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename namespaces @ 05/12/25 15:52:17.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:52:17.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:52:17.221
  STEP: Creating a test namespace @ 05/12/25 15:52:17.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:52:17.306
  STEP: Creating a service in the namespace @ 05/12/25 15:52:17.321
  STEP: Deleting the namespace @ 05/12/25 15:52:17.338
  STEP: Waiting for the namespace to be removed. @ 05/12/25 15:52:17.362
  E0512 15:52:17.903035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:18.904022      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:19.904189      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:20.905620      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:21.905777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:22.907004      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:23.907588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/12/25 15:52:24.373
  STEP: Verifying there is no service in the namespace @ 05/12/25 15:52:24.434
  I0512 15:52:24.442946 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9693" for this suite. @ 05/12/25 15:52:24.454
  STEP: Destroying namespace "nsdeletetest-8235" for this suite. @ 05/12/25 15:52:24.473
  I0512 15:52:24.482803 23 framework.go:370] Namespace nsdeletetest-8235 was already deleted
  STEP: Destroying namespace "nsdeletetest-8471" for this suite. @ 05/12/25 15:52:24.483
• [7.337 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:237
  STEP: Creating a kubernetes client @ 05/12/25 15:52:24.502
  I0512 15:52:24.502637 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 15:52:24.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:52:24.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:52:24.551
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:52:24.56
  E0512 15:52:24.907879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:25.908973      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:26.910051      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:27.910739      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:52:28.628
  I0512 15:52:28.638185 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-3a12138a-bc8c-414f-a463-1180a35e501f container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:52:28.654
  I0512 15:52:28.695657 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8610" for this suite. @ 05/12/25 15:52:28.708
• [4.225 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:443
  STEP: Creating a kubernetes client @ 05/12/25 15:52:28.731
  I0512 15:52:28.731546 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 15:52:28.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:52:28.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:52:28.796
  STEP: set up a multi version CRD @ 05/12/25 15:52:28.802
  I0512 15:52:28.804373 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 15:52:28.911286      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:29.911646      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:30.911763      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:31.912445      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:32.913143      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:33.914179      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:34.914914      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:35.915453      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:36.915616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:37.916713      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:38.917101      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:39.917796      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 05/12/25 15:52:40.858
  STEP: check the unserved version gets removed @ 05/12/25 15:52:40.899
  E0512 15:52:40.918425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:41.918951      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:42.919734      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/12/25 15:52:43.835
  E0512 15:52:43.920487      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:44.921033      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:45.921142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:46.921263      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:47.922377      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:48.923409      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:49.563105 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6985" for this suite. @ 05/12/25 15:52:49.587
• [20.877 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [sig-apps, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 05/12/25 15:52:49.609
  I0512 15:52:49.609511 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename daemonsets @ 05/12/25 15:52:49.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:52:49.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:52:49.677
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/12/25 15:52:49.765
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/25 15:52:49.786
  I0512 15:52:49.833890 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:49.834105 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:49.834258 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:49.862045 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:52:49.862170 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  E0512 15:52:49.924423      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:50.806221 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:50.806730 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:50.807067 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:50.823408 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:52:50.823676 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  E0512 15:52:50.924727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:51.800299 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:51.800466 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:51.800599 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:51.809784 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:52:51.809855 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/12/25 15:52:51.815
  I0512 15:52:51.911020 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:51.911945 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:51.912725 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 15:52:51.924907      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:51.925463 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:52:51.926162 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  I0512 15:52:52.864320 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:52.865521 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:52.866037 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:52.876894 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 3
  I0512 15:52:52.877266 23 fixtures.go:130] Node opscontrol-jaku1-worker-0 is running 0 daemon pod, expected 1
  E0512 15:52:52.925637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:53.865445 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:53.865585 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:53.865666 23 fixtures.go:89] DaemonSet pods can't tolerate node opscontrol-jaku1-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  I0512 15:52:53.897065 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 4
  I0512 15:52:53.897176 23 fixtures.go:135] Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/12/25 15:52:53.897
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/25 15:52:53.911
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6431, will wait for the garbage collector to delete the pods @ 05/12/25 15:52:53.911
  E0512 15:52:53.925974      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:53.992835 23 resources.go:139] Deleting DaemonSet.extensions daemon-set took: 22.159749ms
  I0512 15:52:54.193636 23 resources.go:163] Terminating DaemonSet.extensions daemon-set pods took: 200.796917ms
  E0512 15:52:54.927029      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:55.504070 23 fixtures.go:125] Number of nodes with available pods controlled by daemonset daemon-set: 0
  I0512 15:52:55.504141 23 fixtures.go:135] Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  I0512 15:52:55.513163 23 daemon_set.go:135] daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42335"},"items":null}

  I0512 15:52:55.522972 23 daemon_set.go:140] pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42335"},"items":null}

  I0512 15:52:55.579725 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6431" for this suite. @ 05/12/25 15:52:55.592
• [6.004 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should reject mutating webhook configurations with invalid match conditions [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:838
  STEP: Creating a kubernetes client @ 05/12/25 15:52:55.614
  I0512 15:52:55.614642 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:52:55.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:52:55.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:52:55.671
  STEP: Setting up server cert @ 05/12/25 15:52:55.77
  E0512 15:52:55.928037      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:52:56.871
  STEP: Deploying the webhook pod @ 05/12/25 15:52:56.889
  STEP: Wait for the deployment to be ready @ 05/12/25 15:52:56.918
  E0512 15:52:56.928640      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:52:56.957128 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 15:52:57.928915      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:52:58.928964      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 15:52:58.985
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:52:59.027
  E0512 15:52:59.930131      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:00.028745 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: creating a mutating webhook with match conditions @ 05/12/25 15:53:00.045
  I0512 15:53:00.216656 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1211" for this suite. @ 05/12/25 15:53:00.254
  STEP: Destroying namespace "webhook-markers-1375" for this suite. @ 05/12/25 15:53:00.297
• [4.727 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:116
  STEP: Creating a kubernetes client @ 05/12/25 15:53:00.373
  I0512 15:53:00.373925 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 15:53:00.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:00.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:00.446
  STEP: Creating a pod to test substitution in volume subpath @ 05/12/25 15:53:00.457
  E0512 15:53:00.930807      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:01.931588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:02.932801      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:03.933919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:53:04.512
  I0512 15:53:04.517610 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod var-expansion-96d5b208-6708-4ed1-815a-12a5e620f38e container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 15:53:04.546
  I0512 15:53:04.576254 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5264" for this suite. @ 05/12/25 15:53:04.585
• [4.224 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/12/25 15:53:04.603
  I0512 15:53:04.603743 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:53:04.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:04.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:04.647
  STEP: Creating projection with secret that has name projected-secret-test-map-d8f88a29-ca14-4d99-9667-f0ec900f185a @ 05/12/25 15:53:04.651
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:53:04.662
  E0512 15:53:04.934001      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:05.934303      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:06.935100      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:07.935401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:53:08.709
  I0512 15:53:08.719518 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-secrets-939accf7-a592-4e24-99a8-6ae31467fc76 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:53:08.736
  I0512 15:53:08.773685 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-408" for this suite. @ 05/12/25 15:53:08.785
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/12/25 15:53:08.815
  I0512 15:53:08.815816 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:53:08.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:08.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:08.872
  E0512 15:53:08.936136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating secret with name secret-test-3811955c-f229-4892-b99a-7f93dd617d27 @ 05/12/25 15:53:08.985
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:53:08.998
  E0512 15:53:09.936489      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:10.937605      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:11.937840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:12.938015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:53:13.058
  I0512 15:53:13.065867 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-secrets-d8dd786b-18aa-4e65-af29-9ce029de5ef5 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:53:13.083
  I0512 15:53:13.117811 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7625" for this suite. @ 05/12/25 15:53:13.128
  STEP: Destroying namespace "secret-namespace-4826" for this suite. @ 05/12/25 15:53:13.142
• [4.341 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance] [sig-network, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/network/networking.go:125
  STEP: Creating a kubernetes client @ 05/12/25 15:53:13.157
  I0512 15:53:13.157865 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/25 15:53:13.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:13.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:13.195
  STEP: Performing setup for networking test in namespace pod-network-test-2413 @ 05/12/25 15:53:13.201
  STEP: creating a selector @ 05/12/25 15:53:13.201
  STEP: Creating the service pods in kubernetes @ 05/12/25 15:53:13.201
  I0512 15:53:13.201446 23 helper.go:48] Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0512 15:53:13.941893      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:14.942458      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:15.943762      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:16.944756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:17.945686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:18.946430      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:19.946522      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:20.946748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:21.946967      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:22.947152      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:23.947671      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:24.947879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:25.948101      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:26.948255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:27.948756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:28.948899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:29.949842      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:30.950283      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:31.950806      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:32.951718      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:33.951950      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:34.952052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/12/25 15:53:35.503
  E0512 15:53:35.952200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:36.952778      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:37.589702 23 utils.go:803] Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  I0512 15:53:37.589771 23 utils.go:496] Going to poll 10.233.68.115 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:53:37.601672 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.68.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2413 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:53:37.602003 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:53:37.603545 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:53:37.603940 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2413/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.68.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0512 15:53:37.952702      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:38.736034 23 utils.go:513] Found all 1 expected endpoints: [netserver-0]
  I0512 15:53:38.736138 23 utils.go:496] Going to poll 10.233.69.181 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:53:38.745016 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.69.181 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2413 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:53:38.745100 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:53:38.746238 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:53:38.746432 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2413/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.69.181+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0512 15:53:38.953779      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:39.882050 23 utils.go:513] Found all 1 expected endpoints: [netserver-1]
  I0512 15:53:39.883357 23 utils.go:496] Going to poll 10.233.70.59 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:53:39.891937 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.70.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2413 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:53:39.892335 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:53:39.894042 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:53:39.894676 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2413/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.70.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0512 15:53:39.954464      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:40.954195      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:41.020275 23 utils.go:513] Found all 1 expected endpoints: [netserver-2]
  I0512 15:53:41.020483 23 utils.go:496] Going to poll 10.233.67.52 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  I0512 15:53:41.027800 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.67.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2413 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 15:53:41.027890 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 15:53:41.029413 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 15:53:41.029579 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2413/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.67.52+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0512 15:53:41.954202      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:42.145173 23 utils.go:513] Found all 1 expected endpoints: [netserver-3]
  I0512 15:53:42.145673 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2413" for this suite. @ 05/12/25 15:53:42.156
• [29.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/12/25 15:53:42.18
  I0512 15:53:42.180922 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svc-latency @ 05/12/25 15:53:42.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:42.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:42.227
  I0512 15:53:42.235923 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-2200 @ 05/12/25 15:53:42.238
  I0512 15:53:42.256873      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2200, replica count: 1
  E0512 15:53:42.955066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:43.309493      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0512 15:53:43.955532      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:44.310431      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 15:53:44.449141 23 service_latency.go:356] Created: latency-svc-xtq54
  I0512 15:53:44.457458 23 service_latency.go:363] Got endpoints: latency-svc-xtq54 [46.858888ms]
  I0512 15:53:44.512199 23 service_latency.go:356] Created: latency-svc-wzbf4
  I0512 15:53:44.531885 23 service_latency.go:363] Got endpoints: latency-svc-wzbf4 [72.560082ms]
  I0512 15:53:44.541744 23 service_latency.go:356] Created: latency-svc-6tk8m
  I0512 15:53:44.579619 23 service_latency.go:356] Created: latency-svc-ptxrb
  I0512 15:53:44.605059 23 service_latency.go:363] Got endpoints: latency-svc-6tk8m [145.944483ms]
  I0512 15:53:44.626688 23 service_latency.go:363] Got endpoints: latency-svc-ptxrb [166.364952ms]
  I0512 15:53:44.650479 23 service_latency.go:356] Created: latency-svc-44hzt
  I0512 15:53:44.668891 23 service_latency.go:363] Got endpoints: latency-svc-44hzt [209.121768ms]
  I0512 15:53:44.678229 23 service_latency.go:356] Created: latency-svc-7zdqt
  I0512 15:53:44.693346 23 service_latency.go:363] Got endpoints: latency-svc-7zdqt [233.458623ms]
  I0512 15:53:44.707836 23 service_latency.go:356] Created: latency-svc-xth96
  I0512 15:53:44.723350 23 service_latency.go:356] Created: latency-svc-bjb4c
  I0512 15:53:44.737025 23 service_latency.go:363] Got endpoints: latency-svc-xth96 [278.537055ms]
  I0512 15:53:44.738582 23 service_latency.go:363] Got endpoints: latency-svc-bjb4c [280.147647ms]
  I0512 15:53:44.757876 23 service_latency.go:356] Created: latency-svc-27jvb
  I0512 15:53:44.781104 23 service_latency.go:363] Got endpoints: latency-svc-27jvb [321.779267ms]
  I0512 15:53:44.785903 23 service_latency.go:356] Created: latency-svc-zlhzb
  I0512 15:53:44.810751 23 service_latency.go:356] Created: latency-svc-lg9gj
  I0512 15:53:44.813965 23 service_latency.go:363] Got endpoints: latency-svc-zlhzb [355.430338ms]
  I0512 15:53:44.826617 23 service_latency.go:356] Created: latency-svc-z9vsq
  I0512 15:53:44.838382 23 service_latency.go:363] Got endpoints: latency-svc-lg9gj [378.802006ms]
  I0512 15:53:44.852737 23 service_latency.go:363] Got endpoints: latency-svc-z9vsq [392.779672ms]
  I0512 15:53:44.854908 23 service_latency.go:356] Created: latency-svc-msrk7
  I0512 15:53:44.867964 23 service_latency.go:363] Got endpoints: latency-svc-msrk7 [407.826732ms]
  I0512 15:53:44.877765 23 service_latency.go:356] Created: latency-svc-vkg7m
  I0512 15:53:44.893263 23 service_latency.go:363] Got endpoints: latency-svc-vkg7m [434.200006ms]
  I0512 15:53:44.896160 23 service_latency.go:356] Created: latency-svc-pk42h
  I0512 15:53:44.915774 23 service_latency.go:356] Created: latency-svc-59znl
  I0512 15:53:44.925081 23 service_latency.go:363] Got endpoints: latency-svc-pk42h [466.303875ms]
  I0512 15:53:44.947854 23 service_latency.go:356] Created: latency-svc-sk79n
  I0512 15:53:44.951245 23 service_latency.go:363] Got endpoints: latency-svc-59znl [491.13003ms]
  E0512 15:53:44.956328      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:44.971084 23 service_latency.go:356] Created: latency-svc-r2h46
  I0512 15:53:44.996461 23 service_latency.go:363] Got endpoints: latency-svc-sk79n [464.476848ms]
  I0512 15:53:45.000743 23 service_latency.go:356] Created: latency-svc-scb49
  I0512 15:53:45.014009 23 service_latency.go:363] Got endpoints: latency-svc-r2h46 [408.073326ms]
  I0512 15:53:45.026737 23 service_latency.go:356] Created: latency-svc-qmdtj
  I0512 15:53:45.028498 23 service_latency.go:363] Got endpoints: latency-svc-scb49 [401.714343ms]
  I0512 15:53:45.052565 23 service_latency.go:363] Got endpoints: latency-svc-qmdtj [383.513906ms]
  I0512 15:53:45.274842 23 service_latency.go:356] Created: latency-svc-wxhtd
  I0512 15:53:45.286188 23 service_latency.go:356] Created: latency-svc-fzk4v
  I0512 15:53:45.293098 23 service_latency.go:356] Created: latency-svc-j29cp
  I0512 15:53:45.301958 23 service_latency.go:356] Created: latency-svc-bth4z
  I0512 15:53:45.302557 23 service_latency.go:356] Created: latency-svc-nhkwm
  I0512 15:53:45.302550 23 service_latency.go:356] Created: latency-svc-ldkhp
  I0512 15:53:45.304057 23 service_latency.go:356] Created: latency-svc-7blrx
  I0512 15:53:45.304239 23 service_latency.go:356] Created: latency-svc-9thmq
  I0512 15:53:45.305223 23 service_latency.go:356] Created: latency-svc-m8wwz
  I0512 15:53:45.306542 23 service_latency.go:356] Created: latency-svc-k9wq5
  I0512 15:53:45.333207 23 service_latency.go:356] Created: latency-svc-t8kck
  I0512 15:53:45.334191 23 service_latency.go:356] Created: latency-svc-ckkvf
  I0512 15:53:45.334282 23 service_latency.go:356] Created: latency-svc-t7txc
  I0512 15:53:45.334368 23 service_latency.go:356] Created: latency-svc-f9q9h
  I0512 15:53:45.334669 23 service_latency.go:356] Created: latency-svc-8kdsd
  I0512 15:53:45.359516 23 service_latency.go:363] Got endpoints: latency-svc-wxhtd [330.884199ms]
  I0512 15:53:45.359886 23 service_latency.go:363] Got endpoints: latency-svc-m8wwz [363.321949ms]
  I0512 15:53:45.360358 23 service_latency.go:363] Got endpoints: latency-svc-j29cp [307.690295ms]
  I0512 15:53:45.360686 23 service_latency.go:363] Got endpoints: latency-svc-9thmq [521.827738ms]
  I0512 15:53:45.360768 23 service_latency.go:363] Got endpoints: latency-svc-fzk4v [579.54821ms]
  I0512 15:53:45.385318 23 service_latency.go:363] Got endpoints: latency-svc-bth4z [491.930371ms]
  I0512 15:53:45.405155 23 service_latency.go:363] Got endpoints: latency-svc-k9wq5 [666.475035ms]
  I0512 15:53:45.423733 23 service_latency.go:363] Got endpoints: latency-svc-t8kck [730.285035ms]
  I0512 15:53:45.443255 23 service_latency.go:363] Got endpoints: latency-svc-ldkhp [491.518661ms]
  I0512 15:53:45.443695 23 service_latency.go:363] Got endpoints: latency-svc-7blrx [590.874322ms]
  I0512 15:53:45.443659 23 service_latency.go:363] Got endpoints: latency-svc-nhkwm [518.499841ms]
  I0512 15:53:45.462962 23 service_latency.go:363] Got endpoints: latency-svc-f9q9h [648.723893ms]
  I0512 15:53:45.463536 23 service_latency.go:356] Created: latency-svc-442wd
  I0512 15:53:45.475597 23 service_latency.go:363] Got endpoints: latency-svc-t7txc [461.496758ms]
  I0512 15:53:45.498771 23 service_latency.go:363] Got endpoints: latency-svc-8kdsd [630.715012ms]
  I0512 15:53:45.507161 23 service_latency.go:363] Got endpoints: latency-svc-ckkvf [770.033605ms]
  I0512 15:53:45.507568 23 service_latency.go:356] Created: latency-svc-x2bs6
  I0512 15:53:45.526598 23 service_latency.go:363] Got endpoints: latency-svc-442wd [166.615321ms]
  I0512 15:53:45.539012 23 service_latency.go:356] Created: latency-svc-grl6b
  I0512 15:53:45.560352 23 service_latency.go:363] Got endpoints: latency-svc-grl6b [200.748596ms]
  I0512 15:53:45.560564 23 service_latency.go:363] Got endpoints: latency-svc-x2bs6 [199.700333ms]
  I0512 15:53:45.576322 23 service_latency.go:356] Created: latency-svc-zjqxk
  I0512 15:53:45.616432 23 service_latency.go:356] Created: latency-svc-mrv7x
  I0512 15:53:45.622228 23 service_latency.go:363] Got endpoints: latency-svc-zjqxk [261.820561ms]
  I0512 15:53:45.655535 23 service_latency.go:363] Got endpoints: latency-svc-mrv7x [294.803229ms]
  I0512 15:53:45.679705 23 service_latency.go:356] Created: latency-svc-n9tfz
  I0512 15:53:45.717935 23 service_latency.go:363] Got endpoints: latency-svc-n9tfz [332.530355ms]
  I0512 15:53:45.719468 23 service_latency.go:356] Created: latency-svc-vzcnv
  I0512 15:53:45.730193 23 service_latency.go:356] Created: latency-svc-kk2zn
  I0512 15:53:45.742739 23 service_latency.go:363] Got endpoints: latency-svc-vzcnv [337.437507ms]
  I0512 15:53:45.749362 23 service_latency.go:363] Got endpoints: latency-svc-kk2zn [325.538136ms]
  I0512 15:53:45.764935 23 service_latency.go:356] Created: latency-svc-sws97
  I0512 15:53:45.776461 23 service_latency.go:363] Got endpoints: latency-svc-sws97 [333.113669ms]
  I0512 15:53:45.795101 23 service_latency.go:356] Created: latency-svc-6gkht
  I0512 15:53:45.807899 23 service_latency.go:363] Got endpoints: latency-svc-6gkht [363.897878ms]
  I0512 15:53:45.808602 23 service_latency.go:356] Created: latency-svc-fnmcf
  I0512 15:53:45.821234 23 service_latency.go:363] Got endpoints: latency-svc-fnmcf [377.264385ms]
  I0512 15:53:45.834991 23 service_latency.go:356] Created: latency-svc-srdmx
  I0512 15:53:45.840779 23 service_latency.go:356] Created: latency-svc-xn74s
  I0512 15:53:45.844720 23 service_latency.go:363] Got endpoints: latency-svc-srdmx [381.664491ms]
  I0512 15:53:45.855789 23 service_latency.go:356] Created: latency-svc-gqnzk
  I0512 15:53:45.861046 23 service_latency.go:363] Got endpoints: latency-svc-xn74s [385.253544ms]
  I0512 15:53:45.879270 23 service_latency.go:363] Got endpoints: latency-svc-gqnzk [380.389924ms]
  E0512 15:53:45.956854      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:45.984202 23 service_latency.go:356] Created: latency-svc-jpn25
  I0512 15:53:45.985136 23 service_latency.go:356] Created: latency-svc-m56bb
  I0512 15:53:45.985353 23 service_latency.go:356] Created: latency-svc-xqdsh
  I0512 15:53:45.985574 23 service_latency.go:356] Created: latency-svc-k6z97
  I0512 15:53:45.987829 23 service_latency.go:356] Created: latency-svc-vbtwl
  I0512 15:53:45.988085 23 service_latency.go:356] Created: latency-svc-tdczt
  I0512 15:53:45.988283 23 service_latency.go:356] Created: latency-svc-5tngw
  I0512 15:53:45.989409 23 service_latency.go:356] Created: latency-svc-bzjvw
  I0512 15:53:46.000144 23 service_latency.go:356] Created: latency-svc-86t96
  I0512 15:53:46.010700 23 service_latency.go:356] Created: latency-svc-7gpf5
  I0512 15:53:46.011744 23 service_latency.go:356] Created: latency-svc-7n822
  I0512 15:53:46.012669 23 service_latency.go:356] Created: latency-svc-vlcdd
  I0512 15:53:46.020451 23 service_latency.go:356] Created: latency-svc-ld862
  I0512 15:53:46.020717 23 service_latency.go:356] Created: latency-svc-t8qrw
  I0512 15:53:46.021265 23 service_latency.go:356] Created: latency-svc-jf5cv
  I0512 15:53:46.034910 23 service_latency.go:363] Got endpoints: latency-svc-7n822 [285.475682ms]
  I0512 15:53:46.043181 23 service_latency.go:363] Got endpoints: latency-svc-tdczt [299.672244ms]
  I0512 15:53:46.043688 23 service_latency.go:363] Got endpoints: latency-svc-m56bb [516.475885ms]
  I0512 15:53:46.043970 23 service_latency.go:363] Got endpoints: latency-svc-vbtwl [182.850757ms]
  I0512 15:53:46.044389 23 service_latency.go:363] Got endpoints: latency-svc-vlcdd [199.220758ms]
  I0512 15:53:46.090511 23 service_latency.go:363] Got endpoints: latency-svc-t8qrw [372.481843ms]
  I0512 15:53:46.090524 23 service_latency.go:363] Got endpoints: latency-svc-7gpf5 [468.217838ms]
  I0512 15:53:46.094684 23 service_latency.go:356] Created: latency-svc-qnbct
  I0512 15:53:46.097450 23 service_latency.go:356] Created: latency-svc-zhvzg
  I0512 15:53:46.137688 23 service_latency.go:363] Got endpoints: latency-svc-xqdsh [361.113818ms]
  I0512 15:53:46.138054 23 service_latency.go:363] Got endpoints: latency-svc-bzjvw [258.145279ms]
  I0512 15:53:46.138132 23 service_latency.go:363] Got endpoints: latency-svc-jf5cv [316.812993ms]
  I0512 15:53:46.138211 23 service_latency.go:363] Got endpoints: latency-svc-ld862 [630.974852ms]
  I0512 15:53:46.138547 23 service_latency.go:363] Got endpoints: latency-svc-k6z97 [578.110417ms]
  I0512 15:53:46.157823 23 service_latency.go:363] Got endpoints: latency-svc-86t96 [502.203691ms]
  I0512 15:53:46.158451 23 service_latency.go:363] Got endpoints: latency-svc-5tngw [597.646736ms]
  I0512 15:53:46.161195 23 service_latency.go:356] Created: latency-svc-9vfdz
  I0512 15:53:46.176387 23 service_latency.go:356] Created: latency-svc-69zmx
  I0512 15:53:46.179471 23 service_latency.go:363] Got endpoints: latency-svc-jpn25 [371.216851ms]
  I0512 15:53:46.194492 23 service_latency.go:356] Created: latency-svc-tbtz7
  I0512 15:53:46.202549 23 service_latency.go:356] Created: latency-svc-vp4sb
  I0512 15:53:46.215298 23 service_latency.go:356] Created: latency-svc-cx9c7
  I0512 15:53:46.218224 23 service_latency.go:363] Got endpoints: latency-svc-qnbct [183.238997ms]
  I0512 15:53:46.236920 23 service_latency.go:356] Created: latency-svc-fmpwq
  I0512 15:53:46.247974 23 service_latency.go:356] Created: latency-svc-tw5kb
  I0512 15:53:46.260754 23 service_latency.go:356] Created: latency-svc-jxnt5
  I0512 15:53:46.264478 23 service_latency.go:363] Got endpoints: latency-svc-zhvzg [221.219856ms]
  I0512 15:53:46.271784 23 service_latency.go:356] Created: latency-svc-nl7mp
  I0512 15:53:46.289523 23 service_latency.go:356] Created: latency-svc-hshw8
  I0512 15:53:46.295403 23 service_latency.go:356] Created: latency-svc-5rb64
  I0512 15:53:46.316660 23 service_latency.go:363] Got endpoints: latency-svc-9vfdz [272.270158ms]
  I0512 15:53:46.324834 23 service_latency.go:356] Created: latency-svc-x8cvz
  I0512 15:53:46.356021 23 service_latency.go:356] Created: latency-svc-fprwm
  I0512 15:53:46.363323 23 service_latency.go:363] Got endpoints: latency-svc-69zmx [319.250351ms]
  I0512 15:53:46.376297 23 service_latency.go:356] Created: latency-svc-smbht
  I0512 15:53:46.388319 23 service_latency.go:356] Created: latency-svc-kdrdk
  I0512 15:53:46.407936 23 service_latency.go:363] Got endpoints: latency-svc-tbtz7 [363.496401ms]
  I0512 15:53:46.423401 23 service_latency.go:356] Created: latency-svc-s5b8f
  I0512 15:53:46.425700 23 service_latency.go:356] Created: latency-svc-hd6z5
  I0512 15:53:46.433987 23 service_latency.go:356] Created: latency-svc-zxsn6
  I0512 15:53:46.460979 23 service_latency.go:363] Got endpoints: latency-svc-vp4sb [369.97211ms]
  I0512 15:53:46.482086 23 service_latency.go:356] Created: latency-svc-v7nct
  I0512 15:53:46.508828 23 service_latency.go:363] Got endpoints: latency-svc-cx9c7 [418.240207ms]
  I0512 15:53:46.526852 23 service_latency.go:356] Created: latency-svc-slwd6
  I0512 15:53:46.560092 23 service_latency.go:363] Got endpoints: latency-svc-fmpwq [421.920948ms]
  I0512 15:53:46.590089 23 service_latency.go:356] Created: latency-svc-fpnlp
  I0512 15:53:46.612932 23 service_latency.go:363] Got endpoints: latency-svc-tw5kb [475.15995ms]
  I0512 15:53:46.631769 23 service_latency.go:356] Created: latency-svc-qsj59
  I0512 15:53:46.662570 23 service_latency.go:363] Got endpoints: latency-svc-jxnt5 [524.247634ms]
  I0512 15:53:46.682217 23 service_latency.go:356] Created: latency-svc-cgnk2
  I0512 15:53:46.713069 23 service_latency.go:363] Got endpoints: latency-svc-nl7mp [574.466224ms]
  I0512 15:53:46.729135 23 service_latency.go:356] Created: latency-svc-4l5cz
  I0512 15:53:46.761957 23 service_latency.go:363] Got endpoints: latency-svc-hshw8 [623.126718ms]
  I0512 15:53:46.779202 23 service_latency.go:356] Created: latency-svc-kcnzk
  I0512 15:53:46.808073 23 service_latency.go:363] Got endpoints: latency-svc-5rb64 [650.163934ms]
  I0512 15:53:46.822298 23 service_latency.go:356] Created: latency-svc-7q4w4
  I0512 15:53:46.860683 23 service_latency.go:363] Got endpoints: latency-svc-x8cvz [702.183696ms]
  I0512 15:53:46.878566 23 service_latency.go:356] Created: latency-svc-kh9qr
  I0512 15:53:46.912319 23 service_latency.go:363] Got endpoints: latency-svc-fprwm [732.741036ms]
  I0512 15:53:46.931634 23 service_latency.go:356] Created: latency-svc-8shwl
  E0512 15:53:46.956918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:46.960667 23 service_latency.go:363] Got endpoints: latency-svc-smbht [741.9259ms]
  I0512 15:53:46.980129 23 service_latency.go:356] Created: latency-svc-ddltv
  I0512 15:53:47.009266 23 service_latency.go:363] Got endpoints: latency-svc-kdrdk [744.388313ms]
  I0512 15:53:47.036596 23 service_latency.go:356] Created: latency-svc-sp8hk
  I0512 15:53:47.065700 23 service_latency.go:363] Got endpoints: latency-svc-s5b8f [702.301873ms]
  I0512 15:53:47.091435 23 service_latency.go:356] Created: latency-svc-mcdcz
  I0512 15:53:47.110680 23 service_latency.go:363] Got endpoints: latency-svc-hd6z5 [793.754884ms]
  I0512 15:53:47.141580 23 service_latency.go:356] Created: latency-svc-sf5bc
  I0512 15:53:47.164953 23 service_latency.go:363] Got endpoints: latency-svc-zxsn6 [756.962792ms]
  I0512 15:53:47.191668 23 service_latency.go:356] Created: latency-svc-cht9x
  I0512 15:53:47.211081 23 service_latency.go:363] Got endpoints: latency-svc-v7nct [750.026042ms]
  I0512 15:53:47.229766 23 service_latency.go:356] Created: latency-svc-mgkxz
  I0512 15:53:47.261891 23 service_latency.go:363] Got endpoints: latency-svc-slwd6 [752.983067ms]
  I0512 15:53:47.278930 23 service_latency.go:356] Created: latency-svc-5f5jv
  I0512 15:53:47.310959 23 service_latency.go:363] Got endpoints: latency-svc-fpnlp [750.810352ms]
  I0512 15:53:47.332354 23 service_latency.go:356] Created: latency-svc-b8rkw
  I0512 15:53:47.358773 23 service_latency.go:363] Got endpoints: latency-svc-qsj59 [745.779307ms]
  I0512 15:53:47.376051 23 service_latency.go:356] Created: latency-svc-69gxc
  I0512 15:53:47.417588 23 service_latency.go:363] Got endpoints: latency-svc-cgnk2 [754.945106ms]
  I0512 15:53:47.443189 23 service_latency.go:356] Created: latency-svc-4fkvg
  I0512 15:53:47.459657 23 service_latency.go:363] Got endpoints: latency-svc-4l5cz [746.511671ms]
  I0512 15:53:47.481536 23 service_latency.go:356] Created: latency-svc-t8pwz
  I0512 15:53:47.511708 23 service_latency.go:363] Got endpoints: latency-svc-kcnzk [748.601446ms]
  I0512 15:53:47.528819 23 service_latency.go:356] Created: latency-svc-pr2hn
  I0512 15:53:47.562915 23 service_latency.go:363] Got endpoints: latency-svc-7q4w4 [754.77882ms]
  I0512 15:53:47.581589 23 service_latency.go:356] Created: latency-svc-tv5sw
  I0512 15:53:47.609545 23 service_latency.go:363] Got endpoints: latency-svc-kh9qr [748.798042ms]
  I0512 15:53:47.633351 23 service_latency.go:356] Created: latency-svc-x2rxb
  I0512 15:53:47.662061 23 service_latency.go:363] Got endpoints: latency-svc-8shwl [748.564948ms]
  I0512 15:53:47.679978 23 service_latency.go:356] Created: latency-svc-bnzgr
  I0512 15:53:47.709060 23 service_latency.go:363] Got endpoints: latency-svc-ddltv [747.517286ms]
  I0512 15:53:47.729475 23 service_latency.go:356] Created: latency-svc-hzxc2
  I0512 15:53:47.763056 23 service_latency.go:363] Got endpoints: latency-svc-sp8hk [753.366572ms]
  I0512 15:53:47.782190 23 service_latency.go:356] Created: latency-svc-4mmv4
  I0512 15:53:47.811749 23 service_latency.go:363] Got endpoints: latency-svc-mcdcz [745.964184ms]
  I0512 15:53:47.833060 23 service_latency.go:356] Created: latency-svc-x2mgk
  I0512 15:53:47.861438 23 service_latency.go:363] Got endpoints: latency-svc-sf5bc [750.691904ms]
  I0512 15:53:47.875790 23 service_latency.go:356] Created: latency-svc-5424w
  I0512 15:53:47.910446 23 service_latency.go:363] Got endpoints: latency-svc-cht9x [745.400193ms]
  I0512 15:53:47.926637 23 service_latency.go:356] Created: latency-svc-8wrkw
  E0512 15:53:47.956953      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:47.961814 23 service_latency.go:363] Got endpoints: latency-svc-mgkxz [750.626255ms]
  I0512 15:53:47.977825 23 service_latency.go:356] Created: latency-svc-94xhb
  I0512 15:53:48.011539 23 service_latency.go:363] Got endpoints: latency-svc-5f5jv [749.5724ms]
  I0512 15:53:48.025638 23 service_latency.go:356] Created: latency-svc-pklcz
  I0512 15:53:48.061718 23 service_latency.go:363] Got endpoints: latency-svc-b8rkw [750.558219ms]
  I0512 15:53:48.077493 23 service_latency.go:356] Created: latency-svc-4l6sb
  I0512 15:53:48.111723 23 service_latency.go:363] Got endpoints: latency-svc-69gxc [752.894089ms]
  I0512 15:53:48.134251 23 service_latency.go:356] Created: latency-svc-qnhxg
  I0512 15:53:48.169708 23 service_latency.go:363] Got endpoints: latency-svc-4fkvg [751.840382ms]
  I0512 15:53:48.193403 23 service_latency.go:356] Created: latency-svc-rsx74
  I0512 15:53:48.217123 23 service_latency.go:363] Got endpoints: latency-svc-t8pwz [757.196857ms]
  I0512 15:53:48.263166 23 service_latency.go:356] Created: latency-svc-zwvfv
  I0512 15:53:48.267150 23 service_latency.go:363] Got endpoints: latency-svc-pr2hn [755.199007ms]
  I0512 15:53:48.293062 23 service_latency.go:356] Created: latency-svc-kmjpl
  I0512 15:53:48.311115 23 service_latency.go:363] Got endpoints: latency-svc-tv5sw [748.035564ms]
  I0512 15:53:48.327019 23 service_latency.go:356] Created: latency-svc-wssqv
  I0512 15:53:48.362626 23 service_latency.go:363] Got endpoints: latency-svc-x2rxb [752.987851ms]
  I0512 15:53:48.379738 23 service_latency.go:356] Created: latency-svc-g6tt2
  I0512 15:53:48.411904 23 service_latency.go:363] Got endpoints: latency-svc-bnzgr [749.678341ms]
  I0512 15:53:48.433956 23 service_latency.go:356] Created: latency-svc-pjqw2
  I0512 15:53:48.465000 23 service_latency.go:363] Got endpoints: latency-svc-hzxc2 [755.802456ms]
  I0512 15:53:48.487536 23 service_latency.go:356] Created: latency-svc-zx2kw
  I0512 15:53:48.513085 23 service_latency.go:363] Got endpoints: latency-svc-4mmv4 [749.782691ms]
  I0512 15:53:48.530625 23 service_latency.go:356] Created: latency-svc-krk6m
  I0512 15:53:48.561223 23 service_latency.go:363] Got endpoints: latency-svc-x2mgk [749.114666ms]
  I0512 15:53:48.577390 23 service_latency.go:356] Created: latency-svc-kdsk7
  I0512 15:53:48.619916 23 service_latency.go:363] Got endpoints: latency-svc-5424w [758.387605ms]
  I0512 15:53:48.641276 23 service_latency.go:356] Created: latency-svc-dnw5g
  I0512 15:53:48.665608 23 service_latency.go:363] Got endpoints: latency-svc-8wrkw [754.822079ms]
  I0512 15:53:48.694171 23 service_latency.go:356] Created: latency-svc-sddz7
  I0512 15:53:48.707848 23 service_latency.go:363] Got endpoints: latency-svc-94xhb [744.961934ms]
  I0512 15:53:48.725029 23 service_latency.go:356] Created: latency-svc-pfz8j
  I0512 15:53:48.765718 23 service_latency.go:363] Got endpoints: latency-svc-pklcz [754.093813ms]
  I0512 15:53:48.784101 23 service_latency.go:356] Created: latency-svc-hwn9t
  I0512 15:53:48.810005 23 service_latency.go:363] Got endpoints: latency-svc-4l6sb [747.607978ms]
  I0512 15:53:48.832601 23 service_latency.go:356] Created: latency-svc-5fh5v
  I0512 15:53:48.863306 23 service_latency.go:363] Got endpoints: latency-svc-qnhxg [751.522989ms]
  I0512 15:53:48.891367 23 service_latency.go:356] Created: latency-svc-fmtdl
  I0512 15:53:48.909535 23 service_latency.go:363] Got endpoints: latency-svc-rsx74 [739.739966ms]
  I0512 15:53:48.933061 23 service_latency.go:356] Created: latency-svc-fd8pt
  E0512 15:53:48.957112      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:48.964188 23 service_latency.go:363] Got endpoints: latency-svc-zwvfv [746.589887ms]
  I0512 15:53:48.986295 23 service_latency.go:356] Created: latency-svc-5hl2q
  I0512 15:53:49.010960 23 service_latency.go:363] Got endpoints: latency-svc-kmjpl [743.619144ms]
  I0512 15:53:49.030609 23 service_latency.go:356] Created: latency-svc-g9fjv
  I0512 15:53:49.059899 23 service_latency.go:363] Got endpoints: latency-svc-wssqv [748.719125ms]
  I0512 15:53:49.077743 23 service_latency.go:356] Created: latency-svc-h5587
  I0512 15:53:49.124977 23 service_latency.go:363] Got endpoints: latency-svc-g6tt2 [762.222192ms]
  I0512 15:53:49.145842 23 service_latency.go:356] Created: latency-svc-fglnh
  I0512 15:53:49.163258 23 service_latency.go:363] Got endpoints: latency-svc-pjqw2 [751.214065ms]
  I0512 15:53:49.186314 23 service_latency.go:356] Created: latency-svc-h8wmh
  I0512 15:53:49.212770 23 service_latency.go:363] Got endpoints: latency-svc-zx2kw [747.193179ms]
  I0512 15:53:49.240411 23 service_latency.go:356] Created: latency-svc-kk2p5
  I0512 15:53:49.261317 23 service_latency.go:363] Got endpoints: latency-svc-krk6m [748.041983ms]
  I0512 15:53:49.287355 23 service_latency.go:356] Created: latency-svc-p6kvm
  I0512 15:53:49.313624 23 service_latency.go:363] Got endpoints: latency-svc-kdsk7 [752.326906ms]
  I0512 15:53:49.336953 23 service_latency.go:356] Created: latency-svc-fr67h
  I0512 15:53:49.363217 23 service_latency.go:363] Got endpoints: latency-svc-dnw5g [742.968123ms]
  I0512 15:53:49.385141 23 service_latency.go:356] Created: latency-svc-f9qh6
  I0512 15:53:49.416653 23 service_latency.go:363] Got endpoints: latency-svc-sddz7 [750.988026ms]
  I0512 15:53:49.440426 23 service_latency.go:356] Created: latency-svc-bs9mq
  I0512 15:53:49.461980 23 service_latency.go:363] Got endpoints: latency-svc-pfz8j [753.335993ms]
  I0512 15:53:49.479804 23 service_latency.go:356] Created: latency-svc-jj5j7
  I0512 15:53:49.516938 23 service_latency.go:363] Got endpoints: latency-svc-hwn9t [750.841777ms]
  I0512 15:53:49.537873 23 service_latency.go:356] Created: latency-svc-87sq7
  I0512 15:53:49.561144 23 service_latency.go:363] Got endpoints: latency-svc-5fh5v [750.550346ms]
  I0512 15:53:49.576884 23 service_latency.go:356] Created: latency-svc-lhwgs
  I0512 15:53:49.612575 23 service_latency.go:363] Got endpoints: latency-svc-fmtdl [749.181701ms]
  I0512 15:53:49.631025 23 service_latency.go:356] Created: latency-svc-cfl58
  I0512 15:53:49.666240 23 service_latency.go:363] Got endpoints: latency-svc-fd8pt [756.622834ms]
  I0512 15:53:49.682157 23 service_latency.go:356] Created: latency-svc-sw6bt
  I0512 15:53:49.715230 23 service_latency.go:363] Got endpoints: latency-svc-5hl2q [748.789522ms]
  I0512 15:53:49.733318 23 service_latency.go:356] Created: latency-svc-55r2j
  I0512 15:53:49.761222 23 service_latency.go:363] Got endpoints: latency-svc-g9fjv [750.17542ms]
  I0512 15:53:49.779500 23 service_latency.go:356] Created: latency-svc-dd4gj
  I0512 15:53:49.812980 23 service_latency.go:363] Got endpoints: latency-svc-h5587 [752.996821ms]
  I0512 15:53:49.833987 23 service_latency.go:356] Created: latency-svc-h7b4h
  I0512 15:53:49.860115 23 service_latency.go:363] Got endpoints: latency-svc-fglnh [734.212878ms]
  I0512 15:53:49.878190 23 service_latency.go:356] Created: latency-svc-k7qjd
  I0512 15:53:49.912501 23 service_latency.go:363] Got endpoints: latency-svc-h8wmh [749.183013ms]
  I0512 15:53:49.937026 23 service_latency.go:356] Created: latency-svc-bkp5b
  E0512 15:53:49.958092      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:49.960955 23 service_latency.go:363] Got endpoints: latency-svc-kk2p5 [747.356812ms]
  I0512 15:53:49.983805 23 service_latency.go:356] Created: latency-svc-jfhtf
  I0512 15:53:50.011142 23 service_latency.go:363] Got endpoints: latency-svc-p6kvm [749.733191ms]
  I0512 15:53:50.035740 23 service_latency.go:356] Created: latency-svc-wgnbm
  I0512 15:53:50.065875 23 service_latency.go:363] Got endpoints: latency-svc-fr67h [752.076454ms]
  I0512 15:53:50.090563 23 service_latency.go:356] Created: latency-svc-wfjdp
  I0512 15:53:50.110971 23 service_latency.go:363] Got endpoints: latency-svc-f9qh6 [746.659663ms]
  I0512 15:53:50.129455 23 service_latency.go:356] Created: latency-svc-jj8v2
  I0512 15:53:50.161123 23 service_latency.go:363] Got endpoints: latency-svc-bs9mq [743.240209ms]
  I0512 15:53:50.179463 23 service_latency.go:356] Created: latency-svc-kkdrw
  I0512 15:53:50.212127 23 service_latency.go:363] Got endpoints: latency-svc-jj5j7 [750.086923ms]
  I0512 15:53:50.232188 23 service_latency.go:356] Created: latency-svc-b2m6x
  I0512 15:53:50.261508 23 service_latency.go:363] Got endpoints: latency-svc-87sq7 [744.500781ms]
  I0512 15:53:50.276249 23 service_latency.go:356] Created: latency-svc-kl794
  I0512 15:53:50.309443 23 service_latency.go:363] Got endpoints: latency-svc-lhwgs [747.365415ms]
  I0512 15:53:50.325430 23 service_latency.go:356] Created: latency-svc-hq6cv
  I0512 15:53:50.363855 23 service_latency.go:363] Got endpoints: latency-svc-cfl58 [751.190021ms]
  I0512 15:53:50.384545 23 service_latency.go:356] Created: latency-svc-pg8wb
  I0512 15:53:50.412113 23 service_latency.go:363] Got endpoints: latency-svc-sw6bt [744.85466ms]
  I0512 15:53:50.429890 23 service_latency.go:356] Created: latency-svc-b2g7t
  I0512 15:53:50.461016 23 service_latency.go:363] Got endpoints: latency-svc-55r2j [745.051715ms]
  I0512 15:53:50.484657 23 service_latency.go:356] Created: latency-svc-k8cbq
  I0512 15:53:50.516754 23 service_latency.go:363] Got endpoints: latency-svc-dd4gj [754.56122ms]
  I0512 15:53:50.557164 23 service_latency.go:356] Created: latency-svc-4mpgj
  I0512 15:53:50.577244 23 service_latency.go:363] Got endpoints: latency-svc-h7b4h [763.605041ms]
  I0512 15:53:50.617116 23 service_latency.go:363] Got endpoints: latency-svc-k7qjd [756.913313ms]
  I0512 15:53:50.618242 23 service_latency.go:356] Created: latency-svc-7l7rs
  I0512 15:53:50.652118 23 service_latency.go:356] Created: latency-svc-g4nnv
  I0512 15:53:50.678185 23 service_latency.go:363] Got endpoints: latency-svc-bkp5b [765.601307ms]
  I0512 15:53:50.701219 23 service_latency.go:356] Created: latency-svc-vj8nh
  I0512 15:53:50.713750 23 service_latency.go:363] Got endpoints: latency-svc-jfhtf [752.269514ms]
  I0512 15:53:50.756142 23 service_latency.go:356] Created: latency-svc-hp7c4
  I0512 15:53:50.779520 23 service_latency.go:363] Got endpoints: latency-svc-wgnbm [768.034968ms]
  I0512 15:53:50.817133 23 service_latency.go:356] Created: latency-svc-7gs55
  I0512 15:53:50.819684 23 service_latency.go:363] Got endpoints: latency-svc-wfjdp [753.451054ms]
  I0512 15:53:50.848736 23 service_latency.go:356] Created: latency-svc-lrlzl
  I0512 15:53:50.867457 23 service_latency.go:363] Got endpoints: latency-svc-jj8v2 [756.389788ms]
  I0512 15:53:50.901104 23 service_latency.go:356] Created: latency-svc-tdp4f
  I0512 15:53:50.917636 23 service_latency.go:363] Got endpoints: latency-svc-kkdrw [756.420171ms]
  I0512 15:53:50.942744 23 service_latency.go:356] Created: latency-svc-x4fls
  E0512 15:53:50.958242      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:50.976502 23 service_latency.go:363] Got endpoints: latency-svc-b2m6x [763.613097ms]
  I0512 15:53:50.997830 23 service_latency.go:356] Created: latency-svc-xhfv2
  I0512 15:53:51.013803 23 service_latency.go:363] Got endpoints: latency-svc-kl794 [752.200835ms]
  I0512 15:53:51.032941 23 service_latency.go:356] Created: latency-svc-wfxjw
  I0512 15:53:51.067223 23 service_latency.go:363] Got endpoints: latency-svc-hq6cv [757.687438ms]
  I0512 15:53:51.090146 23 service_latency.go:356] Created: latency-svc-cn827
  I0512 15:53:51.115857 23 service_latency.go:363] Got endpoints: latency-svc-pg8wb [751.896428ms]
  I0512 15:53:51.136625 23 service_latency.go:356] Created: latency-svc-rgq4s
  I0512 15:53:51.162281 23 service_latency.go:363] Got endpoints: latency-svc-b2g7t [750.096014ms]
  I0512 15:53:51.178375 23 service_latency.go:356] Created: latency-svc-fztk5
  I0512 15:53:51.209782 23 service_latency.go:363] Got endpoints: latency-svc-k8cbq [748.675809ms]
  I0512 15:53:51.226965 23 service_latency.go:356] Created: latency-svc-9kn82
  I0512 15:53:51.265218 23 service_latency.go:363] Got endpoints: latency-svc-4mpgj [748.143925ms]
  I0512 15:53:51.286158 23 service_latency.go:356] Created: latency-svc-7wkjq
  I0512 15:53:51.311896 23 service_latency.go:363] Got endpoints: latency-svc-7l7rs [734.287221ms]
  I0512 15:53:51.331378 23 service_latency.go:356] Created: latency-svc-sf7bn
  I0512 15:53:51.359854 23 service_latency.go:363] Got endpoints: latency-svc-g4nnv [742.386115ms]
  I0512 15:53:51.374169 23 service_latency.go:356] Created: latency-svc-x2mjx
  I0512 15:53:51.414187 23 service_latency.go:363] Got endpoints: latency-svc-vj8nh [735.874514ms]
  I0512 15:53:51.429831 23 service_latency.go:356] Created: latency-svc-4gs4v
  I0512 15:53:51.464315 23 service_latency.go:363] Got endpoints: latency-svc-hp7c4 [750.464228ms]
  I0512 15:53:51.488511 23 service_latency.go:356] Created: latency-svc-v8v95
  I0512 15:53:51.508812 23 service_latency.go:363] Got endpoints: latency-svc-7gs55 [729.16224ms]
  I0512 15:53:51.526985 23 service_latency.go:356] Created: latency-svc-2dclf
  I0512 15:53:51.564155 23 service_latency.go:363] Got endpoints: latency-svc-lrlzl [744.410743ms]
  I0512 15:53:51.590659 23 service_latency.go:356] Created: latency-svc-gq6nz
  I0512 15:53:51.610617 23 service_latency.go:363] Got endpoints: latency-svc-tdp4f [743.079376ms]
  I0512 15:53:51.633155 23 service_latency.go:356] Created: latency-svc-mxskp
  I0512 15:53:51.657468 23 service_latency.go:363] Got endpoints: latency-svc-x4fls [739.767187ms]
  I0512 15:53:51.676464 23 service_latency.go:356] Created: latency-svc-m9krp
  I0512 15:53:51.711567 23 service_latency.go:363] Got endpoints: latency-svc-xhfv2 [734.662069ms]
  I0512 15:53:51.732244 23 service_latency.go:356] Created: latency-svc-lqb5p
  I0512 15:53:51.761608 23 service_latency.go:363] Got endpoints: latency-svc-wfxjw [747.353945ms]
  I0512 15:53:51.775859 23 service_latency.go:356] Created: latency-svc-mhh4p
  I0512 15:53:51.811877 23 service_latency.go:363] Got endpoints: latency-svc-cn827 [744.161506ms]
  I0512 15:53:51.824842 23 service_latency.go:356] Created: latency-svc-r55qt
  I0512 15:53:51.860826 23 service_latency.go:363] Got endpoints: latency-svc-rgq4s [744.872623ms]
  I0512 15:53:51.882276 23 service_latency.go:356] Created: latency-svc-d29b5
  I0512 15:53:51.914041 23 service_latency.go:363] Got endpoints: latency-svc-fztk5 [751.670436ms]
  I0512 15:53:51.929882 23 service_latency.go:356] Created: latency-svc-n7n2p
  E0512 15:53:51.959035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:51.961680 23 service_latency.go:363] Got endpoints: latency-svc-9kn82 [751.811687ms]
  I0512 15:53:51.975356 23 service_latency.go:356] Created: latency-svc-m7w9b
  I0512 15:53:52.016206 23 service_latency.go:363] Got endpoints: latency-svc-7wkjq [750.912196ms]
  I0512 15:53:52.037726 23 service_latency.go:356] Created: latency-svc-6mnf5
  I0512 15:53:52.059292 23 service_latency.go:363] Got endpoints: latency-svc-sf7bn [747.339132ms]
  I0512 15:53:52.077888 23 service_latency.go:356] Created: latency-svc-bmh8d
  I0512 15:53:52.109562 23 service_latency.go:363] Got endpoints: latency-svc-x2mjx [749.63875ms]
  I0512 15:53:52.123715 23 service_latency.go:356] Created: latency-svc-pkz48
  I0512 15:53:52.164677 23 service_latency.go:363] Got endpoints: latency-svc-4gs4v [750.407388ms]
  I0512 15:53:52.179827 23 service_latency.go:356] Created: latency-svc-lr24z
  I0512 15:53:52.209582 23 service_latency.go:363] Got endpoints: latency-svc-v8v95 [745.171275ms]
  I0512 15:53:52.230767 23 service_latency.go:356] Created: latency-svc-6kdft
  I0512 15:53:52.260117 23 service_latency.go:363] Got endpoints: latency-svc-2dclf [750.912513ms]
  I0512 15:53:52.279582 23 service_latency.go:356] Created: latency-svc-bm7t8
  I0512 15:53:52.309240 23 service_latency.go:363] Got endpoints: latency-svc-gq6nz [744.835784ms]
  I0512 15:53:52.379007 23 service_latency.go:363] Got endpoints: latency-svc-mxskp [768.061408ms]
  I0512 15:53:52.409737 23 service_latency.go:363] Got endpoints: latency-svc-m9krp [752.190706ms]
  I0512 15:53:52.464066 23 service_latency.go:363] Got endpoints: latency-svc-lqb5p [752.321915ms]
  I0512 15:53:52.507906 23 service_latency.go:363] Got endpoints: latency-svc-mhh4p [746.222429ms]
  I0512 15:53:52.566252 23 service_latency.go:363] Got endpoints: latency-svc-r55qt [754.181396ms]
  I0512 15:53:52.614230 23 service_latency.go:363] Got endpoints: latency-svc-d29b5 [753.073495ms]
  I0512 15:53:52.669711 23 service_latency.go:363] Got endpoints: latency-svc-n7n2p [754.973252ms]
  I0512 15:53:52.710266 23 service_latency.go:363] Got endpoints: latency-svc-m7w9b [748.501964ms]
  I0512 15:53:52.764805 23 service_latency.go:363] Got endpoints: latency-svc-6mnf5 [747.436689ms]
  I0512 15:53:52.817932 23 service_latency.go:363] Got endpoints: latency-svc-bmh8d [757.187018ms]
  I0512 15:53:52.867477 23 service_latency.go:363] Got endpoints: latency-svc-pkz48 [756.71082ms]
  I0512 15:53:52.914015 23 service_latency.go:363] Got endpoints: latency-svc-lr24z [749.199606ms]
  E0512 15:53:52.960011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:53:52.961638 23 service_latency.go:363] Got endpoints: latency-svc-6kdft [751.978362ms]
  I0512 15:53:53.013770 23 service_latency.go:363] Got endpoints: latency-svc-bm7t8 [752.731222ms]
  I0512 15:53:53.014849 23 service_latency.go:114] Latencies: [72.560082ms 145.944483ms 166.364952ms 166.615321ms 182.850757ms 183.238997ms 199.220758ms 199.700333ms 200.748596ms 209.121768ms 221.219856ms 233.458623ms 258.145279ms 261.820561ms 272.270158ms 278.537055ms 280.147647ms 285.475682ms 294.803229ms 299.672244ms 307.690295ms 316.812993ms 319.250351ms 321.779267ms 325.538136ms 330.884199ms 332.530355ms 333.113669ms 337.437507ms 355.430338ms 361.113818ms 363.321949ms 363.496401ms 363.897878ms 369.97211ms 371.216851ms 372.481843ms 377.264385ms 378.802006ms 380.389924ms 381.664491ms 383.513906ms 385.253544ms 392.779672ms 401.714343ms 407.826732ms 408.073326ms 418.240207ms 421.920948ms 434.200006ms 461.496758ms 464.476848ms 466.303875ms 468.217838ms 475.15995ms 491.13003ms 491.518661ms 491.930371ms 502.203691ms 516.475885ms 518.499841ms 521.827738ms 524.247634ms 574.466224ms 578.110417ms 579.54821ms 590.874322ms 597.646736ms 623.126718ms 630.715012ms 630.974852ms 648.723893ms 650.163934ms 666.475035ms 702.183696ms 702.301873ms 729.16224ms 730.285035ms 732.741036ms 734.212878ms 734.287221ms 734.662069ms 735.874514ms 739.739966ms 739.767187ms 741.9259ms 742.386115ms 742.968123ms 743.079376ms 743.240209ms 743.619144ms 744.161506ms 744.388313ms 744.410743ms 744.500781ms 744.835784ms 744.85466ms 744.872623ms 744.961934ms 745.051715ms 745.171275ms 745.400193ms 745.779307ms 745.964184ms 746.222429ms 746.511671ms 746.589887ms 746.659663ms 747.193179ms 747.339132ms 747.353945ms 747.356812ms 747.365415ms 747.436689ms 747.517286ms 747.607978ms 748.035564ms 748.041983ms 748.143925ms 748.501964ms 748.564948ms 748.601446ms 748.675809ms 748.719125ms 748.789522ms 748.798042ms 749.114666ms 749.181701ms 749.183013ms 749.199606ms 749.5724ms 749.63875ms 749.678341ms 749.733191ms 749.782691ms 750.026042ms 750.086923ms 750.096014ms 750.17542ms 750.407388ms 750.464228ms 750.550346ms 750.558219ms 750.626255ms 750.691904ms 750.810352ms 750.841777ms 750.912196ms 750.912513ms 750.988026ms 751.190021ms 751.214065ms 751.522989ms 751.670436ms 751.811687ms 751.840382ms 751.896428ms 751.978362ms 752.076454ms 752.190706ms 752.200835ms 752.269514ms 752.321915ms 752.326906ms 752.731222ms 752.894089ms 752.983067ms 752.987851ms 752.996821ms 753.073495ms 753.335993ms 753.366572ms 753.451054ms 754.093813ms 754.181396ms 754.56122ms 754.77882ms 754.822079ms 754.945106ms 754.973252ms 755.199007ms 755.802456ms 756.389788ms 756.420171ms 756.622834ms 756.71082ms 756.913313ms 756.962792ms 757.187018ms 757.196857ms 757.687438ms 758.387605ms 762.222192ms 763.605041ms 763.613097ms 765.601307ms 768.034968ms 768.061408ms 770.033605ms 793.754884ms]
  I0512 15:53:53.015084 23 service_latency.go:118] 50 %ile: 745.171275ms
  I0512 15:53:53.015264 23 service_latency.go:119] 90 %ile: 755.199007ms
  I0512 15:53:53.015435 23 service_latency.go:120] 99 %ile: 770.033605ms
  I0512 15:53:53.016137 23 service_latency.go:121] Total sample count: 200
  I0512 15:53:53.018223 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-2200" for this suite. @ 05/12/25 15:53:53.032
• [10.864 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/service_accounts.go:649
  STEP: Creating a kubernetes client @ 05/12/25 15:53:53.045
  I0512 15:53:53.045571 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/25 15:53:53.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:53.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:53.075
  STEP: creating a ServiceAccount @ 05/12/25 15:53:53.08
  STEP: watching for the ServiceAccount to be added @ 05/12/25 15:53:53.095
  STEP: patching the ServiceAccount @ 05/12/25 15:53:53.099
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/12/25 15:53:53.113
  STEP: deleting the ServiceAccount @ 05/12/25 15:53:53.119
  I0512 15:53:53.146498 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5095" for this suite. @ 05/12/25 15:53:53.159
• [0.132 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/12/25 15:53:53.177
  I0512 15:53:53.177397 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:53:53.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:53.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:53.211
  STEP: Creating projection with secret that has name projected-secret-test-map-822e8df0-80cc-452b-82e7-87246338193f @ 05/12/25 15:53:53.214
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:53:53.223
  E0512 15:53:53.961309      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:54.962004      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:55.963084      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:56.963498      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:53:57.275
  I0512 15:53:57.283117 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-secrets-a8f67f91-3cef-4409-b38c-7e1de146b66c container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:53:57.297
  I0512 15:53:57.347320 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6344" for this suite. @ 05/12/25 15:53:57.36
• [4.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should allow to use the pod failure policy on exit code to fail the job early [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/job.go:107
  STEP: Creating a kubernetes client @ 05/12/25 15:53:57.38
  I0512 15:53:57.380880 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename job @ 05/12/25 15:53:57.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:53:57.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:53:57.437
  STEP: Looking for a node to schedule job pod @ 05/12/25 15:53:57.445
  STEP: Creating a job @ 05/12/25 15:53:57.462
  STEP: Ensuring job fails @ 05/12/25 15:53:57.485
  E0512 15:53:57.964347      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:58.965284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:53:59.965897      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:00.966579      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:01.966692      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:02.967257      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:03.967382      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:04.968486      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:05.505902 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3795" for this suite. @ 05/12/25 15:54:05.525
• [8.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/12/25 15:54:05.565
  I0512 15:54:05.566749 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename proxy @ 05/12/25 15:54:05.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:54:05.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:54:05.62
  STEP: starting an echo server on multiple ports @ 05/12/25 15:54:05.666
  STEP: creating replication controller proxy-service-pthhq in namespace proxy-3137 @ 05/12/25 15:54:05.667
  I0512 15:54:05.680706      23 runners.go:193] Created replication controller with name: proxy-service-pthhq, namespace: proxy-3137, replica count: 1
  E0512 15:54:05.969688      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:06.732958      23 runners.go:193] proxy-service-pthhq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0512 15:54:06.969859      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:07.733553      23 runners.go:193] proxy-service-pthhq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 15:54:07.745987 23 proxy.go:608] Endpoint proxy-3137/proxy-service-pthhq is not ready yet
  E0512 15:54:07.970347      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:08.971555      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:09.754783 23 proxy.go:608] Endpoint proxy-3137/proxy-service-pthhq is not ready yet
  E0512 15:54:09.972068      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:10.972919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:11.766445 23 proxy.go:608] Endpoint proxy-3137/proxy-service-pthhq is not ready yet
  E0512 15:54:11.973923      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:12.974248      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:13.779042 23 proxy.go:608] Endpoint proxy-3137/proxy-service-pthhq is not ready yet
  E0512 15:54:13.974784      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:14.974856      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:15.791217 23 proxy.go:230] setup took 10.159805487s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/12/25 15:54:15.792
  I0512 15:54:15.845442 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 51.975214ms)
  I0512 15:54:15.846796 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 53.604444ms)
  I0512 15:54:15.846938 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 53.857907ms)
  I0512 15:54:15.853308 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 59.791417ms)
  I0512 15:54:15.854079 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 61.094394ms)
  I0512 15:54:15.856094 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 62.212574ms)
  I0512 15:54:15.856265 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 62.876141ms)
  I0512 15:54:15.868712 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 75.144482ms)
  I0512 15:54:15.869064 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 75.796042ms)
  I0512 15:54:15.869080 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 76.324215ms)
  I0512 15:54:15.869743 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 75.994612ms)
  I0512 15:54:15.873950 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 80.120493ms)
  I0512 15:54:15.875496 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 82.75519ms)
  I0512 15:54:15.876010 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 82.352632ms)
  I0512 15:54:15.876245 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 82.599769ms)
  I0512 15:54:15.877144 23 proxy.go:558] (0) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 83.448083ms)
  I0512 15:54:15.903631 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 26.208189ms)
  I0512 15:54:15.910495 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 32.829225ms)
  I0512 15:54:15.913746 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 35.057236ms)
  I0512 15:54:15.914052 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 36.190856ms)
  I0512 15:54:15.914113 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 36.07222ms)
  I0512 15:54:15.926514 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 47.876116ms)
  I0512 15:54:15.927101 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 48.50099ms)
  I0512 15:54:15.927518 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 50.035286ms)
  I0512 15:54:15.930345 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 53.043654ms)
  I0512 15:54:15.931089 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 52.263487ms)
  I0512 15:54:15.931615 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 53.714851ms)
  I0512 15:54:15.935175 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 56.788271ms)
  I0512 15:54:15.935631 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 56.899641ms)
  I0512 15:54:15.936133 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 57.319261ms)
  I0512 15:54:15.938252 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 60.01534ms)
  I0512 15:54:15.943967 23 proxy.go:558] (1) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 65.086678ms)
  I0512 15:54:15.965360 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 20.657034ms)
  E0512 15:54:15.976234      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:15.978604 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 32.895052ms)
  I0512 15:54:15.978793 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 34.277819ms)
  I0512 15:54:15.982444 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 36.382002ms)
  I0512 15:54:15.983246 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 37.285682ms)
  I0512 15:54:15.986145 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 40.280809ms)
  I0512 15:54:15.986245 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 41.895642ms)
  I0512 15:54:15.986289 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 41.376032ms)
  I0512 15:54:15.986463 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 42.168835ms)
  I0512 15:54:15.986497 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 40.712642ms)
  I0512 15:54:15.986527 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 40.358415ms)
  I0512 15:54:15.990824 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 46.47475ms)
  I0512 15:54:15.993361 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 47.592096ms)
  I0512 15:54:15.994409 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 48.443903ms)
  I0512 15:54:15.994975 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 48.843793ms)
  I0512 15:54:15.998446 23 proxy.go:558] (2) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 52.429533ms)
  I0512 15:54:16.013146 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 14.403115ms)
  I0512 15:54:16.019707 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 20.002092ms)
  I0512 15:54:16.020328 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 21.028652ms)
  I0512 15:54:16.022271 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 23.042425ms)
  I0512 15:54:16.023711 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 24.273655ms)
  I0512 15:54:16.026882 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 27.3739ms)
  I0512 15:54:16.027921 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 27.93242ms)
  I0512 15:54:16.028782 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 30.226906ms)
  I0512 15:54:16.028792 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 28.926036ms)
  I0512 15:54:16.028862 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 29.231123ms)
  I0512 15:54:16.028887 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 30.0409ms)
  I0512 15:54:16.029390 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 29.780713ms)
  I0512 15:54:16.031779 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 32.702683ms)
  I0512 15:54:16.036451 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 37.270059ms)
  I0512 15:54:16.037033 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 37.640303ms)
  I0512 15:54:16.038739 23 proxy.go:558] (3) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 40.076463ms)
  I0512 15:54:16.054981 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 16.074332ms)
  I0512 15:54:16.060155 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 21.006602ms)
  I0512 15:54:16.060240 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 20.680205ms)
  I0512 15:54:16.060930 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 21.389535ms)
  I0512 15:54:16.061800 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 22.596979ms)
  I0512 15:54:16.060159 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 21.112572ms)
  I0512 15:54:16.064338 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 25.002878ms)
  I0512 15:54:16.069002 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 29.687155ms)
  I0512 15:54:16.069143 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 29.768892ms)
  I0512 15:54:16.069928 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 30.737879ms)
  I0512 15:54:16.071600 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 32.317809ms)
  I0512 15:54:16.073430 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 34.060293ms)
  I0512 15:54:16.073425 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 34.166913ms)
  I0512 15:54:16.075502 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 35.927829ms)
  I0512 15:54:16.075524 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 36.069423ms)
  I0512 15:54:16.076141 23 proxy.go:558] (4) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 37.177327ms)
  I0512 15:54:16.105424 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 28.101598ms)
  I0512 15:54:16.105409 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 28.469422ms)
  I0512 15:54:16.108910 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 31.675229ms)
  I0512 15:54:16.122108 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 45.010961ms)
  I0512 15:54:16.122364 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 45.689601ms)
  I0512 15:54:16.124290 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 47.033618ms)
  I0512 15:54:16.131156 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 54.895561ms)
  I0512 15:54:16.131161 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 53.757647ms)
  I0512 15:54:16.131261 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 53.910111ms)
  I0512 15:54:16.132142 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 55.002167ms)
  I0512 15:54:16.132296 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 55.50179ms)
  I0512 15:54:16.133659 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 56.742444ms)
  I0512 15:54:16.136560 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 59.243305ms)
  I0512 15:54:16.137215 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 60.183812ms)
  I0512 15:54:16.139099 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 62.120158ms)
  I0512 15:54:16.140378 23 proxy.go:558] (5) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 63.966775ms)
  I0512 15:54:16.178989 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 38.48537ms)
  I0512 15:54:16.183631 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 42.40996ms)
  I0512 15:54:16.183626 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 42.63055ms)
  I0512 15:54:16.183708 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 42.900507ms)
  I0512 15:54:16.183749 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 42.86241ms)
  I0512 15:54:16.183783 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 42.78079ms)
  I0512 15:54:16.190809 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 49.671287ms)
  I0512 15:54:16.191349 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 50.247687ms)
  I0512 15:54:16.191420 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 50.353017ms)
  I0512 15:54:16.191513 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 50.450687ms)
  I0512 15:54:16.191566 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 50.682757ms)
  I0512 15:54:16.192585 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 51.613751ms)
  I0512 15:54:16.192595 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 51.451777ms)
  I0512 15:54:16.192649 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 51.501604ms)
  I0512 15:54:16.193565 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 52.468058ms)
  I0512 15:54:16.193939 23 proxy.go:558] (6) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 53.099417ms)
  I0512 15:54:16.223433 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 26.838475ms)
  I0512 15:54:16.223551 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 26.733165ms)
  I0512 15:54:16.223591 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 27.080856ms)
  I0512 15:54:16.223882 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 26.829502ms)
  I0512 15:54:16.223936 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 27.302809ms)
  I0512 15:54:16.224841 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 28.140448ms)
  I0512 15:54:16.229075 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 32.312992ms)
  I0512 15:54:16.242604 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 45.674903ms)
  I0512 15:54:16.244923 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 48.058927ms)
  I0512 15:54:16.247407 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 50.431233ms)
  I0512 15:54:16.247484 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 50.41923ms)
  I0512 15:54:16.248258 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 51.274647ms)
  I0512 15:54:16.248401 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 51.890857ms)
  I0512 15:54:16.248962 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 51.766586ms)
  I0512 15:54:16.253831 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 56.710504ms)
  I0512 15:54:16.256926 23 proxy.go:558] (7) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 60.052624ms)
  I0512 15:54:16.273945 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 16.655351ms)
  I0512 15:54:16.275126 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 17.772654ms)
  I0512 15:54:16.277620 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 20.484089ms)
  I0512 15:54:16.278231 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 21.152599ms)
  I0512 15:54:16.278757 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 21.391202ms)
  I0512 15:54:16.278893 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 21.418629ms)
  I0512 15:54:16.279577 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 22.096345ms)
  I0512 15:54:16.280830 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 23.066545ms)
  I0512 15:54:16.282730 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 25.111128ms)
  I0512 15:54:16.286357 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 28.777762ms)
  I0512 15:54:16.286851 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 29.731406ms)
  I0512 15:54:16.289652 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 31.992709ms)
  I0512 15:54:16.290193 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 32.569913ms)
  I0512 15:54:16.290822 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 33.15005ms)
  I0512 15:54:16.293030 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 35.46913ms)
  I0512 15:54:16.294800 23 proxy.go:558] (8) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 37.267506ms)
  I0512 15:54:16.308343 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 13.009471ms)
  I0512 15:54:16.313380 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 17.129158ms)
  I0512 15:54:16.314054 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 19.081208ms)
  I0512 15:54:16.314070 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 18.693051ms)
  I0512 15:54:16.315388 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 19.081941ms)
  I0512 15:54:16.316606 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 19.285888ms)
  I0512 15:54:16.316872 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 21.452952ms)
  I0512 15:54:16.322374 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 27.120982ms)
  I0512 15:54:16.322387 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 27.391185ms)
  I0512 15:54:16.326628 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 31.716089ms)
  I0512 15:54:16.326632 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 30.916669ms)
  I0512 15:54:16.326903 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 30.758264ms)
  I0512 15:54:16.326995 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 30.57984ms)
  I0512 15:54:16.327021 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 31.565751ms)
  I0512 15:54:16.327059 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 31.550136ms)
  I0512 15:54:16.326675 23 proxy.go:558] (9) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 31.119706ms)
  I0512 15:54:16.344787 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 14.366867ms)
  I0512 15:54:16.348390 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 18.127122ms)
  I0512 15:54:16.351680 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 20.991981ms)
  I0512 15:54:16.351688 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 20.893091ms)
  I0512 15:54:16.351940 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 21.628525ms)
  I0512 15:54:16.354012 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 23.644662ms)
  I0512 15:54:16.355358 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 25.151149ms)
  I0512 15:54:16.355957 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 25.233095ms)
  I0512 15:54:16.358037 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 27.29449ms)
  I0512 15:54:16.358044 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 27.253611ms)
  I0512 15:54:16.358297 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 28.192869ms)
  I0512 15:54:16.358345 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 27.632838ms)
  I0512 15:54:16.358339 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 28.181853ms)
  I0512 15:54:16.360548 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 29.931303ms)
  I0512 15:54:16.361676 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 30.839419ms)
  I0512 15:54:16.374773 23 proxy.go:558] (10) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 44.328958ms)
  I0512 15:54:16.408022 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 32.448929ms)
  I0512 15:54:16.408022 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 32.414093ms)
  I0512 15:54:16.408376 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 32.658144ms)
  I0512 15:54:16.409164 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 33.818142ms)
  I0512 15:54:16.409730 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 34.486093ms)
  I0512 15:54:16.418107 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 42.950739ms)
  I0512 15:54:16.419684 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 44.400817ms)
  I0512 15:54:16.420412 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 44.976793ms)
  I0512 15:54:16.421306 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 46.39857ms)
  I0512 15:54:16.421726 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 46.001933ms)
  I0512 15:54:16.421953 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 46.538646ms)
  I0512 15:54:16.422104 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 46.76515ms)
  I0512 15:54:16.422210 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 46.565553ms)
  I0512 15:54:16.422187 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 46.28725ms)
  I0512 15:54:16.422482 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 46.99119ms)
  I0512 15:54:16.422284 23 proxy.go:558] (11) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 47.183183ms)
  I0512 15:54:16.435582 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 12.449191ms)
  I0512 15:54:16.439545 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 16.335225ms)
  I0512 15:54:16.440976 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 17.597157ms)
  I0512 15:54:16.441865 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 18.539334ms)
  I0512 15:54:16.442237 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 18.645798ms)
  I0512 15:54:16.443305 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 19.757656ms)
  I0512 15:54:16.443359 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 19.949382ms)
  I0512 15:54:16.443445 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 19.930434ms)
  I0512 15:54:16.443359 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 19.989508ms)
  I0512 15:54:16.444021 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 20.844849ms)
  I0512 15:54:16.444629 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 21.063492ms)
  I0512 15:54:16.448163 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 24.661897ms)
  I0512 15:54:16.448185 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 24.892032ms)
  I0512 15:54:16.448646 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 25.175079ms)
  I0512 15:54:16.452180 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 28.752332ms)
  I0512 15:54:16.452245 23 proxy.go:558] (12) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 28.799901ms)
  I0512 15:54:16.476146 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 23.363202ms)
  I0512 15:54:16.476916 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 23.266288ms)
  I0512 15:54:16.477168 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 24.276392ms)
  I0512 15:54:16.477796 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 24.030764ms)
  I0512 15:54:16.477871 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 23.966666ms)
  I0512 15:54:16.478038 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 24.728908ms)
  I0512 15:54:16.478250 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 25.252708ms)
  I0512 15:54:16.478931 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 24.818911ms)
  I0512 15:54:16.485819 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 33.190033ms)
  I0512 15:54:16.486747 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 34.447483ms)
  I0512 15:54:16.486856 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 34.383908ms)
  I0512 15:54:16.491848 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 37.706116ms)
  I0512 15:54:16.492451 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 38.902669ms)
  I0512 15:54:16.492982 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 39.734759ms)
  I0512 15:54:16.493072 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 38.99877ms)
  I0512 15:54:16.493252 23 proxy.go:558] (13) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 40.155008ms)
  I0512 15:54:16.508644 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 15.214659ms)
  I0512 15:54:16.509700 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 15.635768ms)
  I0512 15:54:16.510252 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 16.74981ms)
  I0512 15:54:16.517057 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 23.221058ms)
  I0512 15:54:16.517530 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 23.659205ms)
  I0512 15:54:16.520593 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 26.448515ms)
  I0512 15:54:16.520741 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 26.994169ms)
  I0512 15:54:16.521970 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 27.86381ms)
  I0512 15:54:16.523038 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 29.480602ms)
  I0512 15:54:16.523085 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 29.152916ms)
  I0512 15:54:16.523152 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 29.154988ms)
  I0512 15:54:16.525785 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 31.747969ms)
  I0512 15:54:16.525814 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 32.372865ms)
  I0512 15:54:16.526665 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 32.751844ms)
  I0512 15:54:16.528044 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 33.99042ms)
  I0512 15:54:16.528865 23 proxy.go:558] (14) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 34.674752ms)
  I0512 15:54:16.542143 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 11.593903ms)
  I0512 15:54:16.551563 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 20.217531ms)
  I0512 15:54:16.554203 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 23.666765ms)
  I0512 15:54:16.554251 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 23.180945ms)
  I0512 15:54:16.557912 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 27.272611ms)
  I0512 15:54:16.558524 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 26.824903ms)
  I0512 15:54:16.558623 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 27.664197ms)
  I0512 15:54:16.558635 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 27.610548ms)
  I0512 15:54:16.558740 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 27.562421ms)
  I0512 15:54:16.560992 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 29.079218ms)
  I0512 15:54:16.563066 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 32.169079ms)
  I0512 15:54:16.563088 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 32.572099ms)
  I0512 15:54:16.563531 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 31.673482ms)
  I0512 15:54:16.564007 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 32.837542ms)
  I0512 15:54:16.564395 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 32.453431ms)
  I0512 15:54:16.564389 23 proxy.go:558] (15) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 33.562689ms)
  I0512 15:54:16.575211 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 10.57151ms)
  I0512 15:54:16.582203 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 16.885374ms)
  I0512 15:54:16.582783 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 17.701979ms)
  I0512 15:54:16.593054 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 27.799455ms)
  I0512 15:54:16.593075 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 27.660871ms)
  I0512 15:54:16.593884 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 28.425886ms)
  I0512 15:54:16.595025 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 29.321921ms)
  I0512 15:54:16.595453 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 30.81002ms)
  I0512 15:54:16.596308 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 31.13525ms)
  I0512 15:54:16.597349 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 32.205604ms)
  I0512 15:54:16.601514 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 36.018126ms)
  I0512 15:54:16.601588 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 35.93129ms)
  I0512 15:54:16.601617 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 36.581259ms)
  I0512 15:54:16.601895 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 36.928597ms)
  I0512 15:54:16.601892 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 37.02156ms)
  I0512 15:54:16.602246 23 proxy.go:558] (16) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 36.695801ms)
  I0512 15:54:16.617970 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 15.320316ms)
  I0512 15:54:16.618024 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 15.621523ms)
  I0512 15:54:16.622864 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 20.401075ms)
  I0512 15:54:16.626669 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 23.179042ms)
  I0512 15:54:16.627132 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 23.489278ms)
  I0512 15:54:16.627493 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 24.125039ms)
  I0512 15:54:16.629461 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 26.888789ms)
  I0512 15:54:16.630477 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 26.852377ms)
  I0512 15:54:16.633077 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 29.549482ms)
  I0512 15:54:16.635993 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 32.480416ms)
  I0512 15:54:16.636210 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 32.756163ms)
  I0512 15:54:16.636224 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 33.26027ms)
  I0512 15:54:16.637363 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 33.677718ms)
  I0512 15:54:16.639824 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 36.555046ms)
  I0512 15:54:16.641076 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 37.82113ms)
  I0512 15:54:16.641083 23 proxy.go:558] (17) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 37.758809ms)
  I0512 15:54:16.662776 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 21.58993ms)
  I0512 15:54:16.663492 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 22.065378ms)
  I0512 15:54:16.664133 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 22.542211ms)
  I0512 15:54:16.665096 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 22.289698ms)
  I0512 15:54:16.667839 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 25.199338ms)
  I0512 15:54:16.675210 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 32.490959ms)
  I0512 15:54:16.676167 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 33.673945ms)
  I0512 15:54:16.676201 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 33.631432ms)
  I0512 15:54:16.676276 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 33.397928ms)
  I0512 15:54:16.676238 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 33.831025ms)
  I0512 15:54:16.676576 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 34.272119ms)
  I0512 15:54:16.676799 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 35.045843ms)
  I0512 15:54:16.677367 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 35.304399ms)
  I0512 15:54:16.677720 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 35.483585ms)
  I0512 15:54:16.678960 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 36.671269ms)
  I0512 15:54:16.679004 23 proxy.go:558] (18) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 36.641339ms)
  I0512 15:54:16.700372 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:162/proxy/: bar (200; 20.939664ms)
  I0512 15:54:16.709953 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:443/proxy/tlsrewritem... (200; 30.469313ms)
  I0512 15:54:16.710216 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:160/proxy/: foo (200; 30.914252ms)
  I0512 15:54:16.710284 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:1080/proxy/rewriteme">... (200; 30.748043ms)
  I0512 15:54:16.710342 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr/proxy/rewriteme">test</a> (200; 30.752104ms)
  I0512 15:54:16.710506 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/: <a href="/api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:1080/proxy/rewriteme">test<... (200; 30.874936ms)
  I0512 15:54:16.710579 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:460/proxy/: tls baz (200; 31.252606ms)
  I0512 15:54:16.710646 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/proxy-service-pthhq-79sbr:160/proxy/: foo (200; 31.189957ms)
  I0512 15:54:16.710901 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/https:proxy-service-pthhq-79sbr:462/proxy/: tls qux (200; 31.242542ms)
  I0512 15:54:16.711022 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/pods/http:proxy-service-pthhq-79sbr:162/proxy/: bar (200; 31.65057ms)
  I0512 15:54:16.711927 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname1/proxy/: tls baz (200; 32.279495ms)
  I0512 15:54:16.716083 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname1/proxy/: foo (200; 36.567141ms)
  I0512 15:54:16.716598 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname2/proxy/: bar (200; 36.967637ms)
  I0512 15:54:16.716666 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/services/http:proxy-service-pthhq:portname1/proxy/: foo (200; 37.097486ms)
  I0512 15:54:16.716702 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/services/https:proxy-service-pthhq:tlsportname2/proxy/: tls qux (200; 37.34062ms)
  I0512 15:54:16.716763 23 proxy.go:558] (19) /api/v1/namespaces/proxy-3137/services/proxy-service-pthhq:portname2/proxy/: bar (200; 37.243723ms)
  STEP: deleting ReplicationController proxy-service-pthhq in namespace proxy-3137, will wait for the garbage collector to delete the pods @ 05/12/25 15:54:16.717
  I0512 15:54:16.791973 23 resources.go:139] Deleting ReplicationController proxy-service-pthhq took: 17.343741ms
  I0512 15:54:16.894388 23 resources.go:163] Terminating ReplicationController proxy-service-pthhq pods took: 102.416104ms
  E0512 15:54:16.976289      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:17.795755 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-3137" for this suite. @ 05/12/25 15:54:17.806
• [12.253 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:207
  STEP: Creating a kubernetes client @ 05/12/25 15:54:17.821
  I0512 15:54:17.821600 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:54:17.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:54:17.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:54:17.874
  STEP: Setting up server cert @ 05/12/25 15:54:17.958
  E0512 15:54:17.977073      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:18.977174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:54:19.355
  STEP: Deploying the webhook pod @ 05/12/25 15:54:19.376
  STEP: Wait for the deployment to be ready @ 05/12/25 15:54:19.401
  I0512 15:54:19.417469 23 deployment.go:222] new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0512 15:54:19.977859      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:20.978253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 15:54:21.452
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:54:21.471
  E0512 15:54:21.978768      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:22.473205 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/12/25 15:54:22.492
  STEP: create a pod @ 05/12/25 15:54:22.528
  E0512 15:54:22.980921      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:23.981293      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/12/25 15:54:24.572
  I0512 15:54:24.572645 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=webhook-5249 attach --namespace=webhook-5249 to-be-attached-pod -i -c=container1'
  I0512 15:54:24.808625 23 builder.go:135] rc: 1
  E0512 15:54:24.982674      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:25.066318 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5249" for this suite. @ 05/12/25 15:54:25.091
  STEP: Destroying namespace "webhook-markers-296" for this suite. @ 05/12/25 15:54:25.139
• [7.348 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:2216
  STEP: Creating a kubernetes client @ 05/12/25 15:54:25.17
  I0512 15:54:25.170582 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:54:25.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:54:25.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:54:25.251
  STEP: creating service in namespace services-9626 @ 05/12/25 15:54:25.265
  STEP: creating service affinity-nodeport-transition in namespace services-9626 @ 05/12/25 15:54:25.266
  STEP: creating replication controller affinity-nodeport-transition in namespace services-9626 @ 05/12/25 15:54:25.317
  I0512 15:54:25.339277      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9626, replica count: 3
  E0512 15:54:25.982980      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:26.983361      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:27.983818      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:28.394788      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 15:54:28.428075 23 resource.go:361] Creating new exec pod
  E0512 15:54:28.988761      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:29.989743      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:30.990239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:31.491571 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-9626 exec execpod-affinitygclmv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  I0512 15:54:31.787363 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  I0512 15:54:31.787440 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:54:31.787560 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-9626 exec execpod-affinitygclmv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.6.83 80'
  E0512 15:54:31.990696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:32.075558 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.6.83 80\nConnection to 10.233.6.83 80 port [tcp/http] succeeded!\n"
  I0512 15:54:32.075646 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:54:32.076211 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-9626 exec execpod-affinitygclmv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.76 32500'
  I0512 15:54:32.342506 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.76 32500\nConnection to 10.62.16.76 32500 port [tcp/*] succeeded!\n"
  I0512 15:54:32.342605 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:54:32.342802 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-9626 exec execpod-affinitygclmv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.77 32500'
  I0512 15:54:32.638194 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.77 32500\nConnection to 10.62.16.77 32500 port [tcp/*] succeeded!\n"
  I0512 15:54:32.638297 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:54:32.685344 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-9626 exec execpod-affinitygclmv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.62.16.75:32500/ ; done'
  E0512 15:54:32.991128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:33.365897 23 builder.go:146] stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n"
  I0512 15:54:33.366079 23 builder.go:147] stdout: "\naffinity-nodeport-transition-fdmwf\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-xpht9\naffinity-nodeport-transition-fdmwf\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-xpht9\naffinity-nodeport-transition-fdmwf\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-xpht9\naffinity-nodeport-transition-fdmwf\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-xpht9\naffinity-nodeport-transition-fdmwf\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-xpht9\naffinity-nodeport-transition-fdmwf"
  I0512 15:54:33.366169 23 service.go:242] Received response from host: affinity-nodeport-transition-fdmwf
  I0512 15:54:33.366211 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.366247 23 service.go:242] Received response from host: affinity-nodeport-transition-xpht9
  I0512 15:54:33.366267 23 service.go:242] Received response from host: affinity-nodeport-transition-fdmwf
  I0512 15:54:33.366286 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.366305 23 service.go:242] Received response from host: affinity-nodeport-transition-xpht9
  I0512 15:54:33.366323 23 service.go:242] Received response from host: affinity-nodeport-transition-fdmwf
  I0512 15:54:33.366343 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.366364 23 service.go:242] Received response from host: affinity-nodeport-transition-xpht9
  I0512 15:54:33.366383 23 service.go:242] Received response from host: affinity-nodeport-transition-fdmwf
  I0512 15:54:33.366402 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.366420 23 service.go:242] Received response from host: affinity-nodeport-transition-xpht9
  I0512 15:54:33.366439 23 service.go:242] Received response from host: affinity-nodeport-transition-fdmwf
  I0512 15:54:33.366463 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.366491 23 service.go:242] Received response from host: affinity-nodeport-transition-xpht9
  I0512 15:54:33.366510 23 service.go:242] Received response from host: affinity-nodeport-transition-fdmwf
  I0512 15:54:33.392124 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-9626 exec execpod-affinitygclmv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.62.16.75:32500/ ; done'
  I0512 15:54:33.943819 23 builder.go:146] stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:32500/\n"
  I0512 15:54:33.943964 23 builder.go:147] stdout: "\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l\naffinity-nodeport-transition-4jh2l"
  I0512 15:54:33.944027 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944069 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944106 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944149 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944319 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944369 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944411 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944442 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944477 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944626 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944781 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944823 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944856 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944888 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.944922 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.945015 23 service.go:242] Received response from host: affinity-nodeport-transition-4jh2l
  I0512 15:54:33.945266 23 service.go:4061] Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9626, will wait for the garbage collector to delete the pods @ 05/12/25 15:54:33.974
  E0512 15:54:33.992647      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:34.058849 23 resources.go:139] Deleting ReplicationController affinity-nodeport-transition took: 21.017222ms
  I0512 15:54:34.159974 23 resources.go:163] Terminating ReplicationController affinity-nodeport-transition pods took: 101.121114ms
  E0512 15:54:34.993765      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:35.994195      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:36.994481      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:37.517414 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9626" for this suite. @ 05/12/25 15:54:37.541
• [12.390 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/job.go:1148
  STEP: Creating a kubernetes client @ 05/12/25 15:54:37.565
  I0512 15:54:37.565467 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename job @ 05/12/25 15:54:37.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:54:37.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:54:37.624
  STEP: Creating a suspended job @ 05/12/25 15:54:37.647
  STEP: Patching the Job @ 05/12/25 15:54:37.661
  STEP: Watching for Job to be patched @ 05/12/25 15:54:37.717
  I0512 15:54:37.721166 23 job.go:1330] Event ADDED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-job-label:e2e-hcsbx] and annotations: map[]
  I0512 15:54:37.721235 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-job-label:e2e-hcsbx] and annotations: map[]
  I0512 15:54:37.721704 23 job.go:1333] Event MODIFIED found for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[]
  STEP: Updating the job @ 05/12/25 15:54:37.722
  STEP: Watching for Job to be updated @ 05/12/25 15:54:37.752
  I0512 15:54:37.757558 23 job.go:1333] Event MODIFIED found for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:37.758245 23 job.go:1226] Found Job annotations: map[string]string{"updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/12/25 15:54:37.758
  I0512 15:54:37.767789 23 job.go:1233] Job: e2e-hcsbx as labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx]
  STEP: Waiting for job to complete @ 05/12/25 15:54:37.767
  E0512 15:54:37.994823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:38.995519      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:39.995899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:41.000801      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:42.001652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:43.002256      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:44.003169      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:45.004257      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:46.004883      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:47.005768      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:48.006269      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:49.006561      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 05/12/25 15:54:49.789
  STEP: Watching for Job to be deleted @ 05/12/25 15:54:49.81
  I0512 15:54:49.813803 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:49.814110 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:49.814679 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:49.815144 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:49.816468 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:49.817041 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:49.817412 23 job.go:1330] Event MODIFIED observed for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  I0512 15:54:49.817859 23 job.go:1333] Event DELETED found for Job e2e-hcsbx in namespace job-6827 with labels: map[e2e-hcsbx:patched e2e-job-label:e2e-hcsbx] and annotations: map[updated:true]
  STEP: Relist jobs to confirm deletion @ 05/12/25 15:54:49.818
  I0512 15:54:49.827220 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6827" for this suite. @ 05/12/25 15:54:49.851
• [12.348 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:905
  STEP: Creating a kubernetes client @ 05/12/25 15:54:49.918
  I0512 15:54:49.918407 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 15:54:49.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:54:49.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:54:49.971
  STEP: Creating service test in namespace statefulset-1469 @ 05/12/25 15:54:49.977
  STEP: Creating statefulset ss in namespace statefulset-1469 @ 05/12/25 15:54:49.989
  E0512 15:54:50.006805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:54:50.015914 23 wait.go:40] Found 0 stateful pods, waiting for 1
  E0512 15:54:51.007137      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:52.007927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:53.008360      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:54.009178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:55.009552      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:56.010350      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:57.010487      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:58.010650      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:54:59.010819      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:00.010961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:55:00.018604 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/12/25 15:55:00.042
  STEP: updating a scale subresource @ 05/12/25 15:55:00.051
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/12/25 15:55:00.065
  STEP: Patch a scale subresource @ 05/12/25 15:55:00.072
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/12/25 15:55:00.106
  I0512 15:55:00.124737 23 statefulset.go:138] Deleting all statefulset in ns statefulset-1469
  I0512 15:55:00.144065 23 rest.go:150] Scaling statefulset ss to 0
  E0512 15:55:01.011756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:02.012258      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:03.012996      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:04.014127      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:05.015236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:06.015913      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:07.016271      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:08.016936      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:09.017357      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:10.018166      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:55:10.264166 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 15:55:10.271479 23 rest.go:88] Deleting statefulset ss
  I0512 15:55:10.301557 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1469" for this suite. @ 05/12/25 15:55:10.314
• [20.415 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:125
  STEP: Creating a kubernetes client @ 05/12/25 15:55:10.334
  I0512 15:55:10.334319 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 15:55:10.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:55:10.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:55:10.403
  STEP: Creating configMap with name configmap-test-upd-9afaf5f7-d29d-42df-841c-a2af8eb656be @ 05/12/25 15:55:10.422
  STEP: Creating the pod @ 05/12/25 15:55:10.432
  E0512 15:55:11.020569      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:12.021536      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-9afaf5f7-d29d-42df-841c-a2af8eb656be @ 05/12/25 15:55:12.497
  STEP: waiting to observe update in volume @ 05/12/25 15:55:12.508
  E0512 15:55:13.021778      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:14.022196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:15.022207      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:16.023089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:17.023985      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:18.024024      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:19.024414      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:20.025299      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:21.025443      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:22.025611      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:23.025917      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:24.026574      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:25.027586      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:26.028388      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:27.028674      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:28.029073      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:29.029966      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:30.030828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:31.031217      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:32.031776      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:33.032748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:34.033297      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:35.034503      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:36.034719      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:37.035694      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:38.035863      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:39.036958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:40.037953      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:41.039002      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:42.039352      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:43.039362      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:44.040148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:45.041098      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:46.041252      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:47.042233      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:48.042757      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:49.043078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:50.043948      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:51.044969      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:52.045598      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:53.046503      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:54.047026      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:55.047482      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:56.047732      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:57.048814      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:58.048986      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:55:59.049786      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:00.050538      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:01.050837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:02.051255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:03.052081      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:04.052296      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:05.053385      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:06.053634      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:07.054047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:08.054080      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:09.055057      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:10.055139      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:11.055588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:12.056444      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:13.056887      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:14.057154      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:15.057929      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:16.058351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:17.058479      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:18.059009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:19.059855      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:20.059852      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:21.060374      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:22.060646      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:23.061201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:24.062199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:25.063221      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:26.063593      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:27.063702      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:28.064799      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:29.065262      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:56:29.270836 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-136" for this suite. @ 05/12/25 15:56:29.283
• [78.973 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/replica_set.go:177
  STEP: Creating a kubernetes client @ 05/12/25 15:56:29.307
  I0512 15:56:29.307649 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replicaset @ 05/12/25 15:56:29.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:56:29.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:56:29.417
  STEP: Create a Replicaset @ 05/12/25 15:56:29.436
  STEP: Verify that the required pods have come up. @ 05/12/25 15:56:29.449
  I0512 15:56:29.459684 23 resource.go:87] Pod name sample-pod: Found 0 pods out of 1
  E0512 15:56:30.065960      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:31.066435      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:32.066860      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:33.067335      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:34.067961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:56:34.471149 23 resource.go:87] Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/25 15:56:34.471
  STEP: Getting /status @ 05/12/25 15:56:34.471
  I0512 15:56:34.483037 23 replica_set.go:643] Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/12/25 15:56:34.483
  I0512 15:56:34.550406 23 replica_set.go:663] updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/12/25 15:56:34.55
  I0512 15:56:34.555140 23 replica_set.go:689] Observed &ReplicaSet event: ADDED
  I0512 15:56:34.555721 23 replica_set.go:689] Observed &ReplicaSet event: MODIFIED
  I0512 15:56:34.556242 23 replica_set.go:689] Observed &ReplicaSet event: MODIFIED
  I0512 15:56:34.556691 23 replica_set.go:689] Observed &ReplicaSet event: MODIFIED
  I0512 15:56:34.556868 23 replica_set.go:682] Found replicaset test-rs in namespace replicaset-1303 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  I0512 15:56:34.556946 23 replica_set.go:693] Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/12/25 15:56:34.557
  I0512 15:56:34.557283 23 replica_set.go:697] Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  I0512 15:56:34.570727 23 replica_set.go:701] Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/12/25 15:56:34.57
  I0512 15:56:34.575939 23 replica_set.go:725] Observed &ReplicaSet event: ADDED
  I0512 15:56:34.576684 23 replica_set.go:725] Observed &ReplicaSet event: MODIFIED
  I0512 15:56:34.577272 23 replica_set.go:725] Observed &ReplicaSet event: MODIFIED
  I0512 15:56:34.577882 23 replica_set.go:725] Observed &ReplicaSet event: MODIFIED
  I0512 15:56:34.578195 23 replica_set.go:721] Observed replicaset test-rs in namespace replicaset-1303 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  I0512 15:56:34.578507 23 replica_set.go:725] Observed &ReplicaSet event: MODIFIED
  I0512 15:56:34.578687 23 replica_set.go:718] Found replicaset test-rs in namespace replicaset-1303 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  I0512 15:56:34.578827 23 replica_set.go:729] Replicaset test-rs has a patched status
  I0512 15:56:34.579131 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1303" for this suite. @ 05/12/25 15:56:34.592
• [5.301 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:940
  STEP: Creating a kubernetes client @ 05/12/25 15:56:34.609
  I0512 15:56:34.609426 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 15:56:34.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:56:34.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:56:34.67
  STEP: Creating a ResourceQuota @ 05/12/25 15:56:34.679
  STEP: Getting a ResourceQuota @ 05/12/25 15:56:34.695
  STEP: Updating a ResourceQuota @ 05/12/25 15:56:34.708
  STEP: Verifying a ResourceQuota was modified @ 05/12/25 15:56:34.728
  STEP: Deleting a ResourceQuota @ 05/12/25 15:56:34.738
  STEP: Verifying the deleted ResourceQuota @ 05/12/25 15:56:34.777
  I0512 15:56:34.786354 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9207" for this suite. @ 05/12/25 15:56:34.811
• [0.222 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/service_accounts.go:531
  STEP: Creating a kubernetes client @ 05/12/25 15:56:34.831
  I0512 15:56:34.831821 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/25 15:56:34.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:56:34.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:56:34.889
  I0512 15:56:34.929459 23 service_accounts.go:618] created pod
  E0512 15:56:35.068614      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:36.069132      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:37.069709      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:38.070354      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:56:38.966
  E0512 15:56:39.071102      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:40.071358      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:41.072138      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:42.072738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:43.073503      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:44.073995      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:45.074972      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:46.075116      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:47.075326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:48.075810      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:49.076332      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:50.077327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:51.077950      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:52.078481      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:53.078927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:54.079071      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:55.080076      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:56.080567      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:57.080823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:58.082081      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:56:59.082651      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:00.083519      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:01.084201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:02.084755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:03.085008      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:04.085498      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:05.086591      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:06.086836      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:07.087311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:08.088063      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:08.967376 23 service_accounts.go:624] polling logs
  I0512 15:57:08.987746 23 service_accounts.go:634] Pod logs: 
  I0512 15:56:35.928899       1 log.go:245] OK: Got token
  I0512 15:56:35.929098       1 log.go:245] validating with in-cluster discovery
  I0512 15:56:35.929980       1 log.go:245] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0512 15:56:35.931193       1 log.go:245] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7798:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:(*jwt.NumericDate)(0xc000486f90), NotBefore:(*jwt.NumericDate)(0xc000487080), IssuedAt:(*jwt.NumericDate)(0xc000486fa0), ID:"75949993-0730-4b6f-990d-d43fa4f20e66"}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7798", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1babb652-a268-499e-be93-a0e5232376cf"}}}
  I0512 15:56:35.965190       1 log.go:245] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0512 15:56:35.978307       1 log.go:245] OK: Validated signature on JWT
  I0512 15:56:35.978781       1 log.go:245] OK: Got valid claims from token!
  I0512 15:56:35.978917       1 log.go:245] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7798:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:(*jwt.NumericDate)(0xc0002c91f8), NotBefore:(*jwt.NumericDate)(0xc0002c9220), IssuedAt:(*jwt.NumericDate)(0xc0002c9200), ID:"75949993-0730-4b6f-990d-d43fa4f20e66"}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7798", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1babb652-a268-499e-be93-a0e5232376cf"}}}

  I0512 15:57:08.987895 23 service_accounts.go:638] completed pod
  I0512 15:57:09.005577 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7798" for this suite. @ 05/12/25 15:57:09.021
• [34.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/downwardapi.go:92
  STEP: Creating a kubernetes client @ 05/12/25 15:57:09.048
  I0512 15:57:09.048374 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 15:57:09.05
  E0512 15:57:09.088382      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:57:09.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:57:09.117
  STEP: Creating a pod to test downward api env vars @ 05/12/25 15:57:09.128
  E0512 15:57:10.089849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:11.090159      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:12.090851      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:13.091212      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:57:13.241
  I0512 15:57:13.250967 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downward-api-0dc68f3c-1928-44ce-95b7-9b6b79f19a9f container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 15:57:13.28
  I0512 15:57:13.329976 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8355" for this suite. @ 05/12/25 15:57:13.346
• [4.314 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/node/security_context.go:302
  STEP: Creating a kubernetes client @ 05/12/25 15:57:13.362
  I0512 15:57:13.362867 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename security-context @ 05/12/25 15:57:13.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:57:13.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:57:13.421
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/12/25 15:57:13.428
  E0512 15:57:14.091844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:15.092894      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:16.093201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:17.094081      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:57:17.484
  I0512 15:57:17.492489 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod security-context-c39324fb-221e-40bf-adfe-e3b2bb037eea container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:57:17.505
  I0512 15:57:17.532278 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8963" for this suite. @ 05/12/25 15:57:17.552
• [4.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to create and update mutating webhook configurations with match conditions [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:763
  STEP: Creating a kubernetes client @ 05/12/25 15:57:17.568
  I0512 15:57:17.568268 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 15:57:17.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:57:17.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:57:17.612
  STEP: Setting up server cert @ 05/12/25 15:57:17.709
  E0512 15:57:18.095188      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:19.096001      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 15:57:19.211
  STEP: Deploying the webhook pod @ 05/12/25 15:57:19.249
  STEP: Wait for the deployment to be ready @ 05/12/25 15:57:19.279
  I0512 15:57:19.297950 23 deployment.go:222] new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0512 15:57:20.097180      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:21.097592      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 15:57:21.331
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 15:57:21.367
  E0512 15:57:22.098268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:22.368962 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: creating a mutating webhook with match conditions @ 05/12/25 15:57:22.387
  STEP: verifying the mutating webhook match conditions @ 05/12/25 15:57:22.404
  STEP: updating the mutating webhook match conditions @ 05/12/25 15:57:22.412
  STEP: verifying the mutating webhook match conditions @ 05/12/25 15:57:22.428
  I0512 15:57:22.561958 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6801" for this suite. @ 05/12/25 15:57:22.583
  STEP: Destroying namespace "webhook-markers-8756" for this suite. @ 05/12/25 15:57:22.625
• [5.099 seconds]
------------------------------
[sig-network] Services should serve endpoints on same port and different protocols [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:3703
  STEP: Creating a kubernetes client @ 05/12/25 15:57:22.668
  I0512 15:57:22.668502 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:57:22.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:57:22.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:57:22.764
  STEP: creating service multiprotocol-test in namespace services-7437 @ 05/12/25 15:57:22.778
  STEP: creating pod pod1 in namespace services-7437 @ 05/12/25 15:57:22.811
  STEP: Creating pod pod1 in namespace services-7437 @ 05/12/25 15:57:22.811
  E0512 15:57:23.099291      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:24.099817      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multiprotocol-test in namespace services-7437 to expose endpoints map[pod1:[{tcp-port 0 80 TCP } {udp-port 0 80 UDP }]] @ 05/12/25 15:57:24.886
  I0512 15:57:24.917598 23 service.go:4392] successfully validated that service multiprotocol-test in namespace services-7437 exposes endpoints map[pod1:[{tcp-port 0 80 TCP } {udp-port 0 80 UDP }]]
  STEP: Checking if the Service forwards traffic to the TCP and UDP port @ 05/12/25 15:57:24.917
  I0512 15:57:24.917793 23 resource.go:361] Creating new exec pod
  E0512 15:57:25.101417      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:26.102182      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:26.967103 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80'
  E0512 15:57:27.103291      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:27.283930 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.203 80\nConnection to 10.233.40.203 80 port [tcp/http] succeeded!\n"
  I0512 15:57:27.284066 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:57:27.284253 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80'
  E0512 15:57:28.103104      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:29.103430      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:30.104398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:31.104828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:31.584197 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -u -w 2 10.233.40.203 80\nConnection to 10.233.40.203 80 port [udp/*] succeeded!\n"
  I0512 15:57:31.584280 23 builder.go:147] stdout: "pod1"
  STEP: Checking if the Service forwards traffic to TCP only @ 05/12/25 15:57:31.584
  I0512 15:57:31.608994 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80'
  I0512 15:57:31.957398 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.203 80\nConnection to 10.233.40.203 80 port [tcp/http] succeeded!\n"
  I0512 15:57:31.957525 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 15:57:31.957801 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80'
  E0512 15:57:32.105042      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:32.325743 23 builder.go:135] rc: 1
  I0512 15:57:32.325873 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -u -w 2 10.233.40.203 80
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0512 15:57:33.106437      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:33.958053 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80'
  E0512 15:57:34.107429      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:34.298087 23 builder.go:135] rc: 1
  I0512 15:57:34.298238 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -u -w 2 10.233.40.203 80
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0512 15:57:35.108728      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:35.958583 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80'
  E0512 15:57:36.108937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:36.297883 23 builder.go:135] rc: 1
  I0512 15:57:36.298110 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -u -w 2 10.233.40.203 80
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0512 15:57:37.110005      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:37.958220 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80'
  E0512 15:57:38.110511      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:38.281839 23 builder.go:135] rc: 1
  I0512 15:57:38.281956 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -u -w 2 10.233.40.203 80
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  STEP: Checking if the Service forwards traffic to UDP only @ 05/12/25 15:57:38.282
  I0512 15:57:38.307144 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80'
  I0512 15:57:38.678173 23 builder.go:135] rc: 1
  I0512 15:57:38.678323 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -u -w 2 10.233.40.203 80
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0512 15:57:39.110651      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:40.111537      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:40.307420 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -u -w 2 10.233.40.203 80'
  E0512 15:57:41.112308      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:42.112957      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:43.113140      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:44.113546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:44.619158 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -u -w 2 10.233.40.203 80\nConnection to 10.233.40.203 80 port [udp/*] succeeded!\n"
  I0512 15:57:44.619301 23 builder.go:147] stdout: "pod1"
  I0512 15:57:44.619499 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80'
  E0512 15:57:45.114478      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:45.913127 23 builder.go:135] rc: 1
  I0512 15:57:45.913268 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.233.40.203 80
  nc: connect to 10.233.40.203 port 80 (tcp) failed: Connection refused
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0512 15:57:46.114624      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:46.620675 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80'
  E0512 15:57:47.115359      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:47.996114 23 builder.go:135] rc: 1
  I0512 15:57:47.996274 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.233.40.203 80
  nc: connect to 10.233.40.203 port 80 (tcp) failed: Connection refused
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0512 15:57:48.115642      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:48.619576 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80'
  E0512 15:57:49.116089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:49.912128 23 builder.go:135] rc: 1
  I0512 15:57:49.912262 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.233.40.203 80
  nc: connect to 10.233.40.203 port 80 (tcp) failed: Connection refused
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0512 15:57:50.116787      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:50.619706 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80'
  E0512 15:57:51.117482      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:51.931079 23 builder.go:135] rc: 1
  I0512 15:57:51.931209 23 util.go:239] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-7437 exec execpods2jcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.203 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.233.40.203 80
  nc: connect to 10.233.40.203 port 80 (tcp) failed: Connection refused
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  I0512 15:57:51.932284 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7437" for this suite. @ 05/12/25 15:57:51.947
• [29.295 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:55
  STEP: Creating a kubernetes client @ 05/12/25 15:57:51.964
  I0512 15:57:51.964671 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:57:51.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:57:52.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:57:52.006
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 15:57:52.011
  E0512 15:57:52.118550      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:53.119015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:54.119960      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:55.121088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:57:56.067
  I0512 15:57:56.075321 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-ec6146db-606c-471d-85c8-4d3d82e4d2d2 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 15:57:56.092
  E0512 15:57:56.121872      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:57:56.130953 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6608" for this suite. @ 05/12/25 15:57:56.143
• [4.194 seconds]
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance] [sig-architecture, Conformance]
k8s.io/kubernetes/test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/12/25 15:57:56.158
  I0512 15:57:56.158599 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename conformance-tests @ 05/12/25 15:57:56.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:57:56.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:57:56.209
  STEP: Getting node addresses @ 05/12/25 15:57:56.214
  I0512 15:57:56.214892 23 helper.go:48] Waiting up to 10m0s for all (but 0) nodes to be schedulable
  I0512 15:57:56.245431 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-3871" for this suite. @ 05/12/25 15:57:56.254
• [0.111 seconds]
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/preemption.go:221
  STEP: Creating a kubernetes client @ 05/12/25 15:57:56.271
  I0512 15:57:56.272226 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/25 15:57:56.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:57:56.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:57:56.337
  I0512 15:57:56.384131 23 wait.go:50] Waiting up to 1m0s for all nodes to be ready
  E0512 15:57:57.122137      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:58.122259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:57:59.122602      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:00.122789      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:01.123054      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:02.124088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:03.124486      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:04.124989      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:05.125284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:06.125849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:07.126889      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:08.127389      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:09.127535      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:10.128409      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:11.128748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:12.128781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:13.129958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:14.130538      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:15.130600      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:16.130850      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:17.131271      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:18.132313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:19.132971      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:20.133924      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:21.134085      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:22.134294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:23.135007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:24.135559      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:25.135677      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:26.135926      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:27.136316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:28.136473      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:29.136714      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:30.137266      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:31.137766      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:32.137839      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:33.138815      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:34.139296      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:35.139585      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:36.140211      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:37.140828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:38.141099      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:39.141355      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:40.141511      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:41.141727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:42.141868      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:43.142170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:44.142436      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:45.142744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:46.143438      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:47.143861      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:48.144210      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:49.144414      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:50.145506      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:51.145821      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:52.146280      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:53.147029      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:54.147323      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:55.147670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:56.148236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:58:56.394113 23 util.go:393] Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/12/25 15:58:56.399
  I0512 15:58:56.466625 23 preemption.go:266] Created pod: pod0-0-sched-preemption-low-priority
  I0512 15:58:56.478713 23 preemption.go:266] Created pod: pod0-1-sched-preemption-medium-priority
  I0512 15:58:56.554186 23 preemption.go:266] Created pod: pod1-0-sched-preemption-medium-priority
  I0512 15:58:56.579841 23 preemption.go:266] Created pod: pod1-1-sched-preemption-medium-priority
  I0512 15:58:56.639465 23 preemption.go:266] Created pod: pod2-0-sched-preemption-medium-priority
  I0512 15:58:56.655305 23 preemption.go:266] Created pod: pod2-1-sched-preemption-medium-priority
  I0512 15:58:56.734568 23 preemption.go:266] Created pod: pod3-0-sched-preemption-medium-priority
  I0512 15:58:56.756263 23 preemption.go:266] Created pod: pod3-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/12/25 15:58:56.756
  E0512 15:58:57.149148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:58.149848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:58:59.151278      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:00.151326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/12/25 15:59:00.873
  E0512 15:59:01.151610      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:02.151961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:03.152621      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:04.153401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:05.153767      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:05.201662 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-8599" for this suite. @ 05/12/25 15:59:05.212
• [68.957 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/12/25 15:59:05.23
  I0512 15:59:05.230907 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 15:59:05.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:05.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:05.277
  STEP: Creating secret with name secret-test-133bd320-b47e-4396-b97f-4aa43b3f54e0 @ 05/12/25 15:59:05.286
  STEP: Creating a pod to test consume secrets @ 05/12/25 15:59:05.298
  E0512 15:59:06.154035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:07.154232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:08.155181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:09.154945      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:59:09.389
  I0512 15:59:09.397282 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-secrets-e3fea0cd-6b32-44c0-8480-3cfd454515b2 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 15:59:09.422
  I0512 15:59:09.475050 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1053" for this suite. @ 05/12/25 15:59:09.499
• [4.288 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] VolumeAttachment Conformance should run through the lifecycle of a VolumeAttachment [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/volume_attachment.go:57
  STEP: Creating a kubernetes client @ 05/12/25 15:59:09.521
  I0512 15:59:09.521915 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename volumeattachment @ 05/12/25 15:59:09.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:09.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:09.604
  STEP: Create VolumeAttachment "va-e2e-4wsnl" on node "opscontrol-jaku1-worker-3" @ 05/12/25 15:59:09.622
  STEP: Get VolumeAttachment "va-e2e-4wsnl" on node "opscontrol-jaku1-worker-3" @ 05/12/25 15:59:09.636
  STEP: Patch VolumeAttachment "va-e2e-4wsnl" on node "opscontrol-jaku1-worker-3" @ 05/12/25 15:59:09.652
  STEP: List VolumeAttachments with "va-e2e-4wsnl=patched" label @ 05/12/25 15:59:09.666
  STEP: Delete VolumeAttachment "va-e2e-4wsnl" on node "opscontrol-jaku1-worker-3" @ 05/12/25 15:59:09.673
  STEP: Confirm deletion of VolumeAttachment "va-e2e-4wsnl" on node "opscontrol-jaku1-worker-3" @ 05/12/25 15:59:09.692
  STEP: Create VolumeAttachment "va-e2e-rc42r" on node "opscontrol-jaku1-worker-0" @ 05/12/25 15:59:09.735
  STEP: Update the VolumeAttachment "va-e2e-rc42r" on node "opscontrol-jaku1-worker-0" with label "va-e2e=updated" @ 05/12/25 15:59:09.747
  STEP: Create VolumeAttachment "va-e2e-wj96v" on node "opscontrol-jaku1-worker-0" @ 05/12/25 15:59:09.837
  STEP: Update the VolumeAttachment "va-e2e-wj96v" on node "opscontrol-jaku1-worker-0" with label "va-e2e=updated" @ 05/12/25 15:59:09.849
  STEP: DeleteCollection of VolumeAttachments with "va-e2e=updated" label @ 05/12/25 15:59:09.863
  STEP: Confirm deleteCollection of VolumeAttachments with "va-e2e=updated" label @ 05/12/25 15:59:09.896
  I0512 15:59:09.903828 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "volumeattachment-4867" for this suite. @ 05/12/25 15:59:09.942
• [0.448 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/12/25 15:59:09.972
  I0512 15:59:09.972783 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 15:59:09.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:10.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:10.026
  STEP: creating a Pod with a static label @ 05/12/25 15:59:10.04
  STEP: watching for Pod to be ready @ 05/12/25 15:59:10.061
  I0512 15:59:10.064959 23 pods.go:945] observed Pod pod-test in namespace pods-228 in phase Pending with labels: map[test-pod-static:true] & conditions []
  I0512 15:59:10.079518 23 pods.go:945] observed Pod pod-test in namespace pods-228 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC  }]
  I0512 15:59:10.123739 23 pods.go:945] observed Pod pod-test in namespace pods-228 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodReadyToStartContainers False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC  }]
  E0512 15:59:10.156157      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:11.157006      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:12.146097 23 pods.go:948] Found Pod pod-test in namespace pods-228 in phase Running with labels: map[test-pod-static:true] & conditions [{PodReadyToStartContainers True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:12 +0000 UTC  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2025-05-12 15:59:10 +0000 UTC  }]
  E0512 15:59:12.157817      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: patching the Pod with a new Label and updated data @ 05/12/25 15:59:12.162
  STEP: getting the Pod and ensuring that it's patched @ 05/12/25 15:59:12.187
  STEP: replacing the Pod's status Ready condition to False @ 05/12/25 15:59:12.196
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/12/25 15:59:12.225
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/12/25 15:59:12.225
  STEP: watching for the Pod to be deleted @ 05/12/25 15:59:12.253
  I0512 15:59:12.257859 23 pods.go:1058] observed event type MODIFIED
  E0512 15:59:13.158670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:14.158813      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:14.169262 23 pods.go:1058] observed event type MODIFIED
  I0512 15:59:14.537228 23 pods.go:1058] observed event type MODIFIED
  E0512 15:59:15.161082      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:15.182488 23 pods.go:1058] observed event type MODIFIED
  I0512 15:59:15.200185 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-228" for this suite. @ 05/12/25 15:59:15.211
• [5.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:79
  STEP: Creating a kubernetes client @ 05/12/25 15:59:15.235
  I0512 15:59:15.236011 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 15:59:15.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:15.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:15.283
  STEP: Counting existing ResourceQuota @ 05/12/25 15:59:15.293
  E0512 15:59:16.161302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:17.162051      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:18.162609      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:19.163103      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:20.164229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/25 15:59:20.307
  STEP: Ensuring resource quota status is calculated @ 05/12/25 15:59:20.321
  E0512 15:59:21.164622      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:22.164804      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:22.329972 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8825" for this suite. @ 05/12/25 15:59:22.343
• [7.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:3209
  STEP: Creating a kubernetes client @ 05/12/25 15:59:22.361
  I0512 15:59:22.361891 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 15:59:22.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:22.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:22.411
  STEP: creating an Endpoint @ 05/12/25 15:59:22.426
  STEP: waiting for available Endpoint @ 05/12/25 15:59:22.437
  STEP: listing all Endpoints @ 05/12/25 15:59:22.445
  STEP: updating the Endpoint @ 05/12/25 15:59:22.453
  STEP: fetching the Endpoint @ 05/12/25 15:59:22.475
  STEP: patching the Endpoint @ 05/12/25 15:59:22.483
  STEP: fetching the Endpoint @ 05/12/25 15:59:22.506
  STEP: deleting the Endpoint by Collection @ 05/12/25 15:59:22.515
  STEP: waiting for Endpoint deletion @ 05/12/25 15:59:22.537
  STEP: fetching the Endpoint @ 05/12/25 15:59:22.54
  I0512 15:59:22.548787 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7711" for this suite. @ 05/12/25 15:59:22.559
• [0.214 seconds]
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance] [sig-instrumentation, Conformance]
k8s.io/kubernetes/test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/12/25 15:59:22.576
  I0512 15:59:22.576793 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename events @ 05/12/25 15:59:22.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:22.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:22.629
  STEP: Create set of events @ 05/12/25 15:59:22.649
  STEP: get a list of Events with a label in the current namespace @ 05/12/25 15:59:22.716
  STEP: delete a list of events @ 05/12/25 15:59:22.736
  I0512 15:59:22.736775 23 events.go:224] requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/12/25 15:59:22.818
  I0512 15:59:22.830002 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4206" for this suite. @ 05/12/25 15:59:22.841
• [0.283 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:210
  STEP: Creating a kubernetes client @ 05/12/25 15:59:22.862
  I0512 15:59:22.862319 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 15:59:22.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:22.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:22.93
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/12/25 15:59:22.936
  E0512 15:59:23.165303      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:24.166262      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:25.166457      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:26.166589      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 15:59:26.996
  I0512 15:59:27.002073 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-fd5db550-f647-41d5-b274-9821fdae095d container test-container: <nil>
  STEP: delete the pod @ 05/12/25 15:59:27.019
  I0512 15:59:27.061913 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7935" for this suite. @ 05/12/25 15:59:27.078
• [4.249 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/field_validation.go:287
  STEP: Creating a kubernetes client @ 05/12/25 15:59:27.112
  I0512 15:59:27.112446 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename field-validation @ 05/12/25 15:59:27.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:27.162
  E0512 15:59:27.166574      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:27.168
  I0512 15:59:27.174100 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 15:59:28.167560      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:29.168458      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:30.169036      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:31.169090      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:32.170023      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:33.170290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:34.170978      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:35.171090      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:35.411219 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8998" for this suite. @ 05/12/25 15:59:35.423
• [8.326 seconds]
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/disruption.go:351
  STEP: Creating a kubernetes client @ 05/12/25 15:59:35.438
  I0512 15:59:35.438181 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename disruption @ 05/12/25 15:59:35.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:35.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:35.478
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/12/25 15:59:35.483
  STEP: Waiting for the pdb to be processed @ 05/12/25 15:59:35.492
  E0512 15:59:36.172199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:37.172873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/12/25 15:59:37.517
  STEP: Waiting for all pods to be running @ 05/12/25 15:59:37.518
  I0512 15:59:37.527954 23 disruption.go:680] pods: 0 < 3
  E0512 15:59:38.174588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:39.175274      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/12/25 15:59:39.529
  STEP: Updating the pdb to allow a pod to be evicted @ 05/12/25 15:59:39.569
  STEP: Waiting for the pdb to be processed @ 05/12/25 15:59:39.59
  E0512 15:59:40.175835      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:41.176243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/12/25 15:59:41.601
  STEP: Waiting for all pods to be running @ 05/12/25 15:59:41.601
  STEP: Waiting for the pdb to observed all healthy pods @ 05/12/25 15:59:41.61
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/12/25 15:59:41.684
  STEP: Waiting for the pdb to be processed @ 05/12/25 15:59:41.733
  E0512 15:59:42.177070      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:43.177317      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 05/12/25 15:59:43.744
  STEP: locating a running pod @ 05/12/25 15:59:43.763
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/12/25 15:59:43.805
  STEP: Waiting for the pdb to be deleted @ 05/12/25 15:59:43.826
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/12/25 15:59:43.837
  STEP: Waiting for all pods to be running @ 05/12/25 15:59:43.837
  I0512 15:59:43.922627 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3629" for this suite. @ 05/12/25 15:59:43.962
• [8.565 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/security_context.go:531
  STEP: Creating a kubernetes client @ 05/12/25 15:59:44.004
  I0512 15:59:44.004886 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename security-context-test @ 05/12/25 15:59:44.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:44.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:44.075
  E0512 15:59:44.178061      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:45.178197      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:46.179243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:47.179451      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:48.179439 23 security_context.go:538] Got logs for pod "busybox-privileged-false-8bfbec57-b281-45c4-8bcb-98e8016d067b": "ip: RTNETLINK answers: Operation not permitted\n"
  I0512 15:59:48.180089 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 15:59:48.179569      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "security-context-test-9839" for this suite. @ 05/12/25 15:59:48.195
• [4.221 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/12/25 15:59:48.226
  I0512 15:59:48.226729 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename containers @ 05/12/25 15:59:48.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:48.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:48.285
  E0512 15:59:49.180731      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:50.181202      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:50.377737 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-527" for this suite. @ 05/12/25 15:59:50.389
• [2.179 seconds]
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:132
  STEP: Creating a kubernetes client @ 05/12/25 15:59:50.405
  I0512 15:59:50.405627 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 15:59:50.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:50.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:50.442
  STEP: Creating the pod @ 05/12/25 15:59:50.452
  E0512 15:59:51.181539      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:52.182385      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:53.040967 23 pod_client.go:173] Successfully updated pod "labelsupdate573adb38-0779-4e2a-863b-5ef4f68a4072"
  E0512 15:59:53.183316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:54.184496      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 15:59:55.074120 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2026" for this suite. @ 05/12/25 15:59:55.09
• [4.701 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:86
  STEP: Creating a kubernetes client @ 05/12/25 15:59:55.106
  I0512 15:59:55.106500 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/25 15:59:55.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 15:59:55.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 15:59:55.173
  I0512 15:59:55.180703 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 15:59:55.184586      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:56.185232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:57.185713      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:58.186223      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 15:59:59.187024      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:00.187178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:01.187543      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:02.187578      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:03.187602      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:04.188088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:05.188575      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:06.189500      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:07.189730      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:08.190001      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:09.190748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:10.191152      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:11.191351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:12.191532      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:13.192643      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:14.193471      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:15.193756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:16.194781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:17.195113      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:18.195450      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:19.195387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:20.195448      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:21.196438      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:22.196952      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:23.198073      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:24.198670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:25.199498      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:26.200631      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:27.201556      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:28.202288      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:29.202997      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:30.203681      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:31.204277      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:32.205116      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:33.205321      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:34.205473      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:35.205744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:36.206690      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:37.207553      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:38.208708      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:39.209174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:40.210047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:41.210990      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:42.211261      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:43.211562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:44.212432      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:45.213493      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:46.214195      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:47.214494      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:48.214693      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:49.215737      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:50.216673      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:51.217684      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:52.218542      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:53.219523      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:54.219400      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:55.219916      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:56.220871      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:00:56.627864 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8359" for this suite. @ 05/12/25 16:00:56.637
• [61.549 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:130
  STEP: Creating a kubernetes client @ 05/12/25 16:00:56.658
  I0512 16:00:56.658830 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 16:00:56.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:00:56.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:00:56.702
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/12/25 16:00:56.707
  E0512 16:00:57.221813      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:58.221954      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:00:59.222128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:00.222168      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:01:00.75
  I0512 16:01:00.756595 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-1fe03dcc-9861-49bd-94d0-857aa6ee0c9a container test-container: <nil>
  STEP: delete the pod @ 05/12/25 16:01:00.769
  I0512 16:01:00.800671 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7086" for this suite. @ 05/12/25 16:01:00.808
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance] [sig-instrumentation, Conformance]
k8s.io/kubernetes/test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/12/25 16:01:00.825
  I0512 16:01:00.825364 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename events @ 05/12/25 16:01:00.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:01:00.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:01:00.861
  STEP: creating a test event @ 05/12/25 16:01:00.866
  STEP: listing events in all namespaces @ 05/12/25 16:01:00.878
  STEP: listing events in test namespace @ 05/12/25 16:01:00.915
  STEP: listing events with field selection filtering on source @ 05/12/25 16:01:00.921
  STEP: listing events with field selection filtering on reportingController @ 05/12/25 16:01:00.927
  STEP: getting the test event @ 05/12/25 16:01:00.941
  STEP: patching the test event @ 05/12/25 16:01:00.953
  STEP: getting the test event @ 05/12/25 16:01:00.97
  STEP: updating the test event @ 05/12/25 16:01:00.976
  STEP: getting the test event @ 05/12/25 16:01:00.989
  STEP: deleting the test event @ 05/12/25 16:01:00.996
  STEP: listing events in all namespaces @ 05/12/25 16:01:01.011
  STEP: listing events in test namespace @ 05/12/25 16:01:01.04
  I0512 16:01:01.046080 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7494" for this suite. @ 05/12/25 16:01:01.056
• [0.247 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:215
  STEP: Creating a kubernetes client @ 05/12/25 16:01:01.073
  I0512 16:01:01.074103 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 16:01:01.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:01:01.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:01:01.11
  STEP: Creating pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577 @ 05/12/25 16:01:01.116
  E0512 16:01:01.222519      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:02.222831      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 16:01:03.162
  I0512 16:01:03.170394 23 container_probe.go:1749] Initial restart count of pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 is 0
  I0512 16:01:03.178419 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:03.223603      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:04.223899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:05.186693 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:05.223828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:06.224244      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:07.195533 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:07.225232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:08.225378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:09.202864 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:09.226162      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:10.226211      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:11.214607 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:11.226905      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:12.226872      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:13.223628 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:13.227853      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:14.228103      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:15.228574      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:15.232502 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:16.228958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:17.229346      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:17.245659 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:18.229608      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:19.229788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:19.252868 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:20.230007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:21.230331      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:21.260063 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:22.230938      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:23.231238      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:23.270915 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:24.231714      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:25.232133      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:25.280091 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:26.232236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:27.232430      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:27.289051 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:28.232688      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:29.232976      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:29.299571 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:30.234034      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:31.234236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:31.307560 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:32.234624      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:33.235201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:33.317478 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:34.235414      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:35.236468      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:35.327505 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:36.236783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:37.237344      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:37.345271 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:38.238032      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:39.238510      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:39.355441 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:40.239226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:41.239667      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:41.363164 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:42.240007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:43.240637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:43.375404 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:44.241412      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:45.242375      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:45.386238 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:46.242550      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:47.242786      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:47.394170 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:48.243032      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:49.243640      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:49.405104 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:50.244487      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:51.244749      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:51.415249 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:52.245119      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:53.245891      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:53.425354 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:54.247082      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:55.248025      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:55.433944 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:56.248907      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:57.249563      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:57.443322 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:01:58.249963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:01:59.250169      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:01:59.452260 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:00.250497      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:01.251048      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:01.462802 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:02.251220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:03.251708      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:03.472145 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:04.252326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:05.253040      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:05.483882 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:06.253984      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:07.255138      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:07.491623 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:08.255717      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:09.256770      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:09.501333 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:10.256968      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:11.257660      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:11.510103 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:12.257954      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:13.258239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:13.526908 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:14.258727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:15.258791      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:15.540935 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:16.259756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:17.260348      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:17.553340 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:18.260434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:19.260668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:19.561786 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:20.260861      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:21.261034      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:21.569781 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:22.261294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:23.262190      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:23.581723 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:24.262112      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:25.263079      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:25.589816 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:26.263993      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:27.265826      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:27.599303 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:28.266157      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:29.266680      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:29.609170 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:30.266776      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:31.267316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:31.618689 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:32.268506      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:33.269073      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:33.636149 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:34.269272      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:35.270027      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:35.646814 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:36.270350      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:37.270584      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:37.657702 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:38.271552      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:39.272002      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:39.667755 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:40.272375      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:41.272745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:41.678283 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:42.272862      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:43.273982      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:43.692842 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:44.274558      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:45.275594      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:45.701386 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:46.276022      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:47.276727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:47.710280 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:48.276824      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:49.277713      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:49.720731 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:50.277707      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:51.277875      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:51.729472 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:52.278159      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:53.279102      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:53.745905 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:54.279517      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:55.280698      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:55.751930 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:56.280803      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:57.281368      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:57.762610 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:02:58.281562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:02:59.282292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:02:59.771854 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:00.283462      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:01.283944      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:01.781595 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:02.284094      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:03.285343      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:03.796686 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:04.286031      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:05.286998      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:05.808652 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:06.287958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:07.288470      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:07.816043 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:08.288891      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:09.290025      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:09.826526 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:10.290401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:11.291391      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:11.834935 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:12.291287      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:13.292188      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:13.845208 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:14.292864      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:15.293606      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:15.853986 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:16.294538      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:17.295020      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:17.863013 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:18.296088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:19.296358      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:19.869885 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:20.297138      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:21.297229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:21.884899 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:22.298063      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:23.298561      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:23.897608 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:24.299579      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:25.300376      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:25.906052 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:26.300985      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:27.301436      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:27.914020 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:28.301752      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:29.302425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:29.922397 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:30.302765      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:31.303414      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:31.937797 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:32.304466      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:33.304836      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:33.951223 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:34.305698      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:35.305788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:35.962925 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:36.306236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:37.306553      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:37.974322 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:38.306660      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:39.307471      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:39.987662 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:40.307942      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:41.308708      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:41.997206 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:42.310309      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:43.310983      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:44.014393 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:44.311658      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:45.312294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:46.025713 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:46.313100      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:47.313554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:48.038823 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:48.313974      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:49.314904      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:50.050898 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:50.315764      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:51.315964      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:52.060126 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:52.316274      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:53.316466      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:54.070960 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:54.316712      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:55.317408      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:56.086913 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:56.318598      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:57.319178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:03:58.104757 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:03:58.320046      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:03:59.320855      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:00.114893 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:00.321146      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:01.321881      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:02.124442 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:02.322673      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:03.322833      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:04.134335 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:04.323523      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:05.324655      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:06.143550 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:06.325266      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:07.325891      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:08.153349 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:08.326395      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:09.327043      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:10.163932 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:10.328177      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:11.328845      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:12.172963 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:12.329274      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:13.330278      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:14.183455 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:14.330747      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:15.331063      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:16.194282 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:16.331551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:17.332078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:18.209798 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:18.332291      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:19.332610      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:20.221707 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:20.333019      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:21.333832      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:22.239466 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:22.334287      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:23.334491      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:24.249509 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:24.335232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:25.335603      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:26.258042 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:26.335722      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:27.335937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:28.268992 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:28.336926      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:29.337152      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:30.278383 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:30.337673      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:31.337727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:32.287224 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:32.338837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:33.339148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:34.296602 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:34.340064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:35.340287      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:36.303614 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:36.341241      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:37.341492      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:38.313153 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:38.342576      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:39.342979      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:40.323908 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:40.343982      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:41.344351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:42.332750 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:42.344777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:43.345239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:44.343046 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:44.345267      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:45.345495      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:46.346456      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:46.353370 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:47.347290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:48.348257      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:48.375346 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:49.349293      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:50.350361      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:50.384376 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:51.350617      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:52.351604      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:52.403651 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:53.351750      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:54.352220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:54.413505 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:55.352981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:56.353143      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:56.425982 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:57.353489      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:04:58.353698      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:04:58.436720 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:04:59.354573      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:00.355650      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:00.447463 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:05:01.356383      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:02.356922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:02.457260 23 container_probe.go:1759] Get pod test-webserver-ae9302cd-e8da-4864-8088-10e2f951dba2 in namespace container-probe-2577
  E0512 16:05:03.357214      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:04.357941      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting the pod @ 05/12/25 16:05:04.457
  I0512 16:05:04.497593 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2577" for this suite. @ 05/12/25 16:05:04.519
• [243.470 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/12/25 16:05:04.55
  I0512 16:05:04.552142 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 16:05:04.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:04.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:04.629
  STEP: Creating secret with name secret-test-map-195a60b8-78e9-4ba1-b464-2ca5b4524bce @ 05/12/25 16:05:04.639
  STEP: Creating a pod to test consume secrets @ 05/12/25 16:05:04.655
  E0512 16:05:05.358187      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:06.358539      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:07.358780      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:08.359340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:05:08.71
  I0512 16:05:08.719398 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-secrets-629a281c-ee60-4c0c-bb9c-c77bf47044f0 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 16:05:08.754
  I0512 16:05:08.798404 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4089" for this suite. @ 05/12/25 16:05:08.812
• [4.284 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/12/25 16:05:08.834
  I0512 16:05:08.834360 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename watch @ 05/12/25 16:05:08.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:08.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:08.881
  STEP: creating a watch on configmaps with a certain label @ 05/12/25 16:05:08.887
  STEP: creating a new configmap @ 05/12/25 16:05:08.889
  STEP: modifying the configmap once @ 05/12/25 16:05:08.899
  STEP: changing the label value of the configmap @ 05/12/25 16:05:08.92
  STEP: Expecting to observe a delete notification for the watched object @ 05/12/25 16:05:08.955
  I0512 16:05:08.955548 23 watch.go:431] Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2613  843145f0-24e1-466f-b4f2-f75a577318c1 48809 0 2025-05-12 16:05:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2025-05-12 16:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:05:08.955850 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2613  843145f0-24e1-466f-b4f2-f75a577318c1 48811 0 2025-05-12 16:05:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2025-05-12 16:05:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:05:08.955990 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2613  843145f0-24e1-466f-b4f2-f75a577318c1 48812 0 2025-05-12 16:05:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2025-05-12 16:05:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/12/25 16:05:08.956
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/12/25 16:05:08.971
  E0512 16:05:09.360237      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:10.360606      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:11.361556      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:12.361358      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:13.361513      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:14.362060      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:15.362832      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:16.363070      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:17.364200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:18.364263      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 05/12/25 16:05:18.972
  STEP: modifying the configmap a third time @ 05/12/25 16:05:18.999
  STEP: deleting the configmap @ 05/12/25 16:05:19.026
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/12/25 16:05:19.042
  I0512 16:05:19.042212 23 watch.go:431] Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2613  843145f0-24e1-466f-b4f2-f75a577318c1 48874 0 2025-05-12 16:05:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2025-05-12 16:05:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:05:19.042475 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2613  843145f0-24e1-466f-b4f2-f75a577318c1 48875 0 2025-05-12 16:05:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2025-05-12 16:05:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:05:19.042619 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2613  843145f0-24e1-466f-b4f2-f75a577318c1 48877 0 2025-05-12 16:05:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2025-05-12 16:05:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:05:19.042814 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2613" for this suite. @ 05/12/25 16:05:19.055
• [10.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:270
  STEP: Creating a kubernetes client @ 05/12/25 16:05:19.071
  I0512 16:05:19.071155 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/25 16:05:19.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:19.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:19.109
  I0512 16:05:19.114572 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:05:19.366260      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:20.367507      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:21.368255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:22.368934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:23.368970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:24.369103      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:25.370022      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:26.371114      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:26.981266 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4554" for this suite. @ 05/12/25 16:05:26.991
• [7.936 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:132
  STEP: Creating a kubernetes client @ 05/12/25 16:05:27.007
  I0512 16:05:27.007494 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:05:27.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:27.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:27.065
  STEP: Creating the pod @ 05/12/25 16:05:27.072
  E0512 16:05:27.371336      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:28.372082      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:29.373086      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:29.667556 23 pod_client.go:173] Successfully updated pod "labelsupdate5863703f-8a6b-4b81-97c6-61755902a194"
  E0512 16:05:30.374266      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:31.374686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:31.695633 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-385" for this suite. @ 05/12/25 16:05:31.706
• [4.715 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:263
  STEP: Creating a kubernetes client @ 05/12/25 16:05:31.726
  I0512 16:05:31.726951 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:05:31.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:31.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:31.768
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 16:05:31.775
  E0512 16:05:32.375452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:33.375738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:34.376062      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:35.376254      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:05:35.838
  I0512 16:05:35.849470 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-3441edc1-a8c4-4989-ab94-4463273da694 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 16:05:35.871
  I0512 16:05:35.912395 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4704" for this suite. @ 05/12/25 16:05:35.923
• [4.215 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should support RuntimeClasses API operations [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtimeclass.go:191
  STEP: Creating a kubernetes client @ 05/12/25 16:05:35.947
  I0512 16:05:35.948127 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/25 16:05:35.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:35.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:36.002
  STEP: getting /apis @ 05/12/25 16:05:36.011
  STEP: getting /apis/node.k8s.io @ 05/12/25 16:05:36.027
  STEP: getting /apis/node.k8s.io/v1 @ 05/12/25 16:05:36.029
  STEP: creating @ 05/12/25 16:05:36.034
  STEP: watching @ 05/12/25 16:05:36.092
  I0512 16:05:36.092430 23 runtimeclass.go:275] starting watch
  STEP: getting @ 05/12/25 16:05:36.117
  STEP: listing @ 05/12/25 16:05:36.125
  STEP: patching @ 05/12/25 16:05:36.134
  STEP: updating @ 05/12/25 16:05:36.145
  I0512 16:05:36.158657 23 runtimeclass.go:305] waiting for watch events with expected annotations
  STEP: deleting @ 05/12/25 16:05:36.159
  STEP: deleting a collection @ 05/12/25 16:05:36.189
  I0512 16:05:36.269028 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1993" for this suite. @ 05/12/25 16:05:36.283
• [0.355 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance] [sig-auth, Conformance]
k8s.io/kubernetes/test/e2e/auth/service_accounts.go:277
  STEP: Creating a kubernetes client @ 05/12/25 16:05:36.303
  I0512 16:05:36.303908 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/25 16:05:36.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:36.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:36.359
  STEP: Creating a pod to test service account token:  @ 05/12/25 16:05:36.368
  E0512 16:05:36.376895      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:37.377922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:38.378136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:39.379233      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:40.380148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:05:40.419
  I0512 16:05:40.428750 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod test-pod-88557c47-1753-43e7-9257-5489d3f9d758 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:05:40.441
  I0512 16:05:40.477588 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7209" for this suite. @ 05/12/25 16:05:40.49
• [4.200 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/security_context.go:612
  STEP: Creating a kubernetes client @ 05/12/25 16:05:40.504
  I0512 16:05:40.505020 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename security-context-test @ 05/12/25 16:05:40.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:40.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:40.549
  E0512 16:05:41.381208      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:42.381562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:43.381722      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:44.382382      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:45.383486      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:46.383675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:46.632030 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9304" for this suite. @ 05/12/25 16:05:46.649
• [6.163 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/configmap.go:171
  STEP: Creating a kubernetes client @ 05/12/25 16:05:46.668
  I0512 16:05:46.668061 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 16:05:46.67
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:46.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:46.713
  STEP: creating a ConfigMap @ 05/12/25 16:05:46.72
  STEP: fetching the ConfigMap @ 05/12/25 16:05:46.732
  STEP: patching the ConfigMap @ 05/12/25 16:05:46.738
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/12/25 16:05:46.755
  STEP: deleting the ConfigMap by collection with a label selector @ 05/12/25 16:05:46.764
  STEP: listing all ConfigMaps in test namespace @ 05/12/25 16:05:46.783
  I0512 16:05:46.792604 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2782" for this suite. @ 05/12/25 16:05:46.805
• [0.161 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:146
  STEP: Creating a kubernetes client @ 05/12/25 16:05:46.831
  I0512 16:05:46.832112 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/25 16:05:46.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:46.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:46.883
  I0512 16:05:46.891227 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:05:47.384419      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:48.384758      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:49.384685      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:50.385274      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:51.385686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:52.386686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:52.497616 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-702" for this suite. @ 05/12/25 16:05:52.51
• [5.702 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/cronjob.go:341
  STEP: Creating a kubernetes client @ 05/12/25 16:05:52.535
  I0512 16:05:52.535244 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename cronjob @ 05/12/25 16:05:52.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:52.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:52.58
  STEP: Creating a cronjob @ 05/12/25 16:05:52.586
  STEP: creating @ 05/12/25 16:05:52.586
  STEP: getting @ 05/12/25 16:05:52.604
  STEP: listing @ 05/12/25 16:05:52.61
  STEP: watching @ 05/12/25 16:05:52.619
  I0512 16:05:52.619459 23 cronjob.go:370] starting watch
  STEP: cluster-wide listing @ 05/12/25 16:05:52.623
  STEP: cluster-wide watching @ 05/12/25 16:05:52.634
  I0512 16:05:52.634961 23 cronjob.go:382] starting watch
  STEP: patching @ 05/12/25 16:05:52.638
  STEP: updating @ 05/12/25 16:05:52.662
  I0512 16:05:52.696749 23 cronjob.go:406] waiting for watch events with expected annotations
  I0512 16:05:52.697328 23 cronjob.go:420] saw patched and updated annotations
  STEP: patching /status @ 05/12/25 16:05:52.698
  STEP: updating /status @ 05/12/25 16:05:52.726
  STEP: get /status @ 05/12/25 16:05:52.758
  STEP: deleting @ 05/12/25 16:05:52.769
  STEP: deleting a collection @ 05/12/25 16:05:52.812
  I0512 16:05:52.841043 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7312" for this suite. @ 05/12/25 16:05:52.859
• [0.341 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/endpointslice.go:356
  STEP: Creating a kubernetes client @ 05/12/25 16:05:52.876
  I0512 16:05:52.876908 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename endpointslice @ 05/12/25 16:05:52.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:52.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:52.912
  STEP: getting /apis @ 05/12/25 16:05:52.918
  STEP: getting /apis/discovery.k8s.io @ 05/12/25 16:05:52.935
  STEP: getting /apis/discovery.k8s.iov1 @ 05/12/25 16:05:52.937
  STEP: creating @ 05/12/25 16:05:52.939
  STEP: getting @ 05/12/25 16:05:52.979
  STEP: listing @ 05/12/25 16:05:52.986
  STEP: watching @ 05/12/25 16:05:52.993
  I0512 16:05:52.993710 23 endpointslice.go:447] starting watch
  STEP: cluster-wide listing @ 05/12/25 16:05:52.995
  STEP: cluster-wide watching @ 05/12/25 16:05:53.004
  I0512 16:05:53.004296 23 endpointslice.go:459] starting watch
  STEP: patching @ 05/12/25 16:05:53.007
  STEP: updating @ 05/12/25 16:05:53.02
  I0512 16:05:53.042954 23 endpointslice.go:482] waiting for watch events with expected annotations
  I0512 16:05:53.043096 23 endpointslice.go:495] saw patched and updated annotations
  STEP: deleting @ 05/12/25 16:05:53.043
  STEP: deleting a collection @ 05/12/25 16:05:53.068
  I0512 16:05:53.118381 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2134" for this suite. @ 05/12/25 16:05:53.133
• [0.273 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/12/25 16:05:53.152
  I0512 16:05:53.152900 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 16:05:53.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:53.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:53.198
  I0512 16:05:53.251062 23 resource.go:87] Pod name cleanup-pod: Found 0 pods out of 1
  E0512 16:05:53.387611      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:54.388226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:55.389220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:56.389934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:05:57.391094      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:58.269107 23 resource.go:87] Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/25 16:05:58.269
  I0512 16:05:58.269749 23 deployment.go:841] Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/12/25 16:05:58.304
  I0512 16:05:58.341554 23 deployment.go:633] Deployment "test-cleanup-deployment":
  (v1.Deployment) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=23) "test-cleanup-deployment",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-3117",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "be71fc26-78b4-4b9a-8106-aa57ee747d4f",
      ResourceVersion: (string) (len=5) "49251",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662758,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=1) {
        (string) (len=4) "name": (string) (len=11) "cleanup-pod"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) <nil>,
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662758,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=637) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 7d 7d 2c  |},"f:name":{}}},|
              00000030  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 70 72 6f  |"f:spec":{"f:pro|
              00000040  67 72 65 73 73 44 65 61  64 6c 69 6e 65 53 65 63  |gressDeadlineSec|
              00000050  6f 6e 64 73 22 3a 7b 7d  2c 22 66 3a 72 65 70 6c  |onds":{},"f:repl|
              00000060  69 63 61 73 22 3a 7b 7d  2c 22 66 3a 72 65 76 69  |icas":{},"f:revi|
              00000070  73 69 6f 6e 48 69 73 74  6f 72 79 4c 69 6d 69 74  |sionHistoryLimit|
              00000080  22 3a 7b 7d 2c 22 66 3a  73 65 6c 65 63 74 6f 72  |":{},"f:selector|
              00000090  22 3a 7b 7d 2c 22 66 3a  73 74 72 61 74 65 67 79  |":{},"f:strategy|
              000000a0  22 3a 7b 22 66 3a 72 6f  6c 6c 69 6e 67 55 70 64  |":{"f:rollingUpd|
              000000b0  61 74 65 22 3a 7b 22 2e  22 3a 7b 7d 2c 22 66 3a  |ate":{".":{},"f:|
              000000c0  6d 61 78 53 75 72 67 65  22 3a 7b 7d 2c 22 66 3a  |maxSurge":{},"f:|
              000000d0  6d 61 78 55 6e 61 76 61  69 6c 61 62 6c 65 22 3a  |maxUnavailable":|
              000000e0  7b 7d 7d 2c 22 66 3a 74  79 70 65 22 3a 7b 7d 7d  |{}},"f:type":{}}|
              000000f0  2c 22 66 3a 74 65 6d 70  6c 61 74 65 22 3a 7b 22  |,"f:template":{"|
              00000100  66 3a 6d 65 74 61 64 61  74 61 22 3a 7b 22 66 3a  |f:metadata":{"f:|
              00000110  6c 61 62 65 6c 73 22 3a  7b 22 2e 22 3a 7b 7d 2c  |labels":{".":{},|
              00000120  22 66 3a 6e 61 6d 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:name":{}}},"f|
              00000130  3a 73 70 65 63 22 3a 7b  22 66 3a 63 6f 6e 74 61  |:spec":{"f:conta|
              00000140  69 6e 65 72 73 22 3a 7b  22 6b 3a 7b 5c 22 6e 61  |iners":{"k:{\"na|
              00000150  6d 65 5c 22 3a 5c 22 61  67 6e 68 6f 73 74 5c 22  |me\":\"agnhost\"|
              00000160  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000170  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000180  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000190  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              000001a0  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              000001b0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001c0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              000001d0  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              000001e0  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              000001f0  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              00000200  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              00000210  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              00000220  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              00000230  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              00000240  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              00000250  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000260  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000270  63 6f 6e 64 73 22 3a 7b  7d 7d 7d 7d 7d           |conds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.DeploymentSpec) {
      Replicas: (*int32)(1),
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=1) {
          (string) (len=4) "name": (string) (len=11) "cleanup-pod"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=1) {
            (string) (len=4) "name": (string) (len=11) "cleanup-pod"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=7) "agnhost",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      },
      Strategy: (v1.DeploymentStrategy) {
        Type: (v1.DeploymentStrategyType) (len=13) "RollingUpdate",
        RollingUpdate: (*v1.RollingUpdateDeployment)({
          MaxUnavailable: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          }),
          MaxSurge: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          })
        })
      },
      MinReadySeconds: (int32) 0,
      RevisionHistoryLimit: (*int32)(0),
      Paused: (bool) false,
      ProgressDeadlineSeconds: (*int32)(600)
    },
    Status: (v1.DeploymentStatus) {
      ObservedGeneration: (int64) 0,
      Replicas: (int32) 0,
      UpdatedReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      UnavailableReplicas: (int32) 0,
      Conditions: ([]v1.DeploymentCondition) <nil>,
      CollisionCount: (*int32)(<nil>)
    }
  }


  I0512 16:05:58.364784 23 deployment.go:39] New ReplicaSet "test-cleanup-deployment-898f8f847" of Deployment "test-cleanup-deployment":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=33) "test-cleanup-deployment-898f8f847",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-3117",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "e2cbc097-cf4c-41ae-addc-1bfe96b50a1e",
      ResourceVersion: (string) (len=5) "49253",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662758,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=11) "cleanup-pod",
        (string) (len=17) "pod-template-hash": (string) (len=9) "898f8f847"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=23) "test-cleanup-deployment",
          UID: (types.UID) (len=36) "be71fc26-78b4-4b9a-8106-aa57ee747d4f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662758,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=783) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 62 65 37 31 66 63  32 36 2d 37 38 62 34 2d  |\"be71fc26-78b4-|
              00000120  34 62 39 61 2d 38 31 30  36 2d 61 61 35 37 65 65  |4b9a-8106-aa57ee|
              00000130  37 34 37 64 34 66 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |747d4f\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 72 65 70  |"f:spec":{"f:rep|
              00000150  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 73 65 6c  |licas":{},"f:sel|
              00000160  65 63 74 6f 72 22 3a 7b  7d 2c 22 66 3a 74 65 6d  |ector":{},"f:tem|
              00000170  70 6c 61 74 65 22 3a 7b  22 66 3a 6d 65 74 61 64  |plate":{"f:metad|
              00000180  61 74 61 22 3a 7b 22 66  3a 6c 61 62 65 6c 73 22  |ata":{"f:labels"|
              00000190  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6e 61 6d 65  |:{".":{},"f:name|
              000001a0  22 3a 7b 7d 2c 22 66 3a  70 6f 64 2d 74 65 6d 70  |":{},"f:pod-temp|
              000001b0  6c 61 74 65 2d 68 61 73  68 22 3a 7b 7d 7d 7d 2c  |late-hash":{}}},|
              000001c0  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 63 6f 6e  |"f:spec":{"f:con|
              000001d0  74 61 69 6e 65 72 73 22  3a 7b 22 6b 3a 7b 5c 22  |tainers":{"k:{\"|
              000001e0  6e 61 6d 65 5c 22 3a 5c  22 61 67 6e 68 6f 73 74  |name\":\"agnhost|
              000001f0  5c 22 7d 22 3a 7b 22 2e  22 3a 7b 7d 2c 22 66 3a  |\"}":{".":{},"f:|
              00000200  69 6d 61 67 65 22 3a 7b  7d 2c 22 66 3a 69 6d 61  |image":{},"f:ima|
              00000210  67 65 50 75 6c 6c 50 6f  6c 69 63 79 22 3a 7b 7d  |gePullPolicy":{}|
              00000220  2c 22 66 3a 6e 61 6d 65  22 3a 7b 7d 2c 22 66 3a  |,"f:name":{},"f:|
              00000230  72 65 73 6f 75 72 63 65  73 22 3a 7b 7d 2c 22 66  |resources":{},"f|
              00000240  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              00000250  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000260  69 6f 6e 4d 65 73 73 61  67 65 50 61 74 68 22 3a  |ionMessagePath":|
              00000270  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000280  6e 4d 65 73 73 61 67 65  50 6f 6c 69 63 79 22 3a  |nMessagePolicy":|
              00000290  7b 7d 7d 7d 2c 22 66 3a  64 6e 73 50 6f 6c 69 63  |{}}},"f:dnsPolic|
              000002a0  79 22 3a 7b 7d 2c 22 66  3a 72 65 73 74 61 72 74  |y":{},"f:restart|
              000002b0  50 6f 6c 69 63 79 22 3a  7b 7d 2c 22 66 3a 73 63  |Policy":{},"f:sc|
              000002c0  68 65 64 75 6c 65 72 4e  61 6d 65 22 3a 7b 7d 2c  |hedulerName":{},|
              000002d0  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              000002e0  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              000002f0  61 74 69 6f 6e 47 72 61  63 65 50 65 72 69 6f 64  |ationGracePeriod|
              00000300  53 65 63 6f 6e 64 73 22  3a 7b 7d 7d 7d 7d 7d     |Seconds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(1),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=11) "cleanup-pod",
          (string) (len=17) "pod-template-hash": (string) (len=9) "898f8f847"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=11) "cleanup-pod",
            (string) (len=17) "pod-template-hash": (string) (len=9) "898f8f847"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=7) "agnhost",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 0,
      FullyLabeledReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 0,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 16:05:58.367417 23 deployment.go:44] All old ReplicaSets of Deployment "test-cleanup-deployment":
  I0512 16:05:58.368045 23 deployment.go:47] (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=23) "test-cleanup-controller",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-3117",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "8d06dc30-13a4-4d29-881c-856cd57006b9",
      ResourceVersion: (string) (len=5) "49252",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662753,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=11) "cleanup-pod",
        (string) (len=3) "pod": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=23) "test-cleanup-deployment",
          UID: (types.UID) (len=36) "be71fc26-78b4-4b9a-8106-aa57ee747d4f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=3) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662753,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=483) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              00000030  3a 70 6f 64 22 3a 7b 7d  7d 7d 2c 22 66 3a 73 70  |:pod":{}}},"f:sp|
              00000040  65 63 22 3a 7b 22 66 3a  72 65 70 6c 69 63 61 73  |ec":{"f:replicas|
              00000050  22 3a 7b 7d 2c 22 66 3a  73 65 6c 65 63 74 6f 72  |":{},"f:selector|
              00000060  22 3a 7b 7d 2c 22 66 3a  74 65 6d 70 6c 61 74 65  |":{},"f:template|
              00000070  22 3a 7b 22 66 3a 6d 65  74 61 64 61 74 61 22 3a  |":{"f:metadata":|
              00000080  7b 22 66 3a 6c 61 62 65  6c 73 22 3a 7b 22 2e 22  |{"f:labels":{"."|
              00000090  3a 7b 7d 2c 22 66 3a 6e  61 6d 65 22 3a 7b 7d 2c  |:{},"f:name":{},|
              000000a0  22 66 3a 70 6f 64 22 3a  7b 7d 7d 7d 2c 22 66 3a  |"f:pod":{}}},"f:|
              000000b0  73 70 65 63 22 3a 7b 22  66 3a 63 6f 6e 74 61 69  |spec":{"f:contai|
              000000c0  6e 65 72 73 22 3a 7b 22  6b 3a 7b 5c 22 6e 61 6d  |ners":{"k:{\"nam|
              000000d0  65 5c 22 3a 5c 22 68 74  74 70 64 5c 22 7d 22 3a  |e\":\"httpd\"}":|
              000000e0  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |{".":{},"f:image|
              000000f0  22 3a 7b 7d 2c 22 66 3a  69 6d 61 67 65 50 75 6c  |":{},"f:imagePul|
              00000100  6c 50 6f 6c 69 63 79 22  3a 7b 7d 2c 22 66 3a 6e  |lPolicy":{},"f:n|
              00000110  61 6d 65 22 3a 7b 7d 2c  22 66 3a 72 65 73 6f 75  |ame":{},"f:resou|
              00000120  72 63 65 73 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |rces":{},"f:term|
              00000130  69 6e 61 74 69 6f 6e 4d  65 73 73 61 67 65 50 61  |inationMessagePa|
              00000140  74 68 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |th":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 6f 6c 69  |ationMessagePoli|
              00000160  63 79 22 3a 7b 7d 7d 7d  2c 22 66 3a 64 6e 73 50  |cy":{}}},"f:dnsP|
              00000170  6f 6c 69 63 79 22 3a 7b  7d 2c 22 66 3a 72 65 73  |olicy":{},"f:res|
              00000180  74 61 72 74 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |tartPolicy":{},"|
              00000190  66 3a 73 63 68 65 64 75  6c 65 72 4e 61 6d 65 22  |f:schedulerName"|
              000001a0  3a 7b 7d 2c 22 66 3a 73  65 63 75 72 69 74 79 43  |:{},"f:securityC|
              000001b0  6f 6e 74 65 78 74 22 3a  7b 7d 2c 22 66 3a 74 65  |ontext":{},"f:te|
              000001c0  72 6d 69 6e 61 74 69 6f  6e 47 72 61 63 65 50 65  |rminationGracePe|
              000001d0  72 69 6f 64 53 65 63 6f  6e 64 73 22 3a 7b 7d 7d  |riodSeconds":{}}|
              000001e0  7d 7d 7d                                          |}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662754,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=130) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  61 76 61 69 6c 61 62 6c  65 52 65 70 6c 69 63 61  |availableReplica|
              00000020  73 22 3a 7b 7d 2c 22 66  3a 66 75 6c 6c 79 4c 61  |s":{},"f:fullyLa|
              00000030  62 65 6c 65 64 52 65 70  6c 69 63 61 73 22 3a 7b  |beledReplicas":{|
              00000040  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              00000050  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              00000060  65 61 64 79 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |eadyReplicas":{}|
              00000070  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000080  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662758,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=103) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6f 77 6e 65 72 52  65 66 65 72 65 6e 63 65  |f:ownerReference|
              00000020  73 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 6b 3a 7b 5c  |s":{".":{},"k:{\|
              00000030  22 75 69 64 5c 22 3a 5c  22 62 65 37 31 66 63 32  |"uid\":\"be71fc2|
              00000040  36 2d 37 38 62 34 2d 34  62 39 61 2d 38 31 30 36  |6-78b4-4b9a-8106|
              00000050  2d 61 61 35 37 65 65 37  34 37 64 34 66 5c 22 7d  |-aa57ee747d4f\"}|
              00000060  22 3a 7b 7d 7d 7d 7d                              |":{}}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(1),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=3) "pod": (string) (len=5) "httpd",
          (string) (len=4) "name": (string) (len=11) "cleanup-pod"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=3) "pod": (string) (len=5) "httpd",
            (string) (len=4) "name": (string) (len=11) "cleanup-pod"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)(<nil>),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 1,
      FullyLabeledReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      ObservedGeneration: (int64) 1,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  E0512 16:05:58.391743      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:05:58.417104 23 deployment.go:67] Pod "test-cleanup-controller-jm8c4" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=29) "test-cleanup-controller-jm8c4",
      GenerateName: (string) (len=24) "test-cleanup-controller-",
      Namespace: (string) (len=15) "deployment-3117",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "e2bcbe24-97f0-4fc4-90cc-dd968935672d",
      ResourceVersion: (string) (len=5) "49233",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662753,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=11) "cleanup-pod",
        (string) (len=3) "pod": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=23) "test-cleanup-controller",
          UID: (types.UID) (len=36) "8d06dc30-13a4-4d29-881c-856cd57006b9",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662753,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=500) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  22 3a 7b 7d 7d 2c 22 66  |},"f:pod":{}},"f|
              00000050  3a 6f 77 6e 65 72 52 65  66 65 72 65 6e 63 65 73  |:ownerReferences|
              00000060  22 3a 7b 22 2e 22 3a 7b  7d 2c 22 6b 3a 7b 5c 22  |":{".":{},"k:{\"|
              00000070  75 69 64 5c 22 3a 5c 22  38 64 30 36 64 63 33 30  |uid\":\"8d06dc30|
              00000080  2d 31 33 61 34 2d 34 64  32 39 2d 38 38 31 63 2d  |-13a4-4d29-881c-|
              00000090  38 35 36 63 64 35 37 30  30 36 62 39 5c 22 7d 22  |856cd57006b9\"}"|
              000000a0  3a 7b 7d 7d 7d 2c 22 66  3a 73 70 65 63 22 3a 7b  |:{}}},"f:spec":{|
              000000b0  22 66 3a 63 6f 6e 74 61  69 6e 65 72 73 22 3a 7b  |"f:containers":{|
              000000c0  22 6b 3a 7b 5c 22 6e 61  6d 65 5c 22 3a 5c 22 68  |"k:{\"name\":\"h|
              000000d0  74 74 70 64 5c 22 7d 22  3a 7b 22 2e 22 3a 7b 7d  |ttpd\"}":{".":{}|
              000000e0  2c 22 66 3a 69 6d 61 67  65 22 3a 7b 7d 2c 22 66  |,"f:image":{},"f|
              000000f0  3a 69 6d 61 67 65 50 75  6c 6c 50 6f 6c 69 63 79  |:imagePullPolicy|
              00000100  22 3a 7b 7d 2c 22 66 3a  6e 61 6d 65 22 3a 7b 7d  |":{},"f:name":{}|
              00000110  2c 22 66 3a 72 65 73 6f  75 72 63 65 73 22 3a 7b  |,"f:resources":{|
              00000120  7d 2c 22 66 3a 74 65 72  6d 69 6e 61 74 69 6f 6e  |},"f:termination|
              00000130  4d 65 73 73 61 67 65 50  61 74 68 22 3a 7b 7d 2c  |MessagePath":{},|
              00000140  22 66 3a 74 65 72 6d 69  6e 61 74 69 6f 6e 4d 65  |"f:terminationMe|
              00000150  73 73 61 67 65 50 6f 6c  69 63 79 22 3a 7b 7d 7d  |ssagePolicy":{}}|
              00000160  7d 2c 22 66 3a 64 6e 73  50 6f 6c 69 63 79 22 3a  |},"f:dnsPolicy":|
              00000170  7b 7d 2c 22 66 3a 65 6e  61 62 6c 65 53 65 72 76  |{},"f:enableServ|
              00000180  69 63 65 4c 69 6e 6b 73  22 3a 7b 7d 2c 22 66 3a  |iceLinks":{},"f:|
              00000190  72 65 73 74 61 72 74 50  6f 6c 69 63 79 22 3a 7b  |restartPolicy":{|
              000001a0  7d 2c 22 66 3a 73 63 68  65 64 75 6c 65 72 4e 61  |},"f:schedulerNa|
              000001b0  6d 65 22 3a 7b 7d 2c 22  66 3a 73 65 63 75 72 69  |me":{},"f:securi|
              000001c0  74 79 43 6f 6e 74 65 78  74 22 3a 7b 7d 2c 22 66  |tyContext":{},"f|
              000001d0  3a 74 65 72 6d 69 6e 61  74 69 6f 6e 47 72 61 63  |:terminationGrac|
              000001e0  65 50 65 72 69 6f 64 53  65 63 6f 6e 64 73 22 3a  |ePeriodSeconds":|
              000001f0  7b 7d 7d 7d                                       |{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662754,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 39 2e  32 32 31 5c 22 7d 22 3a  |.233.69.221\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-5sfsd",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-5sfsd",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)(<nil>),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662754,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662753,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662754,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662754,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662753,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) (len=13) "10.233.69.221",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.69.221"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662753,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882662754,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://ae6430b812cd4eff5cf5841e2128d0357cc852fa618d4e1aea493dda74a5099c",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-5sfsd",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:05:58.435075 23 deployment.go:67] Pod "test-cleanup-deployment-898f8f847-7rqzd" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=39) "test-cleanup-deployment-898f8f847-7rqzd",
      GenerateName: (string) (len=34) "test-cleanup-deployment-898f8f847-",
      Namespace: (string) (len=15) "deployment-3117",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "af77bf11-25f8-46f9-aed8-8ef9f7d9103a",
      ResourceVersion: (string) (len=5) "49254",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662758,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=11) "cleanup-pod",
        (string) (len=17) "pod-template-hash": (string) (len=9) "898f8f847"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=33) "test-cleanup-deployment-898f8f847",
          UID: (types.UID) (len=36) "e2cbc097-cf4c-41ae-addc-1bfe96b50a1e",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662758,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=539) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 65 32  63 62 63 30 39 37 2d 63  |d\":\"e2cbc097-c|
              00000090  66 34 63 2d 34 31 61 65  2d 61 64 64 63 2d 31 62  |f4c-41ae-addc-1b|
              000000a0  66 65 39 36 62 35 30 61  31 65 5c 22 7d 22 3a 7b  |fe96b50a1e\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 61 67 6e  |:{\"name\":\"agn|
              000000e0  68 6f 73 74 5c 22 7d 22  3a 7b 22 2e 22 3a 7b 7d  |host\"}":{".":{}|
              000000f0  2c 22 66 3a 69 6d 61 67  65 22 3a 7b 7d 2c 22 66  |,"f:image":{},"f|
              00000100  3a 69 6d 61 67 65 50 75  6c 6c 50 6f 6c 69 63 79  |:imagePullPolicy|
              00000110  22 3a 7b 7d 2c 22 66 3a  6e 61 6d 65 22 3a 7b 7d  |":{},"f:name":{}|
              00000120  2c 22 66 3a 72 65 73 6f  75 72 63 65 73 22 3a 7b  |,"f:resources":{|
              00000130  7d 2c 22 66 3a 73 65 63  75 72 69 74 79 43 6f 6e  |},"f:securityCon|
              00000140  74 65 78 74 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |text":{},"f:term|
              00000150  69 6e 61 74 69 6f 6e 4d  65 73 73 61 67 65 50 61  |inationMessagePa|
              00000160  74 68 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |th":{},"f:termin|
              00000170  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 6f 6c 69  |ationMessagePoli|
              00000180  63 79 22 3a 7b 7d 7d 7d  2c 22 66 3a 64 6e 73 50  |cy":{}}},"f:dnsP|
              00000190  6f 6c 69 63 79 22 3a 7b  7d 2c 22 66 3a 65 6e 61  |olicy":{},"f:ena|
              000001a0  62 6c 65 53 65 72 76 69  63 65 4c 69 6e 6b 73 22  |bleServiceLinks"|
              000001b0  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              000001c0  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              000001d0  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              000001e0  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              000001f0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000200  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000210  63 6f 6e 64 73 22 3a 7b  7d 7d 7d                 |conds":{}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-sjhrh",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=7) "agnhost",
          Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-sjhrh",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) "",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) <nil>,
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) "",
      HostIPs: ([]v1.HostIP) <nil>,
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)(<nil>),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) <nil>,
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:05:58.438071 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3117" for this suite. @ 05/12/25 16:05:58.47
• [5.355 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes should run through the lifecycle of a CSIDriver [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/csi_inline.go:157
  STEP: Creating a kubernetes client @ 05/12/25 16:05:58.508
  I0512 16:05:58.508122 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/12/25 16:05:58.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:58.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:58.561
  STEP: Creating two CSIDrivers @ 05/12/25 16:05:58.567
  STEP: Getting "inline-driver-0cb28bab-da7f-4dc0-a6d7-56636ef28a6f" & "inline-driver-455b7027-4b82-448e-9eeb-a897a4233528" @ 05/12/25 16:05:58.613
  STEP: Patching the CSIDriver "inline-driver-455b7027-4b82-448e-9eeb-a897a4233528" @ 05/12/25 16:05:58.63
  STEP: Updating the CSIDriver "inline-driver-455b7027-4b82-448e-9eeb-a897a4233528" @ 05/12/25 16:05:58.65
  STEP: Listing all CSIDrivers with the labelSelector: "e2e-test=csiinlinevolumes-453" @ 05/12/25 16:05:58.683
  STEP: Deleting CSIDriver "inline-driver-0cb28bab-da7f-4dc0-a6d7-56636ef28a6f" @ 05/12/25 16:05:58.694
  STEP: Confirm deletion of CSIDriver "inline-driver-0cb28bab-da7f-4dc0-a6d7-56636ef28a6f" @ 05/12/25 16:05:58.72
  STEP: Deleting CSIDriver "inline-driver-455b7027-4b82-448e-9eeb-a897a4233528" via DeleteCollection @ 05/12/25 16:05:58.732
  STEP: Confirm deletion of CSIDriver "inline-driver-455b7027-4b82-448e-9eeb-a897a4233528" @ 05/12/25 16:05:58.767
  I0512 16:05:58.778455 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-453" for this suite. @ 05/12/25 16:05:58.79
• [0.304 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:58
  STEP: Creating a kubernetes client @ 05/12/25 16:05:58.813
  I0512 16:05:58.813342 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:05:58.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:05:58.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:05:58.871
  STEP: Creating configMap with name projected-configmap-test-volume-15292432-9c67-4936-bc51-785fe5493fd8 @ 05/12/25 16:05:58.883
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:05:58.904
  E0512 16:05:59.392037      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:00.392250      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:01.392466      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:02.392752      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:06:02.965
  I0512 16:06:02.975367 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-configmaps-71dcba63-4660-412e-87bd-63a3128db8e3 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:06:02.991
  I0512 16:06:03.033287 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4215" for this suite. @ 05/12/25 16:06:03.051
• [4.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtimeclass.go:158
  STEP: Creating a kubernetes client @ 05/12/25 16:06:03.085
  I0512 16:06:03.086057 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/25 16:06:03.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:03.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:03.145
  STEP: Deleting RuntimeClass runtimeclass-4857-delete-me @ 05/12/25 16:06:03.168
  STEP: Waiting for the RuntimeClass to disappear @ 05/12/25 16:06:03.186
  I0512 16:06:03.216963 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4857" for this suite. @ 05/12/25 16:06:03.235
• [0.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1365
  STEP: Creating a kubernetes client @ 05/12/25 16:06:03.254
  I0512 16:06:03.254684 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:06:03.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:03.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:03.312
  STEP: validating cluster-info @ 05/12/25 16:06:03.322
  I0512 16:06:03.322419 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-9733 cluster-info'
  E0512 16:06:03.393321      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:03.497582 23 builder.go:146] stderr: ""
  I0512 16:06:03.497722 23 builder.go:147] stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  I0512 16:06:03.498094 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9733" for this suite. @ 05/12/25 16:06:03.535
• [0.303 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/garbage_collector.go:539
  STEP: Creating a kubernetes client @ 05/12/25 16:06:03.558
  I0512 16:06:03.558790 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename gc @ 05/12/25 16:06:03.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:03.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:03.654
  STEP: create the deployment @ 05/12/25 16:06:03.665
  W0512 16:06:03.679237      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/12/25 16:06:03.679
  STEP: delete the deployment @ 05/12/25 16:06:04.219
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/12/25 16:06:04.256
  E0512 16:06:04.394056      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/25 16:06:04.806
  I0512 16:06:05.168914 23 garbage_collector.go:265] For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  I0512 16:06:05.169943 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5056" for this suite. @ 05/12/25 16:06:05.191
• [1.657 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:237
  STEP: Creating a kubernetes client @ 05/12/25 16:06:05.219
  I0512 16:06:05.219352 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 16:06:05.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:05.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:05.255
  I0512 16:06:05.263095 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:06:05.394796      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:06.395934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:07.396631      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:08.397358      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:09.398273      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:10.398260      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:11.398907      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:12.399841      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:13.400056      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/12/25 16:06:14.179
  I0512 16:06:14.180854 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-2535 --namespace=crd-publish-openapi-2535 create -f -'
  E0512 16:06:14.400814      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:14.440599 23 builder.go:146] stderr: ""
  I0512 16:06:14.440677 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  I0512 16:06:14.440763 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-2535 --namespace=crd-publish-openapi-2535 delete e2e-test-crd-publish-openapi-3435-crds test-cr'
  I0512 16:06:14.666614 23 builder.go:146] stderr: ""
  I0512 16:06:14.666737 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  I0512 16:06:14.666959 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-2535 --namespace=crd-publish-openapi-2535 apply -f -'
  I0512 16:06:14.913934 23 builder.go:146] stderr: ""
  I0512 16:06:14.914053 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  I0512 16:06:14.914334 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-2535 --namespace=crd-publish-openapi-2535 delete e2e-test-crd-publish-openapi-3435-crds test-cr'
  I0512 16:06:15.126117 23 builder.go:146] stderr: ""
  I0512 16:06:15.126269 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-3435-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/12/25 16:06:15.126
  I0512 16:06:15.126570 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-2535 explain e2e-test-crd-publish-openapi-3435-crds'
  I0512 16:06:15.298365 23 builder.go:146] stderr: ""
  I0512 16:06:15.298495 23 builder.go:147] stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-3435-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0512 16:06:15.401424      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:16.402461      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:17.403595      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:18.403885      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:18.828503 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2535" for this suite. @ 05/12/25 16:06:18.857
• [13.658 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:350
  STEP: Creating a kubernetes client @ 05/12/25 16:06:18.878
  I0512 16:06:18.878111 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:06:18.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:18.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:18.927
  STEP: creating a replication controller @ 05/12/25 16:06:18.935
  I0512 16:06:18.935538 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 create -f -'
  I0512 16:06:19.245938 23 builder.go:146] stderr: ""
  I0512 16:06:19.246014 23 builder.go:147] stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/25 16:06:19.246
  I0512 16:06:19.246162 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0512 16:06:19.404492      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:19.431882 23 builder.go:146] stderr: ""
  I0512 16:06:19.431954 23 builder.go:147] stdout: "update-demo-nautilus-hj686 update-demo-nautilus-sb56d "
  I0512 16:06:19.432034 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-hj686 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 16:06:19.556819 23 builder.go:146] stderr: ""
  I0512 16:06:19.556895 23 builder.go:147] stdout: ""
  I0512 16:06:19.556922 23 kubectl.go:2502] update-demo-nautilus-hj686 is created but not running
  E0512 16:06:20.405399      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:21.405800      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:22.406734      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:23.407672      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:24.408135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:24.557181 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 16:06:24.736803 23 builder.go:146] stderr: ""
  I0512 16:06:24.736926 23 builder.go:147] stdout: "update-demo-nautilus-hj686 update-demo-nautilus-sb56d "
  I0512 16:06:24.737502 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-hj686 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 16:06:24.902148 23 builder.go:146] stderr: ""
  I0512 16:06:24.902242 23 builder.go:147] stdout: "true"
  I0512 16:06:24.902401 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-hj686 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  I0512 16:06:25.080283 23 builder.go:146] stderr: ""
  I0512 16:06:25.080374 23 builder.go:147] stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  I0512 16:06:25.080405 23 kubectl.go:2393] validating pod update-demo-nautilus-hj686
  I0512 16:06:25.089389 23 kubectl.go:2413] got data: {
    "image": "nautilus.jpg"
  }

  I0512 16:06:25.089514 23 kubectl.go:2418] Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  I0512 16:06:25.089593 23 kubectl.go:2520] update-demo-nautilus-hj686 is verified up and running
  I0512 16:06:25.090187 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-sb56d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 16:06:25.257874 23 builder.go:146] stderr: ""
  I0512 16:06:25.258204 23 builder.go:147] stdout: "true"
  I0512 16:06:25.258318 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-sb56d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0512 16:06:25.408435      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:25.425558 23 builder.go:146] stderr: ""
  I0512 16:06:25.425720 23 builder.go:147] stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  I0512 16:06:25.425766 23 kubectl.go:2393] validating pod update-demo-nautilus-sb56d
  I0512 16:06:25.441349 23 kubectl.go:2413] got data: {
    "image": "nautilus.jpg"
  }

  I0512 16:06:25.441513 23 kubectl.go:2418] Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  I0512 16:06:25.441561 23 kubectl.go:2520] update-demo-nautilus-sb56d is verified up and running
  STEP: scaling down the replication controller @ 05/12/25 16:06:25.441
  I0512 16:06:25.446873 23 kubectl.go:319] scanned /root for discovery docs: <nil>
  I0512 16:06:25.447009 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0512 16:06:26.408726      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:26.651440 23 builder.go:146] stderr: ""
  I0512 16:06:26.651527 23 builder.go:147] stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/25 16:06:26.651
  I0512 16:06:26.651700 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 16:06:26.827237 23 builder.go:146] stderr: ""
  I0512 16:06:26.827390 23 builder.go:147] stdout: "update-demo-nautilus-hj686 update-demo-nautilus-sb56d "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 05/12/25 16:06:26.827
  E0512 16:06:27.409073      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:28.410014      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:29.410655      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:30.410760      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:31.411182      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:31.827985 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 16:06:31.987618 23 builder.go:146] stderr: ""
  I0512 16:06:31.987794 23 builder.go:147] stdout: "update-demo-nautilus-sb56d "
  I0512 16:06:31.987923 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-sb56d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 16:06:32.149150 23 builder.go:146] stderr: ""
  I0512 16:06:32.149237 23 builder.go:147] stdout: "true"
  I0512 16:06:32.149355 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-sb56d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  I0512 16:06:32.312977 23 builder.go:146] stderr: ""
  I0512 16:06:32.313065 23 builder.go:147] stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  I0512 16:06:32.313094 23 kubectl.go:2393] validating pod update-demo-nautilus-sb56d
  I0512 16:06:32.330765 23 kubectl.go:2413] got data: {
    "image": "nautilus.jpg"
  }

  I0512 16:06:32.330909 23 kubectl.go:2418] Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  I0512 16:06:32.330941 23 kubectl.go:2520] update-demo-nautilus-sb56d is verified up and running
  STEP: scaling up the replication controller @ 05/12/25 16:06:32.33
  I0512 16:06:32.334495 23 kubectl.go:319] scanned /root for discovery docs: <nil>
  I0512 16:06:32.334588 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0512 16:06:32.412253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:33.412798      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:33.535826 23 builder.go:146] stderr: ""
  I0512 16:06:33.535932 23 builder.go:147] stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/25 16:06:33.536
  I0512 16:06:33.536244 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 16:06:33.713455 23 builder.go:146] stderr: ""
  I0512 16:06:33.713550 23 builder.go:147] stdout: "update-demo-nautilus-m7zc8 update-demo-nautilus-sb56d "
  I0512 16:06:33.713764 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-m7zc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 16:06:33.894172 23 builder.go:146] stderr: ""
  I0512 16:06:33.894308 23 builder.go:147] stdout: ""
  I0512 16:06:33.894576 23 kubectl.go:2502] update-demo-nautilus-m7zc8 is created but not running
  E0512 16:06:34.413936      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:35.414420      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:36.415149      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:37.415309      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:38.415774      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:38.895679 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  I0512 16:06:39.055628 23 builder.go:146] stderr: ""
  I0512 16:06:39.055698 23 builder.go:147] stdout: "update-demo-nautilus-m7zc8 update-demo-nautilus-sb56d "
  I0512 16:06:39.055790 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-m7zc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  I0512 16:06:39.211811 23 builder.go:146] stderr: ""
  I0512 16:06:39.211885 23 builder.go:147] stdout: "true"
  I0512 16:06:39.212020 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-m7zc8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  I0512 16:06:39.382724 23 builder.go:146] stderr: ""
  I0512 16:06:39.383040 23 builder.go:147] stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  I0512 16:06:39.383075 23 kubectl.go:2393] validating pod update-demo-nautilus-m7zc8
  I0512 16:06:39.399033 23 kubectl.go:2413] got data: {
    "image": "nautilus.jpg"
  }

  I0512 16:06:39.399163 23 kubectl.go:2418] Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  I0512 16:06:39.399195 23 kubectl.go:2520] update-demo-nautilus-m7zc8 is verified up and running
  I0512 16:06:39.399284 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-sb56d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0512 16:06:39.416815      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:39.546042 23 builder.go:146] stderr: ""
  I0512 16:06:39.546129 23 builder.go:147] stdout: "true"
  I0512 16:06:39.546263 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods update-demo-nautilus-sb56d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  I0512 16:06:39.707906 23 builder.go:146] stderr: ""
  I0512 16:06:39.708195 23 builder.go:147] stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  I0512 16:06:39.708233 23 kubectl.go:2393] validating pod update-demo-nautilus-sb56d
  I0512 16:06:39.721791 23 kubectl.go:2413] got data: {
    "image": "nautilus.jpg"
  }

  I0512 16:06:39.721911 23 kubectl.go:2418] Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  I0512 16:06:39.721947 23 kubectl.go:2520] update-demo-nautilus-sb56d is verified up and running
  STEP: using delete to clean up resources @ 05/12/25 16:06:39.721
  I0512 16:06:39.722489 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 delete --grace-period=0 --force -f -'
  I0512 16:06:39.893161 23 builder.go:146] stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  I0512 16:06:39.893233 23 builder.go:147] stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  I0512 16:06:39.893328 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get rc,svc -l name=update-demo --no-headers'
  I0512 16:06:40.112244 23 builder.go:146] stderr: "No resources found in kubectl-5522 namespace.\n"
  I0512 16:06:40.112801 23 builder.go:147] stdout: ""
  I0512 16:06:40.112932 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-5522 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  I0512 16:06:40.308352 23 builder.go:146] stderr: ""
  I0512 16:06:40.308438 23 builder.go:147] stdout: ""
  I0512 16:06:40.308667 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5522" for this suite. @ 05/12/25 16:06:40.353
• [21.507 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/security_context.go:350
  STEP: Creating a kubernetes client @ 05/12/25 16:06:40.388
  I0512 16:06:40.388274 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename security-context-test @ 05/12/25 16:06:40.389
  E0512 16:06:40.417628      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:40.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:40.456
  E0512 16:06:41.417893      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:42.418398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:43.419054      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:44.420176      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:44.518012 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9040" for this suite. @ 05/12/25 16:06:44.529
• [4.155 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtimeclass.go:106
  STEP: Creating a kubernetes client @ 05/12/25 16:06:44.544
  I0512 16:06:44.544247 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/25 16:06:44.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:44.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:44.585
  E0512 16:06:45.420456      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:46.420659      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:46.650218 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3270" for this suite. @ 05/12/25 16:06:46.661
• [2.134 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:98
  STEP: Creating a kubernetes client @ 05/12/25 16:06:46.678
  I0512 16:06:46.678531 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename aggregator @ 05/12/25 16:06:46.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:06:46.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:06:46.732
  I0512 16:06:46.740742 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Registering the sample API server. @ 05/12/25 16:06:46.742
  E0512 16:06:47.421661      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:47.982047 23 helpers.go:161] Found ClusterRoles; assuming RBAC is enabled.
  I0512 16:06:48.043850 23 deployment.go:222] new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  E0512 16:06:48.422040      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:49.422972      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:50.200447 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:06:50.423659      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:51.424374      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:52.219713 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:06:52.424601      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:53.424998      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:54.218725 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:06:54.425316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:55.426349      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:56.209814 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:06:56.427197      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:57.427561      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:06:58.211285 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:06:58.427551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:06:59.428369      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:00.217827 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:00.429132      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:01.429842      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:02.210004 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:02.429982      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:03.430786      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:04.211775 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:04.431603      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:05.431878      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:06.211516 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:06.432733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:07.432971      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:08.210836 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:08.433992      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:09.434253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:10.209345 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:10.435238      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:11.435485      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:12.208870 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:12.436492      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:13.436878      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:14.214471 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:14.437638      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:15.437742      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:16.215296 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:16.438837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:17.439695      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:18.209359 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:18.440259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:19.440406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:20.212874 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:20.441166      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:21.441689      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:22.209950 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:22.441967      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:23.442224      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:24.211055 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:24.443216      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:25.443710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:26.211424 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:26.443567      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:27.443966      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:28.209366 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:28.444417      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:29.444282      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:30.211147 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:30.445104      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:31.446243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:32.208203 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:32.447094      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:33.447903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:34.210179 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:34.448836      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:35.449523      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:36.218741 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:36.450208      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:37.450696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:38.211020 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:38.451279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:39.451876      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:40.212334 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:40.452880      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:41.453582      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:42.213802 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:42.454240      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:43.454914      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:44.213842 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:44.455177      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:45.455395      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:46.208864 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:46.456181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:47.456936      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:48.215406 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:48.457619      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:49.458060      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:50.208173 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 6, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-c4bc74fb5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:07:50.458190      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:51.459126      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:52.368456 23 aggregator.go:755] Waited 138.82243ms for the sample-apiserver to be ready to handle requests.
  E0512 16:07:52.460009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/12/25 16:07:52.465
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/12/25 16:07:52.474
  STEP: List APIServices @ 05/12/25 16:07:52.491
  I0512 16:07:52.499739 23 aggregator.go:556] Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/12/25 16:07:52.499
  I0512 16:07:52.530833 23 aggregator.go:581] APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/12/25 16:07:52.53
  I0512 16:07:52.557360 23 aggregator.go:607] updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2025, time.May, 12, 16, 7, 52, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/12/25 16:07:52.558
  I0512 16:07:52.571702 23 aggregator.go:625] Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2025-05-12 16:07:52 +0000 UTC Passed all checks passed}
  I0512 16:07:52.571790 23 aggregator.go:621] Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  I0512 16:07:52.571821 23 aggregator.go:631] Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/12/25 16:07:52.571
  I0512 16:07:52.592870 23 aggregator.go:647] Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete flunders resource "dynamic-flunder-1233010854" @ 05/12/25 16:07:52.592
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/12/25 16:07:52.618
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/12/25 16:07:52.636
  STEP: Patch APIService Status @ 05/12/25 16:07:52.645
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/12/25 16:07:52.68
  I0512 16:07:52.691178 23 aggregator.go:725] Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2025-05-12 16:07:52 +0000 UTC Passed all checks passed}
  I0512 16:07:52.691279 23 aggregator.go:725] Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  I0512 16:07:52.691316 23 aggregator.go:721] Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  I0512 16:07:52.691371 23 aggregator.go:731] Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "v1alpha1.wardle.example.com=updated" @ 05/12/25 16:07:52.691
  STEP: Confirm that the generated APIService has been deleted @ 05/12/25 16:07:52.711
  I0512 16:07:52.711876 23 aggregator.go:792] Requesting list of APIServices to confirm quantity
  I0512 16:07:52.723283 23 aggregator.go:802] Found 0 APIService with label "v1alpha1.wardle.example.com=updated"
  I0512 16:07:52.723361 23 aggregator.go:744] APIService v1alpha1.wardle.example.com has been deleted.
  I0512 16:07:53.096585 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-3166" for this suite. @ 05/12/25 16:07:53.117
• [66.463 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:796
  STEP: Creating a kubernetes client @ 05/12/25 16:07:53.145
  I0512 16:07:53.146120 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 16:07:53.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:07:53.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:07:53.225
  STEP: Creating service test in namespace statefulset-5812 @ 05/12/25 16:07:53.233
  STEP: Looking for a node to schedule stateful set and pod @ 05/12/25 16:07:53.248
  STEP: Creating pod with conflicting port in namespace statefulset-5812 @ 05/12/25 16:07:53.26
  STEP: Waiting until pod test-pod will start running in namespace statefulset-5812 @ 05/12/25 16:07:53.289
  E0512 16:07:53.460289      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:54.460924      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-5812 @ 05/12/25 16:07:55.308
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5812 @ 05/12/25 16:07:55.329
  I0512 16:07:55.392193 23 statefulset.go:869] Observed stateful pod in namespace: statefulset-5812, name: ss-0, uid: a944d693-4c51-4053-b147-16844e600800, status phase: Pending. Waiting for statefulset controller to delete.
  I0512 16:07:55.430961 23 statefulset.go:869] Observed stateful pod in namespace: statefulset-5812, name: ss-0, uid: a944d693-4c51-4053-b147-16844e600800, status phase: Failed. Waiting for statefulset controller to delete.
  I0512 16:07:55.458169 23 statefulset.go:869] Observed stateful pod in namespace: statefulset-5812, name: ss-0, uid: a944d693-4c51-4053-b147-16844e600800, status phase: Failed. Waiting for statefulset controller to delete.
  E0512 16:07:55.461366      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:55.463857 23 statefulset.go:863] Observed delete event for stateful pod ss-0 in namespace statefulset-5812
  STEP: Removing pod with conflicting port in namespace statefulset-5812 @ 05/12/25 16:07:55.463
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5812 and will be in running state @ 05/12/25 16:07:55.515
  E0512 16:07:56.461573      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:57.462160      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:07:57.538003 23 statefulset.go:138] Deleting all statefulset in ns statefulset-5812
  I0512 16:07:57.548047 23 rest.go:150] Scaling statefulset ss to 0
  E0512 16:07:58.462933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:07:59.462810      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:00.463146      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:01.463201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:02.463434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:03.464431      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:04.465108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:05.466300      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:06.466706      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:07.467083      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:07.576118 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 16:08:07.584010 23 rest.go:88] Deleting statefulset ss
  I0512 16:08:07.628776 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5812" for this suite. @ 05/12/25 16:08:07.643
• [14.520 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance] [sig-network, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/network/networking.go:85
  STEP: Creating a kubernetes client @ 05/12/25 16:08:07.665
  I0512 16:08:07.665895 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/25 16:08:07.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:08:07.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:08:07.722
  STEP: Performing setup for networking test in namespace pod-network-test-7047 @ 05/12/25 16:08:07.731
  STEP: creating a selector @ 05/12/25 16:08:07.731
  STEP: Creating the service pods in kubernetes @ 05/12/25 16:08:07.731
  I0512 16:08:07.731531 23 helper.go:48] Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0512 16:08:08.468218      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:09.469414      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:10.470555      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:11.471061      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:12.471696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:13.472913      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:14.473161      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:15.473580      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:16.474548      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:17.474972      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:18.475877      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:19.476337      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/12/25 16:08:19.957
  E0512 16:08:20.476663      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:21.477195      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:21.999015 23 utils.go:803] Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  I0512 16:08:21.999281 23 networking.go:42] Breadth first check of 10.233.68.123 on host 10.62.16.75...
  I0512 16:08:22.006015 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.69.229:9080/dial?request=hostname&protocol=http&host=10.233.68.123&port=8083&tries=1'] Namespace:pod-network-test-7047 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:08:22.006115 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:08:22.007232 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:08:22.007404 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7047/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.69.229%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.68.123%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  I0512 16:08:22.142680 23 utils.go:356] Waiting for responses: map[]
  I0512 16:08:22.143143 23 utils.go:360] reached 10.233.68.123 after 0/1 tries
  I0512 16:08:22.143177 23 networking.go:42] Breadth first check of 10.233.69.228 on host 10.62.16.76...
  I0512 16:08:22.151075 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.69.229:9080/dial?request=hostname&protocol=http&host=10.233.69.228&port=8083&tries=1'] Namespace:pod-network-test-7047 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:08:22.151144 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:08:22.152173 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:08:22.152310 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7047/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.69.229%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.69.228%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  I0512 16:08:22.281137 23 utils.go:356] Waiting for responses: map[]
  I0512 16:08:22.281374 23 utils.go:360] reached 10.233.69.228 after 0/1 tries
  I0512 16:08:22.281413 23 networking.go:42] Breadth first check of 10.233.70.62 on host 10.62.16.77...
  I0512 16:08:22.295268 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.69.229:9080/dial?request=hostname&protocol=http&host=10.233.70.62&port=8083&tries=1'] Namespace:pod-network-test-7047 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:08:22.295374 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:08:22.296401 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:08:22.296552 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7047/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.69.229%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.70.62%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  I0512 16:08:22.416700 23 utils.go:356] Waiting for responses: map[]
  I0512 16:08:22.416790 23 utils.go:360] reached 10.233.70.62 after 0/1 tries
  I0512 16:08:22.416819 23 networking.go:42] Breadth first check of 10.233.67.61 on host 10.62.16.78...
  I0512 16:08:22.422483 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.69.229:9080/dial?request=hostname&protocol=http&host=10.233.67.61&port=8083&tries=1'] Namespace:pod-network-test-7047 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:08:22.422555 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:08:22.423714 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:08:22.424050 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7047/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.69.229%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.67.61%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0512 16:08:22.477376      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:22.553787 23 utils.go:356] Waiting for responses: map[]
  I0512 16:08:22.553889 23 utils.go:360] reached 10.233.67.61 after 0/1 tries
  I0512 16:08:22.553916 23 networking.go:53] Going to retry 0 out of 4 pods....
  I0512 16:08:22.554338 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7047" for this suite. @ 05/12/25 16:08:22.566
• [14.916 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:125
  STEP: Creating a kubernetes client @ 05/12/25 16:08:22.582
  I0512 16:08:22.582796 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:08:22.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:08:22.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:08:22.628
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-ddc728fa-2c58-4f29-b5f2-dfce96ee5896 @ 05/12/25 16:08:22.668
  STEP: Creating the pod @ 05/12/25 16:08:22.679
  E0512 16:08:23.477723      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:24.478306      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-ddc728fa-2c58-4f29-b5f2-dfce96ee5896 @ 05/12/25 16:08:24.761
  STEP: waiting to observe update in volume @ 05/12/25 16:08:24.772
  E0512 16:08:25.478457      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:26.479153      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:26.802404 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-270" for this suite. @ 05/12/25 16:08:26.813
• [4.246 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/downwardapi.go:219
  STEP: Creating a kubernetes client @ 05/12/25 16:08:26.829
  I0512 16:08:26.829645 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:08:26.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:08:26.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:08:26.892
  STEP: Creating a pod to test downward api env vars @ 05/12/25 16:08:26.897
  E0512 16:08:27.479687      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:28.480069      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:29.481022      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:30.481250      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:08:30.954
  I0512 16:08:30.962321 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downward-api-c0e0ced3-bb58-4a60-aa81-d02ea4007725 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 16:08:30.976
  I0512 16:08:31.014912 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7306" for this suite. @ 05/12/25 16:08:31.029
• [4.213 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/12/25 16:08:31.043
  I0512 16:08:31.044007 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 16:08:31.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:08:31.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:08:31.094
  I0512 16:08:31.100490 23 deployment.go:754] Creating replica set "test-rolling-update-controller" (going to be adopted)
  I0512 16:08:31.131098 23 resource.go:87] Pod name sample-pod: Found 0 pods out of 1
  E0512 16:08:31.481318      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:32.482352      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:33.483137      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:34.483226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:35.483495      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:36.139813 23 resource.go:87] Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/25 16:08:36.139
  I0512 16:08:36.139988 23 deployment.go:763] Creating deployment "test-rolling-update-deployment"
  I0512 16:08:36.150870 23 deployment.go:769] Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  I0512 16:08:36.171989 23 deployment.go:222] new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0512 16:08:36.484682      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:37.484939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:38.193396 23 deployment.go:773] Ensuring status for deployment "test-rolling-update-deployment" is the expected
  I0512 16:08:38.200180 23 deployment.go:778] Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  I0512 16:08:38.234530 23 deployment.go:633] Deployment "test-rolling-update-deployment":
  (v1.Deployment) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=30) "test-rolling-update-deployment",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-4396",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "5e12013e-aaad-4671-a396-bffee5c03c03",
      ResourceVersion: (string) (len=5) "50590",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662916,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=1) {
        (string) (len=4) "name": (string) (len=10) "sample-pod"
      },
      Annotations: (map[string]string) (len=1) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=19) "3546343826724305833"
      },
      OwnerReferences: ([]v1.OwnerReference) <nil>,
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=637) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 7d 7d 2c  |},"f:name":{}}},|
              00000030  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 70 72 6f  |"f:spec":{"f:pro|
              00000040  67 72 65 73 73 44 65 61  64 6c 69 6e 65 53 65 63  |gressDeadlineSec|
              00000050  6f 6e 64 73 22 3a 7b 7d  2c 22 66 3a 72 65 70 6c  |onds":{},"f:repl|
              00000060  69 63 61 73 22 3a 7b 7d  2c 22 66 3a 72 65 76 69  |icas":{},"f:revi|
              00000070  73 69 6f 6e 48 69 73 74  6f 72 79 4c 69 6d 69 74  |sionHistoryLimit|
              00000080  22 3a 7b 7d 2c 22 66 3a  73 65 6c 65 63 74 6f 72  |":{},"f:selector|
              00000090  22 3a 7b 7d 2c 22 66 3a  73 74 72 61 74 65 67 79  |":{},"f:strategy|
              000000a0  22 3a 7b 22 66 3a 72 6f  6c 6c 69 6e 67 55 70 64  |":{"f:rollingUpd|
              000000b0  61 74 65 22 3a 7b 22 2e  22 3a 7b 7d 2c 22 66 3a  |ate":{".":{},"f:|
              000000c0  6d 61 78 53 75 72 67 65  22 3a 7b 7d 2c 22 66 3a  |maxSurge":{},"f:|
              000000d0  6d 61 78 55 6e 61 76 61  69 6c 61 62 6c 65 22 3a  |maxUnavailable":|
              000000e0  7b 7d 7d 2c 22 66 3a 74  79 70 65 22 3a 7b 7d 7d  |{}},"f:type":{}}|
              000000f0  2c 22 66 3a 74 65 6d 70  6c 61 74 65 22 3a 7b 22  |,"f:template":{"|
              00000100  66 3a 6d 65 74 61 64 61  74 61 22 3a 7b 22 66 3a  |f:metadata":{"f:|
              00000110  6c 61 62 65 6c 73 22 3a  7b 22 2e 22 3a 7b 7d 2c  |labels":{".":{},|
              00000120  22 66 3a 6e 61 6d 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:name":{}}},"f|
              00000130  3a 73 70 65 63 22 3a 7b  22 66 3a 63 6f 6e 74 61  |:spec":{"f:conta|
              00000140  69 6e 65 72 73 22 3a 7b  22 6b 3a 7b 5c 22 6e 61  |iners":{"k:{\"na|
              00000150  6d 65 5c 22 3a 5c 22 61  67 6e 68 6f 73 74 5c 22  |me\":\"agnhost\"|
              00000160  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000170  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000180  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000190  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              000001a0  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              000001b0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001c0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              000001d0  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              000001e0  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              000001f0  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              00000200  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              00000210  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              00000220  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              00000230  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              00000240  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              00000250  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000260  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000270  63 6f 6e 64 73 22 3a 7b  7d 7d 7d 7d 7d           |conds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=514) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 72 65 76 69 73  69 6f 6e 22 3a 7b 7d 7d  |io/revision":{}}|
              00000050  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 22 66  |},"f:status":{"f|
              00000060  3a 61 76 61 69 6c 61 62  6c 65 52 65 70 6c 69 63  |:availableReplic|
              00000070  61 73 22 3a 7b 7d 2c 22  66 3a 63 6f 6e 64 69 74  |as":{},"f:condit|
              00000080  69 6f 6e 73 22 3a 7b 22  2e 22 3a 7b 7d 2c 22 6b  |ions":{".":{},"k|
              00000090  3a 7b 5c 22 74 79 70 65  5c 22 3a 5c 22 41 76 61  |:{\"type\":\"Ava|
              000000a0  69 6c 61 62 6c 65 5c 22  7d 22 3a 7b 22 2e 22 3a  |ilable\"}":{".":|
              000000b0  7b 7d 2c 22 66 3a 6c 61  73 74 54 72 61 6e 73 69  |{},"f:lastTransi|
              000000c0  74 69 6f 6e 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |tionTime":{},"f:|
              000000d0  6c 61 73 74 55 70 64 61  74 65 54 69 6d 65 22 3a  |lastUpdateTime":|
              000000e0  7b 7d 2c 22 66 3a 6d 65  73 73 61 67 65 22 3a 7b  |{},"f:message":{|
              000000f0  7d 2c 22 66 3a 72 65 61  73 6f 6e 22 3a 7b 7d 2c  |},"f:reason":{},|
              00000100  22 66 3a 73 74 61 74 75  73 22 3a 7b 7d 2c 22 66  |"f:status":{},"f|
              00000110  3a 74 79 70 65 22 3a 7b  7d 7d 2c 22 6b 3a 7b 5c  |:type":{}},"k:{\|
              00000120  22 74 79 70 65 5c 22 3a  5c 22 50 72 6f 67 72 65  |"type\":\"Progre|
              00000130  73 73 69 6e 67 5c 22 7d  22 3a 7b 22 2e 22 3a 7b  |ssing\"}":{".":{|
              00000140  7d 2c 22 66 3a 6c 61 73  74 54 72 61 6e 73 69 74  |},"f:lastTransit|
              00000150  69 6f 6e 54 69 6d 65 22  3a 7b 7d 2c 22 66 3a 6c  |ionTime":{},"f:l|
              00000160  61 73 74 55 70 64 61 74  65 54 69 6d 65 22 3a 7b  |astUpdateTime":{|
              00000170  7d 2c 22 66 3a 6d 65 73  73 61 67 65 22 3a 7b 7d  |},"f:message":{}|
              00000180  2c 22 66 3a 72 65 61 73  6f 6e 22 3a 7b 7d 2c 22  |,"f:reason":{},"|
              00000190  66 3a 73 74 61 74 75 73  22 3a 7b 7d 2c 22 66 3a  |f:status":{},"f:|
              000001a0  74 79 70 65 22 3a 7b 7d  7d 7d 2c 22 66 3a 6f 62  |type":{}}},"f:ob|
              000001b0  73 65 72 76 65 64 47 65  6e 65 72 61 74 69 6f 6e  |servedGeneration|
              000001c0  22 3a 7b 7d 2c 22 66 3a  72 65 61 64 79 52 65 70  |":{},"f:readyRep|
              000001d0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 72 65 70  |licas":{},"f:rep|
              000001e0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 75 70 64  |licas":{},"f:upd|
              000001f0  61 74 65 64 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |atedReplicas":{}|
              00000200  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.DeploymentSpec) {
      Replicas: (*int32)(1),
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=1) {
          (string) (len=4) "name": (string) (len=10) "sample-pod"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=1) {
            (string) (len=4) "name": (string) (len=10) "sample-pod"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=7) "agnhost",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      },
      Strategy: (v1.DeploymentStrategy) {
        Type: (v1.DeploymentStrategyType) (len=13) "RollingUpdate",
        RollingUpdate: (*v1.RollingUpdateDeployment)({
          MaxUnavailable: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          }),
          MaxSurge: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          })
        })
      },
      MinReadySeconds: (int32) 0,
      RevisionHistoryLimit: (*int32)(10),
      Paused: (bool) false,
      ProgressDeadlineSeconds: (*int32)(600)
    },
    Status: (v1.DeploymentStatus) {
      ObservedGeneration: (int64) 1,
      Replicas: (int32) 1,
      UpdatedReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      UnavailableReplicas: (int32) 0,
      Conditions: ([]v1.DeploymentCondition) (len=2) {
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=9) "Available",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=24) "MinimumReplicasAvailable",
          Message: (string) (len=36) "Deployment has minimum availability."
        },
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=11) "Progressing",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=22) "NewReplicaSetAvailable",
          Message: (string) (len=83) "ReplicaSet \"test-rolling-update-deployment-56bb5bb765\" has successfully progressed."
        }
      },
      CollisionCount: (*int32)(<nil>)
    }
  }


  I0512 16:08:38.260877 23 deployment.go:39] New ReplicaSet "test-rolling-update-deployment-56bb5bb765" of Deployment "test-rolling-update-deployment":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=41) "test-rolling-update-deployment-56bb5bb765",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-4396",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "deab869b-cdd3-4967-b3b1-70f3f381ee94",
      ResourceVersion: (string) (len=5) "50580",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662916,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=10) "sample-pod",
        (string) (len=17) "pod-template-hash": (string) (len=10) "56bb5bb765"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=19) "3546343826724305833",
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=30) "test-rolling-update-deployment",
          UID: (types.UID) (len=36) "5e12013e-aaad-4671-a396-bffee5c03c03",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=783) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 35 65 31 32 30 31  33 65 2d 61 61 61 64 2d  |\"5e12013e-aaad-|
              00000120  34 36 37 31 2d 61 33 39  36 2d 62 66 66 65 65 35  |4671-a396-bffee5|
              00000130  63 30 33 63 30 33 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |c03c03\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 72 65 70  |"f:spec":{"f:rep|
              00000150  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 73 65 6c  |licas":{},"f:sel|
              00000160  65 63 74 6f 72 22 3a 7b  7d 2c 22 66 3a 74 65 6d  |ector":{},"f:tem|
              00000170  70 6c 61 74 65 22 3a 7b  22 66 3a 6d 65 74 61 64  |plate":{"f:metad|
              00000180  61 74 61 22 3a 7b 22 66  3a 6c 61 62 65 6c 73 22  |ata":{"f:labels"|
              00000190  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6e 61 6d 65  |:{".":{},"f:name|
              000001a0  22 3a 7b 7d 2c 22 66 3a  70 6f 64 2d 74 65 6d 70  |":{},"f:pod-temp|
              000001b0  6c 61 74 65 2d 68 61 73  68 22 3a 7b 7d 7d 7d 2c  |late-hash":{}}},|
              000001c0  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 63 6f 6e  |"f:spec":{"f:con|
              000001d0  74 61 69 6e 65 72 73 22  3a 7b 22 6b 3a 7b 5c 22  |tainers":{"k:{\"|
              000001e0  6e 61 6d 65 5c 22 3a 5c  22 61 67 6e 68 6f 73 74  |name\":\"agnhost|
              000001f0  5c 22 7d 22 3a 7b 22 2e  22 3a 7b 7d 2c 22 66 3a  |\"}":{".":{},"f:|
              00000200  69 6d 61 67 65 22 3a 7b  7d 2c 22 66 3a 69 6d 61  |image":{},"f:ima|
              00000210  67 65 50 75 6c 6c 50 6f  6c 69 63 79 22 3a 7b 7d  |gePullPolicy":{}|
              00000220  2c 22 66 3a 6e 61 6d 65  22 3a 7b 7d 2c 22 66 3a  |,"f:name":{},"f:|
              00000230  72 65 73 6f 75 72 63 65  73 22 3a 7b 7d 2c 22 66  |resources":{},"f|
              00000240  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              00000250  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000260  69 6f 6e 4d 65 73 73 61  67 65 50 61 74 68 22 3a  |ionMessagePath":|
              00000270  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000280  6e 4d 65 73 73 61 67 65  50 6f 6c 69 63 79 22 3a  |nMessagePolicy":|
              00000290  7b 7d 7d 7d 2c 22 66 3a  64 6e 73 50 6f 6c 69 63  |{}}},"f:dnsPolic|
              000002a0  79 22 3a 7b 7d 2c 22 66  3a 72 65 73 74 61 72 74  |y":{},"f:restart|
              000002b0  50 6f 6c 69 63 79 22 3a  7b 7d 2c 22 66 3a 73 63  |Policy":{},"f:sc|
              000002c0  68 65 64 75 6c 65 72 4e  61 6d 65 22 3a 7b 7d 2c  |hedulerName":{},|
              000002d0  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              000002e0  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              000002f0  61 74 69 6f 6e 47 72 61  63 65 50 65 72 69 6f 64  |ationGracePeriod|
              00000300  53 65 63 6f 6e 64 73 22  3a 7b 7d 7d 7d 7d 7d     |Seconds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=130) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  61 76 61 69 6c 61 62 6c  65 52 65 70 6c 69 63 61  |availableReplica|
              00000020  73 22 3a 7b 7d 2c 22 66  3a 66 75 6c 6c 79 4c 61  |s":{},"f:fullyLa|
              00000030  62 65 6c 65 64 52 65 70  6c 69 63 61 73 22 3a 7b  |beledReplicas":{|
              00000040  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              00000050  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              00000060  65 61 64 79 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |eadyReplicas":{}|
              00000070  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000080  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(1),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=10) "sample-pod",
          (string) (len=17) "pod-template-hash": (string) (len=10) "56bb5bb765"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=10) "sample-pod",
            (string) (len=17) "pod-template-hash": (string) (len=10) "56bb5bb765"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=7) "agnhost",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 1,
      FullyLabeledReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      ObservedGeneration: (int64) 1,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 16:08:38.263177 23 deployment.go:44] All old ReplicaSets of Deployment "test-rolling-update-deployment":
  I0512 16:08:38.263666 23 deployment.go:47] (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=30) "test-rolling-update-controller",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-4396",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "4fc35150-36ed-4875-a918-0b2123651220",
      ResourceVersion: (string) (len=5) "50589",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662911,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=10) "sample-pod",
        (string) (len=3) "pod": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=19) "3546343826724305832"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=30) "test-rolling-update-deployment",
          UID: (types.UID) (len=36) "5e12013e-aaad-4671-a396-bffee5c03c03",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=3) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662911,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=533) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 72 65 76 69 73  69 6f 6e 22 3a 7b 7d 7d  |io/revision":{}}|
              00000050  2c 22 66 3a 6c 61 62 65  6c 73 22 3a 7b 22 2e 22  |,"f:labels":{"."|
              00000060  3a 7b 7d 2c 22 66 3a 6e  61 6d 65 22 3a 7b 7d 2c  |:{},"f:name":{},|
              00000070  22 66 3a 70 6f 64 22 3a  7b 7d 7d 7d 2c 22 66 3a  |"f:pod":{}}},"f:|
              00000080  73 70 65 63 22 3a 7b 22  66 3a 73 65 6c 65 63 74  |spec":{"f:select|
              00000090  6f 72 22 3a 7b 7d 2c 22  66 3a 74 65 6d 70 6c 61  |or":{},"f:templa|
              000000a0  74 65 22 3a 7b 22 66 3a  6d 65 74 61 64 61 74 61  |te":{"f:metadata|
              000000b0  22 3a 7b 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |":{"f:labels":{"|
              000000c0  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              000000d0  7d 2c 22 66 3a 70 6f 64  22 3a 7b 7d 7d 7d 2c 22  |},"f:pod":{}}},"|
              000000e0  66 3a 73 70 65 63 22 3a  7b 22 66 3a 63 6f 6e 74  |f:spec":{"f:cont|
              000000f0  61 69 6e 65 72 73 22 3a  7b 22 6b 3a 7b 5c 22 6e  |ainers":{"k:{\"n|
              00000100  61 6d 65 5c 22 3a 5c 22  68 74 74 70 64 5c 22 7d  |ame\":\"httpd\"}|
              00000110  22 3a 7b 22 2e 22 3a 7b  7d 2c 22 66 3a 69 6d 61  |":{".":{},"f:ima|
              00000120  67 65 22 3a 7b 7d 2c 22  66 3a 69 6d 61 67 65 50  |ge":{},"f:imageP|
              00000130  75 6c 6c 50 6f 6c 69 63  79 22 3a 7b 7d 2c 22 66  |ullPolicy":{},"f|
              00000140  3a 6e 61 6d 65 22 3a 7b  7d 2c 22 66 3a 72 65 73  |:name":{},"f:res|
              00000150  6f 75 72 63 65 73 22 3a  7b 7d 2c 22 66 3a 74 65  |ources":{},"f:te|
              00000160  72 6d 69 6e 61 74 69 6f  6e 4d 65 73 73 61 67 65  |rminationMessage|
              00000170  50 61 74 68 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |Path":{},"f:term|
              00000180  69 6e 61 74 69 6f 6e 4d  65 73 73 61 67 65 50 6f  |inationMessagePo|
              00000190  6c 69 63 79 22 3a 7b 7d  7d 7d 2c 22 66 3a 64 6e  |licy":{}}},"f:dn|
              000001a0  73 50 6f 6c 69 63 79 22  3a 7b 7d 2c 22 66 3a 72  |sPolicy":{},"f:r|
              000001b0  65 73 74 61 72 74 50 6f  6c 69 63 79 22 3a 7b 7d  |estartPolicy":{}|
              000001c0  2c 22 66 3a 73 63 68 65  64 75 6c 65 72 4e 61 6d  |,"f:schedulerNam|
              000001d0  65 22 3a 7b 7d 2c 22 66  3a 73 65 63 75 72 69 74  |e":{},"f:securit|
              000001e0  79 43 6f 6e 74 65 78 74  22 3a 7b 7d 2c 22 66 3a  |yContext":{},"f:|
              000001f0  74 65 72 6d 69 6e 61 74  69 6f 6e 47 72 61 63 65  |terminationGrace|
              00000200  50 65 72 69 6f 64 53 65  63 6f 6e 64 73 22 3a 7b  |PeriodSeconds":{|
              00000210  7d 7d 7d 7d 7d                                    |}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=242) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 66 3a 64 65 70 6c 6f  79 6d 65 6e 74 2e 6b 75  |"f:deployment.ku|
              00000030  62 65 72 6e 65 74 65 73  2e 69 6f 2f 64 65 73 69  |bernetes.io/desi|
              00000040  72 65 64 2d 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |red-replicas":{}|
              00000050  2c 22 66 3a 64 65 70 6c  6f 79 6d 65 6e 74 2e 6b  |,"f:deployment.k|
              00000060  75 62 65 72 6e 65 74 65  73 2e 69 6f 2f 6d 61 78  |ubernetes.io/max|
              00000070  2d 72 65 70 6c 69 63 61  73 22 3a 7b 7d 7d 2c 22  |-replicas":{}},"|
              00000080  66 3a 6f 77 6e 65 72 52  65 66 65 72 65 6e 63 65  |f:ownerReference|
              00000090  73 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 6b 3a 7b 5c  |s":{".":{},"k:{\|
              000000a0  22 75 69 64 5c 22 3a 5c  22 35 65 31 32 30 31 33  |"uid\":\"5e12013|
              000000b0  65 2d 61 61 61 64 2d 34  36 37 31 2d 61 33 39 36  |e-aaad-4671-a396|
              000000c0  2d 62 66 66 65 65 35 63  30 33 63 30 33 5c 22 7d  |-bffee5c03c03\"}|
              000000d0  22 3a 7b 7d 7d 7d 2c 22  66 3a 73 70 65 63 22 3a  |":{}}},"f:spec":|
              000000e0  7b 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |{"f:replicas":{}|
              000000f0  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=56) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  6f 62 73 65 72 76 65 64  47 65 6e 65 72 61 74 69  |observedGenerati|
              00000020  6f 6e 22 3a 7b 7d 2c 22  66 3a 72 65 70 6c 69 63  |on":{},"f:replic|
              00000030  61 73 22 3a 7b 7d 7d 7d                           |as":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(0),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=10) "sample-pod",
          (string) (len=3) "pod": (string) (len=5) "httpd"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=10) "sample-pod",
            (string) (len=3) "pod": (string) (len=5) "httpd"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)(<nil>),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 0,
      FullyLabeledReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 16:08:38.277544 23 deployment.go:67] Pod "test-rolling-update-deployment-56bb5bb765-72j9p" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=47) "test-rolling-update-deployment-56bb5bb765-72j9p",
      GenerateName: (string) (len=42) "test-rolling-update-deployment-56bb5bb765-",
      Namespace: (string) (len=15) "deployment-4396",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "4cd8053d-a332-4bd9-9a3b-e00d8dd4c07f",
      ResourceVersion: (string) (len=5) "50579",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662916,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=10) "sample-pod",
        (string) (len=17) "pod-template-hash": (string) (len=10) "56bb5bb765"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=41) "test-rolling-update-deployment-56bb5bb765",
          UID: (types.UID) (len=36) "deab869b-cdd3-4967-b3b1-70f3f381ee94",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=539) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 64 65  61 62 38 36 39 62 2d 63  |d\":\"deab869b-c|
              00000090  64 64 33 2d 34 39 36 37  2d 62 33 62 31 2d 37 30  |dd3-4967-b3b1-70|
              000000a0  66 33 66 33 38 31 65 65  39 34 5c 22 7d 22 3a 7b  |f3f381ee94\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 61 67 6e  |:{\"name\":\"agn|
              000000e0  68 6f 73 74 5c 22 7d 22  3a 7b 22 2e 22 3a 7b 7d  |host\"}":{".":{}|
              000000f0  2c 22 66 3a 69 6d 61 67  65 22 3a 7b 7d 2c 22 66  |,"f:image":{},"f|
              00000100  3a 69 6d 61 67 65 50 75  6c 6c 50 6f 6c 69 63 79  |:imagePullPolicy|
              00000110  22 3a 7b 7d 2c 22 66 3a  6e 61 6d 65 22 3a 7b 7d  |":{},"f:name":{}|
              00000120  2c 22 66 3a 72 65 73 6f  75 72 63 65 73 22 3a 7b  |,"f:resources":{|
              00000130  7d 2c 22 66 3a 73 65 63  75 72 69 74 79 43 6f 6e  |},"f:securityCon|
              00000140  74 65 78 74 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |text":{},"f:term|
              00000150  69 6e 61 74 69 6f 6e 4d  65 73 73 61 67 65 50 61  |inationMessagePa|
              00000160  74 68 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |th":{},"f:termin|
              00000170  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 6f 6c 69  |ationMessagePoli|
              00000180  63 79 22 3a 7b 7d 7d 7d  2c 22 66 3a 64 6e 73 50  |cy":{}}},"f:dnsP|
              00000190  6f 6c 69 63 79 22 3a 7b  7d 2c 22 66 3a 65 6e 61  |olicy":{},"f:ena|
              000001a0  62 6c 65 53 65 72 76 69  63 65 4c 69 6e 6b 73 22  |bleServiceLinks"|
              000001b0  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              000001c0  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              000001d0  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              000001e0  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              000001f0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000200  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000210  63 6f 6e 64 73 22 3a 7b  7d 7d 7d                 |conds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 39 2e  32 33 33 5c 22 7d 22 3a  |.233.69.233\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-4ngdn",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=7) "agnhost",
          Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-4ngdn",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662917,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882662916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) (len=13) "10.233.69.233",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.69.233"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882662916,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=7) "agnhost",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882662917,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
          ImageID: (string) (len=111) "registry.k8s.io/e2e-test-images/agnhost@sha256:b173c7d0ffe3d805d49f4dfe48375169b7b8d2e1feb81783efd61eb9d08042e6",
          ContainerID: (string) (len=77) "containerd://804e5ded05dc14daf02039e162951284cc252a686dce2062c562ad33c9aff875",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-4ngdn",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:08:38.284058 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4396" for this suite. @ 05/12/25 16:08:38.295
• [7.272 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:75
  STEP: Creating a kubernetes client @ 05/12/25 16:08:38.316
  I0512 16:08:38.316376 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:08:38.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:08:38.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:08:38.373
  STEP: Creating configMap with name projected-configmap-test-volume-634db330-5715-4f06-b9cb-325fd649f7c2 @ 05/12/25 16:08:38.379
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:08:38.389
  E0512 16:08:38.485916      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:39.486236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:40.486451      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:41.486971      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:08:42.445
  I0512 16:08:42.453471 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-configmaps-f8d78eaf-7578-4f55-a903-425ad38c8a17 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:08:42.47
  E0512 16:08:42.487241      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:42.513941 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9155" for this suite. @ 05/12/25 16:08:42.525
• [4.228 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/rc.go:425
  STEP: Creating a kubernetes client @ 05/12/25 16:08:42.545
  I0512 16:08:42.545240 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replication-controller @ 05/12/25 16:08:42.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:08:42.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:08:42.592
  STEP: Creating ReplicationController "e2e-rc-jzzv6" @ 05/12/25 16:08:42.599
  I0512 16:08:42.612739 23 rc.go:792] Get Replication Controller "e2e-rc-jzzv6" to confirm replicas
  E0512 16:08:43.491306      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:43.613480 23 rc.go:792] Get Replication Controller "e2e-rc-jzzv6" to confirm replicas
  I0512 16:08:43.627063 23 rc.go:801] Found 1 replicas for "e2e-rc-jzzv6" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-jzzv6" @ 05/12/25 16:08:43.627
  STEP: Updating a scale subresource @ 05/12/25 16:08:43.638
  STEP: Verifying replicas where modified for replication controller "e2e-rc-jzzv6" @ 05/12/25 16:08:43.661
  I0512 16:08:43.661356 23 rc.go:792] Get Replication Controller "e2e-rc-jzzv6" to confirm replicas
  E0512 16:08:44.491866      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:44.661801 23 rc.go:792] Get Replication Controller "e2e-rc-jzzv6" to confirm replicas
  I0512 16:08:44.674107 23 rc.go:801] Found 2 replicas for "e2e-rc-jzzv6" replication controller
  I0512 16:08:44.674538 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2702" for this suite. @ 05/12/25 16:08:44.686
• [2.161 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:324
  STEP: Creating a kubernetes client @ 05/12/25 16:08:44.707
  I0512 16:08:44.707066 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 16:08:44.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:08:44.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:08:44.773
  STEP: Creating service test in namespace statefulset-7158 @ 05/12/25 16:08:44.779
  STEP: Creating a new StatefulSet @ 05/12/25 16:08:44.79
  I0512 16:08:44.818644 23 wait.go:40] Found 0 stateful pods, waiting for 3
  E0512 16:08:45.492509      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:46.493003      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:47.493567      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:48.494210      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:49.494785      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:50.495385      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:51.495779      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:52.497064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:53.497512      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:54.497867      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:08:54.818646 23 wait.go:50] Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 16:08:54.818779 23 wait.go:50] Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  I0512 16:08:54.819414 23 wait.go:50] Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  I0512 16:08:54.919845 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-7158 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 16:08:55.245267 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 16:08:55.245404 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 16:08:55.245486 23 statefulset.go:2450] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/12/25 16:08:55.289
  I0512 16:08:55.347934 23 statefulset.go:2507] Updating stateful set ss2
  STEP: Creating a new revision @ 05/12/25 16:08:55.348
  E0512 16:08:55.498916      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:56.499084      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:57.499418      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:58.499637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:08:59.500262      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:00.500679      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:01.500849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:02.501664      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:03.501792      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:04.502001      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 05/12/25 16:09:05.369
  I0512 16:09:05.380654 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-7158 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0512 16:09:05.502477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:09:05.663735 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  I0512 16:09:05.663822 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 16:09:05.663853 23 statefulset.go:2474] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0512 16:09:06.502972      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:07.503444      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:08.503864      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:09.504359      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:10.505069      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:11.505902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:12.506313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:13.506701      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:14.507167      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:15.507929      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 05/12/25 16:09:15.705
  I0512 16:09:15.706224 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-7158 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  I0512 16:09:16.033883 23 builder.go:146] stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  I0512 16:09:16.033977 23 builder.go:147] stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  I0512 16:09:16.034009 23 statefulset.go:2450] stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0512 16:09:16.508810      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:17.509292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:18.509718      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:19.510199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:20.510310      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:21.510495      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:22.510722      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:23.511267      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:24.511784      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:25.511963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:09:26.076458 23 statefulset.go:2507] Updating stateful set ss2
  E0512 16:09:26.513021      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:27.513683      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:28.514398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:29.514634      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:30.515780      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:31.516872      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:32.517525      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:33.518343      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:34.519300      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:35.520005      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 05/12/25 16:09:36.099
  I0512 16:09:36.109510 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=statefulset-7158 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  I0512 16:09:36.392156 23 builder.go:146] stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  I0512 16:09:36.392266 23 builder.go:147] stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  I0512 16:09:36.392730 23 statefulset.go:2474] stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0512 16:09:36.520176      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:37.520490      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:38.520947      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:39.521163      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:40.521353      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:41.521536      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:42.521759      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:43.522019      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:44.522262      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:45.522498      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:09:46.426834 23 statefulset.go:138] Deleting all statefulset in ns statefulset-7158
  I0512 16:09:46.433787 23 rest.go:150] Scaling statefulset ss2 to 0
  E0512 16:09:46.523167      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:47.523441      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:48.523920      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:49.524857      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:50.525011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:51.526336      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:52.527000      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:53.527799      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:54.528243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:55.528656      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:09:56.469960 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 16:09:56.476368 23 rest.go:88] Deleting statefulset ss2
  I0512 16:09:56.519979 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 16:09:56.528640      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "statefulset-7158" for this suite. @ 05/12/25 16:09:56.53
• [71.843 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1723
  STEP: Creating a kubernetes client @ 05/12/25 16:09:56.55
  I0512 16:09:56.550947 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:09:56.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:09:56.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:09:56.605
  I0512 16:09:56.612837 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-3566 version'
  I0512 16:09:56.753148 23 builder.go:146] stderr: ""
  I0512 16:09:56.753225 23 builder.go:147] stdout: "Client Version: v1.31.4\nKustomize Version: v5.4.2\nServer Version: v1.31.4\n"
  I0512 16:09:56.753721 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3566" for this suite. @ 05/12/25 16:09:56.764
• [0.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/12/25 16:09:56.781
  I0512 16:09:56.781841 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/25 16:09:56.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:09:56.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:09:56.88
  I0512 16:09:56.975662 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3808" for this suite. @ 05/12/25 16:09:56.985
• [0.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ValidatingAdmissionPolicy [Privileged:ClusterAdmin] should validate against a Deployment [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/validatingadmissionpolicy.go:77
  STEP: Creating a kubernetes client @ 05/12/25 16:09:57.007
  I0512 16:09:57.007329 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename validating-admission-policy @ 05/12/25 16:09:57.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:09:57.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:09:57.074
  STEP: creating the policy @ 05/12/25 16:09:57.107
  STEP: waiting until the marker is denied @ 05/12/25 16:09:57.143
  E0512 16:09:57.529179      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: testing a replicated Deployment to be allowed @ 05/12/25 16:09:57.988
  STEP: testing a non-replicated ReplicaSet not to be denied @ 05/12/25 16:09:58.011
  I0512 16:09:58.131148 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "validating-admission-policy-1357" for this suite. @ 05/12/25 16:09:58.164
• [1.176 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/node/security_context.go:337
  STEP: Creating a kubernetes client @ 05/12/25 16:09:58.183
  I0512 16:09:58.183264 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename security-context @ 05/12/25 16:09:58.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:09:58.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:09:58.261
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/12/25 16:09:58.267
  E0512 16:09:58.530269      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:09:59.530555      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:00.530724      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:01.531273      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:10:02.331
  I0512 16:10:02.342238 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod security-context-de90ccec-db7b-4e80-b96b-63e7fa0eda12 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 16:10:02.357
  I0512 16:10:02.402888 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4643" for this suite. @ 05/12/25 16:10:02.416
• [4.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/endpointslice.go:105
  STEP: Creating a kubernetes client @ 05/12/25 16:10:02.435
  I0512 16:10:02.435712 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename endpointslice @ 05/12/25 16:10:02.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:02.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:02.489
  E0512 16:10:02.532190      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:03.532759      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:04.533087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:04.631382 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8714" for this suite. @ 05/12/25 16:10:04.642
• [2.226 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:282
  STEP: Creating a kubernetes client @ 05/12/25 16:10:04.662
  I0512 16:10:04.662242 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:10:04.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:04.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:04.747
  STEP: Setting up server cert @ 05/12/25 16:10:04.827
  E0512 16:10:05.533750      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:10:05.828
  STEP: Deploying the webhook pod @ 05/12/25 16:10:05.839
  STEP: Wait for the deployment to be ready @ 05/12/25 16:10:05.857
  I0512 16:10:05.871368 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:10:06.535022      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:07.535664      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:10:07.902
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:10:07.918
  E0512 16:10:08.535943      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:08.919037 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  I0512 16:10:08.953430 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:10:09.536744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:10.537149      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:11.538189      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:12.539302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:13.540128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4075-crds.webhook.example.com via the AdmissionRegistration API @ 05/12/25 16:10:14.475
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/12/25 16:10:14.529
  E0512 16:10:14.540255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:15.540364      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:16.540586      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:17.282264 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-92" for this suite. @ 05/12/25 16:10:17.291
  STEP: Destroying namespace "webhook-markers-5655" for this suite. @ 05/12/25 16:10:17.305
• [12.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should locate the groupVersion and a resource within each APIGroup [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:172
  STEP: Creating a kubernetes client @ 05/12/25 16:10:17.323
  I0512 16:10:17.323932 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename discovery @ 05/12/25 16:10:17.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:17.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:17.37
  STEP: Setting up server cert @ 05/12/25 16:10:17.379
  E0512 16:10:17.541139      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Requesting APIResourceList from "/api/v1" @ 05/12/25 16:10:18.006
  STEP: Requesting APIResourceList from "/apis/admissionregistration.k8s.io/v1" @ 05/12/25 16:10:18.011
  STEP: Requesting APIResourceList from "/apis/apiextensions.k8s.io/v1" @ 05/12/25 16:10:18.014
  STEP: Requesting APIResourceList from "/apis/apiregistration.k8s.io/v1" @ 05/12/25 16:10:18.016
  STEP: Requesting APIResourceList from "/apis/apps/v1" @ 05/12/25 16:10:18.019
  STEP: Requesting APIResourceList from "/apis/authentication.k8s.io/v1" @ 05/12/25 16:10:18.022
  STEP: Requesting APIResourceList from "/apis/authorization.k8s.io/v1" @ 05/12/25 16:10:18.024
  STEP: Requesting APIResourceList from "/apis/autoscaling/v1" @ 05/12/25 16:10:18.027
  STEP: Requesting APIResourceList from "/apis/autoscaling/v2" @ 05/12/25 16:10:18.029
  STEP: Requesting APIResourceList from "/apis/batch/v1" @ 05/12/25 16:10:18.032
  STEP: Requesting APIResourceList from "/apis/certificates.k8s.io/v1" @ 05/12/25 16:10:18.035
  STEP: Requesting APIResourceList from "/apis/coordination.k8s.io/v1" @ 05/12/25 16:10:18.038
  STEP: Requesting APIResourceList from "/apis/discovery.k8s.io/v1" @ 05/12/25 16:10:18.04
  STEP: Requesting APIResourceList from "/apis/events.k8s.io/v1" @ 05/12/25 16:10:18.043
  STEP: Requesting APIResourceList from "/apis/networking.k8s.io/v1" @ 05/12/25 16:10:18.045
  STEP: Requesting APIResourceList from "/apis/node.k8s.io/v1" @ 05/12/25 16:10:18.047
  STEP: Requesting APIResourceList from "/apis/policy/v1" @ 05/12/25 16:10:18.05
  STEP: Requesting APIResourceList from "/apis/scheduling.k8s.io/v1" @ 05/12/25 16:10:18.052
  STEP: Requesting APIResourceList from "/apis/storage.k8s.io/v1" @ 05/12/25 16:10:18.055
  I0512 16:10:18.058490 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-6109" for this suite. @ 05/12/25 16:10:18.067
• [0.759 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/12/25 16:10:18.085
  I0512 16:10:18.085404 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-runtime @ 05/12/25 16:10:18.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:18.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:18.162
  STEP: create the container @ 05/12/25 16:10:18.173
  W0512 16:10:18.209563      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/12/25 16:10:18.209
  E0512 16:10:18.542030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:19.542142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:20.542716      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/12/25 16:10:21.292
  STEP: the container should be terminated @ 05/12/25 16:10:21.299
  STEP: the termination message should be set @ 05/12/25 16:10:21.299
  I0512 16:10:21.299378 23 runtime.go:167] Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/12/25 16:10:21.299
  I0512 16:10:21.328227 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5141" for this suite. @ 05/12/25 16:10:21.351
• [3.294 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:110
  STEP: Creating a kubernetes client @ 05/12/25 16:10:21.378
  I0512 16:10:21.378907 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 16:10:21.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:21.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:21.42
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/12/25 16:10:21.428
  E0512 16:10:21.543750      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:22.544654      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:23.545633      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:24.545802      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:10:25.481
  I0512 16:10:25.492362 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-578b6840-20e7-40ef-a46f-e4f0e2c0201f container test-container: <nil>
  STEP: delete the pod @ 05/12/25 16:10:25.516
  I0512 16:10:25.543848 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 16:10:25.545824      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-8649" for this suite. @ 05/12/25 16:10:25.554
• [4.190 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/rc.go:70
  STEP: Creating a kubernetes client @ 05/12/25 16:10:25.569
  I0512 16:10:25.569959 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replication-controller @ 05/12/25 16:10:25.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:25.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:25.634
  STEP: Creating replication controller my-hostname-basic-7c94d09f-8a3f-45ea-94a3-f942fe15c845 @ 05/12/25 16:10:25.642
  I0512 16:10:25.659016 23 resource.go:87] Pod name my-hostname-basic-7c94d09f-8a3f-45ea-94a3-f942fe15c845: Found 0 pods out of 1
  E0512 16:10:26.546034      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:27.547069      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:28.547098      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:29.547847      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:30.548460      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:30.671122 23 resource.go:87] Pod name my-hostname-basic-7c94d09f-8a3f-45ea-94a3-f942fe15c845: Found 1 pods out of 1
  I0512 16:10:30.671227 23 rc.go:507] Ensuring all pods for ReplicationController "my-hostname-basic-7c94d09f-8a3f-45ea-94a3-f942fe15c845" are running
  I0512 16:10:30.680084 23 rc.go:523] Pod "my-hostname-basic-7c94d09f-8a3f-45ea-94a3-f942fe15c845-bqlcn" is running and ready(conditions: [{Type:PodReadyToStartContainers Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 16:10:27 +0000 UTC Reason: Message:} {Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 16:10:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 16:10:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 16:10:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2025-05-12 16:10:25 +0000 UTC Reason: Message:}])
  I0512 16:10:30.680175 23 rc.go:531] Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/12/25 16:10:30.68
  I0512 16:10:30.797294 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9509" for this suite. @ 05/12/25 16:10:30.811
• [5.258 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance] [sig-storage, Serial, Conformance]
k8s.io/kubernetes/test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/12/25 16:10:30.829
  I0512 16:10:30.829945 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/12/25 16:10:30.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:30.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:30.884
  STEP: Creating 50 configmaps @ 05/12/25 16:10:30.891
  STEP: Creating RC which spawns configmap-volume pods @ 05/12/25 16:10:31.445
  I0512 16:10:31.480788 23 resource.go:87] Pod name wrapped-volume-race-074457ad-4d2a-431c-8fcc-9c5393cad880: Found 0 pods out of 5
  E0512 16:10:31.549349      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:32.550404      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:33.551043      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:34.551937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:35.552313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:36.495687 23 resource.go:87] Pod name wrapped-volume-race-074457ad-4d2a-431c-8fcc-9c5393cad880: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/12/25 16:10:36.495
  E0512 16:10:36.552307      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/12/25 16:10:36.562
  I0512 16:10:36.602454 23 resource.go:87] Pod name wrapped-volume-race-94996cb8-3058-4fda-b9fc-d8c587edeb0c: Found 0 pods out of 5
  E0512 16:10:37.552820      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:38.553520      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:39.553896      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:40.553965      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:41.554311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:41.618611 23 resource.go:87] Pod name wrapped-volume-race-94996cb8-3058-4fda-b9fc-d8c587edeb0c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/12/25 16:10:41.618
  STEP: Creating RC which spawns configmap-volume pods @ 05/12/25 16:10:41.669
  I0512 16:10:41.719513 23 resource.go:87] Pod name wrapped-volume-race-e050a078-9aa9-44fc-b667-501a827db91c: Found 1 pods out of 5
  E0512 16:10:42.556939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:43.557103      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:44.557743      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:45.558566      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:46.559568      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:46.737535 23 resource.go:87] Pod name wrapped-volume-race-e050a078-9aa9-44fc-b667-501a827db91c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/12/25 16:10:46.737
  STEP: deleting ReplicationController wrapped-volume-race-e050a078-9aa9-44fc-b667-501a827db91c in namespace emptydir-wrapper-853, will wait for the garbage collector to delete the pods @ 05/12/25 16:10:46.791
  I0512 16:10:46.870544 23 resources.go:139] Deleting ReplicationController wrapped-volume-race-e050a078-9aa9-44fc-b667-501a827db91c took: 20.606936ms
  I0512 16:10:47.071949 23 resources.go:163] Terminating ReplicationController wrapped-volume-race-e050a078-9aa9-44fc-b667-501a827db91c pods took: 201.398112ms
  E0512 16:10:47.560500      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:48.561343      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-94996cb8-3058-4fda-b9fc-d8c587edeb0c in namespace emptydir-wrapper-853, will wait for the garbage collector to delete the pods @ 05/12/25 16:10:48.872
  I0512 16:10:48.957089 23 resources.go:139] Deleting ReplicationController wrapped-volume-race-94996cb8-3058-4fda-b9fc-d8c587edeb0c took: 23.119106ms
  I0512 16:10:49.158497 23 resources.go:163] Terminating ReplicationController wrapped-volume-race-94996cb8-3058-4fda-b9fc-d8c587edeb0c pods took: 201.399956ms
  E0512 16:10:49.562068      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:50.562241      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-074457ad-4d2a-431c-8fcc-9c5393cad880 in namespace emptydir-wrapper-853, will wait for the garbage collector to delete the pods @ 05/12/25 16:10:50.759
  I0512 16:10:50.855648 23 resources.go:139] Deleting ReplicationController wrapped-volume-race-074457ad-4d2a-431c-8fcc-9c5393cad880 took: 18.35692ms
  I0512 16:10:51.056018 23 resources.go:163] Terminating ReplicationController wrapped-volume-race-074457ad-4d2a-431c-8fcc-9c5393cad880 pods took: 200.332137ms
  E0512 16:10:51.563003      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:52.563828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/12/25 16:10:53.056
  E0512 16:10:53.564791      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:53.907400 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-wrapper-853" for this suite. @ 05/12/25 16:10:53.92
• [23.108 seconds]
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/12/25 16:10:53.938
  I0512 16:10:53.938402 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename watch @ 05/12/25 16:10:53.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:53.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:53.994
  STEP: creating a new configmap @ 05/12/25 16:10:54.005
  STEP: modifying the configmap once @ 05/12/25 16:10:54.016
  STEP: modifying the configmap a second time @ 05/12/25 16:10:54.036
  STEP: deleting the configmap @ 05/12/25 16:10:54.054
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/12/25 16:10:54.067
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/12/25 16:10:54.071
  I0512 16:10:54.072871 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1390  454ddb46-02e5-4e96-a08a-2d7c5578dd16 52426 0 2025-05-12 16:10:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2025-05-12 16:10:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:10:54.073891 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1390  454ddb46-02e5-4e96-a08a-2d7c5578dd16 52427 0 2025-05-12 16:10:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2025-05-12 16:10:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:10:54.074745 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1390" for this suite. @ 05/12/25 16:10:54.087
• [0.178 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:1260
  STEP: Creating a kubernetes client @ 05/12/25 16:10:54.117
  I0512 16:10:54.117174 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 16:10:54.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:10:54.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:10:54.171
  STEP: creating service nodeport-test with type=NodePort in namespace services-6821 @ 05/12/25 16:10:54.177
  STEP: creating replication controller nodeport-test in namespace services-6821 @ 05/12/25 16:10:54.227
  I0512 16:10:54.247236      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6821, replica count: 2
  E0512 16:10:54.568182      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:55.569464      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:56.569310      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:10:57.297999      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 16:10:57.298151 23 resource.go:361] Creating new exec pod
  E0512 16:10:57.569689      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:58.569863      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:10:59.570192      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:11:00.367893 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6821 exec execpodvnb9s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  E0512 16:11:00.570827      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:11:00.750247 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  I0512 16:11:00.750603 23 builder.go:147] stdout: "nodeport-test-6tpd4"
  I0512 16:11:00.750771 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6821 exec execpodvnb9s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.7 80'
  I0512 16:11:01.083471 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.7 80\nConnection to 10.233.55.7 80 port [tcp/http] succeeded!\n"
  I0512 16:11:01.083601 23 builder.go:147] stdout: "nodeport-test-2ghkl"
  I0512 16:11:01.083814 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6821 exec execpodvnb9s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.75 31078'
  I0512 16:11:01.406576 23 builder.go:146] stderr: "+ nc -v -t -w 2 10.62.16.75 31078\n+ echo hostName\nConnection to 10.62.16.75 31078 port [tcp/*] succeeded!\n"
  I0512 16:11:01.406699 23 builder.go:147] stdout: ""
  E0512 16:11:01.571027      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:11:02.083877 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6821 exec execpodvnb9s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.75 31078'
  I0512 16:11:02.394326 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.75 31078\nConnection to 10.62.16.75 31078 port [tcp/*] succeeded!\n"
  I0512 16:11:02.394420 23 builder.go:147] stdout: "nodeport-test-2ghkl"
  I0512 16:11:02.394554 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6821 exec execpodvnb9s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.78 31078'
  E0512 16:11:02.571149      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:11:02.680735 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.78 31078\nConnection to 10.62.16.78 31078 port [tcp/*] succeeded!\n"
  I0512 16:11:02.680828 23 builder.go:147] stdout: ""
  I0512 16:11:03.395470 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6821 exec execpodvnb9s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.78 31078'
  E0512 16:11:03.572634      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:11:03.691285 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.78 31078\nConnection to 10.62.16.78 31078 port [tcp/*] succeeded!\n"
  I0512 16:11:03.691410 23 builder.go:147] stdout: ""
  I0512 16:11:04.394958 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6821 exec execpodvnb9s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.78 31078'
  E0512 16:11:04.573032      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:11:04.686748 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.78 31078\nConnection to 10.62.16.78 31078 port [tcp/*] succeeded!\n"
  I0512 16:11:04.686833 23 builder.go:147] stdout: "nodeport-test-6tpd4"
  I0512 16:11:04.687064 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6821" for this suite. @ 05/12/25 16:11:04.698
• [10.603 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/preemption.go:814
  STEP: Creating a kubernetes client @ 05/12/25 16:11:04.72
  I0512 16:11:04.720133 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/25 16:11:04.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:11:04.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:11:04.777
  I0512 16:11:04.812850 23 wait.go:50] Waiting up to 1m0s for all nodes to be ready
  E0512 16:11:05.574217      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:06.574497      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:07.575141      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:08.575345      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:09.575727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:10.575861      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:11.576292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:12.576511      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:13.576858      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:14.577813      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:15.578301      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:16.578226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:17.578663      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:18.579249      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:19.580123      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:20.580688      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:21.580849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:22.581091      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:23.581327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:24.582246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:25.583011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:26.583549      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:27.583809      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:28.584428      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:29.584942      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:30.585612      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:31.586131      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:32.586934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:33.587282      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:34.588047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:35.588329      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:36.588882      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:37.589865      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:38.590108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:39.591313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:40.592301      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:41.592821      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:42.593931      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:43.594412      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:44.594828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:45.595152      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:46.595225      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:47.595657      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:48.596135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:49.597131      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:50.597933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:51.598600      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:52.599323      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:53.599524      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:54.599785      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:55.600261      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:56.600295      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:57.601064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:58.601535      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:11:59.602227      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:00.602483      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:01.602606      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:02.603187      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:03.603505      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:04.603937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:04.829189 23 util.go:393] Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/12/25 16:12:04.842
  I0512 16:12:04.842477 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/12/25 16:12:04.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:12:04.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:12:04.892
  I0512 16:12:04.954320 23 preemption.go:820] PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  I0512 16:12:04.962794 23 preemption.go:826] PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  I0512 16:12:05.256390 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6227" for this suite. @ 05/12/25 16:12:05.274
  I0512 16:12:05.295699 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-5813" for this suite. @ 05/12/25 16:12:05.309
• [60.615 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/job.go:878
  STEP: Creating a kubernetes client @ 05/12/25 16:12:05.348
  I0512 16:12:05.348744 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename job @ 05/12/25 16:12:05.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:12:05.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:12:05.406
  STEP: Creating a job @ 05/12/25 16:12:05.413
  STEP: Ensuring active pods == parallelism @ 05/12/25 16:12:05.425
  E0512 16:12:05.604594      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:06.605393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete a job @ 05/12/25 16:12:07.444
  STEP: deleting Job.batch foo in namespace job-2567, will wait for the garbage collector to delete the pods @ 05/12/25 16:12:07.444
  I0512 16:12:07.520760 23 resources.go:139] Deleting Job.batch foo took: 15.873972ms
  E0512 16:12:07.606627      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:07.621010 23 resources.go:163] Terminating Job.batch foo pods took: 100.24927ms
  E0512 16:12:08.607348      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 05/12/25 16:12:09.021
  I0512 16:12:09.032002 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2567" for this suite. @ 05/12/25 16:12:09.046
• [3.719 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:199
  STEP: Creating a kubernetes client @ 05/12/25 16:12:09.07
  I0512 16:12:09.070375 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 16:12:09.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:12:09.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:12:09.134
  STEP: Creating pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249 @ 05/12/25 16:12:09.142
  E0512 16:12:09.607903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:10.608590      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 16:12:11.185
  I0512 16:12:11.195021 23 container_probe.go:1749] Initial restart count of pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 is 0
  I0512 16:12:11.203004 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:11.609720      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:12.610640      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:13.217782 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:13.611277      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:14.611462      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:15.227727 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:15.612422      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:16.612912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:17.236585 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:17.613402      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:18.613762      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:19.245398 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:19.614769      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:20.615009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:21.254183 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:21.615764      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:22.616326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:23.268016 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:23.616608      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:24.617650      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:25.278540 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:25.618020      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:26.618996      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:27.286287 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:27.619908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:28.620282      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:29.295607 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:29.621290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:30.622543      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:31.305284 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  I0512 16:12:31.305386 23 container_probe.go:1763] Restart count of pod container-probe-1249/liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 is now 1 (20.110043651s elapsed)
  E0512 16:12:31.622669      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:32.623026      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:33.313042 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:33.623187      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:34.624372      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:35.333878 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:35.625404      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:36.626487      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:37.341951 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:37.626587      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:38.627136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:39.351049 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:39.627986      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:40.628100      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:41.359033 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:41.628262      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:42.628398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:43.370172 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:43.629041      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:44.629912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:45.380060 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:45.630990      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:46.631108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:47.391733 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:47.631336      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:48.631539      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:49.400897 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:49.632968      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:50.633249      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:51.413395 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  I0512 16:12:51.413504 23 container_probe.go:1763] Restart count of pod container-probe-1249/liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 is now 2 (40.218160904s elapsed)
  E0512 16:12:51.633797      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:52.634030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:53.423044 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:53.635381      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:54.636093      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:55.431149 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:55.636710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:56.637663      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:57.442078 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:57.638552      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:12:58.638857      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:12:59.451826 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:12:59.639596      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:00.640129      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:01.462313 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:01.640352      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:02.641585      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:03.469828 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:03.642136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:04.643380      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:05.481180 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:05.643310      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:06.643636      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:07.493077 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:07.644164      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:08.645509      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:09.503054 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:09.646444      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:10.646833      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:11.515856 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  I0512 16:13:11.516087 23 container_probe.go:1763] Restart count of pod container-probe-1249/liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 is now 3 (1m0.320741207s elapsed)
  E0512 16:13:11.647081      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:12.647893      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:13.528288 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:13.649321      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:14.649220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:15.539239 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:15.649378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:16.650307      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:17.547303 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:17.650467      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:18.651100      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:19.558650 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:19.651804      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:20.652580      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:21.568051 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:21.653509      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:22.653840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:23.578347 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:23.654572      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:24.654632      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:25.587109 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:25.654681      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:26.654901      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:27.595985 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:27.655685      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:28.655904      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:29.609090 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:29.656779      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:30.657408      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:31.615957 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  I0512 16:13:31.616395 23 container_probe.go:1763] Restart count of pod container-probe-1249/liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 is now 4 (1m20.421051555s elapsed)
  E0512 16:13:31.658066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:32.658762      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:33.629080 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:33.659482      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:34.659770      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:35.640104 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:35.660477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:36.660805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:37.648365 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:37.661519      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:38.661672      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:39.658854 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:39.662071      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:40.662419      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:41.662842      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:41.667058 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:42.663254      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:43.664125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:43.678046 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:44.664976      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:45.665254      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:45.687669 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:46.665464      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:47.665613      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:47.696157 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:48.665830      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:49.667222      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:49.707493 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:50.667495      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:51.669841      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:51.717279 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:52.669964      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:53.670942      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:53.732622 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:54.671005      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:55.671158      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:55.742375 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:56.671918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:57.672652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:57.752188 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:13:58.673043      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:13:59.673941      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:13:59.761038 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:00.674330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:01.675098      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:01.770551 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:02.675268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:03.675686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:03.786427 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:04.676810      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:05.676870      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:05.803068 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:06.677196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:07.677762      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:07.814551 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:08.677939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:09.678755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:09.825377 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:10.679636      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:11.680148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:11.835211 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:12.680920      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:13.681111      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:13.844614 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:14.681751      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:15.682302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:15.856246 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:16.682398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:17.683086      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:17.866401 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:18.683325      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:19.683782      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:19.876890 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:20.684110      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:21.684620      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:21.885045 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:22.684503      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:23.685144      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:23.894433 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:24.686474      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:25.686092      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:25.903711 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:26.686365      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:27.686642      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:27.916493 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:28.687796      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:29.688127      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:29.924737 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:30.688977      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:31.689259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:31.932565 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:32.689650      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:33.690193      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:33.944844 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  E0512 16:14:34.690800      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:35.691028      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:35.954244 23 container_probe.go:1759] Get pod liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 in namespace container-probe-1249
  I0512 16:14:35.954370 23 container_probe.go:1763] Restart count of pod container-probe-1249/liveness-58b50d42-a5b2-44e4-98e4-4f773eb5fc17 is now 5 (2m24.759023962s elapsed)
  STEP: deleting the pod @ 05/12/25 16:14:35.954
  I0512 16:14:35.979703 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1249" for this suite. @ 05/12/25 16:14:35.992
• [146.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:184
  STEP: Creating a kubernetes client @ 05/12/25 16:14:36.012
  I0512 16:14:36.012456 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 16:14:36.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:14:36.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:14:36.062
  STEP: Creating pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404 @ 05/12/25 16:14:36.072
  E0512 16:14:36.691638      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:37.692837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 16:14:38.119
  I0512 16:14:38.136693 23 container_probe.go:1749] Initial restart count of pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d is 0
  I0512 16:14:38.148596 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:38.693153      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:39.694051      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:40.160438 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:40.695026      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:41.695504      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:42.173645 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:42.695841      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:43.695900      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:44.187865 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:44.696858      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:45.697202      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:46.198544 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:46.697757      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:47.697981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:48.211261 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:48.699151      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:49.700039      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:50.236064 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:50.701183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:51.701275      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:52.245365 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:52.702204      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:53.703102      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:54.255078 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:54.703932      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:55.704425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:56.263099 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:56.704962      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:57.705374      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:14:58.279670 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:14:58.705675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:14:59.705923      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:00.291171 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:00.706726      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:01.707333      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:02.303726 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:02.708198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:03.708662      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:04.315473 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:04.709156      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:05.709596      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:06.327019 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:06.710547      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:07.710970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:08.334739 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:08.711827      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:09.712101      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:10.341984 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:10.712307      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:11.712743      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:12.349562 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:12.713619      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:13.713406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:14.362473 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:14.713706      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:15.714150      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:16.376490 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:16.714624      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:17.714881      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:18.385445 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:18.715164      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:19.715988      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:20.401993 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:20.716056      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:21.717037      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:22.413073 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:22.717954      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:23.718133      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:24.420442 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:24.718340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:25.718361      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:26.428120 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:26.719239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:27.719467      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:28.439043 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:28.720006      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:29.720800      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:30.449367 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:30.721637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:31.722305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:32.458686 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:32.723036      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:33.723330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:34.466660 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:34.724176      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:35.724578      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:36.475100 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:36.725129      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:37.725328      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:38.487323 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:38.726217      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:39.727124      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:40.494859 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:40.727149      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:41.727884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:42.504110 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:42.729211      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:43.729996      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:44.513040 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:44.730698      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:45.731442      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:46.521798 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:46.732150      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:47.732434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:48.530724 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:48.732928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:49.734158      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:50.539699 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:50.734954      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:51.735501      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:52.551917 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:52.736391      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:53.736856      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:54.560348 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:54.737728      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:55.738358      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:56.571513 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:56.739585      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:57.739750      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:15:58.581228 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:15:58.740919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:15:59.741798      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:00.594445 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:00.742584      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:01.743511      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:02.604630 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:02.743665      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:03.744715      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:04.613454 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:04.744845      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:05.744922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:06.621343 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:06.745736      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:07.746176      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:08.628139 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:08.746776      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:09.747392      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:10.637402 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:10.747499      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:11.747912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:12.651525 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:12.748192      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:13.748741      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:14.661371 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:14.749405      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:15.749657      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:16.669566 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:16.750815      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:17.751490      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:18.677432 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:18.751603      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:19.751788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:20.687548 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:20.752805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:21.753908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:22.695986 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:22.754125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:23.754809      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:24.706799 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:24.755558      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:25.755838      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:26.717291 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:26.755881      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:27.756179      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:28.727431 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:28.757246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:29.758025      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:30.738145 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:30.758189      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:31.758889      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:32.747325 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:32.759584      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:33.759839      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:34.756392 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:34.760385      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:35.761076      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:36.761173      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:36.769678 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:37.761729      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:38.762306      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:38.780234 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:39.763279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:40.763779      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:40.789725 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:41.763906      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:42.764469      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:42.798183 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:43.764903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:44.765130      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:44.806775 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:45.765362      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:46.765433      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:46.816008 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:47.765826      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:48.766486      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:48.828424 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:49.766699      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:50.766807      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:50.838853 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:51.767170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:52.767935      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:52.849950 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:53.768933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:54.769946      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:54.858120 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:55.770153      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:56.770612      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:56.867906 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:57.770865      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:16:58.771519      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:16:58.877619 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:16:59.772038      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:00.772615      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:00.886413 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:01.773330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:02.774217      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:02.892675 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:03.774768      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:04.775591      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:04.902035 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:05.776025      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:06.777013      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:06.910669 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:07.777098      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:08.778035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:08.919288 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:09.778570      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:10.778623      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:10.928627 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:11.778873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:12.779121      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:12.946020 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:13.780046      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:14.780889      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:14.954230 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:15.781236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:16.781396      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:16.964007 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:17.781654      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:18.781745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:18.976805 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:19.781998      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:20.782324      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:20.987731 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:21.782546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:22.782683      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:22.997393 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:23.783148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:24.784170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:25.007797 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:25.784448      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:26.784827      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:27.017090 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:27.785032      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:28.785591      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:29.027844 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:29.786622      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:30.786906      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:31.036393 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:31.787184      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:32.787538      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:33.050109 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:33.788121      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:34.787902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:35.056844 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:35.788396      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:36.788917      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:37.064275 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:37.789258      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:38.789692      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:39.074283 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:39.790393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:40.790965      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:41.085851 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:41.791636      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:42.791927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:43.099207 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:43.792103      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:44.793306      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:45.112594 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:45.793935      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:46.794379      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:47.122399 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:47.795436      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:48.796084      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:49.133071 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:49.796921      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:50.797332      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:51.147422 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:51.797891      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:52.798278      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:53.155605 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:53.798595      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:54.798799      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:55.167617 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:55.800171      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:56.800505      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:57.182598 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:57.801657      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:17:58.802281      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:17:59.193365 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:17:59.802803      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:00.803221      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:01.207213 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:01.804436      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:02.804670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:03.219024 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:03.804819      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:04.804958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:05.231335 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:05.806101      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:06.806490      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:07.244359 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:07.806438      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:08.806903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:09.251543 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:09.807518      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:10.807866      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:11.262406 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:11.808115      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:12.808633      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:13.272210 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:13.809178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:14.810160      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:15.279794 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:15.810879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:16.811127      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:17.291464 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:17.811371      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:18.811621      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:19.302346 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:19.811758      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:20.812076      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:21.315313 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:21.812791      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:22.812970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:23.327377 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:23.813264      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:24.814219      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:25.338511 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:25.815514      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:26.816078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:27.347367 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:27.816993      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:28.817803      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:29.355953 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:29.817819      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:30.818300      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:31.368822 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:31.818449      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:32.818870      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:33.385414 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:33.819156      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:34.820016      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:35.393988 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:35.820922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:36.821101      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:37.404824 23 container_probe.go:1759] Get pod liveness-1c90f398-ef17-41e3-82bc-334b175f1b0d in namespace container-probe-4404
  E0512 16:18:37.821398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:38.821735      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting the pod @ 05/12/25 16:18:39.406
  I0512 16:18:39.442567 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-4404" for this suite. @ 05/12/25 16:18:39.465
• [243.475 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtimeclass.go:57
  STEP: Creating a kubernetes client @ 05/12/25 16:18:39.488
  I0512 16:18:39.488750 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/25 16:18:39.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:18:39.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:18:39.549
  I0512 16:18:39.578978 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2742" for this suite. @ 05/12/25 16:18:39.592
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
  STEP: Creating a kubernetes client @ 05/12/25 16:18:39.611
  I0512 16:18:39.611186 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:18:39.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:18:39.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:18:39.664
  STEP: creating Agnhost RC @ 05/12/25 16:18:39.67
  I0512 16:18:39.671037 23 kubectl.go:1537] namespace kubectl-4211
  I0512 16:18:39.671167 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-4211 create -f -'
  E0512 16:18:39.821775      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:39.969964 23 builder.go:146] stderr: ""
  I0512 16:18:39.970050 23 builder.go:147] stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/12/25 16:18:39.97
  E0512 16:18:40.822261      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:40.978166 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:18:40.978257 23 framework.go:733] Found 0 / 1
  E0512 16:18:41.823476      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:41.991758 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:18:41.991824 23 framework.go:733] Found 1 / 1
  I0512 16:18:41.991861 23 framework.go:742] WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  I0512 16:18:42.004380 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:18:42.004444 23 framework.go:765] ForEach: Found 1 pods from the filter.  Now looping through them.
  I0512 16:18:42.004465 23 kubectl.go:1544] wait on agnhost-primary startup in kubectl-4211 
  I0512 16:18:42.004708 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-4211 logs agnhost-primary-nbv89 agnhost-primary'
  I0512 16:18:42.185829 23 builder.go:146] stderr: ""
  I0512 16:18:42.185934 23 builder.go:147] stdout: "Paused\n"
  STEP: exposing RC @ 05/12/25 16:18:42.186
  I0512 16:18:42.186100 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-4211 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  I0512 16:18:42.371818 23 builder.go:146] stderr: ""
  I0512 16:18:42.371913 23 builder.go:147] stdout: "service/rm2 exposed\n"
  I0512 16:18:42.381653 23 utils.go:1203] Service rm2 in namespace kubectl-4211 found.
  E0512 16:18:42.824466      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:43.825370      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: exposing service @ 05/12/25 16:18:44.397
  I0512 16:18:44.397234 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-4211 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  I0512 16:18:44.569753 23 builder.go:146] stderr: ""
  I0512 16:18:44.569812 23 builder.go:147] stdout: "service/rm3 exposed\n"
  I0512 16:18:44.586343 23 utils.go:1203] Service rm3 in namespace kubectl-4211 found.
  E0512 16:18:44.826389      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:45.827713      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:46.604463 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4211" for this suite. @ 05/12/25 16:18:46.613
• [7.024 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/field_validation.go:115
  STEP: Creating a kubernetes client @ 05/12/25 16:18:46.637
  I0512 16:18:46.637824 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename field-validation @ 05/12/25 16:18:46.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:18:46.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:18:46.693
  STEP: apply creating a deployment @ 05/12/25 16:18:46.701
  I0512 16:18:46.743087 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2486" for this suite. @ 05/12/25 16:18:46.762
• [0.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [sig-scheduling, Serial, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/12/25 16:18:46.789
  I0512 16:18:46.789839 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename sched-pred @ 05/12/25 16:18:46.791
  E0512 16:18:46.828053      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:18:46.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:18:46.85
  I0512 16:18:46.879503 23 helper.go:122] Waiting up to 1m0s for all (but 0) nodes to be ready
  I0512 16:18:46.901487 23 util.go:393] Waiting for terminating namespaces to be deleted...
  I0512 16:18:46.910301 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-0 before test
  I0512 16:18:46.926144 23 predicates.go:957] harbor-core-569d44cfd7-46qkt from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.926699 23 predicates.go:959] 	Container core ready: true, restart count 1
  I0512 16:18:46.927233 23 predicates.go:957] harbor-database-0 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.927697 23 predicates.go:959] 	Container database ready: true, restart count 0
  I0512 16:18:46.928159 23 predicates.go:957] harbor-redis-0 from harbor started at 2025-05-12 15:18:25 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.928545 23 predicates.go:959] 	Container redis ready: true, restart count 0
  I0512 16:18:46.928960 23 predicates.go:957] kube-flannel-m6mdj from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.929354 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 16:18:46.929719 23 predicates.go:957] kube-proxy-ksc6h from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.930250 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 16:18:46.930870 23 predicates.go:957] metrics-server-c5b7b4dc-sb7sz from kube-system started at 2025-05-12 15:18:24 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.931454 23 predicates.go:959] 	Container metrics-server ready: true, restart count 0
  I0512 16:18:46.931855 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-0 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.932225 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 16:18:46.932633 23 predicates.go:957] nodelocaldns-fp5rs from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.933017 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 16:18:46.933408 23 predicates.go:957] vsphere-csi-node-zmnvq from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 16:18:46.933753 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 16:18:46.934155 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 16:18:46.934449 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 16:18:46.934818 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-9znn4 from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 16:18:46.935161 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 16:18:46.935548 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 16:18:46.935907 23 predicates.go:957] vault-1 from vault started at 2025-05-12 14:25:29 +0000 UTC (3 container statuses recorded)
  I0512 16:18:46.936329 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 16:18:46.936667 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 16:18:46.936996 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 16:18:46.937334 23 predicates.go:957] vault-configurer-59545bd678-lmkzg from vault started at 2025-05-12 14:24:44 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.937639 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 16:18:46.938011 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-1 before test
  I0512 16:18:46.957954 23 predicates.go:957] kube-flannel-8bzqg from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.958001 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 16:18:46.958021 23 predicates.go:957] kube-proxy-g82r5 from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.958032 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 16:18:46.958044 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-1 from kube-system started at 2025-05-12 15:07:22 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.958054 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 16:18:46.958065 23 predicates.go:957] nodelocaldns-v22ld from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.958074 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 16:18:46.958085 23 predicates.go:957] vsphere-csi-node-4z66x from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 16:18:46.958094 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 16:18:46.958104 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 16:18:46.958113 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 16:18:46.958123 23 predicates.go:957] agnhost-primary-nbv89 from kubectl-4211 started at 2025-05-12 16:18:40 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.958132 23 predicates.go:959] 	Container agnhost-primary ready: true, restart count 0
  I0512 16:18:46.958142 23 predicates.go:957] sonobuoy from sonobuoy started at 2025-05-12 14:52:15 +0000 UTC (1 container statuses recorded)
  I0512 16:18:46.958151 23 predicates.go:959] 	Container kube-sonobuoy ready: true, restart count 0
  I0512 16:18:46.958161 23 predicates.go:957] sonobuoy-e2e-job-0883294174af4fef from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 16:18:46.958174 23 predicates.go:959] 	Container e2e ready: true, restart count 0
  I0512 16:18:46.958199 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 16:18:46.958211 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-b5spt from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 16:18:46.958222 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 16:18:46.958231 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 16:18:46.958241 23 predicates.go:957] vault-0 from vault started at 2025-05-12 15:18:55 +0000 UTC (3 container statuses recorded)
  I0512 16:18:46.958250 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 16:18:46.958259 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 16:18:46.958268 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 16:18:46.958279 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-2 before test
  I0512 16:18:47.027819 23 predicates.go:957] harbor-registry-5584d97487-9cst2 from harbor started at 2025-05-12 14:20:33 +0000 UTC (2 container statuses recorded)
  I0512 16:18:47.027890 23 predicates.go:959] 	Container registry ready: true, restart count 0
  I0512 16:18:47.027911 23 predicates.go:959] 	Container registryctl ready: true, restart count 0
  I0512 16:18:47.027934 23 predicates.go:957] kube-flannel-sfqr5 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.027951 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 16:18:47.027970 23 predicates.go:957] kube-proxy-95b2r from kube-system started at 2025-05-12 14:12:58 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.027986 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 16:18:47.028005 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-2 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.028020 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 16:18:47.028043 23 predicates.go:957] nodelocaldns-5lcd4 from kube-system started at 2025-05-12 14:14:57 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.028077 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 16:18:47.028115 23 predicates.go:957] vsphere-csi-node-sqm2v from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 16:18:47.028146 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 16:18:47.028172 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 16:18:47.028199 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 16:18:47.028229 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-qbt8k from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 16:18:47.028302 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 16:18:47.028319 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 16:18:47.028338 23 predicates.go:957] vault-2 from vault started at 2025-05-12 14:26:08 +0000 UTC (3 container statuses recorded)
  I0512 16:18:47.028380 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 16:18:47.028409 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 16:18:47.028443 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 16:18:47.028464 23 predicates.go:957] velero-5f4c979ccf-4c5mx from velero started at 2025-05-12 14:22:48 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.028548 23 predicates.go:959] 	Container velero ready: true, restart count 0
  I0512 16:18:47.028614 23 predicates.go:119] 
  Logging pods the apiserver thinks is on node opscontrol-jaku1-worker-3 before test
  I0512 16:18:47.042476 23 predicates.go:957] harbor-jobservice-79c749f9bc-qm554 from harbor started at 2025-05-12 14:20:33 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.042868 23 predicates.go:959] 	Container jobservice ready: true, restart count 2
  I0512 16:18:47.043175 23 predicates.go:957] harbor-portal-757685fc6b-7smbd from harbor started at 2025-05-12 14:20:31 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.043528 23 predicates.go:959] 	Container portal ready: true, restart count 0
  I0512 16:18:47.043747 23 predicates.go:957] kube-flannel-pglr4 from kube-system started at 2025-05-12 14:14:26 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.044065 23 predicates.go:959] 	Container kube-flannel ready: true, restart count 0
  I0512 16:18:47.044338 23 predicates.go:957] kube-proxy-5nnvw from kube-system started at 2025-05-12 14:12:59 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.044620 23 predicates.go:959] 	Container kube-proxy ready: true, restart count 0
  I0512 16:18:47.044842 23 predicates.go:957] nginx-proxy-opscontrol-jaku1-worker-3 from kube-system started at 2025-05-12 15:07:23 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.045287 23 predicates.go:959] 	Container nginx-proxy ready: true, restart count 0
  I0512 16:18:47.045767 23 predicates.go:957] nodelocaldns-v58mx from kube-system started at 2025-05-12 14:14:58 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.046008 23 predicates.go:959] 	Container node-cache ready: true, restart count 0
  I0512 16:18:47.046234 23 predicates.go:957] vsphere-csi-node-blmnp from kube-system started at 2025-05-12 15:13:32 +0000 UTC (3 container statuses recorded)
  I0512 16:18:47.046442 23 predicates.go:959] 	Container liveness-probe ready: true, restart count 0
  I0512 16:18:47.046595 23 predicates.go:959] 	Container node-driver-registrar ready: true, restart count 0
  I0512 16:18:47.046761 23 predicates.go:959] 	Container vsphere-csi-node ready: true, restart count 0
  I0512 16:18:47.046969 23 predicates.go:957] sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-rbcss from sonobuoy started at 2025-05-12 14:52:22 +0000 UTC (2 container statuses recorded)
  I0512 16:18:47.047198 23 predicates.go:959] 	Container sonobuoy-worker ready: true, restart count 0
  I0512 16:18:47.047400 23 predicates.go:959] 	Container systemd-logs ready: true, restart count 0
  I0512 16:18:47.047601 23 predicates.go:957] vault-3 from vault started at 2025-05-12 14:27:02 +0000 UTC (3 container statuses recorded)
  I0512 16:18:47.047786 23 predicates.go:959] 	Container bank-vaults ready: true, restart count 0
  I0512 16:18:47.048017 23 predicates.go:959] 	Container vault ready: true, restart count 0
  I0512 16:18:47.048255 23 predicates.go:959] 	Container velero-fsfreeze ready: true, restart count 0
  I0512 16:18:47.048495 23 predicates.go:957] vault-operator-56c68d678-2gr95 from vault started at 2025-05-12 14:24:33 +0000 UTC (1 container statuses recorded)
  I0512 16:18:47.048756 23 predicates.go:959] 	Container vault-operator ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/12/25 16:18:47.049
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.183ed3ee96203090], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/7 nodes are available: 7 Preemption is not helpful for scheduling.] @ 05/12/25 16:18:47.153
  E0512 16:18:47.828844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:18:48.158875 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-406" for this suite. @ 05/12/25 16:18:48.176
• [1.405 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/subpath.go:105
  STEP: Creating a kubernetes client @ 05/12/25 16:18:48.195
  I0512 16:18:48.195638 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename subpath @ 05/12/25 16:18:48.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:18:48.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:18:48.271
  STEP: Setting up data @ 05/12/25 16:18:48.28
  STEP: Creating pod pod-subpath-test-projected-99d4 @ 05/12/25 16:18:48.314
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/25 16:18:48.315
  E0512 16:18:48.828877      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:49.829796      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:50.830478      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:51.830902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:52.831015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:53.831411      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:54.831566      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:55.832281      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:56.833057      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:57.833450      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:58.833804      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:18:59.833978      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:00.834187      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:01.834759      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:02.835947      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:03.835944      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:04.836281      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:05.836585      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:06.837662      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:07.837473      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:08.837991      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:09.838289      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:10.838834      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:11.839094      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:19:12.491
  I0512 16:19:12.499886 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-subpath-test-projected-99d4 container test-container-subpath-projected-99d4: <nil>
  STEP: delete the pod @ 05/12/25 16:19:12.535
  STEP: Deleting pod pod-subpath-test-projected-99d4 @ 05/12/25 16:19:12.578
  I0512 16:19:12.578570 23 delete.go:62] Deleting pod "pod-subpath-test-projected-99d4" in namespace "subpath-9761"
  I0512 16:19:12.587821 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9761" for this suite. @ 05/12/25 16:19:12.601
• [24.444 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance] [sig-node, Conformance]
k8s.io/kubernetes/test/e2e/common/node/podtemplates.go:123
  STEP: Creating a kubernetes client @ 05/12/25 16:19:12.642
  I0512 16:19:12.642077 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename podtemplate @ 05/12/25 16:19:12.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:12.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:12.716
  STEP: Create set of pod templates @ 05/12/25 16:19:12.725
  I0512 16:19:12.736973 23 podtemplates.go:143] created test-podtemplate-1
  I0512 16:19:12.756046 23 podtemplates.go:143] created test-podtemplate-2
  I0512 16:19:12.769710 23 podtemplates.go:143] created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/12/25 16:19:12.769
  STEP: delete collection of pod templates @ 05/12/25 16:19:12.781
  I0512 16:19:12.781623 23 podtemplates.go:158] requesting DeleteCollection of pod templates
  E0512 16:19:12.839658      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: check that the list of pod templates matches the requested quantity @ 05/12/25 16:19:12.846
  I0512 16:19:12.847235 23 podtemplates.go:219] requesting list of pod templates to confirm quantity
  I0512 16:19:12.888924 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5639" for this suite. @ 05/12/25 16:19:12.901
• [0.277 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:3619
  STEP: Creating a kubernetes client @ 05/12/25 16:19:12.919
  I0512 16:19:12.919608 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 16:19:12.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:12.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:13
  STEP: creating a collection of services @ 05/12/25 16:19:13.007
  I0512 16:19:13.007844 23 service.go:3655] Creating e2e-svc-a-srh4j
  I0512 16:19:13.036317 23 service.go:3655] Creating e2e-svc-b-j2rwj
  I0512 16:19:13.081119 23 service.go:3655] Creating e2e-svc-c-whvwh
  STEP: deleting service collection @ 05/12/25 16:19:13.117
  I0512 16:19:13.212949 23 service.go:3690] Collection of services has been deleted
  I0512 16:19:13.214601 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6878" for this suite. @ 05/12/25 16:19:13.229
• [0.342 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/init_container.go:256
  STEP: Creating a kubernetes client @ 05/12/25 16:19:13.261
  I0512 16:19:13.261637 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename init-container @ 05/12/25 16:19:13.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:13.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:13.325
  STEP: creating the pod @ 05/12/25 16:19:13.333
  I0512 16:19:13.333294 23 init_container.go:294] PodSpec: initContainers in spec.initContainers
  E0512 16:19:13.840420      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:14.840370      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:15.840800      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:16.599097 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4222" for this suite. @ 05/12/25 16:19:16.619
• [3.377 seconds]
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/configmap.go:47
  STEP: Creating a kubernetes client @ 05/12/25 16:19:16.639
  I0512 16:19:16.639621 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 16:19:16.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:16.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:16.699
  STEP: Creating configMap configmap-6144/configmap-test-df6e903d-c51c-4700-a1df-45614d2255f9 @ 05/12/25 16:19:16.708
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:19:16.723
  E0512 16:19:16.841469      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:17.842056      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:18.842428      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:19.844209      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:19:20.79
  I0512 16:19:20.802048 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-configmaps-124c4378-03ae-4310-b83f-cb7ba42e6899 container env-test: <nil>
  STEP: delete the pod @ 05/12/25 16:19:20.826
  E0512 16:19:20.843652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:20.906814 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6144" for this suite. @ 05/12/25 16:19:20.918
• [4.300 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:59
  STEP: Creating a kubernetes client @ 05/12/25 16:19:20.94
  I0512 16:19:20.940055 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/25 16:19:20.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:20.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:21
  I0512 16:19:21.005532 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:19:21.844943      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:22.845043      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:23.845508      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:24.846517      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:25.847446      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:26.848442      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:27.086809 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7540" for this suite. @ 05/12/25 16:19:27.1
• [6.182 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:424
  STEP: Creating a kubernetes client @ 05/12/25 16:19:27.122
  I0512 16:19:27.122835 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 16:19:27.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:27.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:27.2
  STEP: Creating configMap with name configmap-test-volume-77363a3a-03d3-4612-bd37-0177202cd07f @ 05/12/25 16:19:27.208
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:19:27.225
  E0512 16:19:27.848838      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:28.849010      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:29.849837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:30.850103      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:19:31.303
  I0512 16:19:31.318853 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-configmaps-deabacf8-3268-449d-9163-9bfaaa30f570 container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 16:19:31.336
  I0512 16:19:31.383139 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2708" for this suite. @ 05/12/25 16:19:31.396
• [4.295 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/replica_set.go:144
  STEP: Creating a kubernetes client @ 05/12/25 16:19:31.42
  I0512 16:19:31.420279 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replicaset @ 05/12/25 16:19:31.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:31.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:31.484
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/12/25 16:19:31.49
  I0512 16:19:31.519543 23 resource.go:87] Pod name sample-pod: Found 0 pods out of 1
  E0512 16:19:31.851268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:32.851753      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:33.852657      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:34.852826      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:35.853307      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:36.532233 23 resource.go:87] Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/25 16:19:36.532
  STEP: getting scale subresource @ 05/12/25 16:19:36.532
  STEP: updating a scale subresource @ 05/12/25 16:19:36.541
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/12/25 16:19:36.552
  STEP: Patch a scale subresource @ 05/12/25 16:19:36.562
  I0512 16:19:36.595645 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4123" for this suite. @ 05/12/25 16:19:36.615
• [5.220 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/12/25 16:19:36.639
  I0512 16:19:36.639777 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 16:19:36.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:36.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:36.723
  STEP: creating a Deployment @ 05/12/25 16:19:36.742
  STEP: waiting for Deployment to be created @ 05/12/25 16:19:36.761
  STEP: waiting for all Replicas to be Ready @ 05/12/25 16:19:36.764
  I0512 16:19:36.768082 23 deployment.go:246] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  I0512 16:19:36.768139 23 deployment.go:248] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  I0512 16:19:36.789527 23 deployment.go:246] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  I0512 16:19:36.789601 23 deployment.go:248] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  I0512 16:19:36.852794 23 deployment.go:246] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  I0512 16:19:36.852868 23 deployment.go:248] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0512 16:19:36.853714      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:36.936401 23 deployment.go:246] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  I0512 16:19:36.936482 23 deployment.go:248] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  I0512 16:19:37.742345 23 deployment.go:246] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  I0512 16:19:37.742421 23 deployment.go:248] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0512 16:19:37.854486      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:37.908595 23 deployment.go:248] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/12/25 16:19:37.908
  I0512 16:19:37.935968 23 deployment.go:290] observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/12/25 16:19:37.936
  I0512 16:19:37.951440 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.952650 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.953579 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.954477 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.955445 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.956357 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.957320 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.958146 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 0
  I0512 16:19:37.959264 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:37.961378 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:37.962148 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:37.962883 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:37.963665 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:37.964448 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:37.970376 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:37.970448 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:38.035985 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:38.036758 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:38.096830 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:38.096989 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:38.135886 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:38.136154 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  E0512 16:19:38.854551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:39.808920 23 deployment.go:309] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:39.809152 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  E0512 16:19:39.855377      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:39.924180 23 deployment.go:311] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  STEP: listing Deployments @ 05/12/25 16:19:39.924
  I0512 16:19:39.933776 23 deployment.go:327] Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/12/25 16:19:39.934
  I0512 16:19:39.975676 23 deployment.go:360] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/12/25 16:19:39.975
  I0512 16:19:39.995140 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  I0512 16:19:40.030474 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  I0512 16:19:40.117693 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  I0512 16:19:40.177775 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0512 16:19:40.856571      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:40.900639 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0512 16:19:41.857246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:41.880008 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  I0512 16:19:41.936806 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  I0512 16:19:42.001015 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0512 16:19:42.856953      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:43.004947 23 deployment.go:389] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/12/25 16:19:43.119
  STEP: fetching the DeploymentStatus @ 05/12/25 16:19:43.148
  I0512 16:19:43.164021 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:43.164226 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:43.164342 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:43.165335 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 1
  I0512 16:19:43.165685 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:43.165964 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 3
  I0512 16:19:43.166217 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:43.166463 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 2
  I0512 16:19:43.166700 23 deployment.go:449] observed Deployment test-deployment in namespace deployment-5270 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/12/25 16:19:43.166
  I0512 16:19:43.195272 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.196699 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.196865 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.197882 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.197987 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.198020 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.198102 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.198637 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.198691 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.199077 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.199160 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.199409 23 deployment.go:475] observed event type MODIFIED
  I0512 16:19:43.209516 23 deployment.go:650] Log out all the ReplicaSets if there is no deployment created
  I0512 16:19:43.221099 23 deployment.go:657] ReplicaSet "test-deployment-6c86c7f765":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=26) "test-deployment-6c86c7f765",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-5270",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "d78b53f3-2a83-4c9d-88ba-e03b8e287de8",
      ResourceVersion: (string) (len=5) "55429",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663580,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "6c86c7f765",
        (string) (len=22) "test-deployment-static": (string) (len=4) "true"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "2",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "3",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "3"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=15) "test-deployment",
          UID: (types.UID) (len=36) "4c5a3e1a-affc-4708-b270-d0e78e570946",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=827) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              000000d0  65 2d 68 61 73 68 22 3a  7b 7d 2c 22 66 3a 74 65  |e-hash":{},"f:te|
              000000e0  73 74 2d 64 65 70 6c 6f  79 6d 65 6e 74 2d 73 74  |st-deployment-st|
              000000f0  61 74 69 63 22 3a 7b 7d  7d 2c 22 66 3a 6f 77 6e  |atic":{}},"f:own|
              00000100  65 72 52 65 66 65 72 65  6e 63 65 73 22 3a 7b 22  |erReferences":{"|
              00000110  2e 22 3a 7b 7d 2c 22 6b  3a 7b 5c 22 75 69 64 5c  |.":{},"k:{\"uid\|
              00000120  22 3a 5c 22 34 63 35 61  33 65 31 61 2d 61 66 66  |":\"4c5a3e1a-aff|
              00000130  63 2d 34 37 30 38 2d 62  32 37 30 2d 64 30 65 37  |c-4708-b270-d0e7|
              00000140  38 65 35 37 30 39 34 36  5c 22 7d 22 3a 7b 7d 7d  |8e570946\"}":{}}|
              00000150  7d 2c 22 66 3a 73 70 65  63 22 3a 7b 22 66 3a 72  |},"f:spec":{"f:r|
              00000160  65 70 6c 69 63 61 73 22  3a 7b 7d 2c 22 66 3a 73  |eplicas":{},"f:s|
              00000170  65 6c 65 63 74 6f 72 22  3a 7b 7d 2c 22 66 3a 74  |elector":{},"f:t|
              00000180  65 6d 70 6c 61 74 65 22  3a 7b 22 66 3a 6d 65 74  |emplate":{"f:met|
              00000190  61 64 61 74 61 22 3a 7b  22 66 3a 6c 61 62 65 6c  |adata":{"f:label|
              000001a0  73 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 70 6f  |s":{".":{},"f:po|
              000001b0  64 2d 74 65 6d 70 6c 61  74 65 2d 68 61 73 68 22  |d-template-hash"|
              000001c0  3a 7b 7d 2c 22 66 3a 74  65 73 74 2d 64 65 70 6c  |:{},"f:test-depl|
              000001d0  6f 79 6d 65 6e 74 2d 73  74 61 74 69 63 22 3a 7b  |oyment-static":{|
              000001e0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              00000200  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 74 65 73  |:{\"name\":\"tes|
              00000210  74 2d 64 65 70 6c 6f 79  6d 65 6e 74 5c 22 7d 22  |t-deployment\"}"|
              00000220  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 69 6d 61 67  |:{".":{},"f:imag|
              00000230  65 22 3a 7b 7d 2c 22 66  3a 69 6d 61 67 65 50 75  |e":{},"f:imagePu|
              00000240  6c 6c 50 6f 6c 69 63 79  22 3a 7b 7d 2c 22 66 3a  |llPolicy":{},"f:|
              00000250  6e 61 6d 65 22 3a 7b 7d  2c 22 66 3a 72 65 73 6f  |name":{},"f:reso|
              00000260  75 72 63 65 73 22 3a 7b  7d 2c 22 66 3a 73 65 63  |urces":{},"f:sec|
              00000270  75 72 69 74 79 43 6f 6e  74 65 78 74 22 3a 7b 7d  |urityContext":{}|
              00000280  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000290  65 73 73 61 67 65 50 61  74 68 22 3a 7b 7d 2c 22  |essagePath":{},"|
              000002a0  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 4d 65 73  |f:terminationMes|
              000002b0  73 61 67 65 50 6f 6c 69  63 79 22 3a 7b 7d 7d 7d  |sagePolicy":{}}}|
              000002c0  2c 22 66 3a 64 6e 73 50  6f 6c 69 63 79 22 3a 7b  |,"f:dnsPolicy":{|
              000002d0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000002e0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000002f0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              00000300  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000310  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000320  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000330  6e 64 73 22 3a 7b 7d 7d  7d 7d 7d                 |nds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663582,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=130) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  61 76 61 69 6c 61 62 6c  65 52 65 70 6c 69 63 61  |availableReplica|
              00000020  73 22 3a 7b 7d 2c 22 66  3a 66 75 6c 6c 79 4c 61  |s":{},"f:fullyLa|
              00000030  62 65 6c 65 64 52 65 70  6c 69 63 61 73 22 3a 7b  |beledReplicas":{|
              00000040  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              00000050  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              00000060  65 61 64 79 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |eadyReplicas":{}|
              00000070  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000080  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(2),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=17) "pod-template-hash": (string) (len=10) "6c86c7f765",
          (string) (len=22) "test-deployment-static": (string) (len=4) "true"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=17) "pod-template-hash": (string) (len=10) "6c86c7f765",
            (string) (len=22) "test-deployment-static": (string) (len=4) "true"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=15) "test-deployment",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(1),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 2,
      FullyLabeledReplicas: (int32) 2,
      ReadyReplicas: (int32) 2,
      AvailableReplicas: (int32) 2,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }


  I0512 16:19:43.236230 23 deployment.go:669] pod: "test-deployment-6c86c7f765-fcgpx":
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=32) "test-deployment-6c86c7f765-fcgpx",
      GenerateName: (string) (len=27) "test-deployment-6c86c7f765-",
      Namespace: (string) (len=15) "deployment-5270",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "515b4c8c-c770-42c3-b4fc-5232383fcf52",
      ResourceVersion: (string) (len=5) "55381",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663580,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "6c86c7f765",
        (string) (len=22) "test-deployment-static": (string) (len=4) "true"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=26) "test-deployment-6c86c7f765",
          UID: (types.UID) (len=36) "d78b53f3-2a83-4c9d-88ba-e03b8e287de8",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=565) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 70 6f 64 2d 74 65 6d  |.":{},"f:pod-tem|
              00000040  70 6c 61 74 65 2d 68 61  73 68 22 3a 7b 7d 2c 22  |plate-hash":{},"|
              00000050  66 3a 74 65 73 74 2d 64  65 70 6c 6f 79 6d 65 6e  |f:test-deploymen|
              00000060  74 2d 73 74 61 74 69 63  22 3a 7b 7d 7d 2c 22 66  |t-static":{}},"f|
              00000070  3a 6f 77 6e 65 72 52 65  66 65 72 65 6e 63 65 73  |:ownerReferences|
              00000080  22 3a 7b 22 2e 22 3a 7b  7d 2c 22 6b 3a 7b 5c 22  |":{".":{},"k:{\"|
              00000090  75 69 64 5c 22 3a 5c 22  64 37 38 62 35 33 66 33  |uid\":\"d78b53f3|
              000000a0  2d 32 61 38 33 2d 34 63  39 64 2d 38 38 62 61 2d  |-2a83-4c9d-88ba-|
              000000b0  65 30 33 62 38 65 32 38  37 64 65 38 5c 22 7d 22  |e03b8e287de8\"}"|
              000000c0  3a 7b 7d 7d 7d 2c 22 66  3a 73 70 65 63 22 3a 7b  |:{}}},"f:spec":{|
              000000d0  22 66 3a 63 6f 6e 74 61  69 6e 65 72 73 22 3a 7b  |"f:containers":{|
              000000e0  22 6b 3a 7b 5c 22 6e 61  6d 65 5c 22 3a 5c 22 74  |"k:{\"name\":\"t|
              000000f0  65 73 74 2d 64 65 70 6c  6f 79 6d 65 6e 74 5c 22  |est-deployment\"|
              00000100  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000110  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000120  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000130  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              00000140  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              00000150  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000160  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000170  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              00000180  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000190  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              000001a0  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              000001b0  3a 7b 7d 2c 22 66 3a 65  6e 61 62 6c 65 53 65 72  |:{},"f:enableSer|
              000001c0  76 69 63 65 4c 69 6e 6b  73 22 3a 7b 7d 2c 22 66  |viceLinks":{},"f|
              000001d0  3a 72 65 73 74 61 72 74  50 6f 6c 69 63 79 22 3a  |:restartPolicy":|
              000001e0  7b 7d 2c 22 66 3a 73 63  68 65 64 75 6c 65 72 4e  |{},"f:schedulerN|
              000001f0  61 6d 65 22 3a 7b 7d 2c  22 66 3a 73 65 63 75 72  |ame":{},"f:secur|
              00000200  69 74 79 43 6f 6e 74 65  78 74 22 3a 7b 7d 2c 22  |ityContext":{},"|
              00000210  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 47 72 61  |f:terminationGra|
              00000220  63 65 50 65 72 69 6f 64  53 65 63 6f 6e 64 73 22  |cePeriodSeconds"|
              00000230  3a 7b 7d 7d 7d                                    |:{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=661) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 39 2e  31 32 5c 22 7d 22 3a 7b  |.233.69.12\"}":{|
              00000270  22 2e 22 3a 7b 7d 2c 22  66 3a 69 70 22 3a 7b 7d  |".":{},"f:ip":{}|
              00000280  7d 7d 2c 22 66 3a 73 74  61 72 74 54 69 6d 65 22  |}},"f:startTime"|
              00000290  3a 7b 7d 7d 7d                                    |:{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-d6xmj",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=15) "test-deployment",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-d6xmj",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(1),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) (len=12) "10.233.69.12",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=12) "10.233.69.12"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663580,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=15) "test-deployment",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882663580,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://c7f1c38d13f87ed8b3baa46b11697876f5636e7df831810c54830518d6d2114a",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-d6xmj",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }


  I0512 16:19:43.253016 23 deployment.go:669] pod: "test-deployment-6c86c7f765-jlhgj":
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=32) "test-deployment-6c86c7f765-jlhgj",
      GenerateName: (string) (len=27) "test-deployment-6c86c7f765-",
      Namespace: (string) (len=15) "deployment-5270",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "f2ad3603-2072-4dae-8919-9fc6395c801a",
      ResourceVersion: (string) (len=5) "55428",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663581,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "6c86c7f765",
        (string) (len=22) "test-deployment-static": (string) (len=4) "true"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=26) "test-deployment-6c86c7f765",
          UID: (types.UID) (len=36) "d78b53f3-2a83-4c9d-88ba-e03b8e287de8",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=565) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 70 6f 64 2d 74 65 6d  |.":{},"f:pod-tem|
              00000040  70 6c 61 74 65 2d 68 61  73 68 22 3a 7b 7d 2c 22  |plate-hash":{},"|
              00000050  66 3a 74 65 73 74 2d 64  65 70 6c 6f 79 6d 65 6e  |f:test-deploymen|
              00000060  74 2d 73 74 61 74 69 63  22 3a 7b 7d 7d 2c 22 66  |t-static":{}},"f|
              00000070  3a 6f 77 6e 65 72 52 65  66 65 72 65 6e 63 65 73  |:ownerReferences|
              00000080  22 3a 7b 22 2e 22 3a 7b  7d 2c 22 6b 3a 7b 5c 22  |":{".":{},"k:{\"|
              00000090  75 69 64 5c 22 3a 5c 22  64 37 38 62 35 33 66 33  |uid\":\"d78b53f3|
              000000a0  2d 32 61 38 33 2d 34 63  39 64 2d 38 38 62 61 2d  |-2a83-4c9d-88ba-|
              000000b0  65 30 33 62 38 65 32 38  37 64 65 38 5c 22 7d 22  |e03b8e287de8\"}"|
              000000c0  3a 7b 7d 7d 7d 2c 22 66  3a 73 70 65 63 22 3a 7b  |:{}}},"f:spec":{|
              000000d0  22 66 3a 63 6f 6e 74 61  69 6e 65 72 73 22 3a 7b  |"f:containers":{|
              000000e0  22 6b 3a 7b 5c 22 6e 61  6d 65 5c 22 3a 5c 22 74  |"k:{\"name\":\"t|
              000000f0  65 73 74 2d 64 65 70 6c  6f 79 6d 65 6e 74 5c 22  |est-deployment\"|
              00000100  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000110  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000120  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000130  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              00000140  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              00000150  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000160  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000170  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              00000180  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000190  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              000001a0  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              000001b0  3a 7b 7d 2c 22 66 3a 65  6e 61 62 6c 65 53 65 72  |:{},"f:enableSer|
              000001c0  76 69 63 65 4c 69 6e 6b  73 22 3a 7b 7d 2c 22 66  |viceLinks":{},"f|
              000001d0  3a 72 65 73 74 61 72 74  50 6f 6c 69 63 79 22 3a  |:restartPolicy":|
              000001e0  7b 7d 2c 22 66 3a 73 63  68 65 64 75 6c 65 72 4e  |{},"f:schedulerN|
              000001f0  61 6d 65 22 3a 7b 7d 2c  22 66 3a 73 65 63 75 72  |ame":{},"f:secur|
              00000200  69 74 79 43 6f 6e 74 65  78 74 22 3a 7b 7d 2c 22  |ityContext":{},"|
              00000210  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 47 72 61  |f:terminationGra|
              00000220  63 65 50 65 72 69 6f 64  53 65 63 6f 6e 64 73 22  |cePeriodSeconds"|
              00000230  3a 7b 7d 7d 7d                                    |:{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663582,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 38 2e  31 34 31 5c 22 7d 22 3a  |.233.68.141\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-q8pds",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=15) "test-deployment",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-q8pds",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(1),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663582,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663582,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663582,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663581,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) (len=13) "10.233.68.141",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.68.141"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663581,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=15) "test-deployment",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882663582,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://4f5096e60f71fff0347d173c59e7793cd3a3d28f9d440736a19440c0f3b27237",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-q8pds",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }


  I0512 16:19:43.269191 23 deployment.go:657] ReplicaSet "test-deployment-6ccdbb4d8c":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=26) "test-deployment-6ccdbb4d8c",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-5270",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "abc4de11-9a72-4e5f-af69-a546a8db63ba",
      ResourceVersion: (string) (len=5) "55440",
      Generation: (int64) 4,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663577,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "6ccdbb4d8c",
        (string) (len=22) "test-deployment-static": (string) (len=4) "true"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "2",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "3",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "2"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=15) "test-deployment",
          UID: (types.UID) (len=36) "4c5a3e1a-affc-4708-b270-d0e78e570946",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663582,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=827) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              000000d0  65 2d 68 61 73 68 22 3a  7b 7d 2c 22 66 3a 74 65  |e-hash":{},"f:te|
              000000e0  73 74 2d 64 65 70 6c 6f  79 6d 65 6e 74 2d 73 74  |st-deployment-st|
              000000f0  61 74 69 63 22 3a 7b 7d  7d 2c 22 66 3a 6f 77 6e  |atic":{}},"f:own|
              00000100  65 72 52 65 66 65 72 65  6e 63 65 73 22 3a 7b 22  |erReferences":{"|
              00000110  2e 22 3a 7b 7d 2c 22 6b  3a 7b 5c 22 75 69 64 5c  |.":{},"k:{\"uid\|
              00000120  22 3a 5c 22 34 63 35 61  33 65 31 61 2d 61 66 66  |":\"4c5a3e1a-aff|
              00000130  63 2d 34 37 30 38 2d 62  32 37 30 2d 64 30 65 37  |c-4708-b270-d0e7|
              00000140  38 65 35 37 30 39 34 36  5c 22 7d 22 3a 7b 7d 7d  |8e570946\"}":{}}|
              00000150  7d 2c 22 66 3a 73 70 65  63 22 3a 7b 22 66 3a 72  |},"f:spec":{"f:r|
              00000160  65 70 6c 69 63 61 73 22  3a 7b 7d 2c 22 66 3a 73  |eplicas":{},"f:s|
              00000170  65 6c 65 63 74 6f 72 22  3a 7b 7d 2c 22 66 3a 74  |elector":{},"f:t|
              00000180  65 6d 70 6c 61 74 65 22  3a 7b 22 66 3a 6d 65 74  |emplate":{"f:met|
              00000190  61 64 61 74 61 22 3a 7b  22 66 3a 6c 61 62 65 6c  |adata":{"f:label|
              000001a0  73 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 70 6f  |s":{".":{},"f:po|
              000001b0  64 2d 74 65 6d 70 6c 61  74 65 2d 68 61 73 68 22  |d-template-hash"|
              000001c0  3a 7b 7d 2c 22 66 3a 74  65 73 74 2d 64 65 70 6c  |:{},"f:test-depl|
              000001d0  6f 79 6d 65 6e 74 2d 73  74 61 74 69 63 22 3a 7b  |oyment-static":{|
              000001e0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              00000200  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 74 65 73  |:{\"name\":\"tes|
              00000210  74 2d 64 65 70 6c 6f 79  6d 65 6e 74 5c 22 7d 22  |t-deployment\"}"|
              00000220  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 69 6d 61 67  |:{".":{},"f:imag|
              00000230  65 22 3a 7b 7d 2c 22 66  3a 69 6d 61 67 65 50 75  |e":{},"f:imagePu|
              00000240  6c 6c 50 6f 6c 69 63 79  22 3a 7b 7d 2c 22 66 3a  |llPolicy":{},"f:|
              00000250  6e 61 6d 65 22 3a 7b 7d  2c 22 66 3a 72 65 73 6f  |name":{},"f:reso|
              00000260  75 72 63 65 73 22 3a 7b  7d 2c 22 66 3a 73 65 63  |urces":{},"f:sec|
              00000270  75 72 69 74 79 43 6f 6e  74 65 78 74 22 3a 7b 7d  |urityContext":{}|
              00000280  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000290  65 73 73 61 67 65 50 61  74 68 22 3a 7b 7d 2c 22  |essagePath":{},"|
              000002a0  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 4d 65 73  |f:terminationMes|
              000002b0  73 61 67 65 50 6f 6c 69  63 79 22 3a 7b 7d 7d 7d  |sagePolicy":{}}}|
              000002c0  2c 22 66 3a 64 6e 73 50  6f 6c 69 63 79 22 3a 7b  |,"f:dnsPolicy":{|
              000002d0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000002e0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000002f0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              00000300  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000310  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000320  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000330  6e 64 73 22 3a 7b 7d 7d  7d 7d 7d                 |nds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663583,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=56) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  6f 62 73 65 72 76 65 64  47 65 6e 65 72 61 74 69  |observedGenerati|
              00000020  6f 6e 22 3a 7b 7d 2c 22  66 3a 72 65 70 6c 69 63  |on":{},"f:replic|
              00000030  61 73 22 3a 7b 7d 7d 7d                           |as":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(0),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=17) "pod-template-hash": (string) (len=10) "6ccdbb4d8c",
          (string) (len=22) "test-deployment-static": (string) (len=4) "true"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=17) "pod-template-hash": (string) (len=10) "6ccdbb4d8c",
            (string) (len=22) "test-deployment-static": (string) (len=4) "true"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=15) "test-deployment",
              Image: (string) (len=26) "registry.k8s.io/pause:3.10",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(2),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 0,
      FullyLabeledReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 4,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }


  I0512 16:19:43.294024 23 deployment.go:669] pod: "test-deployment-6ccdbb4d8c-5fh99":
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=32) "test-deployment-6ccdbb4d8c-5fh99",
      GenerateName: (string) (len=27) "test-deployment-6ccdbb4d8c-",
      Namespace: (string) (len=15) "deployment-5270",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "8809f433-1478-488c-996d-8590db8e6d4e",
      ResourceVersion: (string) (len=5) "55434",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663579,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663584,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      DeletionGracePeriodSeconds: (*int64)(2),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "6ccdbb4d8c",
        (string) (len=22) "test-deployment-static": (string) (len=4) "true"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=26) "test-deployment-6ccdbb4d8c",
          UID: (types.UID) (len=36) "abc4de11-9a72-4e5f-af69-a546a8db63ba",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663579,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=565) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 70 6f 64 2d 74 65 6d  |.":{},"f:pod-tem|
              00000040  70 6c 61 74 65 2d 68 61  73 68 22 3a 7b 7d 2c 22  |plate-hash":{},"|
              00000050  66 3a 74 65 73 74 2d 64  65 70 6c 6f 79 6d 65 6e  |f:test-deploymen|
              00000060  74 2d 73 74 61 74 69 63  22 3a 7b 7d 7d 2c 22 66  |t-static":{}},"f|
              00000070  3a 6f 77 6e 65 72 52 65  66 65 72 65 6e 63 65 73  |:ownerReferences|
              00000080  22 3a 7b 22 2e 22 3a 7b  7d 2c 22 6b 3a 7b 5c 22  |":{".":{},"k:{\"|
              00000090  75 69 64 5c 22 3a 5c 22  61 62 63 34 64 65 31 31  |uid\":\"abc4de11|
              000000a0  2d 39 61 37 32 2d 34 65  35 66 2d 61 66 36 39 2d  |-9a72-4e5f-af69-|
              000000b0  61 35 34 36 61 38 64 62  36 33 62 61 5c 22 7d 22  |a546a8db63ba\"}"|
              000000c0  3a 7b 7d 7d 7d 2c 22 66  3a 73 70 65 63 22 3a 7b  |:{}}},"f:spec":{|
              000000d0  22 66 3a 63 6f 6e 74 61  69 6e 65 72 73 22 3a 7b  |"f:containers":{|
              000000e0  22 6b 3a 7b 5c 22 6e 61  6d 65 5c 22 3a 5c 22 74  |"k:{\"name\":\"t|
              000000f0  65 73 74 2d 64 65 70 6c  6f 79 6d 65 6e 74 5c 22  |est-deployment\"|
              00000100  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000110  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000120  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000130  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              00000140  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              00000150  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000160  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000170  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              00000180  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000190  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              000001a0  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              000001b0  3a 7b 7d 2c 22 66 3a 65  6e 61 62 6c 65 53 65 72  |:{},"f:enableSer|
              000001c0  76 69 63 65 4c 69 6e 6b  73 22 3a 7b 7d 2c 22 66  |viceLinks":{},"f|
              000001d0  3a 72 65 73 74 61 72 74  50 6f 6c 69 63 79 22 3a  |:restartPolicy":|
              000001e0  7b 7d 2c 22 66 3a 73 63  68 65 64 75 6c 65 72 4e  |{},"f:schedulerN|
              000001f0  61 6d 65 22 3a 7b 7d 2c  22 66 3a 73 65 63 75 72  |ame":{},"f:secur|
              00000200  69 74 79 43 6f 6e 74 65  78 74 22 3a 7b 7d 2c 22  |ityContext":{},"|
              00000210  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 47 72 61  |f:terminationGra|
              00000220  63 65 50 65 72 69 6f 64  53 65 63 6f 6e 64 73 22  |cePeriodSeconds"|
              00000230  3a 7b 7d 7d 7d                                    |:{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 38 2e  31 34 30 5c 22 7d 22 3a  |.233.68.140\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-wrmm6",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=15) "test-deployment",
          Image: (string) (len=26) "registry.k8s.io/pause:3.10",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-wrmm6",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(2),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663580,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) (len=13) "10.233.68.140",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.68.140"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663580,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=15) "test-deployment",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882663580,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=26) "registry.k8s.io/pause:3.10",
          ImageID: (string) (len=71) "sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136",
          ContainerID: (string) (len=77) "containerd://a25b297c8ca79bba19ddc13d738efcd89bef6728a86176337229dc739b5f75f2",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-wrmm6",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }


  I0512 16:19:43.300294 23 deployment.go:657] ReplicaSet "test-deployment-77bdf6fb4b":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=26) "test-deployment-77bdf6fb4b",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-5270",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "22998341-954b-42f6-a5b2-0706eb7bb565",
      ResourceVersion: (string) (len=5) "55335",
      Generation: (int64) 3,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882663576,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "77bdf6fb4b",
        (string) (len=22) "test-deployment-static": (string) (len=4) "true"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=15) "test-deployment",
          UID: (types.UID) (len=36) "4c5a3e1a-affc-4708-b270-d0e78e570946",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663579,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=827) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              000000d0  65 2d 68 61 73 68 22 3a  7b 7d 2c 22 66 3a 74 65  |e-hash":{},"f:te|
              000000e0  73 74 2d 64 65 70 6c 6f  79 6d 65 6e 74 2d 73 74  |st-deployment-st|
              000000f0  61 74 69 63 22 3a 7b 7d  7d 2c 22 66 3a 6f 77 6e  |atic":{}},"f:own|
              00000100  65 72 52 65 66 65 72 65  6e 63 65 73 22 3a 7b 22  |erReferences":{"|
              00000110  2e 22 3a 7b 7d 2c 22 6b  3a 7b 5c 22 75 69 64 5c  |.":{},"k:{\"uid\|
              00000120  22 3a 5c 22 34 63 35 61  33 65 31 61 2d 61 66 66  |":\"4c5a3e1a-aff|
              00000130  63 2d 34 37 30 38 2d 62  32 37 30 2d 64 30 65 37  |c-4708-b270-d0e7|
              00000140  38 65 35 37 30 39 34 36  5c 22 7d 22 3a 7b 7d 7d  |8e570946\"}":{}}|
              00000150  7d 2c 22 66 3a 73 70 65  63 22 3a 7b 22 66 3a 72  |},"f:spec":{"f:r|
              00000160  65 70 6c 69 63 61 73 22  3a 7b 7d 2c 22 66 3a 73  |eplicas":{},"f:s|
              00000170  65 6c 65 63 74 6f 72 22  3a 7b 7d 2c 22 66 3a 74  |elector":{},"f:t|
              00000180  65 6d 70 6c 61 74 65 22  3a 7b 22 66 3a 6d 65 74  |emplate":{"f:met|
              00000190  61 64 61 74 61 22 3a 7b  22 66 3a 6c 61 62 65 6c  |adata":{"f:label|
              000001a0  73 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 70 6f  |s":{".":{},"f:po|
              000001b0  64 2d 74 65 6d 70 6c 61  74 65 2d 68 61 73 68 22  |d-template-hash"|
              000001c0  3a 7b 7d 2c 22 66 3a 74  65 73 74 2d 64 65 70 6c  |:{},"f:test-depl|
              000001d0  6f 79 6d 65 6e 74 2d 73  74 61 74 69 63 22 3a 7b  |oyment-static":{|
              000001e0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              00000200  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 74 65 73  |:{\"name\":\"tes|
              00000210  74 2d 64 65 70 6c 6f 79  6d 65 6e 74 5c 22 7d 22  |t-deployment\"}"|
              00000220  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 69 6d 61 67  |:{".":{},"f:imag|
              00000230  65 22 3a 7b 7d 2c 22 66  3a 69 6d 61 67 65 50 75  |e":{},"f:imagePu|
              00000240  6c 6c 50 6f 6c 69 63 79  22 3a 7b 7d 2c 22 66 3a  |llPolicy":{},"f:|
              00000250  6e 61 6d 65 22 3a 7b 7d  2c 22 66 3a 72 65 73 6f  |name":{},"f:reso|
              00000260  75 72 63 65 73 22 3a 7b  7d 2c 22 66 3a 73 65 63  |urces":{},"f:sec|
              00000270  75 72 69 74 79 43 6f 6e  74 65 78 74 22 3a 7b 7d  |urityContext":{}|
              00000280  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000290  65 73 73 61 67 65 50 61  74 68 22 3a 7b 7d 2c 22  |essagePath":{},"|
              000002a0  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 4d 65 73  |f:terminationMes|
              000002b0  73 61 67 65 50 6f 6c 69  63 79 22 3a 7b 7d 7d 7d  |sagePolicy":{}}}|
              000002c0  2c 22 66 3a 64 6e 73 50  6f 6c 69 63 79 22 3a 7b  |,"f:dnsPolicy":{|
              000002d0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000002e0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000002f0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              00000300  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000310  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000320  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000330  6e 64 73 22 3a 7b 7d 7d  7d 7d 7d                 |nds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882663579,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=56) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  6f 62 73 65 72 76 65 64  47 65 6e 65 72 61 74 69  |observedGenerati|
              00000020  6f 6e 22 3a 7b 7d 2c 22  66 3a 72 65 70 6c 69 63  |on":{},"f:replic|
              00000030  61 73 22 3a 7b 7d 7d 7d                           |as":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(0),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=17) "pod-template-hash": (string) (len=10) "77bdf6fb4b",
          (string) (len=22) "test-deployment-static": (string) (len=4) "true"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=17) "pod-template-hash": (string) (len=10) "77bdf6fb4b",
            (string) (len=22) "test-deployment-static": (string) (len=4) "true"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=15) "test-deployment",
              Image: (string) (len=44) "registry.k8s.io/e2e-test-images/agnhost:2.52",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(1),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 0,
      FullyLabeledReplicas: (int32) 0,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 3,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }


  I0512 16:19:43.333997 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5270" for this suite. @ 05/12/25 16:19:43.345
• [6.725 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:358
  STEP: Creating a kubernetes client @ 05/12/25 16:19:43.365
  I0512 16:19:43.365568 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 16:19:43.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:19:43.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:19:43.437
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/12/25 16:19:43.448
  I0512 16:19:43.449786 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:19:43.857922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:44.858294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:45.859070      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:46.859173      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:47.860342      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:48.860704      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:49.861640      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:50.862204      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:51.862885      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:19:52.258213 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:19:52.863680      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:53.864994      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:54.865591      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:55.866781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:56.867873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:57.868913      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:58.869123      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:19:59.869844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:00.871611      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:01.872674      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:02.873814      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:03.874456      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:04.875041      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:05.876193      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:06.876900      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:07.877725      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:08.877970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:20:09.757897 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5631" for this suite. @ 05/12/25 16:20:09.782
• [26.432 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:126
  STEP: Creating a kubernetes client @ 05/12/25 16:20:09.797
  I0512 16:20:09.797592 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename discovery @ 05/12/25 16:20:09.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:20:09.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:20:09.851
  E0512 16:20:09.878479      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 05/12/25 16:20:09.885
  E0512 16:20:10.879667      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:20:11.581892 23 discovery.go:139] Checking APIGroup: apiregistration.k8s.io
  I0512 16:20:11.584284 23 discovery.go:147] PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  I0512 16:20:11.584369 23 discovery.go:148] Versions found [{apiregistration.k8s.io/v1 v1}]
  I0512 16:20:11.584396 23 discovery.go:154] apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  I0512 16:20:11.584414 23 discovery.go:139] Checking APIGroup: apps
  I0512 16:20:11.586013 23 discovery.go:147] PreferredVersion.GroupVersion: apps/v1
  I0512 16:20:11.586054 23 discovery.go:148] Versions found [{apps/v1 v1}]
  I0512 16:20:11.586068 23 discovery.go:154] apps/v1 matches apps/v1
  I0512 16:20:11.586081 23 discovery.go:139] Checking APIGroup: events.k8s.io
  I0512 16:20:11.588232 23 discovery.go:147] PreferredVersion.GroupVersion: events.k8s.io/v1
  I0512 16:20:11.588264 23 discovery.go:148] Versions found [{events.k8s.io/v1 v1}]
  I0512 16:20:11.588278 23 discovery.go:154] events.k8s.io/v1 matches events.k8s.io/v1
  I0512 16:20:11.588290 23 discovery.go:139] Checking APIGroup: authentication.k8s.io
  I0512 16:20:11.589923 23 discovery.go:147] PreferredVersion.GroupVersion: authentication.k8s.io/v1
  I0512 16:20:11.589958 23 discovery.go:148] Versions found [{authentication.k8s.io/v1 v1}]
  I0512 16:20:11.589971 23 discovery.go:154] authentication.k8s.io/v1 matches authentication.k8s.io/v1
  I0512 16:20:11.589983 23 discovery.go:139] Checking APIGroup: authorization.k8s.io
  I0512 16:20:11.592122 23 discovery.go:147] PreferredVersion.GroupVersion: authorization.k8s.io/v1
  I0512 16:20:11.592163 23 discovery.go:148] Versions found [{authorization.k8s.io/v1 v1}]
  I0512 16:20:11.592181 23 discovery.go:154] authorization.k8s.io/v1 matches authorization.k8s.io/v1
  I0512 16:20:11.592197 23 discovery.go:139] Checking APIGroup: autoscaling
  I0512 16:20:11.594635 23 discovery.go:147] PreferredVersion.GroupVersion: autoscaling/v2
  I0512 16:20:11.594667 23 discovery.go:148] Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  I0512 16:20:11.594679 23 discovery.go:154] autoscaling/v2 matches autoscaling/v2
  I0512 16:20:11.594691 23 discovery.go:139] Checking APIGroup: batch
  I0512 16:20:11.597397 23 discovery.go:147] PreferredVersion.GroupVersion: batch/v1
  I0512 16:20:11.597450 23 discovery.go:148] Versions found [{batch/v1 v1}]
  I0512 16:20:11.597477 23 discovery.go:154] batch/v1 matches batch/v1
  I0512 16:20:11.597496 23 discovery.go:139] Checking APIGroup: certificates.k8s.io
  I0512 16:20:11.599842 23 discovery.go:147] PreferredVersion.GroupVersion: certificates.k8s.io/v1
  I0512 16:20:11.599901 23 discovery.go:148] Versions found [{certificates.k8s.io/v1 v1}]
  I0512 16:20:11.599920 23 discovery.go:154] certificates.k8s.io/v1 matches certificates.k8s.io/v1
  I0512 16:20:11.599949 23 discovery.go:139] Checking APIGroup: networking.k8s.io
  I0512 16:20:11.602773 23 discovery.go:147] PreferredVersion.GroupVersion: networking.k8s.io/v1
  I0512 16:20:11.603144 23 discovery.go:148] Versions found [{networking.k8s.io/v1 v1}]
  I0512 16:20:11.603426 23 discovery.go:154] networking.k8s.io/v1 matches networking.k8s.io/v1
  I0512 16:20:11.603696 23 discovery.go:139] Checking APIGroup: policy
  I0512 16:20:11.606917 23 discovery.go:147] PreferredVersion.GroupVersion: policy/v1
  I0512 16:20:11.607195 23 discovery.go:148] Versions found [{policy/v1 v1}]
  I0512 16:20:11.607441 23 discovery.go:154] policy/v1 matches policy/v1
  I0512 16:20:11.607712 23 discovery.go:139] Checking APIGroup: rbac.authorization.k8s.io
  I0512 16:20:11.610662 23 discovery.go:147] PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  I0512 16:20:11.610955 23 discovery.go:148] Versions found [{rbac.authorization.k8s.io/v1 v1}]
  I0512 16:20:11.611226 23 discovery.go:154] rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  I0512 16:20:11.611611 23 discovery.go:139] Checking APIGroup: storage.k8s.io
  I0512 16:20:11.614732 23 discovery.go:147] PreferredVersion.GroupVersion: storage.k8s.io/v1
  I0512 16:20:11.615028 23 discovery.go:148] Versions found [{storage.k8s.io/v1 v1}]
  I0512 16:20:11.615270 23 discovery.go:154] storage.k8s.io/v1 matches storage.k8s.io/v1
  I0512 16:20:11.615589 23 discovery.go:139] Checking APIGroup: admissionregistration.k8s.io
  I0512 16:20:11.618924 23 discovery.go:147] PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  I0512 16:20:11.619206 23 discovery.go:148] Versions found [{admissionregistration.k8s.io/v1 v1}]
  I0512 16:20:11.619579 23 discovery.go:154] admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  I0512 16:20:11.619961 23 discovery.go:139] Checking APIGroup: apiextensions.k8s.io
  I0512 16:20:11.623343 23 discovery.go:147] PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  I0512 16:20:11.623391 23 discovery.go:148] Versions found [{apiextensions.k8s.io/v1 v1}]
  I0512 16:20:11.623409 23 discovery.go:154] apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  I0512 16:20:11.623426 23 discovery.go:139] Checking APIGroup: scheduling.k8s.io
  I0512 16:20:11.626193 23 discovery.go:147] PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  I0512 16:20:11.626243 23 discovery.go:148] Versions found [{scheduling.k8s.io/v1 v1}]
  I0512 16:20:11.626263 23 discovery.go:154] scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  I0512 16:20:11.626281 23 discovery.go:139] Checking APIGroup: coordination.k8s.io
  I0512 16:20:11.628998 23 discovery.go:147] PreferredVersion.GroupVersion: coordination.k8s.io/v1
  I0512 16:20:11.629047 23 discovery.go:148] Versions found [{coordination.k8s.io/v1 v1}]
  I0512 16:20:11.629064 23 discovery.go:154] coordination.k8s.io/v1 matches coordination.k8s.io/v1
  I0512 16:20:11.629086 23 discovery.go:139] Checking APIGroup: node.k8s.io
  I0512 16:20:11.632356 23 discovery.go:147] PreferredVersion.GroupVersion: node.k8s.io/v1
  I0512 16:20:11.632925 23 discovery.go:148] Versions found [{node.k8s.io/v1 v1}]
  I0512 16:20:11.633501 23 discovery.go:154] node.k8s.io/v1 matches node.k8s.io/v1
  I0512 16:20:11.634283 23 discovery.go:139] Checking APIGroup: discovery.k8s.io
  I0512 16:20:11.639758 23 discovery.go:147] PreferredVersion.GroupVersion: discovery.k8s.io/v1
  I0512 16:20:11.639815 23 discovery.go:148] Versions found [{discovery.k8s.io/v1 v1}]
  I0512 16:20:11.639835 23 discovery.go:154] discovery.k8s.io/v1 matches discovery.k8s.io/v1
  I0512 16:20:11.639852 23 discovery.go:139] Checking APIGroup: flowcontrol.apiserver.k8s.io
  I0512 16:20:11.643495 23 discovery.go:147] PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1
  I0512 16:20:11.643545 23 discovery.go:148] Versions found [{flowcontrol.apiserver.k8s.io/v1 v1} {flowcontrol.apiserver.k8s.io/v1beta3 v1beta3}]
  I0512 16:20:11.643564 23 discovery.go:154] flowcontrol.apiserver.k8s.io/v1 matches flowcontrol.apiserver.k8s.io/v1
  I0512 16:20:11.643581 23 discovery.go:139] Checking APIGroup: velero.io
  I0512 16:20:11.646525 23 discovery.go:147] PreferredVersion.GroupVersion: velero.io/v1
  I0512 16:20:11.646577 23 discovery.go:148] Versions found [{velero.io/v1 v1} {velero.io/v2alpha1 v2alpha1}]
  I0512 16:20:11.646595 23 discovery.go:154] velero.io/v1 matches velero.io/v1
  I0512 16:20:11.646612 23 discovery.go:139] Checking APIGroup: cns.vmware.com
  I0512 16:20:11.649328 23 discovery.go:147] PreferredVersion.GroupVersion: cns.vmware.com/v1alpha1
  I0512 16:20:11.649375 23 discovery.go:148] Versions found [{cns.vmware.com/v1alpha1 v1alpha1}]
  I0512 16:20:11.649392 23 discovery.go:154] cns.vmware.com/v1alpha1 matches cns.vmware.com/v1alpha1
  I0512 16:20:11.649412 23 discovery.go:139] Checking APIGroup: traefik.containo.us
  I0512 16:20:11.651956 23 discovery.go:147] PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
  I0512 16:20:11.652002 23 discovery.go:148] Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
  I0512 16:20:11.652019 23 discovery.go:154] traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
  I0512 16:20:11.652037 23 discovery.go:139] Checking APIGroup: traefik.io
  I0512 16:20:11.655446 23 discovery.go:147] PreferredVersion.GroupVersion: traefik.io/v1alpha1
  I0512 16:20:11.655925 23 discovery.go:148] Versions found [{traefik.io/v1alpha1 v1alpha1}]
  I0512 16:20:11.656415 23 discovery.go:154] traefik.io/v1alpha1 matches traefik.io/v1alpha1
  I0512 16:20:11.656797 23 discovery.go:139] Checking APIGroup: vault.banzaicloud.com
  I0512 16:20:11.660039 23 discovery.go:147] PreferredVersion.GroupVersion: vault.banzaicloud.com/v1alpha1
  I0512 16:20:11.660086 23 discovery.go:148] Versions found [{vault.banzaicloud.com/v1alpha1 v1alpha1}]
  I0512 16:20:11.660100 23 discovery.go:154] vault.banzaicloud.com/v1alpha1 matches vault.banzaicloud.com/v1alpha1
  I0512 16:20:11.660112 23 discovery.go:139] Checking APIGroup: metrics.k8s.io
  I0512 16:20:11.663055 23 discovery.go:147] PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  I0512 16:20:11.663700 23 discovery.go:148] Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  I0512 16:20:11.664087 23 discovery.go:154] metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  I0512 16:20:11.664591 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-5162" for this suite. @ 05/12/25 16:20:11.675
• [1.895 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/12/25 16:20:11.693
  I0512 16:20:11.693342 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename containers @ 05/12/25 16:20:11.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:20:11.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:20:11.755
  STEP: Creating a pod to test override all @ 05/12/25 16:20:11.764
  E0512 16:20:11.880052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:12.880154      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:13.880852      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:14.881023      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:20:15.824
  I0512 16:20:15.835836 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod client-containers-b72e54da-10a2-4240-b831-8aeecc3a8dcc container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:20:15.875
  E0512 16:20:15.881868      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:20:15.935254 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6777" for this suite. @ 05/12/25 16:20:15.95
• [4.276 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AggregatedDiscovery should support raw aggregated discovery endpoint Accept headers [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/aggregated_discovery.go:151
  STEP: Creating a kubernetes client @ 05/12/25 16:20:15.969
  I0512 16:20:15.969869 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename aggregateddiscovery @ 05/12/25 16:20:15.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:20:16.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:20:16.019
  I0512 16:20:16.037354 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregateddiscovery-6977" for this suite. @ 05/12/25 16:20:16.05
• [0.097 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [sig-api-machinery, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/namespace.go:245
  STEP: Creating a kubernetes client @ 05/12/25 16:20:16.067
  I0512 16:20:16.067871 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename namespaces @ 05/12/25 16:20:16.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:20:16.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:20:16.122
  STEP: Creating a test namespace @ 05/12/25 16:20:16.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:20:16.22
  STEP: Creating a pod in the namespace @ 05/12/25 16:20:16.227
  STEP: Waiting for the pod to have running status @ 05/12/25 16:20:16.247
  E0512 16:20:16.883001      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:17.883440      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 05/12/25 16:20:18.27
  STEP: Waiting for the namespace to be removed. @ 05/12/25 16:20:18.287
  E0512 16:20:18.883593      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:19.884084      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:20.884427      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:21.884838      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:22.885493      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:23.885962      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:24.886087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:25.887115      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:26.887327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:27.888324      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:28.888577      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:29.889095      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:30.889983      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:31.890952      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/12/25 16:20:32.296
  STEP: Verifying there are no pods in the namespace @ 05/12/25 16:20:32.336
  I0512 16:20:32.353763 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3571" for this suite. @ 05/12/25 16:20:32.374
  STEP: Destroying namespace "nsdeletetest-3753" for this suite. @ 05/12/25 16:20:32.398
  I0512 16:20:32.409235 23 framework.go:370] Namespace nsdeletetest-3753 was already deleted
  STEP: Destroying namespace "nsdeletetest-8471" for this suite. @ 05/12/25 16:20:32.409
• [16.356 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] should update the ephemeral containers in an existing pod [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/ephemeral_containers.go:98
  STEP: Creating a kubernetes client @ 05/12/25 16:20:32.423
  I0512 16:20:32.423999 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/12/25 16:20:32.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:20:32.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:20:32.476
  STEP: creating a target pod @ 05/12/25 16:20:32.486
  E0512 16:20:32.892844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:33.892903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/12/25 16:20:34.544
  E0512 16:20:34.893987      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:35.895043      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:36.895257      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:37.896038      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/12/25 16:20:38.609
  I0512 16:20:38.609145 23 exec_util.go:59] ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2154 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:20:38.609173 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:20:38.609939 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:20:38.610037 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-2154/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  I0512 16:20:38.750802 23 exec_util.go:111] Exec stderr: ""
  STEP: checking pod "ephemeral-containers-target-pod" has only one ephemeralcontainer @ 05/12/25 16:20:38.764
  STEP: adding another ephemeralcontainer to pod "ephemeral-containers-target-pod" @ 05/12/25 16:20:38.775
  STEP: checking pod "ephemeral-containers-target-pod" has only two ephemeralcontainers @ 05/12/25 16:20:38.807
  I0512 16:20:38.835502 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-2154" for this suite. @ 05/12/25 16:20:38.851
• [6.458 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/garbage_collector.go:380
  STEP: Creating a kubernetes client @ 05/12/25 16:20:38.882
  I0512 16:20:38.882642 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename gc @ 05/12/25 16:20:38.884
  E0512 16:20:38.896248      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:20:38.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:20:38.936
  STEP: create the rc @ 05/12/25 16:20:38.96
  W0512 16:20:38.972650      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0512 16:20:39.896738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:40.897958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:41.901576      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:42.902543      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:43.906387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:44.909372      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/12/25 16:20:44.982
  STEP: wait for the rc to be deleted @ 05/12/25 16:20:44.997
  E0512 16:20:45.910748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:46.911139      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:47.911716      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:48.912128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:49.913160      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/12/25 16:20:50.007
  E0512 16:20:50.914104      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:51.914562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:52.915314      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:53.915992      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:54.916419      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:55.917035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:56.917434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:57.917477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:58.917821      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:20:59.917922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:00.918198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:01.918427      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:02.918731      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:03.918962      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:04.919885      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:05.920316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:06.920609      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:07.920815      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:08.921546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:09.922092      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:10.922752      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:11.922840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:12.923075      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:13.923284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:14.924410      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:15.924640      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:16.925154      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:17.925452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:18.925608      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:19.926255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/25 16:21:20.039
  I0512 16:21:20.503113 23 garbage_collector.go:265] For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  I0512 16:21:20.503691 23 delete.go:95] Deleting pod "simpletest.rc-2g7jp" in namespace "gc-955"
  I0512 16:21:20.556003 23 delete.go:95] Deleting pod "simpletest.rc-2k28t" in namespace "gc-955"
  I0512 16:21:20.589163 23 delete.go:95] Deleting pod "simpletest.rc-2qnrd" in namespace "gc-955"
  I0512 16:21:20.623223 23 delete.go:95] Deleting pod "simpletest.rc-429h5" in namespace "gc-955"
  I0512 16:21:20.684693 23 delete.go:95] Deleting pod "simpletest.rc-4bb9f" in namespace "gc-955"
  I0512 16:21:20.732032 23 delete.go:95] Deleting pod "simpletest.rc-4p5fx" in namespace "gc-955"
  I0512 16:21:20.808846 23 delete.go:95] Deleting pod "simpletest.rc-56tns" in namespace "gc-955"
  I0512 16:21:20.851503 23 delete.go:95] Deleting pod "simpletest.rc-5jtjs" in namespace "gc-955"
  I0512 16:21:20.916244 23 delete.go:95] Deleting pod "simpletest.rc-5n87z" in namespace "gc-955"
  E0512 16:21:20.928251      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:20.977219 23 delete.go:95] Deleting pod "simpletest.rc-6dkf2" in namespace "gc-955"
  I0512 16:21:21.021291 23 delete.go:95] Deleting pod "simpletest.rc-6ftfm" in namespace "gc-955"
  I0512 16:21:21.062694 23 delete.go:95] Deleting pod "simpletest.rc-6gczc" in namespace "gc-955"
  I0512 16:21:21.114887 23 delete.go:95] Deleting pod "simpletest.rc-6pdpz" in namespace "gc-955"
  I0512 16:21:21.155095 23 delete.go:95] Deleting pod "simpletest.rc-6r5jm" in namespace "gc-955"
  I0512 16:21:21.195802 23 delete.go:95] Deleting pod "simpletest.rc-6rlrb" in namespace "gc-955"
  I0512 16:21:21.246531 23 delete.go:95] Deleting pod "simpletest.rc-6wbpr" in namespace "gc-955"
  I0512 16:21:21.282935 23 delete.go:95] Deleting pod "simpletest.rc-7fl8z" in namespace "gc-955"
  I0512 16:21:21.321827 23 delete.go:95] Deleting pod "simpletest.rc-7kr5w" in namespace "gc-955"
  I0512 16:21:21.371348 23 delete.go:95] Deleting pod "simpletest.rc-7tbls" in namespace "gc-955"
  I0512 16:21:21.401097 23 delete.go:95] Deleting pod "simpletest.rc-98czs" in namespace "gc-955"
  I0512 16:21:21.426410 23 delete.go:95] Deleting pod "simpletest.rc-9brr9" in namespace "gc-955"
  I0512 16:21:21.457099 23 delete.go:95] Deleting pod "simpletest.rc-bg69w" in namespace "gc-955"
  I0512 16:21:21.484814 23 delete.go:95] Deleting pod "simpletest.rc-bvwzh" in namespace "gc-955"
  I0512 16:21:21.515808 23 delete.go:95] Deleting pod "simpletest.rc-c7h89" in namespace "gc-955"
  I0512 16:21:21.547159 23 delete.go:95] Deleting pod "simpletest.rc-cb2cn" in namespace "gc-955"
  I0512 16:21:21.579268 23 delete.go:95] Deleting pod "simpletest.rc-cbvb2" in namespace "gc-955"
  I0512 16:21:21.603179 23 delete.go:95] Deleting pod "simpletest.rc-cm548" in namespace "gc-955"
  I0512 16:21:21.637139 23 delete.go:95] Deleting pod "simpletest.rc-cmm54" in namespace "gc-955"
  I0512 16:21:21.658206 23 delete.go:95] Deleting pod "simpletest.rc-cns5q" in namespace "gc-955"
  I0512 16:21:21.691332 23 delete.go:95] Deleting pod "simpletest.rc-dl9hc" in namespace "gc-955"
  I0512 16:21:21.729121 23 delete.go:95] Deleting pod "simpletest.rc-dpvwc" in namespace "gc-955"
  I0512 16:21:21.760923 23 delete.go:95] Deleting pod "simpletest.rc-fzsbp" in namespace "gc-955"
  I0512 16:21:21.798223 23 delete.go:95] Deleting pod "simpletest.rc-fzvs7" in namespace "gc-955"
  I0512 16:21:21.826192 23 delete.go:95] Deleting pod "simpletest.rc-g64d5" in namespace "gc-955"
  I0512 16:21:21.851322 23 delete.go:95] Deleting pod "simpletest.rc-g8vjf" in namespace "gc-955"
  I0512 16:21:21.886139 23 delete.go:95] Deleting pod "simpletest.rc-gc4t9" in namespace "gc-955"
  I0512 16:21:21.918036 23 delete.go:95] Deleting pod "simpletest.rc-gc6gh" in namespace "gc-955"
  E0512 16:21:21.928181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:21.951430 23 delete.go:95] Deleting pod "simpletest.rc-ggrcc" in namespace "gc-955"
  I0512 16:21:21.982223 23 delete.go:95] Deleting pod "simpletest.rc-gp2ct" in namespace "gc-955"
  I0512 16:21:22.006350 23 delete.go:95] Deleting pod "simpletest.rc-grfhl" in namespace "gc-955"
  I0512 16:21:22.036350 23 delete.go:95] Deleting pod "simpletest.rc-gs7cm" in namespace "gc-955"
  I0512 16:21:22.061248 23 delete.go:95] Deleting pod "simpletest.rc-h74b6" in namespace "gc-955"
  I0512 16:21:22.095938 23 delete.go:95] Deleting pod "simpletest.rc-hgf8v" in namespace "gc-955"
  I0512 16:21:22.151710 23 delete.go:95] Deleting pod "simpletest.rc-hnrqr" in namespace "gc-955"
  I0512 16:21:22.186432 23 delete.go:95] Deleting pod "simpletest.rc-hqh2r" in namespace "gc-955"
  I0512 16:21:22.244866 23 delete.go:95] Deleting pod "simpletest.rc-jm8c2" in namespace "gc-955"
  I0512 16:21:22.294951 23 delete.go:95] Deleting pod "simpletest.rc-jmfgl" in namespace "gc-955"
  I0512 16:21:22.323632 23 delete.go:95] Deleting pod "simpletest.rc-jqd5j" in namespace "gc-955"
  I0512 16:21:22.350611 23 delete.go:95] Deleting pod "simpletest.rc-jt4bm" in namespace "gc-955"
  I0512 16:21:22.396151 23 delete.go:95] Deleting pod "simpletest.rc-jzmbg" in namespace "gc-955"
  I0512 16:21:22.418009 23 delete.go:95] Deleting pod "simpletest.rc-k477z" in namespace "gc-955"
  I0512 16:21:22.460870 23 delete.go:95] Deleting pod "simpletest.rc-kj8jn" in namespace "gc-955"
  I0512 16:21:22.484990 23 delete.go:95] Deleting pod "simpletest.rc-lxf2g" in namespace "gc-955"
  I0512 16:21:22.514560 23 delete.go:95] Deleting pod "simpletest.rc-lxj6r" in namespace "gc-955"
  I0512 16:21:22.553053 23 delete.go:95] Deleting pod "simpletest.rc-mf448" in namespace "gc-955"
  I0512 16:21:22.596576 23 delete.go:95] Deleting pod "simpletest.rc-mf8vf" in namespace "gc-955"
  I0512 16:21:22.624431 23 delete.go:95] Deleting pod "simpletest.rc-mfjhd" in namespace "gc-955"
  I0512 16:21:22.666930 23 delete.go:95] Deleting pod "simpletest.rc-mhgt7" in namespace "gc-955"
  I0512 16:21:22.708057 23 delete.go:95] Deleting pod "simpletest.rc-mkjgq" in namespace "gc-955"
  I0512 16:21:22.729362 23 delete.go:95] Deleting pod "simpletest.rc-mvv8n" in namespace "gc-955"
  I0512 16:21:22.787403 23 delete.go:95] Deleting pod "simpletest.rc-n9wnx" in namespace "gc-955"
  I0512 16:21:22.815138 23 delete.go:95] Deleting pod "simpletest.rc-p8qhb" in namespace "gc-955"
  I0512 16:21:22.903668 23 delete.go:95] Deleting pod "simpletest.rc-pkxhv" in namespace "gc-955"
  E0512 16:21:22.933387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:22.936748 23 delete.go:95] Deleting pod "simpletest.rc-pnfzr" in namespace "gc-955"
  I0512 16:21:22.973988 23 delete.go:95] Deleting pod "simpletest.rc-ps8hf" in namespace "gc-955"
  I0512 16:21:23.008954 23 delete.go:95] Deleting pod "simpletest.rc-ptvb9" in namespace "gc-955"
  I0512 16:21:23.049148 23 delete.go:95] Deleting pod "simpletest.rc-q29xr" in namespace "gc-955"
  I0512 16:21:23.077843 23 delete.go:95] Deleting pod "simpletest.rc-q4sh4" in namespace "gc-955"
  I0512 16:21:23.113897 23 delete.go:95] Deleting pod "simpletest.rc-q8ndt" in namespace "gc-955"
  I0512 16:21:23.150683 23 delete.go:95] Deleting pod "simpletest.rc-qjqrn" in namespace "gc-955"
  I0512 16:21:23.188184 23 delete.go:95] Deleting pod "simpletest.rc-qwk2p" in namespace "gc-955"
  I0512 16:21:23.221112 23 delete.go:95] Deleting pod "simpletest.rc-qxk9c" in namespace "gc-955"
  I0512 16:21:23.253026 23 delete.go:95] Deleting pod "simpletest.rc-r8vnz" in namespace "gc-955"
  I0512 16:21:23.294448 23 delete.go:95] Deleting pod "simpletest.rc-rc8m2" in namespace "gc-955"
  I0512 16:21:23.326425 23 delete.go:95] Deleting pod "simpletest.rc-rwbkn" in namespace "gc-955"
  I0512 16:21:23.351921 23 delete.go:95] Deleting pod "simpletest.rc-rzjhs" in namespace "gc-955"
  I0512 16:21:23.392878 23 delete.go:95] Deleting pod "simpletest.rc-sjjkx" in namespace "gc-955"
  I0512 16:21:23.441126 23 delete.go:95] Deleting pod "simpletest.rc-ss8sd" in namespace "gc-955"
  I0512 16:21:23.484129 23 delete.go:95] Deleting pod "simpletest.rc-svdcl" in namespace "gc-955"
  I0512 16:21:23.515774 23 delete.go:95] Deleting pod "simpletest.rc-sxspv" in namespace "gc-955"
  I0512 16:21:23.560610 23 delete.go:95] Deleting pod "simpletest.rc-tdp49" in namespace "gc-955"
  I0512 16:21:23.604401 23 delete.go:95] Deleting pod "simpletest.rc-tsz85" in namespace "gc-955"
  I0512 16:21:23.639742 23 delete.go:95] Deleting pod "simpletest.rc-tv2vs" in namespace "gc-955"
  I0512 16:21:23.673538 23 delete.go:95] Deleting pod "simpletest.rc-v8j6g" in namespace "gc-955"
  I0512 16:21:23.714104 23 delete.go:95] Deleting pod "simpletest.rc-vgmpc" in namespace "gc-955"
  I0512 16:21:23.738732 23 delete.go:95] Deleting pod "simpletest.rc-vh88p" in namespace "gc-955"
  I0512 16:21:23.767729 23 delete.go:95] Deleting pod "simpletest.rc-vrns8" in namespace "gc-955"
  I0512 16:21:23.804501 23 delete.go:95] Deleting pod "simpletest.rc-vrtjn" in namespace "gc-955"
  I0512 16:21:23.839872 23 delete.go:95] Deleting pod "simpletest.rc-wmz8g" in namespace "gc-955"
  I0512 16:21:23.906069 23 delete.go:95] Deleting pod "simpletest.rc-wnl8j" in namespace "gc-955"
  E0512 16:21:23.933760      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:23.944980 23 delete.go:95] Deleting pod "simpletest.rc-wqfn5" in namespace "gc-955"
  I0512 16:21:23.985471 23 delete.go:95] Deleting pod "simpletest.rc-wr7zs" in namespace "gc-955"
  I0512 16:21:24.010639 23 delete.go:95] Deleting pod "simpletest.rc-wr884" in namespace "gc-955"
  I0512 16:21:24.056088 23 delete.go:95] Deleting pod "simpletest.rc-wsdft" in namespace "gc-955"
  I0512 16:21:24.109753 23 delete.go:95] Deleting pod "simpletest.rc-wvqzl" in namespace "gc-955"
  I0512 16:21:24.165429 23 delete.go:95] Deleting pod "simpletest.rc-xcbbc" in namespace "gc-955"
  I0512 16:21:24.193716 23 delete.go:95] Deleting pod "simpletest.rc-xdx8j" in namespace "gc-955"
  I0512 16:21:24.222776 23 delete.go:95] Deleting pod "simpletest.rc-xs2th" in namespace "gc-955"
  I0512 16:21:24.265006 23 delete.go:95] Deleting pod "simpletest.rc-zbcx7" in namespace "gc-955"
  I0512 16:21:24.292389 23 delete.go:95] Deleting pod "simpletest.rc-zk4j9" in namespace "gc-955"
  I0512 16:21:24.329478 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-955" for this suite. @ 05/12/25 16:21:24.34
• [45.483 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:135
  STEP: Creating a kubernetes client @ 05/12/25 16:21:24.365
  I0512 16:21:24.365941 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 16:21:24.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:21:24.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:21:24.433
  STEP: Creating pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631 @ 05/12/25 16:21:24.439
  E0512 16:21:24.933969      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:25.936078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 16:21:26.487
  I0512 16:21:26.499345 23 container_probe.go:1749] Initial restart count of pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 is 0
  I0512 16:21:26.505795 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:26.936127      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:27.936745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:28.516613 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:28.936862      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:29.936963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:30.531450 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:30.937985      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:31.938360      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:32.541890 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:32.938675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:33.939230      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:34.553037 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:34.939324      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:35.939465      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:36.564227 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:36.939665      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:37.940007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:38.573022 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:38.940797      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:39.940837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:40.583846 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:40.941294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:41.941975      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:42.595362 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:42.942764      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:43.943019      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:44.604663 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:44.944111      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:45.944714      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:46.613059 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:46.945860      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:47.946406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:48.622400 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:48.946869      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:49.947884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:50.632153 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:50.948488      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:51.949213      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:52.646177 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:52.949543      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:53.949644      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:54.658080 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:54.950524      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:55.950849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:56.669453 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:56.951930      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:57.952628      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:21:58.680429 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:21:58.953791      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:21:59.954220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:00.693377 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:00.954894      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:01.955840      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:02.704573 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:02.956796      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:03.957492      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:04.713382 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:04.957769      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:05.957830      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:06.726540 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:06.959200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:07.959908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:08.742599 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:08.960914      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:09.961153      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:10.752670 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:10.961992      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:11.962882      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:12.760431 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:12.963944      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:13.964472      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:14.770516 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  E0512 16:22:14.964671      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:15.964798      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:16.790120 23 container_probe.go:1759] Get pod busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 in namespace container-probe-631
  I0512 16:22:16.790217 23 container_probe.go:1763] Restart count of pod container-probe-631/busybox-9bbc1e0b-3a94-454f-b0b4-2c5208b347d5 is now 1 (50.290737919s elapsed)
  STEP: deleting the pod @ 05/12/25 16:22:16.79
  I0512 16:22:16.826944 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-631" for this suite. @ 05/12/25 16:22:16.845
• [52.514 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/12/25 16:22:16.881
  I0512 16:22:16.881123 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename watch @ 05/12/25 16:22:16.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:22:16.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:22:16.934
  STEP: creating a watch on configmaps with label A @ 05/12/25 16:22:16.947
  STEP: creating a watch on configmaps with label B @ 05/12/25 16:22:16.95
  STEP: creating a watch on configmaps with label A or B @ 05/12/25 16:22:16.955
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/12/25 16:22:16.958
  E0512 16:22:16.965329      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:16.971864 23 watch.go:431] Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58121 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:22:16.972438 23 watch.go:431] Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58121 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/12/25 16:22:16.973
  I0512 16:22:16.996604 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58122 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:22:16.997029 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58122 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/12/25 16:22:16.997
  I0512 16:22:17.019019 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58123 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:22:17.020123 23 watch.go:431] Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58123 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/12/25 16:22:17.021
  I0512 16:22:17.042451 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58124 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:22:17.042672 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7639  8deefeeb-c67c-4473-ae05-e306e3a6cf4e 58124 0 2025-05-12 16:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/12/25 16:22:17.042
  I0512 16:22:17.056314 23 watch.go:431] Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7639  2af30d54-b335-4282-b71e-e445f58df4fb 58125 0 2025-05-12 16:22:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:22:17.057325 23 watch.go:431] Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7639  2af30d54-b335-4282-b71e-e445f58df4fb 58125 0 2025-05-12 16:22:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0512 16:22:17.965514      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:18.966087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:19.966175      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:20.966556      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:21.966638      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:22.967468      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:23.968141      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:24.969176      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:25.969229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:26.969620      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/12/25 16:22:27.058
  I0512 16:22:27.075013 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7639  2af30d54-b335-4282-b71e-e445f58df4fb 58180 0 2025-05-12 16:22:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  I0512 16:22:27.075411 23 watch.go:431] Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7639  2af30d54-b335-4282-b71e-e445f58df4fb 58180 0 2025-05-12 16:22:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2025-05-12 16:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0512 16:22:27.970534      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:28.971114      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:29.971477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:30.972378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:31.972824      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:32.972961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:33.973607      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:34.973904      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:35.974639      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:36.975491      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:37.076452 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7639" for this suite. @ 05/12/25 16:22:37.092
• [20.227 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:298
  STEP: Creating a kubernetes client @ 05/12/25 16:22:37.111
  I0512 16:22:37.112304 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:22:37.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:22:37.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:22:37.166
  STEP: Setting up server cert @ 05/12/25 16:22:37.275
  E0512 16:22:37.975711      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:38.976100      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:22:39.693
  STEP: Deploying the webhook pod @ 05/12/25 16:22:39.734
  STEP: Wait for the deployment to be ready @ 05/12/25 16:22:39.763
  I0512 16:22:39.785964 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:22:39.977236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:40.977710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:22:41.829
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:22:41.856
  E0512 16:22:41.980901      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:42.857720 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/12/25 16:22:42.903
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/12/25 16:22:42.953
  I0512 16:22:42.953989 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:22:42.982076      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:43.178700 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4060" for this suite. @ 05/12/25 16:22:43.2
  STEP: Destroying namespace "webhook-markers-2898" for this suite. @ 05/12/25 16:22:43.214
• [6.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:1066
  STEP: Creating a kubernetes client @ 05/12/25 16:22:43.249
  I0512 16:22:43.249604 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 16:22:43.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:22:43.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:22:43.349
  STEP: Creating resourceQuota "e2e-rq-status-kkccp" @ 05/12/25 16:22:43.366
  I0512 16:22:43.385896 23 resource_quota.go:1102] Resource quota "e2e-rq-status-kkccp" reports spec: hard cpu limit of 500m
  I0512 16:22:43.386068 23 resource_quota.go:1104] Resource quota "e2e-rq-status-kkccp" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-kkccp" /status @ 05/12/25 16:22:43.386
  STEP: Confirm /status for "e2e-rq-status-kkccp" resourceQuota via watch @ 05/12/25 16:22:43.444
  I0512 16:22:43.447926 23 resource_quota.go:1131] observed resourceQuota "e2e-rq-status-kkccp" in namespace "resourcequota-257" with hard status: v1.ResourceList(nil)
  I0512 16:22:43.448107 23 resource_quota.go:1134] Found resourceQuota "e2e-rq-status-kkccp" in namespace "resourcequota-257" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  I0512 16:22:43.448197 23 resource_quota.go:1141] ResourceQuota "e2e-rq-status-kkccp" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/12/25 16:22:43.455
  I0512 16:22:43.468956 23 resource_quota.go:1152] Resource quota "e2e-rq-status-kkccp" reports spec: hard cpu limit of 1
  I0512 16:22:43.469400 23 resource_quota.go:1153] Resource quota "e2e-rq-status-kkccp" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-kkccp" /status @ 05/12/25 16:22:43.469
  STEP: Confirm /status for "e2e-rq-status-kkccp" resourceQuota via watch @ 05/12/25 16:22:43.482
  I0512 16:22:43.487845 23 resource_quota.go:1175] observed resourceQuota "e2e-rq-status-kkccp" in namespace "resourcequota-257" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  I0512 16:22:43.488407 23 resource_quota.go:1178] Found resourceQuota "e2e-rq-status-kkccp" in namespace "resourcequota-257" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  I0512 16:22:43.488621 23 resource_quota.go:1185] ResourceQuota "e2e-rq-status-kkccp" /status was patched
  STEP: Get "e2e-rq-status-kkccp" /status @ 05/12/25 16:22:43.489
  I0512 16:22:43.500645 23 resource_quota.go:1196] Resourcequota "e2e-rq-status-kkccp" reports status: hard cpu of 1
  I0512 16:22:43.500776 23 resource_quota.go:1198] Resourcequota "e2e-rq-status-kkccp" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-kkccp" /status before checking Spec is unchanged @ 05/12/25 16:22:43.513
  I0512 16:22:43.526372 23 resource_quota.go:1218] Resourcequota "e2e-rq-status-kkccp" reports status: hard cpu of 2
  I0512 16:22:43.526547 23 resource_quota.go:1220] Resourcequota "e2e-rq-status-kkccp" reports status: hard memory of 2Gi
  I0512 16:22:43.529893 23 resource_quota.go:1232] Found resourceQuota "e2e-rq-status-kkccp" in namespace "resourcequota-257" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  I0512 16:22:43.538876 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c198), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c1e0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c228), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:22:43.982506      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:44.982753      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:45.983787      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:46.984340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:47.984833      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:48.544296 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c480), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c4f8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c588), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:22:48.985693      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:49.985995      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:50.985852      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:51.986418      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:52.987252      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:53.539196 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d0e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d128), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d158), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:22:53.987395      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:54.987573      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:55.988738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:56.989337      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:57.990030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:22:58.540256 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d338), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d398), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d3c8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:22:58.990898      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:22:59.991642      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:00.992213      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:01.992616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:02.993479      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:03.541632 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d698), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d6e0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d728), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:03.993851      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:04.994092      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:05.994250      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:06.994593      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:07.994901      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:08.538708 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a4ab0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a4b40), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a4ba0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:08.996139      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:09.997051      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:10.997718      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:11.997765      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:12.998013      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:13.544005 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d950), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d998), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2d9c8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:13.999030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:15.000080      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:16.000282      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:17.000725      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:18.001206      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:18.538339 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a4e88), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a4eb8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a4f00), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:19.001785      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:20.002168      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:21.002388      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:22.002569      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:23.002783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:23.538711 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c888), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c8d0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c930), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:24.004089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:25.004766      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:26.004591      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:27.004793      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:28.005181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:28.541628 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cb40), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cb70), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cbb8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:29.005723      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:30.006719      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:31.007122      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:32.007825      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:33.007755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:33.541108 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5380), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a53b0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5410), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:34.007940      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:35.008330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:36.008694      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:37.009747      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:38.010731      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:38.542468 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5668), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5698), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a56e0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:39.010990      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:40.011290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:41.012614      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:42.012927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:43.013707      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:43.544626 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5920), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5980), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a59c8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:44.014117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:45.015045      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:46.016682      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:47.017121      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:48.017618      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:48.538473 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2dd40), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2dd70), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003b2dde8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:49.018758      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:50.019718      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:51.020171      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:52.020686      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:53.021848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:53.543852 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5c98), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5d28), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0044a5d70), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:54.022823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:55.023057      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:56.023406      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:57.023815      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:23:58.024108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:23:58.540084 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f60a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f60f0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6150), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:23:59.024176      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:00.025083      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:01.025602      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:02.026047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:03.026609      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:03.542759 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6390), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f63c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6408), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:04.026833      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:05.027420      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:06.027958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:07.028044      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:08.028349      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:08.539006 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001be030), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001be0a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001be120), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:09.028339      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:10.029475      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:11.029730      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:12.030557      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:13.031393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:13.542448 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f60a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f60f0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6150), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:14.031417      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:15.031676      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:16.032269      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:17.033198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:18.034836      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:18.539752 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001be228), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001be3a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001be4c8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:19.035033      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:20.035316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:21.035594      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:22.036340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:23.037066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:23.541784 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001be9f0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bea38), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001beb10), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:24.038114      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:25.039174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:26.039852      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:27.039915      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:28.040655      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:28.542870 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bee10), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bee88), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bef18), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:29.041263      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:30.042318      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:31.043340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:32.043976      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:33.045002      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:33.545060 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bf1d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bf218), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bf260), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:34.045309      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:35.046250      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:36.046624      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:37.046853      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:38.047078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:38.542488 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bf488), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bf4b8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bf548), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:39.047708      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:40.047732      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:41.048003      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:42.048169      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:43.048414      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:43.541029 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c258), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c2e8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c348), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:44.049137      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:45.049949      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:46.050336      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:47.051117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:48.051174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:48.536873 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bfb90), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bfbd8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bfc38), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:49.051260      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:50.052216      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:51.052835      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:52.053200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:53.053280      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:53.539316 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bfed8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001bff50), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0018), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:54.053827      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:55.053942      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:56.054277      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:57.054546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:24:58.054804      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:24:58.539727 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0318), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0378), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e03a8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:24:59.055766      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:00.056863      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:01.056795      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:02.057534      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:03.058062      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:03.537998 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6408), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6480), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f64b0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:04.058425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:05.059454      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:06.059806      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:07.060698      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:08.061148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:08.538327 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6768), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f67b0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f67f8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:09.061579      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:10.062334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:11.062803      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:12.063778      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:13.064495      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:13.539348 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e05a0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e05d0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0648), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:14.064965      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:15.065371      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:16.065940      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:17.066624      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:18.067108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:18.543609 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c720), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c768), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c7b0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:19.068095      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:20.068066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:21.068472      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:22.069112      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:23.069087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:23.540835 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379ca20), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379ca80), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cab0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:24.070172      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:25.071226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:26.071995      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:27.072680      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:28.073588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:28.538250 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6a20), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6a68), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6ab0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:29.073922      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:30.074788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:31.075286      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:32.076424      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:33.077299      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:33.541126 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0948), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e09c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0a38), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:34.077326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:35.078292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:36.078789      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:37.079668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:38.079944      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:38.540880 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0cf0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0d20), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0d50), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:39.080359      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:40.082064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:41.081710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:42.082236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:43.082851      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:43.539258 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0fc0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e1008), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e1080), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:44.083913      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:45.085008      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:46.086067      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:47.086847      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:48.086968      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:48.541043 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379ce88), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cee8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cf60), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:49.087154      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:50.088359      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:51.088401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:52.088818      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:53.088873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:53.538392 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e1410), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e1470), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e14a0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:54.089246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:55.090055      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:56.090239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:57.090453      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:25:58.090626      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:25:58.539519 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379d218), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379d260), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379d290), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:25:59.091602      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:00.091852      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:01.092155      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:02.093308      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:03.093543      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:03.541022 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e1728), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e17a0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e17e8), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:04.093782      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:05.093931      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:06.094775      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:07.094736      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:08.094964      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:08.541632 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379d680), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379d6b0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379d710), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:09.095353      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:10.096079      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:11.096605      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:12.096861      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:13.097408      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:13.539433 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f60a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f60f0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:14.097836      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:15.098426      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:16.098562      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:17.099258      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:18.100142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:18.539572 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0138), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e01b0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0210), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:19.100303      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:20.101393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:21.101605      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:22.101936      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:23.102097      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:23.542489 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f63a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f63f0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6438), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:24.102763      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:25.103646      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:26.104234      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:27.104728      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:28.105536      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:28.538717 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6600), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6678), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f66f0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:29.106525      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:30.107515      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:31.108136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:32.109009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:33.109424      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:33.538637 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c210), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c240), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c2a0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:34.109819      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:35.110716      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:36.111025      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:37.111696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:38.111945      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:38.537623 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c570), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c5a0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c5d0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:39.112226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:40.113202      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:41.113560      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:42.114096      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:43.114648      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:43.537684 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6930), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6960), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f69c0), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:44.115153      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:45.116199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:46.116581      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:47.117347      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:48.117473      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:48.541297 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6ba0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6be8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6c60), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:49.117781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:50.118823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:51.118959      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:52.119136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:53.119468      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:53.539853 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c8e8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c930), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379c978), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:54.120665      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:55.121551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:56.121755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:57.122453      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:26:58.122953      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:26:58.538686 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6eb8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6f00), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f6f60), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:26:59.123281      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:00.123769      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:01.124564      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:02.124918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:03.124931      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:03.537401 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cba0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cbe8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cc48), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:27:04.126064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:05.127535      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:06.127425      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:07.127894      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:08.128209      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:08.541892 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cf60), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379cfd8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00379d020), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:27:09.128207      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:10.128592      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:11.129223      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:12.129861      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:13.130235      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:13.539226 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f7260), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f72c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0034f7320), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:27:14.130337      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:15.131309      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:16.131693      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:17.132035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:18.132704      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:18.540577 23 resource_quota.go:1263] ResourceQuota "e2e-rq-status-kkccp" Spec and Status does not match: &v1.ResourceQuota{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"e2e-rq-status-kkccp", GenerateName:"", Namespace:"resourcequota-257", SelfLink:"", UID:"598b7a52-eeb8-4b24-b33a-abe110b2307e", ResourceVersion:"58307", Generation:0, CreationTimestamp:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"e2e-rq-label":"e2e-rq-status-kkccp"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e05d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0648), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kube-controller-manager", Operation:"Update", APIVersion:"v1", Time:time.Date(2025, time.May, 12, 16, 22, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037e0690), Subresource:"status"}}}, Spec:v1.ResourceQuotaSpec{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}, Scopes:[]v1.ResourceQuotaScope(nil), ScopeSelector:(*v1.ScopeSelector)(nil)}, Status:v1.ResourceQuotaStatus{Hard:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}, Used:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:0, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"0", Format:"DecimalSI"}}}}
  E0512 16:27:19.132943      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:20.132985      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:21.133251      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:22.133662      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:23.133672      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:23.538551 23 resource_quota.go:1260] ResourceQuota "e2e-rq-status-kkccp" Spec was unchanged and /status reset
  I0512 16:27:23.539563 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-257" for this suite. @ 05/12/25 16:27:23.55
• [280.318 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:329
  STEP: Creating a kubernetes client @ 05/12/25 16:27:23.569
  I0512 16:27:23.569386 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:27:23.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:23.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:23.622
  STEP: Setting up server cert @ 05/12/25 16:27:23.711
  E0512 16:27:24.133879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:27:24.583
  STEP: Deploying the webhook pod @ 05/12/25 16:27:24.599
  STEP: Wait for the deployment to be ready @ 05/12/25 16:27:24.625
  I0512 16:27:24.655451 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:27:25.134903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:26.135775      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:27:26.682
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:27:26.71
  E0512 16:27:27.136754      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:27.710879 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  I0512 16:27:27.736227 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:27:28.137269      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:29.138167      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:30.139330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:31.140668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:32.141094      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:33.141314      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3488-crds.webhook.example.com via the AdmissionRegistration API @ 05/12/25 16:27:33.279
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/12/25 16:27:33.31
  E0512 16:27:34.142134      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:35.143112      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:36.068912 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8707" for this suite. @ 05/12/25 16:27:36.082
  STEP: Destroying namespace "webhook-markers-7888" for this suite. @ 05/12/25 16:27:36.095
• [12.543 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:164
  STEP: Creating a kubernetes client @ 05/12/25 16:27:36.113
  I0512 16:27:36.113240 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:27:36.114
  E0512 16:27:36.143486      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:36.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:36.167
  STEP: Creating the pod @ 05/12/25 16:27:36.176
  E0512 16:27:37.143892      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:38.144130      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:38.799166 23 pod_client.go:173] Successfully updated pod "annotationupdate0e9cc701-ce5e-4eba-b882-87e40fba43dc"
  E0512 16:27:39.145123      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:40.146187      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:27:40.840921 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7763" for this suite. @ 05/12/25 16:27:40.854
• [4.760 seconds]
------------------------------
SSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance] [sig-instrumentation, Conformance]
k8s.io/kubernetes/test/e2e/instrumentation/core_events.go:58
  STEP: Creating a kubernetes client @ 05/12/25 16:27:40.873
  I0512 16:27:40.873873 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename events @ 05/12/25 16:27:40.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:40.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:40.926
  STEP: creating a test event @ 05/12/25 16:27:40.941
  STEP: listing all events in all namespaces @ 05/12/25 16:27:40.963
  STEP: patching the test event @ 05/12/25 16:27:40.974
  STEP: fetching the test event @ 05/12/25 16:27:40.992
  STEP: updating the test event @ 05/12/25 16:27:41.002
  STEP: getting the test event @ 05/12/25 16:27:41.029
  STEP: deleting the test event @ 05/12/25 16:27:41.038
  STEP: listing all events in all namespaces @ 05/12/25 16:27:41.058
  I0512 16:27:41.067020 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7270" for this suite. @ 05/12/25 16:27:41.08
• [0.222 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/downwardapi.go:46
  STEP: Creating a kubernetes client @ 05/12/25 16:27:41.098
  I0512 16:27:41.098831 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:27:41.101
  E0512 16:27:41.147245      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:41.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:41.173
  STEP: Creating a pod to test downward api env vars @ 05/12/25 16:27:41.18
  E0512 16:27:42.147533      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:43.147705      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:44.148077      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:45.148251      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:27:45.239
  I0512 16:27:45.246891 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downward-api-d095aaf6-4472-476b-b953-4325f0f55333 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 16:27:45.263
  I0512 16:27:45.295038 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4873" for this suite. @ 05/12/25 16:27:45.305
• [4.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/disruption.go:90
  STEP: Creating a kubernetes client @ 05/12/25 16:27:45.322
  I0512 16:27:45.322260 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename disruption @ 05/12/25 16:27:45.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:45.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:45.368
  STEP: Creating a kubernetes client @ 05/12/25 16:27:45.374
  I0512 16:27:45.374714 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename disruption-2 @ 05/12/25 16:27:45.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:45.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:45.464
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:27:45.48
  E0512 16:27:46.148458      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:47.148820      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:27:47.499
  E0512 16:27:48.149136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:49.150191      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:27:49.521
  E0512 16:27:50.150939      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:51.151292      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 05/12/25 16:27:51.529
  STEP: listing a collection of PDBs in namespace disruption-5786 @ 05/12/25 16:27:51.537
  STEP: deleting a collection of PDBs @ 05/12/25 16:27:51.545
  STEP: Waiting for the PDB collection to be deleted @ 05/12/25 16:27:51.575
  I0512 16:27:51.583696 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5597" for this suite. @ 05/12/25 16:27:51.595
  I0512 16:27:51.611601 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5786" for this suite. @ 05/12/25 16:27:51.695
• [6.386 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:110
  STEP: Creating a kubernetes client @ 05/12/25 16:27:51.708
  I0512 16:27:51.708881 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 16:27:51.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:51.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:51.756
  STEP: Creating configMap with name configmap-test-volume-map-1c3a351f-28ef-4866-970c-ba46edb6ac6a @ 05/12/25 16:27:51.762
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:27:51.773
  E0512 16:27:52.151595      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:53.152265      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:54.152744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:55.153785      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:27:55.833
  I0512 16:27:55.840462 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-configmaps-4bb5cd7a-8fc7-40fa-9729-b192442a6a40 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:27:55.862
  I0512 16:27:55.905587 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9308" for this suite. @ 05/12/25 16:27:55.917
• [4.231 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:86
  STEP: Creating a kubernetes client @ 05/12/25 16:27:55.94
  I0512 16:27:55.940565 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:27:55.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:27:55.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:27:55.987
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 16:27:55.994
  E0512 16:27:56.154661      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:57.155388      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:58.155546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:27:59.155999      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:28:00.052
  I0512 16:28:00.060370 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-255ca57b-05fc-4da0-b0fa-236bea2a398f container client-container: <nil>
  STEP: delete the pod @ 05/12/25 16:28:00.076
  I0512 16:28:00.120874 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7322" for this suite. @ 05/12/25 16:28:00.134
  E0512 16:28:00.156387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
• [4.223 seconds]
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:175
  STEP: Creating a kubernetes client @ 05/12/25 16:28:00.163
  I0512 16:28:00.163942 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:28:00.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:28:00.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:28:00.225
  STEP: Creating configMap with name cm-test-opt-del-d701ec74-98d7-418e-beb6-bfbd39fb84b3 @ 05/12/25 16:28:00.247
  STEP: Creating configMap with name cm-test-opt-upd-d98b0451-aa3e-4fa1-98a5-a9db60074af3 @ 05/12/25 16:28:00.26
  STEP: Creating the pod @ 05/12/25 16:28:00.271
  E0512 16:28:01.159689      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:02.157135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-d701ec74-98d7-418e-beb6-bfbd39fb84b3 @ 05/12/25 16:28:02.382
  STEP: Updating configmap cm-test-opt-upd-d98b0451-aa3e-4fa1-98a5-a9db60074af3 @ 05/12/25 16:28:02.399
  STEP: Creating configMap with name cm-test-opt-create-2ab11486-4467-418a-9d94-b6864c0cbbcb @ 05/12/25 16:28:02.425
  STEP: waiting to observe update in volume @ 05/12/25 16:28:02.437
  E0512 16:28:03.157812      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:04.158262      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:28:04.511792 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-491" for this suite. @ 05/12/25 16:28:04.521
• [4.373 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/field_validation.go:348
  STEP: Creating a kubernetes client @ 05/12/25 16:28:04.536
  I0512 16:28:04.536904 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename field-validation @ 05/12/25 16:28:04.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:28:04.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:28:04.591
  I0512 16:28:04.602154 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  W0512 16:28:04.605046      23 field_validation.go:421] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc005160a20 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0512 16:28:05.159108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:06.160860      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:07.161580      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:08.162094      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:09.163070      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:10.163205      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:11.163763      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:12.164173      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  W0512 16:28:12.253716      23 warnings.go:70] unknown field "alpha"
  W0512 16:28:12.253775      23 warnings.go:70] unknown field "beta"
  W0512 16:28:12.253787      23 warnings.go:70] unknown field "delta"
  W0512 16:28:12.253799      23 warnings.go:70] unknown field "epsilon"
  W0512 16:28:12.253810      23 warnings.go:70] unknown field "gamma"
  I0512 16:28:12.847287 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6765" for this suite. @ 05/12/25 16:28:12.865
• [8.350 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:164
  STEP: Creating a kubernetes client @ 05/12/25 16:28:12.888
  I0512 16:28:12.888365 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 16:28:12.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:28:12.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:28:12.945
  STEP: Discovering how many secrets are in namespace by default @ 05/12/25 16:28:12.951
  E0512 16:28:13.165196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:14.166250      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:15.167183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:16.167289      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:17.167382      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/12/25 16:28:17.959
  E0512 16:28:18.168316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:19.168768      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:20.168936      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:21.169282      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:22.169336      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/25 16:28:22.973
  STEP: Ensuring resource quota status is calculated @ 05/12/25 16:28:22.99
  E0512 16:28:23.170078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:24.170923      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 05/12/25 16:28:24.999
  STEP: Ensuring resource quota status captures secret creation @ 05/12/25 16:28:25.018
  E0512 16:28:25.171012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:26.171564      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 05/12/25 16:28:27.028
  STEP: Ensuring resource quota status released usage @ 05/12/25 16:28:27.041
  E0512 16:28:27.171705      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:28.172310      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:28:29.049672 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7480" for this suite. @ 05/12/25 16:28:29.059
• [16.185 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:331
  STEP: Creating a kubernetes client @ 05/12/25 16:28:29.077
  I0512 16:28:29.077555 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 16:28:29.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:28:29.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:28:29.13
  E0512 16:28:29.173032      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:30.173598      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:31.174259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:32.174755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:33.175142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:34.175286      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:35.176203      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:36.176946      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:37.177419      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:38.178387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:39.179500      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:40.180470      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:41.181184      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:42.181931      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:43.182484      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:44.183357      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:45.183494      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/12/25 16:28:46.147
  E0512 16:28:46.184551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:47.184753      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:48.185779      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:49.185845      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:50.186221      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/25 16:28:51.157
  STEP: Ensuring resource quota status is calculated @ 05/12/25 16:28:51.165
  E0512 16:28:51.187145      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:52.188203      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 05/12/25 16:28:53.175
  E0512 16:28:53.188132      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status captures configMap creation @ 05/12/25 16:28:53.204
  E0512 16:28:54.190190      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:55.190222      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 05/12/25 16:28:55.213
  STEP: Ensuring resource quota status released usage @ 05/12/25 16:28:55.228
  E0512 16:28:56.190534      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:57.190919      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:28:57.240187 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7684" for this suite. @ 05/12/25 16:28:57.252
• [28.190 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:169
  STEP: Creating a kubernetes client @ 05/12/25 16:28:57.267
  I0512 16:28:57.267064 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 16:28:57.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:28:57.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:28:57.347
  STEP: Creating pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012 @ 05/12/25 16:28:57.353
  E0512 16:28:58.191035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:28:59.191068      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 16:28:59.395
  I0512 16:28:59.404264 23 container_probe.go:1749] Initial restart count of pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 is 0
  I0512 16:28:59.411300 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:00.191340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:01.191964      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:01.421358 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:02.192115      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:03.192289      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:03.430568 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:04.192869      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:05.193966      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:05.441892 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:06.194159      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:07.194340      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:07.452863 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:08.194533      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:09.195066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:09.460982 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:10.195336      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:11.196426      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:11.471081 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:12.196828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:13.197491      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:13.480502 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:14.197715      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:15.198829      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:15.490904 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:16.199801      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:17.200361      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:17.499303 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  E0512 16:29:18.200865      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:19.201095      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:19.509310 23 container_probe.go:1759] Get pod liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 in namespace container-probe-1012
  I0512 16:29:19.509502 23 container_probe.go:1763] Restart count of pod container-probe-1012/liveness-6473e118-b6bc-49f7-96f7-bdbf3ad13955 is now 1 (20.105003336s elapsed)
  STEP: deleting the pod @ 05/12/25 16:29:19.509
  I0512 16:29:19.547191 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1012" for this suite. @ 05/12/25 16:29:19.568
• [22.316 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/job.go:1054
  STEP: Creating a kubernetes client @ 05/12/25 16:29:19.584
  I0512 16:29:19.584203 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename job @ 05/12/25 16:29:19.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:19.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:19.633
  STEP: Creating a job @ 05/12/25 16:29:19.639
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/12/25 16:29:19.652
  E0512 16:29:20.201254      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:21.201774      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/12/25 16:29:21.666
  STEP: updating /status @ 05/12/25 16:29:21.683
  STEP: get /status @ 05/12/25 16:29:21.705
  I0512 16:29:21.712408 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6499" for this suite. @ 05/12/25 16:29:21.723
• [2.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:176
  STEP: Creating a kubernetes client @ 05/12/25 16:29:21.737
  I0512 16:29:21.737417 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 16:29:21.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:21.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:21.8
  STEP: Creating configMap with name configmap-test-upd-bd99c58d-066f-4f72-b2b6-4551242afb6d @ 05/12/25 16:29:21.825
  STEP: Creating the pod @ 05/12/25 16:29:21.837
  E0512 16:29:22.202204      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:23.203273      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 05/12/25 16:29:23.89
  STEP: Waiting for pod with binary data @ 05/12/25 16:29:23.907
  I0512 16:29:23.926728 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9743" for this suite. @ 05/12/25 16:29:23.938
• [2.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:52
  STEP: Creating a kubernetes client @ 05/12/25 16:29:23.965
  I0512 16:29:23.966147 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 16:29:23.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:24.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:24.02
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/12/25 16:29:24.027
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/12/25 16:29:24.027
  STEP: creating a pod to probe DNS @ 05/12/25 16:29:24.027
  STEP: submitting the pod to kubernetes @ 05/12/25 16:29:24.028
  E0512 16:29:24.204267      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:25.205225      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:26.205228      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:27.205555      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/25 16:29:28.118
  STEP: looking for the results for each expected name from probers @ 05/12/25 16:29:28.127
  I0512 16:29:28.169274 23 dns_common.go:527] DNS probes using dns-7869/dns-test-81dc4571-040b-4a73-bc5e-2488d1e11b4c succeeded

  STEP: deleting the pod @ 05/12/25 16:29:28.169
  E0512 16:29:28.206515      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:28.209094 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "dns-7869" for this suite. @ 05/12/25 16:29:28.223
• [4.275 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance] [sig-api-machinery, Serial, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/namespace.go:400
  STEP: Creating a kubernetes client @ 05/12/25 16:29:28.241
  I0512 16:29:28.241119 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename namespaces @ 05/12/25 16:29:28.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:28.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:28.304
  STEP: Creating namespace "e2e-ns-zkw7z" @ 05/12/25 16:29:28.311
  I0512 16:29:28.435807 23 namespace.go:411] Namespace "e2e-ns-zkw7z-3041" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-zkw7z-3041" @ 05/12/25 16:29:28.436
  I0512 16:29:28.454984 23 namespace.go:434] Namespace "e2e-ns-zkw7z-3041" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-zkw7z-3041" @ 05/12/25 16:29:28.455
  I0512 16:29:28.474775 23 namespace.go:463] Namespace "e2e-ns-zkw7z-3041" has []v1.FinalizerName{"kubernetes"}
  I0512 16:29:28.475793 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8510" for this suite. @ 05/12/25 16:29:28.488
  STEP: Destroying namespace "e2e-ns-zkw7z-3041" for this suite. @ 05/12/25 16:29:28.513
• [0.308 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:569
  STEP: Creating a kubernetes client @ 05/12/25 16:29:28.549
  I0512 16:29:28.549738 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:29:28.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:28.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:28.608
  STEP: Setting up server cert @ 05/12/25 16:29:28.7
  E0512 16:29:29.206709      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:29:30.08
  STEP: Deploying the webhook pod @ 05/12/25 16:29:30.122
  STEP: Wait for the deployment to be ready @ 05/12/25 16:29:30.147
  I0512 16:29:30.166521 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:29:30.207472      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:31.208432      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:29:32.189
  E0512 16:29:32.209095      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:29:32.209
  E0512 16:29:33.209639      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:33.209725 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/12/25 16:29:33.382
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/25 16:29:33.464
  STEP: Deleting the collection of validation webhooks @ 05/12/25 16:29:33.523
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/25 16:29:33.685
  I0512 16:29:33.899132 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9059" for this suite. @ 05/12/25 16:29:33.911
  STEP: Destroying namespace "webhook-markers-8894" for this suite. @ 05/12/25 16:29:33.936
• [5.401 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:100
  STEP: Creating a kubernetes client @ 05/12/25 16:29:33.951
  I0512 16:29:33.951380 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:29:33.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:33.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:33.999
  STEP: Creating configMap with name projected-configmap-test-volume-map-39a3dbef-5768-4d72-9def-d0cf083a511d @ 05/12/25 16:29:34.009
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:29:34.023
  E0512 16:29:34.209817      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:35.210744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:36.211668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:37.212563      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:29:38.08
  I0512 16:29:38.090433 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-configmaps-a460be55-e5a0-4c55-a3ed-51b14c5ee0f7 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:29:38.115
  I0512 16:29:38.151626 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9166" for this suite. @ 05/12/25 16:29:38.164
• [4.227 seconds]
------------------------------
S
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/12/25 16:29:38.178
  I0512 16:29:38.178626 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename hostport @ 05/12/25 16:29:38.18
  E0512 16:29:38.213093      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:38.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:38.233
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/12/25 16:29:38.263
  E0512 16:29:39.213984      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:40.214085      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:41.214121      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:42.214360      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.62.16.76 on the node which pod1 resides and expect scheduled @ 05/12/25 16:29:42.334
  E0512 16:29:43.214662      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:44.215109      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.62.16.76 but use UDP protocol on the node which pod2 resides @ 05/12/25 16:29:44.382
  E0512 16:29:45.216335      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:46.217092      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:47.217313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:48.217687      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/12/25 16:29:48.459
  I0512 16:29:48.459666 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.62.16.76 http://127.0.0.1:54323/hostname] Namespace:hostport-3205 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:29:48.460195 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:29:48.462179 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:29:48.462569 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3205/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.62.16.76+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.62.16.76, port: 54323 @ 05/12/25 16:29:48.601
  I0512 16:29:48.601660 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.62.16.76:54323/hostname] Namespace:hostport-3205 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:29:48.601699 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:29:48.602988 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:29:48.603142 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3205/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.62.16.76%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.62.16.76, port: 54323 UDP @ 05/12/25 16:29:48.738
  I0512 16:29:48.739138 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.62.16.76 54323] Namespace:hostport-3205 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:29:48.739400 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:29:48.741216 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:29:48.741624 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3205/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.62.16.76+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0512 16:29:49.218279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:50.218748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:51.219467      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:52.220108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:53.220881      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:53.895988 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-3205" for this suite. @ 05/12/25 16:29:53.908
• [15.741 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/ephemeral_containers.go:51
  STEP: Creating a kubernetes client @ 05/12/25 16:29:53.921
  I0512 16:29:53.921475 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/12/25 16:29:53.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:53.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:53.963
  STEP: creating a target pod @ 05/12/25 16:29:53.973
  E0512 16:29:54.221546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:55.222109      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/12/25 16:29:56.036
  E0512 16:29:56.222500      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:29:57.222918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/12/25 16:29:58.075
  I0512 16:29:58.075682 23 exec_util.go:59] ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3320 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:29:58.075722 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:29:58.077826 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:29:58.078273 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-3320/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  I0512 16:29:58.211205 23 exec_util.go:111] Exec stderr: ""
  E0512 16:29:58.222809      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:29:58.229131 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-3320" for this suite. @ 05/12/25 16:29:58.239
• [4.331 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:312
  STEP: Creating a kubernetes client @ 05/12/25 16:29:58.256
  I0512 16:29:58.256979 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:29:58.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:29:58.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:29:58.307
  STEP: Setting up server cert @ 05/12/25 16:29:58.414
  E0512 16:29:59.223789      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:29:59.397
  STEP: Deploying the webhook pod @ 05/12/25 16:29:59.406
  STEP: Wait for the deployment to be ready @ 05/12/25 16:29:59.432
  I0512 16:29:59.451335 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:30:00.224874      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:01.225268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:01.491910 23 deployment.go:103] deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2025, time.May, 12, 16, 29, 59, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 29, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 29, 59, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 29, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5b9c4f9645\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 16:30:02.225916      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:03.226183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:30:03.504
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:30:03.524
  E0512 16:30:04.226369      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:04.525001 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  I0512 16:30:04.541625 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:30:05.226521      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:06.227178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:07.227348      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:08.228327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:09.228698      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6653-crds.webhook.example.com via the AdmissionRegistration API @ 05/12/25 16:30:10.068
  STEP: Creating a custom resource while v1 is storage version @ 05/12/25 16:30:10.104
  E0512 16:30:10.228927      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:11.229634      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/12/25 16:30:12.181
  E0512 16:30:12.230342      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Patching the custom resource while v2 is storage version @ 05/12/25 16:30:12.265
  I0512 16:30:13.120692 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-823" for this suite. @ 05/12/25 16:30:13.13
  STEP: Destroying namespace "webhook-markers-7398" for this suite. @ 05/12/25 16:30:13.143
• [14.904 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1756
  STEP: Creating a kubernetes client @ 05/12/25 16:30:13.161
  I0512 16:30:13.161805 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:30:13.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:30:13.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:30:13.211
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/25 16:30:13.218
  I0512 16:30:13.218406 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-9999 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  E0512 16:30:13.231012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:13.382100 23 builder.go:146] stderr: ""
  I0512 16:30:13.382181 23 builder.go:147] stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/12/25 16:30:13.382
  I0512 16:30:13.392617 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-9999 delete pods e2e-test-httpd-pod'
  E0512 16:30:14.231615      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:15.232661      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:15.548997 23 builder.go:146] stderr: ""
  I0512 16:30:15.549102 23 builder.go:147] stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  I0512 16:30:15.549396 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9999" for this suite. @ 05/12/25 16:30:15.563
• [2.420 seconds]
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/garbage_collector.go:714
  STEP: Creating a kubernetes client @ 05/12/25 16:30:15.581
  I0512 16:30:15.581972 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename gc @ 05/12/25 16:30:15.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:30:15.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:30:15.63
  STEP: create the rc1 @ 05/12/25 16:30:15.663
  STEP: create the rc2 @ 05/12/25 16:30:15.688
  E0512 16:30:16.233034      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:17.233164      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:18.239125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:19.240784      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:20.241482      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:21.243472      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/12/25 16:30:21.709
  E0512 16:30:22.244359      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/12/25 16:30:22.332
  STEP: wait for the rc to be deleted @ 05/12/25 16:30:22.347
  E0512 16:30:23.245024      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:24.245810      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:25.251675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:26.248237      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:27.248670      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:27.375558 23 garbage_collector.go:762] 63 pods remaining
  I0512 16:30:27.375664 23 garbage_collector.go:769] 63 pods has nil DeletionTimestamp
  I0512 16:30:27.375687 23 garbage_collector.go:770] 
  E0512 16:30:28.248879      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:29.248933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:30.249991      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:31.250195      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:32.250683      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/25 16:30:32.387
  I0512 16:30:32.656295 23 garbage_collector.go:265] For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  I0512 16:30:32.656592 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-27btx" in namespace "gc-6961"
  I0512 16:30:32.709163 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-27ccb" in namespace "gc-6961"
  I0512 16:30:32.762442 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-4lsc7" in namespace "gc-6961"
  I0512 16:30:32.795124 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-4vp47" in namespace "gc-6961"
  I0512 16:30:32.841115 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-4zb7b" in namespace "gc-6961"
  I0512 16:30:32.875857 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-52b9s" in namespace "gc-6961"
  I0512 16:30:32.914395 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-5bkhn" in namespace "gc-6961"
  I0512 16:30:32.980419 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-5zfzq" in namespace "gc-6961"
  I0512 16:30:33.011879 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-6kb42" in namespace "gc-6961"
  I0512 16:30:33.057950 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-75dcf" in namespace "gc-6961"
  I0512 16:30:33.112089 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-78rfl" in namespace "gc-6961"
  I0512 16:30:33.169177 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-7bdpj" in namespace "gc-6961"
  I0512 16:30:33.206733 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-7grzk" in namespace "gc-6961"
  E0512 16:30:33.251549      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:33.252948 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-7mjlg" in namespace "gc-6961"
  I0512 16:30:33.305579 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-7nt6b" in namespace "gc-6961"
  I0512 16:30:33.342160 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-7qpbn" in namespace "gc-6961"
  I0512 16:30:33.381816 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-8bvhh" in namespace "gc-6961"
  I0512 16:30:33.423975 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-99gcm" in namespace "gc-6961"
  I0512 16:30:33.453670 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-9bnj4" in namespace "gc-6961"
  I0512 16:30:33.484277 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-9dl2q" in namespace "gc-6961"
  I0512 16:30:33.515998 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-9hs94" in namespace "gc-6961"
  I0512 16:30:33.565218 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-9vl58" in namespace "gc-6961"
  I0512 16:30:33.604895 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-9z99t" in namespace "gc-6961"
  I0512 16:30:33.642407 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-b4r9h" in namespace "gc-6961"
  I0512 16:30:33.681215 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-bbcs8" in namespace "gc-6961"
  I0512 16:30:33.710475 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-bckpc" in namespace "gc-6961"
  I0512 16:30:33.746163 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-bf2g8" in namespace "gc-6961"
  I0512 16:30:33.768739 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-btcl2" in namespace "gc-6961"
  I0512 16:30:33.824880 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-chcbm" in namespace "gc-6961"
  I0512 16:30:33.871227 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-cj5kz" in namespace "gc-6961"
  I0512 16:30:33.922011 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-ctdv4" in namespace "gc-6961"
  I0512 16:30:33.958687 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-dg86f" in namespace "gc-6961"
  I0512 16:30:33.990552 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-ds7n7" in namespace "gc-6961"
  I0512 16:30:34.017128 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-f2rn8" in namespace "gc-6961"
  I0512 16:30:34.054743 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-fbkvx" in namespace "gc-6961"
  I0512 16:30:34.117175 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-fcknm" in namespace "gc-6961"
  I0512 16:30:34.157062 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-fxc4p" in namespace "gc-6961"
  I0512 16:30:34.183740 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-fzqcd" in namespace "gc-6961"
  I0512 16:30:34.217967 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-g6s5n" in namespace "gc-6961"
  E0512 16:30:34.251932      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:34.268649 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-gcfbt" in namespace "gc-6961"
  I0512 16:30:34.323201 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-gh9gx" in namespace "gc-6961"
  I0512 16:30:34.344052 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-gqps5" in namespace "gc-6961"
  I0512 16:30:34.401808 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-hdz75" in namespace "gc-6961"
  I0512 16:30:34.424852 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-hlsds" in namespace "gc-6961"
  I0512 16:30:34.467061 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-j8kfr" in namespace "gc-6961"
  I0512 16:30:34.502808 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-jz687" in namespace "gc-6961"
  I0512 16:30:34.533589 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-k7rt6" in namespace "gc-6961"
  I0512 16:30:34.570772 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-kg2bh" in namespace "gc-6961"
  I0512 16:30:34.597070 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-kldbr" in namespace "gc-6961"
  I0512 16:30:34.630878 23 delete.go:95] Deleting pod "simpletest-rc-to-be-deleted-klgk8" in namespace "gc-6961"
  I0512 16:30:34.662830 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6961" for this suite. @ 05/12/25 16:30:34.677
• [19.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:2194
  STEP: Creating a kubernetes client @ 05/12/25 16:30:34.703
  I0512 16:30:34.703239 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 16:30:34.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:30:34.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:30:34.765
  STEP: creating service in namespace services-5930 @ 05/12/25 16:30:34.772
  STEP: creating service affinity-nodeport in namespace services-5930 @ 05/12/25 16:30:34.772
  STEP: creating replication controller affinity-nodeport in namespace services-5930 @ 05/12/25 16:30:34.804
  I0512 16:30:34.834853      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5930, replica count: 3
  E0512 16:30:35.252658      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:36.252833      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:37.253632      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:37.886928      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 16:30:37.915489 23 resource.go:361] Creating new exec pod
  E0512 16:30:38.254420      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:39.255122      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:40.256122      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:40.979645 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-5930 exec execpod-affinityl5w8g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0512 16:30:41.257109      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:41.365614 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  I0512 16:30:41.365762 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 16:30:41.366524 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-5930 exec execpod-affinityl5w8g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.58.190 80'
  I0512 16:30:41.701307 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.58.190 80\nConnection to 10.233.58.190 80 port [tcp/http] succeeded!\n"
  I0512 16:30:41.701492 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 16:30:41.701775 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-5930 exec execpod-affinityl5w8g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.75 30484'
  I0512 16:30:42.081958 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.75 30484\nConnection to 10.62.16.75 30484 port [tcp/*] succeeded!\n"
  I0512 16:30:42.082159 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 16:30:42.082499 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-5930 exec execpod-affinityl5w8g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.77 30484'
  E0512 16:30:42.257677      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:42.439304 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.77 30484\nConnection to 10.62.16.77 30484 port [tcp/*] succeeded!\n"
  I0512 16:30:42.439611 23 builder.go:147] stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  I0512 16:30:42.439789 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-5930 exec execpod-affinityl5w8g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.62.16.75:30484/ ; done'
  I0512 16:30:43.034665 23 builder.go:146] stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.62.16.75:30484/\n"
  I0512 16:30:43.034780 23 builder.go:147] stdout: "\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv\naffinity-nodeport-5bnkv"
  I0512 16:30:43.034825 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.034851 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.034872 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.034891 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.034911 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.034930 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.034949 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035163 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035203 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035222 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035242 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035261 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035279 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035297 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035316 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035337 23 service.go:242] Received response from host: affinity-nodeport-5bnkv
  I0512 16:30:43.035487 23 service.go:4061] Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-5930, will wait for the garbage collector to delete the pods @ 05/12/25 16:30:43.077
  I0512 16:30:43.178762 23 resources.go:139] Deleting ReplicationController affinity-nodeport took: 20.110771ms
  E0512 16:30:43.257868      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:43.279646 23 resources.go:163] Terminating ReplicationController affinity-nodeport pods took: 100.876405ms
  E0512 16:30:44.258683      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:45.258889      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:30:46.209157 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5930" for this suite. @ 05/12/25 16:30:46.23
• [11.551 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:376
  STEP: Creating a kubernetes client @ 05/12/25 16:30:46.256
  I0512 16:30:46.257136 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:30:46.260771      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:30:46.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:30:46.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:30:46.343
  STEP: Creating configMap with name projected-configmap-test-volume-2dba5be6-c72f-4308-b5f5-c073ff694ed2 @ 05/12/25 16:30:46.349
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:30:46.36
  E0512 16:30:47.260763      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:48.261654      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:49.263011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:50.262611      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:30:50.435
  I0512 16:30:50.441937 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-configmaps-ba954627-f030-478b-b01b-213788c5b588 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 16:30:50.458
  I0512 16:30:50.491316 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4970" for this suite. @ 05/12/25 16:30:50.501
• [4.260 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/configmap_volume.go:48
  STEP: Creating a kubernetes client @ 05/12/25 16:30:50.518
  I0512 16:30:50.518554 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename configmap @ 05/12/25 16:30:50.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:30:50.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:30:50.565
  STEP: Creating configMap with name configmap-test-volume-a1fd136b-8b26-45d1-92b5-bc050ea6cc4d @ 05/12/25 16:30:50.571
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:30:50.579
  E0512 16:30:51.262731      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:52.263989      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:53.264771      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:54.264934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:30:54.631
  I0512 16:30:54.639362 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-configmaps-c61b612f-a4c0-43a5-a475-bb2677c827a4 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:30:54.657
  I0512 16:30:54.696694 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-180" for this suite. @ 05/12/25 16:30:54.711
• [4.214 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/ingress.go:55
  STEP: Creating a kubernetes client @ 05/12/25 16:30:54.738
  I0512 16:30:54.739101 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename ingress @ 05/12/25 16:30:54.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:30:54.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:30:54.789
  STEP: getting /apis @ 05/12/25 16:30:54.798
  STEP: getting /apis/networking.k8s.io @ 05/12/25 16:30:54.811
  STEP: getting /apis/networking.k8s.iov1 @ 05/12/25 16:30:54.814
  STEP: creating @ 05/12/25 16:30:54.816
  STEP: getting @ 05/12/25 16:30:54.847
  STEP: listing @ 05/12/25 16:30:54.874
  STEP: watching @ 05/12/25 16:30:54.884
  I0512 16:30:54.885004 23 ingress.go:186] starting watch
  STEP: cluster-wide listing @ 05/12/25 16:30:54.888
  STEP: cluster-wide watching @ 05/12/25 16:30:54.894
  I0512 16:30:54.894584 23 ingress.go:198] starting watch
  STEP: patching @ 05/12/25 16:30:54.896
  STEP: updating @ 05/12/25 16:30:54.908
  I0512 16:30:54.937056 23 ingress.go:221] waiting for watch events with expected annotations
  I0512 16:30:54.937177 23 ingress.go:234] saw patched and updated annotations
  STEP: patching /status @ 05/12/25 16:30:54.937
  STEP: updating /status @ 05/12/25 16:30:54.951
  STEP: get /status @ 05/12/25 16:30:54.977
  STEP: deleting @ 05/12/25 16:30:54.984
  STEP: deleting a collection @ 05/12/25 16:30:55.011
  I0512 16:30:55.046775 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-8750" for this suite. @ 05/12/25 16:30:55.056
• [0.333 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/12/25 16:30:55.072
  I0512 16:30:55.072862 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 16:30:55.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:30:55.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:30:55.12
  E0512 16:30:55.265746      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:56.266594      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:57.267348      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:58.268196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:30:59.268589      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:00.268937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:31:01.253
  I0512 16:31:01.260149 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod client-envvars-0015b79d-b777-4800-9254-e8099b707c4c container env3cont: <nil>
  E0512 16:31:01.269472      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: delete the pod @ 05/12/25 16:31:01.277
  I0512 16:31:01.308372 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2141" for this suite. @ 05/12/25 16:31:01.323
• [6.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:1435
  STEP: Creating a kubernetes client @ 05/12/25 16:31:01.344
  I0512 16:31:01.344565 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 16:31:01.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:31:01.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:31:01.395
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3084 @ 05/12/25 16:31:01.401
  STEP: changing the ExternalName service to type=NodePort @ 05/12/25 16:31:01.415
  STEP: creating replication controller externalname-service in namespace services-3084 @ 05/12/25 16:31:01.466
  I0512 16:31:01.479108      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3084, replica count: 2
  E0512 16:31:02.270229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:03.270369      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:04.271243      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:04.532995      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0512 16:31:04.533290 23 resource.go:361] Creating new exec pod
  E0512 16:31:05.271671      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:06.272272      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:07.273348      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:07.608117 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  I0512 16:31:07.929639 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  I0512 16:31:07.930052 23 builder.go:147] stdout: "externalname-service-cn2x4"
  I0512 16:31:07.930198 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.223 80'
  E0512 16:31:08.274221      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:09.275418      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:10.209448 23 builder.go:135] rc: 1
  I0512 16:31:10.209549 23 jig.go:916] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.223 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.233.55.223 80
  nc: connect to 10.233.55.223 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  I0512 16:31:10.209712 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.223 80'
  E0512 16:31:10.276720      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:10.484753 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.223 80\nConnection to 10.233.55.223 80 port [tcp/http] succeeded!\n"
  I0512 16:31:10.484839 23 builder.go:147] stdout: "externalname-service-cn2x4"
  I0512 16:31:10.484999 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.75 31878'
  I0512 16:31:10.771936 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.75 31878\nConnection to 10.62.16.75 31878 port [tcp/*] succeeded!\n"
  I0512 16:31:10.772043 23 builder.go:147] stdout: ""
  E0512 16:31:11.277586      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:11.485105 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.75 31878'
  E0512 16:31:12.277792      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:13.277895      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:13.734440 23 builder.go:135] rc: 1
  I0512 16:31:13.734543 23 jig.go:916] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.75 31878:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.62.16.75 31878
  nc: connect to 10.62.16.75 port 31878 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  I0512 16:31:13.734989 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.75 31878'
  I0512 16:31:14.081698 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.75 31878\nConnection to 10.62.16.75 31878 port [tcp/*] succeeded!\n"
  I0512 16:31:14.081808 23 builder.go:147] stdout: "externalname-service-cn2x4"
  I0512 16:31:14.082307 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.77 31878'
  E0512 16:31:14.278456      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:14.374049 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.77 31878\nConnection to 10.62.16.77 31878 port [tcp/*] succeeded!\n"
  I0512 16:31:14.374186 23 builder.go:147] stdout: ""
  I0512 16:31:15.083281 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.77 31878'
  E0512 16:31:15.278668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:16.278903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:17.280104      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:17.368884 23 builder.go:135] rc: 1
  I0512 16:31:17.368994 23 jig.go:916] Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.77 31878:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.62.16.77 31878
  nc: connect to 10.62.16.77 port 31878 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  I0512 16:31:17.369200 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-3084 exec execpodb6p7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.62.16.77 31878'
  I0512 16:31:17.680388 23 builder.go:146] stderr: "+ echo hostName\n+ nc -v -t -w 2 10.62.16.77 31878\nConnection to 10.62.16.77 31878 port [tcp/*] succeeded!\n"
  I0512 16:31:17.680482 23 builder.go:147] stdout: "externalname-service-cn2x4"
  I0512 16:31:17.680716 23 service.go:1444] Cleaning up the ExternalName to NodePort test service
  I0512 16:31:17.735658 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3084" for this suite. @ 05/12/25 16:31:17.749
• [16.421 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/12/25 16:31:17.766
  I0512 16:31:17.766186 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-runtime @ 05/12/25 16:31:17.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:31:17.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:31:17.84
  STEP: create the container @ 05/12/25 16:31:17.853
  W0512 16:31:17.897832      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/12/25 16:31:17.898
  E0512 16:31:18.280729      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:19.281700      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:20.282733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/12/25 16:31:20.941
  STEP: the container should be terminated @ 05/12/25 16:31:20.948
  STEP: the termination message should be set @ 05/12/25 16:31:20.948
  I0512 16:31:20.948572 23 runtime.go:167] Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/12/25 16:31:20.948
  I0512 16:31:20.986437 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7525" for this suite. @ 05/12/25 16:31:21.002
• [3.252 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] PersistentVolumes CSI Conformance should run through the lifecycle of a PV and a PVC [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/persistent_volumes.go:429
  STEP: Creating a kubernetes client @ 05/12/25 16:31:21.047
  I0512 16:31:21.047985 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pv @ 05/12/25 16:31:21.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:31:21.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:31:21.096
  STEP: Creating initial PV and PVC @ 05/12/25 16:31:21.104
  I0512 16:31:21.104143 23 pv.go:394] Creating a PV followed by a PVC
  STEP: Listing all PVs with the labelSelector: "e2e-pv-pool=pv-432" @ 05/12/25 16:31:21.139
  STEP: Listing PVCs in namespace "pv-432" @ 05/12/25 16:31:21.148
  STEP: Patching the PV "pv-432-xktzs" @ 05/12/25 16:31:21.165
  STEP: Patching the PVC "pvc-f875p" @ 05/12/25 16:31:21.199
  STEP: Getting PV "pv-432-xktzs" @ 05/12/25 16:31:21.22
  STEP: Getting PVC "pvc-f875p" @ 05/12/25 16:31:21.228
  STEP: Deleting PVC "pvc-f875p" @ 05/12/25 16:31:21.238
  STEP: Confirm deletion of PVC "pvc-f875p" @ 05/12/25 16:31:21.253
  E0512 16:31:21.284253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:22.284578      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:23.285447      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting PV "pv-432-xktzs" @ 05/12/25 16:31:23.285
  STEP: Confirm deletion of PV "pv-432-xktzs" @ 05/12/25 16:31:23.308
  E0512 16:31:24.286120      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:25.287189      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Recreating another PV & PVC @ 05/12/25 16:31:25.339
  I0512 16:31:25.339948 23 pv.go:394] Creating a PV followed by a PVC
  STEP: Updating the PV "pv-432-hr27t" @ 05/12/25 16:31:25.37
  STEP: Updating the PVC "pvc-99wts" @ 05/12/25 16:31:25.399
  STEP: Listing PVCs in all namespaces with the labelSelector: "pvc-99wts=updated" @ 05/12/25 16:31:25.424
  STEP: Deleting PVC "pvc-99wts" via DeleteCollection @ 05/12/25 16:31:25.436
  STEP: Confirm deletion of PVC "pvc-99wts" @ 05/12/25 16:31:25.456
  STEP: Deleting PV "pv-432-hr27t" via DeleteCollection @ 05/12/25 16:31:25.498
  STEP: Confirm deletion of PV "pv-432-hr27t" @ 05/12/25 16:31:25.528
  E0512 16:31:26.287551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:27.288326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:27.560324 23 persistent_volumes.go:406] AfterEach: deleting 1 PVCs and 1 PVs...
  I0512 16:31:27.560685 23 pv.go:205] Deleting PersistentVolumeClaim "pvc-99wts"
  I0512 16:31:27.569069 23 pv.go:193] Deleting PersistentVolume "pv-432-hr27t"
  I0512 16:31:27.577801 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pv-432" for this suite. @ 05/12/25 16:31:27.592
• [6.560 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/subpath.go:79
  STEP: Creating a kubernetes client @ 05/12/25 16:31:27.607
  I0512 16:31:27.607595 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename subpath @ 05/12/25 16:31:27.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:31:27.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:31:27.668
  STEP: Setting up data @ 05/12/25 16:31:27.679
  STEP: Creating pod pod-subpath-test-configmap-62lc @ 05/12/25 16:31:27.706
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/25 16:31:27.706
  E0512 16:31:28.289465      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:29.290878      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:30.291996      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:31.292367      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:32.293302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:33.293645      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:34.294016      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:35.294222      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:36.294646      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:37.295216      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:38.295619      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:39.295873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:40.296192      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:41.296837      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:42.296811      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:43.297124      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:44.297522      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:45.298360      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:46.298544      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:47.299199      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:48.299981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:49.300222      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:50.300807      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:51.301683      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:31:51.903
  I0512 16:31:51.911714 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-subpath-test-configmap-62lc container test-container-subpath-configmap-62lc: <nil>
  STEP: delete the pod @ 05/12/25 16:31:51.934
  STEP: Deleting pod pod-subpath-test-configmap-62lc @ 05/12/25 16:31:51.974
  I0512 16:31:51.974098 23 delete.go:62] Deleting pod "pod-subpath-test-configmap-62lc" in namespace "subpath-605"
  I0512 16:31:51.981363 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-605" for this suite. @ 05/12/25 16:31:51.992
• [24.399 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:139
  STEP: Creating a kubernetes client @ 05/12/25 16:31:52.006
  I0512 16:31:52.006723 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-webhook @ 05/12/25 16:31:52.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:31:52.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:31:52.053
  STEP: Setting up server cert @ 05/12/25 16:31:52.059
  E0512 16:31:52.301891      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/12/25 16:31:53.074
  STEP: Deploying the custom resource conversion webhook pod @ 05/12/25 16:31:53.093
  STEP: Wait for the deployment to be ready @ 05/12/25 16:31:53.133
  I0512 16:31:53.167577 23 deployment.go:222] deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0512 16:31:53.302072      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:54.302820      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:31:55.191
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:31:55.212
  E0512 16:31:55.303947      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:31:56.213695 23 util.go:420] Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  I0512 16:31:56.230225 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:31:56.304173      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:57.304781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:58.305130      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:31:59.305847      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:00.306231      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:01.306961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:02.306970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:03.307595      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/12/25 16:32:03.922
  STEP: v2 custom resource should be converted @ 05/12/25 16:32:03.939
  E0512 16:32:04.308645      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:04.609488 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-7469" for this suite. @ 05/12/25 16:32:04.624
• [12.639 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:400
  STEP: Creating a kubernetes client @ 05/12/25 16:32:04.647
  I0512 16:32:04.647889 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:32:04.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:32:04.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:32:04.71
  STEP: Setting up server cert @ 05/12/25 16:32:04.805
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:32:05.3
  E0512 16:32:05.308758      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook pod @ 05/12/25 16:32:05.311
  STEP: Wait for the deployment to be ready @ 05/12/25 16:32:05.344
  I0512 16:32:05.377647 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:32:06.309043      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:07.309626      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:32:07.404
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:32:07.426
  E0512 16:32:08.310759      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:08.428621 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/12/25 16:32:08.447
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/25 16:32:08.481
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/12/25 16:32:08.505
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/25 16:32:08.529
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/12/25 16:32:08.551
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/25 16:32:08.564
  I0512 16:32:08.701913 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4772" for this suite. @ 05/12/25 16:32:08.733
  STEP: Destroying namespace "webhook-markers-6800" for this suite. @ 05/12/25 16:32:08.751
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/12/25 16:32:08.774
  I0512 16:32:08.774550 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 16:32:08.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:32:08.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:32:08.827
  I0512 16:32:08.835061 23 deployment.go:1196] Creating deployment "webserver-deployment"
  I0512 16:32:08.850277 23 deployment.go:1200] Waiting for observed generation 1
  E0512 16:32:09.311669      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:10.312705      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:10.887935 23 deployment.go:1205] Waiting for all required pods to come up
  I0512 16:32:10.900321 23 resource.go:87] Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/12/25 16:32:10.9
  E0512 16:32:11.313157      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:12.313324      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:12.922041 23 deployment.go:1209] Waiting for deployment "webserver-deployment" to complete
  I0512 16:32:12.954115 23 deployment.go:1218] Updating deployment "webserver-deployment" with a non-existent image
  I0512 16:32:12.980195 23 deployment.go:313] Updating deployment webserver-deployment
  I0512 16:32:12.980482 23 deployment.go:1224] Waiting for observed generation 2
  E0512 16:32:13.313800      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:14.314311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:15.004485 23 deployment.go:1234] Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  I0512 16:32:15.013479 23 deployment.go:1239] Waiting for the first rollout's replicaset to have .spec.replicas = 8
  I0512 16:32:15.022392 23 deployment.go:1244] Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  I0512 16:32:15.057221 23 deployment.go:1258] Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  I0512 16:32:15.057360 23 deployment.go:1263] Waiting for the second rollout's replicaset to have .spec.replicas = 5
  I0512 16:32:15.070257 23 deployment.go:1268] Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  I0512 16:32:15.091348 23 deployment.go:1275] Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  I0512 16:32:15.091635 23 deployment.go:1283] Scaling up the deployment "webserver-deployment" from 10 to 30
  I0512 16:32:15.119220 23 deployment.go:313] Updating deployment webserver-deployment
  I0512 16:32:15.119528 23 deployment.go:1289] Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  I0512 16:32:15.151522 23 deployment.go:1297] Verifying that first rollout's replicaset has .spec.replicas = 20
  I0512 16:32:15.160570 23 deployment.go:1303] Verifying that second rollout's replicaset has .spec.replicas = 13
  I0512 16:32:15.197496 23 deployment.go:633] Deployment "webserver-deployment":
  (v1.Deployment) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=20) "webserver-deployment",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "a441d47b-6258-48a7-b4ce-ed136ee51057",
      ResourceVersion: (string) (len=5) "64111",
      Generation: (int64) 3,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=1) {
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) (len=1) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "2"
      },
      OwnerReferences: ([]v1.OwnerReference) <nil>,
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=635) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 7d 7d 2c  |},"f:name":{}}},|
              00000030  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 70 72 6f  |"f:spec":{"f:pro|
              00000040  67 72 65 73 73 44 65 61  64 6c 69 6e 65 53 65 63  |gressDeadlineSec|
              00000050  6f 6e 64 73 22 3a 7b 7d  2c 22 66 3a 72 65 70 6c  |onds":{},"f:repl|
              00000060  69 63 61 73 22 3a 7b 7d  2c 22 66 3a 72 65 76 69  |icas":{},"f:revi|
              00000070  73 69 6f 6e 48 69 73 74  6f 72 79 4c 69 6d 69 74  |sionHistoryLimit|
              00000080  22 3a 7b 7d 2c 22 66 3a  73 65 6c 65 63 74 6f 72  |":{},"f:selector|
              00000090  22 3a 7b 7d 2c 22 66 3a  73 74 72 61 74 65 67 79  |":{},"f:strategy|
              000000a0  22 3a 7b 22 66 3a 72 6f  6c 6c 69 6e 67 55 70 64  |":{"f:rollingUpd|
              000000b0  61 74 65 22 3a 7b 22 2e  22 3a 7b 7d 2c 22 66 3a  |ate":{".":{},"f:|
              000000c0  6d 61 78 53 75 72 67 65  22 3a 7b 7d 2c 22 66 3a  |maxSurge":{},"f:|
              000000d0  6d 61 78 55 6e 61 76 61  69 6c 61 62 6c 65 22 3a  |maxUnavailable":|
              000000e0  7b 7d 7d 2c 22 66 3a 74  79 70 65 22 3a 7b 7d 7d  |{}},"f:type":{}}|
              000000f0  2c 22 66 3a 74 65 6d 70  6c 61 74 65 22 3a 7b 22  |,"f:template":{"|
              00000100  66 3a 6d 65 74 61 64 61  74 61 22 3a 7b 22 66 3a  |f:metadata":{"f:|
              00000110  6c 61 62 65 6c 73 22 3a  7b 22 2e 22 3a 7b 7d 2c  |labels":{".":{},|
              00000120  22 66 3a 6e 61 6d 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:name":{}}},"f|
              00000130  3a 73 70 65 63 22 3a 7b  22 66 3a 63 6f 6e 74 61  |:spec":{"f:conta|
              00000140  69 6e 65 72 73 22 3a 7b  22 6b 3a 7b 5c 22 6e 61  |iners":{"k:{\"na|
              00000150  6d 65 5c 22 3a 5c 22 68  74 74 70 64 5c 22 7d 22  |me\":\"httpd\"}"|
              00000160  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 69 6d 61 67  |:{".":{},"f:imag|
              00000170  65 22 3a 7b 7d 2c 22 66  3a 69 6d 61 67 65 50 75  |e":{},"f:imagePu|
              00000180  6c 6c 50 6f 6c 69 63 79  22 3a 7b 7d 2c 22 66 3a  |llPolicy":{},"f:|
              00000190  6e 61 6d 65 22 3a 7b 7d  2c 22 66 3a 72 65 73 6f  |name":{},"f:reso|
              000001a0  75 72 63 65 73 22 3a 7b  7d 2c 22 66 3a 73 65 63  |urces":{},"f:sec|
              000001b0  75 72 69 74 79 43 6f 6e  74 65 78 74 22 3a 7b 7d  |urityContext":{}|
              000001c0  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              000001d0  65 73 73 61 67 65 50 61  74 68 22 3a 7b 7d 2c 22  |essagePath":{},"|
              000001e0  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 4d 65 73  |f:terminationMes|
              000001f0  73 61 67 65 50 6f 6c 69  63 79 22 3a 7b 7d 7d 7d  |sagePolicy":{}}}|
              00000200  2c 22 66 3a 64 6e 73 50  6f 6c 69 63 79 22 3a 7b  |,"f:dnsPolicy":{|
              00000210  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              00000220  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              00000230  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              00000240  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000250  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000260  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000270  6e 64 73 22 3a 7b 7d 7d  7d 7d 7d                 |nds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=541) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 72 65 76 69 73  69 6f 6e 22 3a 7b 7d 7d  |io/revision":{}}|
              00000050  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 22 66  |},"f:status":{"f|
              00000060  3a 61 76 61 69 6c 61 62  6c 65 52 65 70 6c 69 63  |:availableReplic|
              00000070  61 73 22 3a 7b 7d 2c 22  66 3a 63 6f 6e 64 69 74  |as":{},"f:condit|
              00000080  69 6f 6e 73 22 3a 7b 22  2e 22 3a 7b 7d 2c 22 6b  |ions":{".":{},"k|
              00000090  3a 7b 5c 22 74 79 70 65  5c 22 3a 5c 22 41 76 61  |:{\"type\":\"Ava|
              000000a0  69 6c 61 62 6c 65 5c 22  7d 22 3a 7b 22 2e 22 3a  |ilable\"}":{".":|
              000000b0  7b 7d 2c 22 66 3a 6c 61  73 74 54 72 61 6e 73 69  |{},"f:lastTransi|
              000000c0  74 69 6f 6e 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |tionTime":{},"f:|
              000000d0  6c 61 73 74 55 70 64 61  74 65 54 69 6d 65 22 3a  |lastUpdateTime":|
              000000e0  7b 7d 2c 22 66 3a 6d 65  73 73 61 67 65 22 3a 7b  |{},"f:message":{|
              000000f0  7d 2c 22 66 3a 72 65 61  73 6f 6e 22 3a 7b 7d 2c  |},"f:reason":{},|
              00000100  22 66 3a 73 74 61 74 75  73 22 3a 7b 7d 2c 22 66  |"f:status":{},"f|
              00000110  3a 74 79 70 65 22 3a 7b  7d 7d 2c 22 6b 3a 7b 5c  |:type":{}},"k:{\|
              00000120  22 74 79 70 65 5c 22 3a  5c 22 50 72 6f 67 72 65  |"type\":\"Progre|
              00000130  73 73 69 6e 67 5c 22 7d  22 3a 7b 22 2e 22 3a 7b  |ssing\"}":{".":{|
              00000140  7d 2c 22 66 3a 6c 61 73  74 54 72 61 6e 73 69 74  |},"f:lastTransit|
              00000150  69 6f 6e 54 69 6d 65 22  3a 7b 7d 2c 22 66 3a 6c  |ionTime":{},"f:l|
              00000160  61 73 74 55 70 64 61 74  65 54 69 6d 65 22 3a 7b  |astUpdateTime":{|
              00000170  7d 2c 22 66 3a 6d 65 73  73 61 67 65 22 3a 7b 7d  |},"f:message":{}|
              00000180  2c 22 66 3a 72 65 61 73  6f 6e 22 3a 7b 7d 2c 22  |,"f:reason":{},"|
              00000190  66 3a 73 74 61 74 75 73  22 3a 7b 7d 2c 22 66 3a  |f:status":{},"f:|
              000001a0  74 79 70 65 22 3a 7b 7d  7d 7d 2c 22 66 3a 6f 62  |type":{}}},"f:ob|
              000001b0  73 65 72 76 65 64 47 65  6e 65 72 61 74 69 6f 6e  |servedGeneration|
              000001c0  22 3a 7b 7d 2c 22 66 3a  72 65 61 64 79 52 65 70  |":{},"f:readyRep|
              000001d0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 72 65 70  |licas":{},"f:rep|
              000001e0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 75 6e 61  |licas":{},"f:una|
              000001f0  76 61 69 6c 61 62 6c 65  52 65 70 6c 69 63 61 73  |vailableReplicas|
              00000200  22 3a 7b 7d 2c 22 66 3a  75 70 64 61 74 65 64 52  |":{},"f:updatedR|
              00000210  65 70 6c 69 63 61 73 22  3a 7b 7d 7d 7d           |eplicas":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.DeploymentSpec) {
      Replicas: (*int32)(30),
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=1) {
          (string) (len=4) "name": (string) (len=5) "httpd"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=1) {
            (string) (len=4) "name": (string) (len=5) "httpd"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=13) "webserver:404",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      },
      Strategy: (v1.DeploymentStrategy) {
        Type: (v1.DeploymentStrategyType) (len=13) "RollingUpdate",
        RollingUpdate: (*v1.RollingUpdateDeployment)({
          MaxUnavailable: (*intstr.IntOrString)({
            Type: (intstr.Type) 0,
            IntVal: (int32) 2,
            StrVal: (string) ""
          }),
          MaxSurge: (*intstr.IntOrString)({
            Type: (intstr.Type) 0,
            IntVal: (int32) 3,
            StrVal: (string) ""
          })
        })
      },
      MinReadySeconds: (int32) 0,
      RevisionHistoryLimit: (*int32)(10),
      Paused: (bool) false,
      ProgressDeadlineSeconds: (*int32)(600)
    },
    Status: (v1.DeploymentStatus) {
      ObservedGeneration: (int64) 3,
      Replicas: (int32) 13,
      UpdatedReplicas: (int32) 5,
      ReadyReplicas: (int32) 8,
      AvailableReplicas: (int32) 8,
      UnavailableReplicas: (int32) 5,
      Conditions: ([]v1.DeploymentCondition) (len=2) {
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=11) "Progressing",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=17) "ReplicaSetUpdated",
          Message: (string) (len=60) "ReplicaSet \"webserver-deployment-786f49d774\" is progressing."
        },
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=9) "Available",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=26) "MinimumReplicasUnavailable",
          Message: (string) (len=46) "Deployment does not have minimum availability."
        }
      },
      CollisionCount: (*int32)(<nil>)
    }
  }


  I0512 16:32:15.229253 23 deployment.go:39] New ReplicaSet "webserver-deployment-786f49d774" of Deployment "webserver-deployment":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=31) "webserver-deployment-786f49d774",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
      ResourceVersion: (string) (len=5) "64108",
      Generation: (int64) 3,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664332,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "2",
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=2) "30",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=2) "33"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=20) "webserver-deployment",
          UID: (types.UID) (len=36) "a441d47b-6258-48a7-b4ce-ed136ee51057",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=84) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  66 75 6c 6c 79 4c 61 62  65 6c 65 64 52 65 70 6c  |fullyLabeledRepl|
              00000020  69 63 61 73 22 3a 7b 7d  2c 22 66 3a 6f 62 73 65  |icas":{},"f:obse|
              00000030  72 76 65 64 47 65 6e 65  72 61 74 69 6f 6e 22 3a  |rvedGeneration":|
              00000040  7b 7d 2c 22 66 3a 72 65  70 6c 69 63 61 73 22 3a  |{},"f:replicas":|
              00000050  7b 7d 7d 7d                                       |{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=781) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 61 34 34 31 64 34  37 62 2d 36 32 35 38 2d  |\"a441d47b-6258-|
              00000120  34 38 61 37 2d 62 34 63  65 2d 65 64 31 33 36 65  |48a7-b4ce-ed136e|
              00000130  65 35 31 30 35 37 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |e51057\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 72 65 70  |"f:spec":{"f:rep|
              00000150  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 73 65 6c  |licas":{},"f:sel|
              00000160  65 63 74 6f 72 22 3a 7b  7d 2c 22 66 3a 74 65 6d  |ector":{},"f:tem|
              00000170  70 6c 61 74 65 22 3a 7b  22 66 3a 6d 65 74 61 64  |plate":{"f:metad|
              00000180  61 74 61 22 3a 7b 22 66  3a 6c 61 62 65 6c 73 22  |ata":{"f:labels"|
              00000190  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6e 61 6d 65  |:{".":{},"f:name|
              000001a0  22 3a 7b 7d 2c 22 66 3a  70 6f 64 2d 74 65 6d 70  |":{},"f:pod-temp|
              000001b0  6c 61 74 65 2d 68 61 73  68 22 3a 7b 7d 7d 7d 2c  |late-hash":{}}},|
              000001c0  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 63 6f 6e  |"f:spec":{"f:con|
              000001d0  74 61 69 6e 65 72 73 22  3a 7b 22 6b 3a 7b 5c 22  |tainers":{"k:{\"|
              000001e0  6e 61 6d 65 5c 22 3a 5c  22 68 74 74 70 64 5c 22  |name\":\"httpd\"|
              000001f0  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000200  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000210  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000220  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              00000230  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              00000240  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000250  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000260  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              00000270  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000280  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              00000290  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              000002a0  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              000002b0  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              000002c0  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              000002d0  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              000002e0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              000002f0  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000300  63 6f 6e 64 73 22 3a 7b  7d 7d 7d 7d 7d           |conds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(13),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=5) "httpd",
          (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=5) "httpd",
            (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=13) "webserver:404",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 5,
      FullyLabeledReplicas: (int32) 5,
      ReadyReplicas: (int32) 0,
      AvailableReplicas: (int32) 0,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 16:32:15.232156 23 deployment.go:44] All old ReplicaSets of Deployment "webserver-deployment":
  I0512 16:32:15.233015 23 deployment.go:47] (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=31) "webserver-deployment-64bcfc6446",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
      ResourceVersion: (string) (len=5) "64107",
      Generation: (int64) 3,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=2) "30",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=2) "33",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=20) "webserver-deployment",
          UID: (types.UID) (len=36) "a441d47b-6258-48a7-b4ce-ed136ee51057",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=130) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  61 76 61 69 6c 61 62 6c  65 52 65 70 6c 69 63 61  |availableReplica|
              00000020  73 22 3a 7b 7d 2c 22 66  3a 66 75 6c 6c 79 4c 61  |s":{},"f:fullyLa|
              00000030  62 65 6c 65 64 52 65 70  6c 69 63 61 73 22 3a 7b  |beledReplicas":{|
              00000040  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              00000050  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              00000060  65 61 64 79 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |eadyReplicas":{}|
              00000070  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000080  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=781) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 61 34 34 31 64 34  37 62 2d 36 32 35 38 2d  |\"a441d47b-6258-|
              00000120  34 38 61 37 2d 62 34 63  65 2d 65 64 31 33 36 65  |48a7-b4ce-ed136e|
              00000130  65 35 31 30 35 37 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |e51057\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 72 65 70  |"f:spec":{"f:rep|
              00000150  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 73 65 6c  |licas":{},"f:sel|
              00000160  65 63 74 6f 72 22 3a 7b  7d 2c 22 66 3a 74 65 6d  |ector":{},"f:tem|
              00000170  70 6c 61 74 65 22 3a 7b  22 66 3a 6d 65 74 61 64  |plate":{"f:metad|
              00000180  61 74 61 22 3a 7b 22 66  3a 6c 61 62 65 6c 73 22  |ata":{"f:labels"|
              00000190  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6e 61 6d 65  |:{".":{},"f:name|
              000001a0  22 3a 7b 7d 2c 22 66 3a  70 6f 64 2d 74 65 6d 70  |":{},"f:pod-temp|
              000001b0  6c 61 74 65 2d 68 61 73  68 22 3a 7b 7d 7d 7d 2c  |late-hash":{}}},|
              000001c0  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 63 6f 6e  |"f:spec":{"f:con|
              000001d0  74 61 69 6e 65 72 73 22  3a 7b 22 6b 3a 7b 5c 22  |tainers":{"k:{\"|
              000001e0  6e 61 6d 65 5c 22 3a 5c  22 68 74 74 70 64 5c 22  |name\":\"httpd\"|
              000001f0  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000200  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000210  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000220  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              00000230  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              00000240  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000250  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000260  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              00000270  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000280  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              00000290  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              000002a0  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              000002b0  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              000002c0  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              000002d0  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              000002e0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              000002f0  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000300  63 6f 6e 64 73 22 3a 7b  7d 7d 7d 7d 7d           |conds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(20),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=5) "httpd",
          (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=5) "httpd",
            (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 8,
      FullyLabeledReplicas: (int32) 8,
      ReadyReplicas: (int32) 8,
      AvailableReplicas: (int32) 8,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 16:32:15.271894 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-827r7" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-827r7",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "2a42042f-1cc4-4ea7-8a7d-7cf9e6b15c83",
      ResourceVersion: (string) (len=5) "63992",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 38 2e  31 39 35 5c 22 7d 22 3a  |.233.68.195\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-wpbgs",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-wpbgs",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) (len=13) "10.233.68.195",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.68.195"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664329,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://0f72d2dc74ff431599ee1f4be3e3a32a6c507c8a97f387c55afe21d4e0994908",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-wpbgs",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.278093 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-9s7l6" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-9s7l6",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "7b288385-da9a-4169-922d-95192bc6ab72",
      ResourceVersion: (string) (len=5) "64018",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664329,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664331,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 37 30 2e  31 31 35 5c 22 7d 22 3a  |.233.70.115\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-9z4km",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-9z4km",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-2",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664331,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664331,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664331,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.77",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.77"
        }
      },
      PodIP: (string) (len=13) "10.233.70.115",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.70.115"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664329,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://c97f9f02d701212017f129d2d7577af1020eee6e4ff7fc48d9772112a0a8dfbf",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-9z4km",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.283746 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-bgcr2" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-bgcr2",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "ec5f24fb-290a-4b9c-971c-055f5fb06cf7",
      ResourceVersion: (string) (len=5) "63979",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 37 2e  31 31 36 5c 22 7d 22 3a  |.233.67.116\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-h4w9t",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-h4w9t",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-3",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.78",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.78"
        }
      },
      PodIP: (string) (len=13) "10.233.67.116",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.67.116"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664329,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://4ec666bc2112b545b7c052547501554ececc4dcf81b2c96f590038fbe6415569",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-h4w9t",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.290829 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-bx2zf" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-bx2zf",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "eabd85dd-cc33-4e2e-9491-83b6448643e9",
      ResourceVersion: (string) (len=5) "64122",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664335,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-dznsh",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-dznsh",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=1) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) "",
      HostIPs: ([]v1.HostIP) <nil>,
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)(<nil>),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) <nil>,
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.295518 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-hwv8h" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-hwv8h",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "7052c4e1-4fce-4965-b018-ae7d6071b8fd",
      ResourceVersion: (string) (len=5) "64124",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664335,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-8r9vc",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-8r9vc",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-3",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=1) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) "",
      HostIPs: ([]v1.HostIP) <nil>,
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)(<nil>),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) <nil>,
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.299493 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-pck9b" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-pck9b",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "cec744b3-a2a2-49c4-b584-7e58d3213363",
      ResourceVersion: (string) (len=5) "64006",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 39 2e  31 30 30 5c 22 7d 22 3a  |.233.69.100\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-98lpk",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-98lpk",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) (len=13) "10.233.69.100",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.69.100"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://2d179e0f8313a96cf643ecf70b7840d5ec390d77b8432654fe28de47a2edfe23",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-98lpk",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.306263 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-sn45n" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-sn45n",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "b96dacd2-202c-4818-b86b-480e90ec3a3b",
      ResourceVersion: (string) (len=5) "63972",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 37 30 2e  31 31 34 5c 22 7d 22 3a  |.233.70.114\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-wkjs6",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-wkjs6",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-2",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.77",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.77"
        }
      },
      PodIP: (string) (len=13) "10.233.70.114",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.70.114"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664329,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://7d85695d7f9a2cddaec29f960936afc4789e0f24c40ec43be5bb1a173a8b1daa",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-wkjs6",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.312771 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-stdck" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-stdck",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "71381740-533d-4f39-b384-7b8614b8afc2",
      ResourceVersion: (string) (len=5) "63995",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 38 2e  31 39 34 5c 22 7d 22 3a  |.233.68.194\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-n8lkx",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-n8lkx",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) (len=13) "10.233.68.194",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.68.194"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://50fa6268f8c5841e949930d424289cec0f6d69e73bcb1df64dffbb0b103309f6",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-n8lkx",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  E0512 16:32:15.317937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:15.318468 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-vf4pr" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-vf4pr",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "20867ac5-e982-4fc3-bed5-828b802044d6",
      ResourceVersion: (string) (len=5) "63982",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 37 2e  31 31 35 5c 22 7d 22 3a  |.233.67.115\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-splvq",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-splvq",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-3",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664328,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.78",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.78"
        }
      },
      PodIP: (string) (len=13) "10.233.67.115",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.67.115"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664328,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://a8a1b5b20d7f80848b9351179b3d749ae887472ab868c05a31ca9078e38c3b6d",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-splvq",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.325234 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-vrqp5" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-vrqp5",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "36b15d8a-c47e-47ff-afb0-512b53022092",
      ResourceVersion: (string) (len=5) "63989",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664329,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 38 2e  31 39 36 5c 22 7d 22 3a  |.233.68.196\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-gsj89",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-gsj89",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664330,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664329,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) (len=13) "10.233.68.196",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.68.196"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664329,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664330,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://659286010fad1bcdff25cd769a34338cc7d85beeceb4824c36d28654f9a2d43d",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-gsj89",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.330381 23 deployment.go:67] Pod "webserver-deployment-64bcfc6446-w8v4b" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-64bcfc6446-w8v4b",
      GenerateName: (string) (len=32) "webserver-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "5deef1d3-0bec-4fff-80bf-1773e2cfca42",
      ResourceVersion: (string) (len=5) "64113",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664335,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "5e917b40-0ba9-4979-a089-4e963268e20f",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 35 65  39 31 37 62 34 30 2d 30  |d\":\"5e917b40-0|
              00000090  62 61 39 2d 34 39 37 39  2d 61 30 38 39 2d 34 65  |ba9-4979-a089-4e|
              000000a0  39 36 33 32 36 38 65 32  30 66 5c 22 7d 22 3a 7b  |963268e20f\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-7qsql",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-7qsql",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=1) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) "",
      HostIPs: ([]v1.HostIP) <nil>,
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)(<nil>),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) <nil>,
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.333935 23 deployment.go:67] Pod "webserver-deployment-786f49d774-492gd" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-492gd",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "b651c984-290a-4e5f-b97d-4d3e504af613",
      ResourceVersion: (string) (len=5) "64056",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=624) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 6d 65 73 73 61 67  |me":{},"f:messag|
              00000080  65 22 3a 7b 7d 2c 22 66  3a 72 65 61 73 6f 6e 22  |e":{},"f:reason"|
              00000090  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              000000a0  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000000b0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 49 6e  |k:{\"type\":\"In|
              000000c0  69 74 69 61 6c 69 7a 65  64 5c 22 7d 22 3a 7b 22  |itialized\"}":{"|
              000000d0  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 50 72 6f  |.":{},"f:lastPro|
              000000e0  62 65 54 69 6d 65 22 3a  7b 7d 2c 22 66 3a 6c 61  |beTime":{},"f:la|
              000000f0  73 74 54 72 61 6e 73 69  74 69 6f 6e 54 69 6d 65  |stTransitionTime|
              00000100  22 3a 7b 7d 2c 22 66 3a  73 74 61 74 75 73 22 3a  |":{},"f:status":|
              00000110  7b 7d 2c 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |{},"f:type":{}},|
              00000120  22 6b 3a 7b 5c 22 74 79  70 65 5c 22 3a 5c 22 50  |"k:{\"type\":\"P|
              00000130  6f 64 52 65 61 64 79 54  6f 53 74 61 72 74 43 6f  |odReadyToStartCo|
              00000140  6e 74 61 69 6e 65 72 73  5c 22 7d 22 3a 7b 22 2e  |ntainers\"}":{".|
              00000150  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 50 72 6f 62  |":{},"f:lastProb|
              00000160  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |eTime":{},"f:las|
              00000170  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              00000180  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              00000190  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000001a0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 52 65  |k:{\"type\":\"Re|
              000001b0  61 64 79 5c 22 7d 22 3a  7b 22 2e 22 3a 7b 7d 2c  |ady\"}":{".":{},|
              000001c0  22 66 3a 6c 61 73 74 50  72 6f 62 65 54 69 6d 65  |"f:lastProbeTime|
              000001d0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000001e0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000001f0  66 3a 6d 65 73 73 61 67  65 22 3a 7b 7d 2c 22 66  |f:message":{},"f|
              00000200  3a 72 65 61 73 6f 6e 22  3a 7b 7d 2c 22 66 3a 73  |:reason":{},"f:s|
              00000210  74 61 74 75 73 22 3a 7b  7d 2c 22 66 3a 74 79 70  |tatus":{},"f:typ|
              00000220  65 22 3a 7b 7d 7d 7d 2c  22 66 3a 63 6f 6e 74 61  |e":{}}},"f:conta|
              00000230  69 6e 65 72 53 74 61 74  75 73 65 73 22 3a 7b 7d  |inerStatuses":{}|
              00000240  2c 22 66 3a 68 6f 73 74  49 50 22 3a 7b 7d 2c 22  |,"f:hostIP":{},"|
              00000250  66 3a 68 6f 73 74 49 50  73 22 3a 7b 7d 2c 22 66  |f:hostIPs":{},"f|
              00000260  3a 73 74 61 72 74 54 69  6d 65 22 3a 7b 7d 7d 7d  |:startTime":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-d2xkw",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-d2xkw",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)({
              Reason: (string) (len=17) "ContainerCreating",
              Message: (string) ""
            }),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) false,
          RestartCount: (int32) 0,
          Image: (string) (len=13) "webserver:404",
          ImageID: (string) "",
          ContainerID: (string) "",
          Started: (*bool)(false),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-d2xkw",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.339880 23 deployment.go:67] Pod "webserver-deployment-786f49d774-5hn76" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-5hn76",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "b582a1e2-5e09-40a2-ade5-f4f334c56745",
      ResourceVersion: (string) (len=5) "64047",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664332,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664332,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=624) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 6d 65 73 73 61 67  |me":{},"f:messag|
              00000080  65 22 3a 7b 7d 2c 22 66  3a 72 65 61 73 6f 6e 22  |e":{},"f:reason"|
              00000090  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              000000a0  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000000b0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 49 6e  |k:{\"type\":\"In|
              000000c0  69 74 69 61 6c 69 7a 65  64 5c 22 7d 22 3a 7b 22  |itialized\"}":{"|
              000000d0  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 50 72 6f  |.":{},"f:lastPro|
              000000e0  62 65 54 69 6d 65 22 3a  7b 7d 2c 22 66 3a 6c 61  |beTime":{},"f:la|
              000000f0  73 74 54 72 61 6e 73 69  74 69 6f 6e 54 69 6d 65  |stTransitionTime|
              00000100  22 3a 7b 7d 2c 22 66 3a  73 74 61 74 75 73 22 3a  |":{},"f:status":|
              00000110  7b 7d 2c 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |{},"f:type":{}},|
              00000120  22 6b 3a 7b 5c 22 74 79  70 65 5c 22 3a 5c 22 50  |"k:{\"type\":\"P|
              00000130  6f 64 52 65 61 64 79 54  6f 53 74 61 72 74 43 6f  |odReadyToStartCo|
              00000140  6e 74 61 69 6e 65 72 73  5c 22 7d 22 3a 7b 22 2e  |ntainers\"}":{".|
              00000150  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 50 72 6f 62  |":{},"f:lastProb|
              00000160  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |eTime":{},"f:las|
              00000170  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              00000180  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              00000190  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000001a0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 52 65  |k:{\"type\":\"Re|
              000001b0  61 64 79 5c 22 7d 22 3a  7b 22 2e 22 3a 7b 7d 2c  |ady\"}":{".":{},|
              000001c0  22 66 3a 6c 61 73 74 50  72 6f 62 65 54 69 6d 65  |"f:lastProbeTime|
              000001d0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000001e0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000001f0  66 3a 6d 65 73 73 61 67  65 22 3a 7b 7d 2c 22 66  |f:message":{},"f|
              00000200  3a 72 65 61 73 6f 6e 22  3a 7b 7d 2c 22 66 3a 73  |:reason":{},"f:s|
              00000210  74 61 74 75 73 22 3a 7b  7d 2c 22 66 3a 74 79 70  |tatus":{},"f:typ|
              00000220  65 22 3a 7b 7d 7d 7d 2c  22 66 3a 63 6f 6e 74 61  |e":{}}},"f:conta|
              00000230  69 6e 65 72 53 74 61 74  75 73 65 73 22 3a 7b 7d  |inerStatuses":{}|
              00000240  2c 22 66 3a 68 6f 73 74  49 50 22 3a 7b 7d 2c 22  |,"f:hostIP":{},"|
              00000250  66 3a 68 6f 73 74 49 50  73 22 3a 7b 7d 2c 22 66  |f:hostIPs":{},"f|
              00000260  3a 73 74 61 72 74 54 69  6d 65 22 3a 7b 7d 7d 7d  |:startTime":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-zzw9c",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-zzw9c",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)({
              Reason: (string) (len=17) "ContainerCreating",
              Message: (string) ""
            }),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) false,
          RestartCount: (int32) 0,
          Image: (string) (len=13) "webserver:404",
          ImageID: (string) "",
          ContainerID: (string) "",
          Started: (*bool)(false),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-zzw9c",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.347230 23 deployment.go:67] Pod "webserver-deployment-786f49d774-5v7n5" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-5v7n5",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "95621809-27c2-47e7-b462-fdcfb9ebec31",
      ResourceVersion: (string) (len=5) "64080",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=624) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 6d 65 73 73 61 67  |me":{},"f:messag|
              00000080  65 22 3a 7b 7d 2c 22 66  3a 72 65 61 73 6f 6e 22  |e":{},"f:reason"|
              00000090  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              000000a0  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000000b0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 49 6e  |k:{\"type\":\"In|
              000000c0  69 74 69 61 6c 69 7a 65  64 5c 22 7d 22 3a 7b 22  |itialized\"}":{"|
              000000d0  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 50 72 6f  |.":{},"f:lastPro|
              000000e0  62 65 54 69 6d 65 22 3a  7b 7d 2c 22 66 3a 6c 61  |beTime":{},"f:la|
              000000f0  73 74 54 72 61 6e 73 69  74 69 6f 6e 54 69 6d 65  |stTransitionTime|
              00000100  22 3a 7b 7d 2c 22 66 3a  73 74 61 74 75 73 22 3a  |":{},"f:status":|
              00000110  7b 7d 2c 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |{},"f:type":{}},|
              00000120  22 6b 3a 7b 5c 22 74 79  70 65 5c 22 3a 5c 22 50  |"k:{\"type\":\"P|
              00000130  6f 64 52 65 61 64 79 54  6f 53 74 61 72 74 43 6f  |odReadyToStartCo|
              00000140  6e 74 61 69 6e 65 72 73  5c 22 7d 22 3a 7b 22 2e  |ntainers\"}":{".|
              00000150  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 50 72 6f 62  |":{},"f:lastProb|
              00000160  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |eTime":{},"f:las|
              00000170  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              00000180  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              00000190  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000001a0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 52 65  |k:{\"type\":\"Re|
              000001b0  61 64 79 5c 22 7d 22 3a  7b 22 2e 22 3a 7b 7d 2c  |ady\"}":{".":{},|
              000001c0  22 66 3a 6c 61 73 74 50  72 6f 62 65 54 69 6d 65  |"f:lastProbeTime|
              000001d0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000001e0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000001f0  66 3a 6d 65 73 73 61 67  65 22 3a 7b 7d 2c 22 66  |f:message":{},"f|
              00000200  3a 72 65 61 73 6f 6e 22  3a 7b 7d 2c 22 66 3a 73  |:reason":{},"f:s|
              00000210  74 61 74 75 73 22 3a 7b  7d 2c 22 66 3a 74 79 70  |tatus":{},"f:typ|
              00000220  65 22 3a 7b 7d 7d 7d 2c  22 66 3a 63 6f 6e 74 61  |e":{}}},"f:conta|
              00000230  69 6e 65 72 53 74 61 74  75 73 65 73 22 3a 7b 7d  |inerStatuses":{}|
              00000240  2c 22 66 3a 68 6f 73 74  49 50 22 3a 7b 7d 2c 22  |,"f:hostIP":{},"|
              00000250  66 3a 68 6f 73 74 49 50  73 22 3a 7b 7d 2c 22 66  |f:hostIPs":{},"f|
              00000260  3a 73 74 61 72 74 54 69  6d 65 22 3a 7b 7d 7d 7d  |:startTime":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-kkgkr",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-kkgkr",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-3",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.78",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.78"
        }
      },
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)({
              Reason: (string) (len=17) "ContainerCreating",
              Message: (string) ""
            }),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) false,
          RestartCount: (int32) 0,
          Image: (string) (len=13) "webserver:404",
          ImageID: (string) "",
          ContainerID: (string) "",
          Started: (*bool)(false),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-kkgkr",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.353400 23 deployment.go:67] Pod "webserver-deployment-786f49d774-g5nht" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-g5nht",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "d336916f-7e01-41ba-a137-7af685b08482",
      ResourceVersion: (string) (len=5) "64081",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=624) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 6d 65 73 73 61 67  |me":{},"f:messag|
              00000080  65 22 3a 7b 7d 2c 22 66  3a 72 65 61 73 6f 6e 22  |e":{},"f:reason"|
              00000090  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              000000a0  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000000b0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 49 6e  |k:{\"type\":\"In|
              000000c0  69 74 69 61 6c 69 7a 65  64 5c 22 7d 22 3a 7b 22  |itialized\"}":{"|
              000000d0  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 50 72 6f  |.":{},"f:lastPro|
              000000e0  62 65 54 69 6d 65 22 3a  7b 7d 2c 22 66 3a 6c 61  |beTime":{},"f:la|
              000000f0  73 74 54 72 61 6e 73 69  74 69 6f 6e 54 69 6d 65  |stTransitionTime|
              00000100  22 3a 7b 7d 2c 22 66 3a  73 74 61 74 75 73 22 3a  |":{},"f:status":|
              00000110  7b 7d 2c 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |{},"f:type":{}},|
              00000120  22 6b 3a 7b 5c 22 74 79  70 65 5c 22 3a 5c 22 50  |"k:{\"type\":\"P|
              00000130  6f 64 52 65 61 64 79 54  6f 53 74 61 72 74 43 6f  |odReadyToStartCo|
              00000140  6e 74 61 69 6e 65 72 73  5c 22 7d 22 3a 7b 22 2e  |ntainers\"}":{".|
              00000150  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 50 72 6f 62  |":{},"f:lastProb|
              00000160  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |eTime":{},"f:las|
              00000170  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              00000180  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              00000190  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000001a0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 52 65  |k:{\"type\":\"Re|
              000001b0  61 64 79 5c 22 7d 22 3a  7b 22 2e 22 3a 7b 7d 2c  |ady\"}":{".":{},|
              000001c0  22 66 3a 6c 61 73 74 50  72 6f 62 65 54 69 6d 65  |"f:lastProbeTime|
              000001d0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000001e0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000001f0  66 3a 6d 65 73 73 61 67  65 22 3a 7b 7d 2c 22 66  |f:message":{},"f|
              00000200  3a 72 65 61 73 6f 6e 22  3a 7b 7d 2c 22 66 3a 73  |:reason":{},"f:s|
              00000210  74 61 74 75 73 22 3a 7b  7d 2c 22 66 3a 74 79 70  |tatus":{},"f:typ|
              00000220  65 22 3a 7b 7d 7d 7d 2c  22 66 3a 63 6f 6e 74 61  |e":{}}},"f:conta|
              00000230  69 6e 65 72 53 74 61 74  75 73 65 73 22 3a 7b 7d  |inerStatuses":{}|
              00000240  2c 22 66 3a 68 6f 73 74  49 50 22 3a 7b 7d 2c 22  |,"f:hostIP":{},"|
              00000250  66 3a 68 6f 73 74 49 50  73 22 3a 7b 7d 2c 22 66  |f:hostIPs":{},"f|
              00000260  3a 73 74 61 72 74 54 69  6d 65 22 3a 7b 7d 7d 7d  |:startTime":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-g55wt",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-g55wt",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)({
              Reason: (string) (len=17) "ContainerCreating",
              Message: (string) ""
            }),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) false,
          RestartCount: (int32) 0,
          Image: (string) (len=13) "webserver:404",
          ImageID: (string) "",
          ContainerID: (string) "",
          Started: (*bool)(false),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-g55wt",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.368846 23 deployment.go:67] Pod "webserver-deployment-786f49d774-mr7tl" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-mr7tl",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "f9f7cd1e-e5f4-49c2-a089-fe7af2730c36",
      ResourceVersion: (string) (len=5) "64125",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664335,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-v2jmt",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-v2jmt",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-3",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=1) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) "",
      HostIPs: ([]v1.HostIP) <nil>,
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)(<nil>),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) <nil>,
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.376927 23 deployment.go:67] Pod "webserver-deployment-786f49d774-mvh55" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-mvh55",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "97e6781d-f75a-43a0-9eda-0ba6e71cb1e9",
      ResourceVersion: (string) (len=5) "64119",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664335,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-bzdjj",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-bzdjj",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=1) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) "",
      HostIPs: ([]v1.HostIP) <nil>,
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)(<nil>),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) <nil>,
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.381868 23 deployment.go:67] Pod "webserver-deployment-786f49d774-tvjz7" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-tvjz7",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "1a22ba21-7587-489d-af3f-e41b54f9155d",
      ResourceVersion: (string) (len=5) "64054",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=624) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 6d 65 73 73 61 67  |me":{},"f:messag|
              00000080  65 22 3a 7b 7d 2c 22 66  3a 72 65 61 73 6f 6e 22  |e":{},"f:reason"|
              00000090  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              000000a0  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000000b0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 49 6e  |k:{\"type\":\"In|
              000000c0  69 74 69 61 6c 69 7a 65  64 5c 22 7d 22 3a 7b 22  |itialized\"}":{"|
              000000d0  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 50 72 6f  |.":{},"f:lastPro|
              000000e0  62 65 54 69 6d 65 22 3a  7b 7d 2c 22 66 3a 6c 61  |beTime":{},"f:la|
              000000f0  73 74 54 72 61 6e 73 69  74 69 6f 6e 54 69 6d 65  |stTransitionTime|
              00000100  22 3a 7b 7d 2c 22 66 3a  73 74 61 74 75 73 22 3a  |":{},"f:status":|
              00000110  7b 7d 2c 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |{},"f:type":{}},|
              00000120  22 6b 3a 7b 5c 22 74 79  70 65 5c 22 3a 5c 22 50  |"k:{\"type\":\"P|
              00000130  6f 64 52 65 61 64 79 54  6f 53 74 61 72 74 43 6f  |odReadyToStartCo|
              00000140  6e 74 61 69 6e 65 72 73  5c 22 7d 22 3a 7b 22 2e  |ntainers\"}":{".|
              00000150  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 50 72 6f 62  |":{},"f:lastProb|
              00000160  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |eTime":{},"f:las|
              00000170  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              00000180  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              00000190  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000001a0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 52 65  |k:{\"type\":\"Re|
              000001b0  61 64 79 5c 22 7d 22 3a  7b 22 2e 22 3a 7b 7d 2c  |ady\"}":{".":{},|
              000001c0  22 66 3a 6c 61 73 74 50  72 6f 62 65 54 69 6d 65  |"f:lastProbeTime|
              000001d0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000001e0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000001f0  66 3a 6d 65 73 73 61 67  65 22 3a 7b 7d 2c 22 66  |f:message":{},"f|
              00000200  3a 72 65 61 73 6f 6e 22  3a 7b 7d 2c 22 66 3a 73  |:reason":{},"f:s|
              00000210  74 61 74 75 73 22 3a 7b  7d 2c 22 66 3a 74 79 70  |tatus":{},"f:typ|
              00000220  65 22 3a 7b 7d 7d 7d 2c  22 66 3a 63 6f 6e 74 61  |e":{}}},"f:conta|
              00000230  69 6e 65 72 53 74 61 74  75 73 65 73 22 3a 7b 7d  |inerStatuses":{}|
              00000240  2c 22 66 3a 68 6f 73 74  49 50 22 3a 7b 7d 2c 22  |,"f:hostIP":{},"|
              00000250  66 3a 68 6f 73 74 49 50  73 22 3a 7b 7d 2c 22 66  |f:hostIPs":{},"f|
              00000260  3a 73 74 61 72 74 54 69  6d 65 22 3a 7b 7d 7d 7d  |:startTime":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-vfx5b",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-vfx5b",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-2",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664333,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.77",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.77"
        }
      },
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664333,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)({
              Reason: (string) (len=17) "ContainerCreating",
              Message: (string) ""
            }),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) false,
          RestartCount: (int32) 0,
          Image: (string) (len=13) "webserver:404",
          ImageID: (string) "",
          ContainerID: (string) "",
          Started: (*bool)(false),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-vfx5b",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.388207 23 deployment.go:67] Pod "webserver-deployment-786f49d774-wkj9t" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "webserver-deployment-786f49d774-wkj9t",
      GenerateName: (string) (len=32) "webserver-deployment-786f49d774-",
      Namespace: (string) (len=15) "deployment-1092",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "3056f45a-0001-412c-a622-61514251d7fc",
      ResourceVersion: (string) (len=5) "64126",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664335,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "786f49d774"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "webserver-deployment-786f49d774",
          UID: (types.UID) (len=36) "a1e65a56-5498-420b-aa33-b731d1bacb74",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=1) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 61 31  65 36 35 61 35 36 2d 35  |d\":\"a1e65a56-5|
              00000090  34 39 38 2d 34 32 30 62  2d 61 61 33 33 2d 62 37  |498-420b-aa33-b7|
              000000a0  33 31 64 31 62 61 63 62  37 34 5c 22 7d 22 3a 7b  |31d1bacb74\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-fn54r",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=13) "webserver:404",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-fn54r",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-2",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=1) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664335,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) "",
      HostIPs: ([]v1.HostIP) <nil>,
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)(<nil>),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) <nil>,
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:32:15.391975 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1092" for this suite. @ 05/12/25 16:32:15.464
• [6.727 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:270
  STEP: Creating a kubernetes client @ 05/12/25 16:32:15.504
  I0512 16:32:15.504453 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:32:15.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:32:15.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:32:15.7
  STEP: Setting up server cert @ 05/12/25 16:32:15.844
  E0512 16:32:16.319476      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:17.319897      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:18.320887      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:32:18.609
  STEP: Deploying the webhook pod @ 05/12/25 16:32:18.623
  STEP: Wait for the deployment to be ready @ 05/12/25 16:32:18.643
  I0512 16:32:18.659578 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:32:19.321357      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:20.321997      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:32:20.69
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:32:20.718
  E0512 16:32:21.322454      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:21.719113 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/12/25 16:32:21.741
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/12/25 16:32:21.785
  STEP: Creating a dummy validating-webhook-configuration object @ 05/12/25 16:32:21.82
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/12/25 16:32:21.868
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/12/25 16:32:21.891
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/12/25 16:32:21.918
  I0512 16:32:22.114577 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8887" for this suite. @ 05/12/25 16:32:22.159
  STEP: Destroying namespace "webhook-markers-2356" for this suite. @ 05/12/25 16:32:22.188
• [6.700 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:1034
  STEP: Creating a kubernetes client @ 05/12/25 16:32:22.21
  I0512 16:32:22.210734 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 16:32:22.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:32:22.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:32:22.292
  STEP: Creating service test in namespace statefulset-4021 @ 05/12/25 16:32:22.302
  E0512 16:32:22.322935      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating statefulset ss in namespace statefulset-4021 @ 05/12/25 16:32:22.341
  I0512 16:32:22.362625 23 wait.go:40] Found 0 stateful pods, waiting for 1
  E0512 16:32:23.323305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:24.323695      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:25.324471      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:26.324733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:27.324810      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:28.325149      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:29.325938      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:30.326823      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:31.327239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:32.327652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:32.366176 23 wait.go:50] Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/12/25 16:32:32.379
  STEP: Getting /status @ 05/12/25 16:32:32.409
  I0512 16:32:32.418449 23 statefulset.go:1070] StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/12/25 16:32:32.418
  I0512 16:32:32.440132 23 statefulset.go:1090] updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/12/25 16:32:32.44
  I0512 16:32:32.443900 23 statefulset.go:1118] Observed &StatefulSet event: ADDED
  I0512 16:32:32.443988 23 statefulset.go:1111] Found Statefulset ss in namespace statefulset-4021 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  I0512 16:32:32.444018 23 statefulset.go:1122] Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/12/25 16:32:32.444
  I0512 16:32:32.444125 23 statefulset.go:1126] Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  I0512 16:32:32.459711 23 statefulset.go:1130] Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/12/25 16:32:32.459
  I0512 16:32:32.464121 23 statefulset.go:1155] Observed &StatefulSet event: ADDED
  I0512 16:32:32.464287 23 statefulset.go:138] Deleting all statefulset in ns statefulset-4021
  I0512 16:32:32.472129 23 rest.go:150] Scaling statefulset ss to 0
  E0512 16:32:33.327711      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:34.328167      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:35.329225      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:36.329625      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:37.332847      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:38.333960      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:39.334505      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:40.335434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:41.336018      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:42.336443      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:42.505699 23 wait.go:151] Waiting for statefulset status.replicas updated to 0
  I0512 16:32:42.513271 23 rest.go:88] Deleting statefulset ss
  I0512 16:32:42.556951 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4021" for this suite. @ 05/12/25 16:32:42.57
• [20.381 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:86
  STEP: Creating a kubernetes client @ 05/12/25 16:32:42.591
  I0512 16:32:42.592176 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:32:42.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:32:42.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:32:42.673
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 16:32:42.678
  E0512 16:32:43.336835      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:44.336994      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:45.337069      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:46.337206      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:32:46.723
  I0512 16:32:46.732341 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-15de5528-1b3b-4cc7-b8fc-62672de7e080 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 16:32:46.75
  I0512 16:32:46.792575 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3103" for this suite. @ 05/12/25 16:32:46.802
• [4.223 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/disruption.go:167
  STEP: Creating a kubernetes client @ 05/12/25 16:32:46.817
  I0512 16:32:46.817591 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename disruption @ 05/12/25 16:32:46.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:32:46.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:32:46.873
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:32:46.894
  E0512 16:32:47.338380      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:48.339135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 05/12/25 16:32:48.905
  STEP: Waiting for all pods to be running @ 05/12/25 16:32:48.927
  I0512 16:32:48.939974 23 disruption.go:691] running pods: 0 < 1
  E0512 16:32:49.340234      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:50.340616      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/12/25 16:32:50.938
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:32:50.965
  STEP: Patching PodDisruptionBudget status @ 05/12/25 16:32:50.982
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:32:50.998
  I0512 16:32:51.005109 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7585" for this suite. @ 05/12/25 16:32:51.017
• [4.214 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:174
  STEP: Creating a kubernetes client @ 05/12/25 16:32:51.033
  I0512 16:32:51.033774 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-webhook @ 05/12/25 16:32:51.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:32:51.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:32:51.078
  STEP: Setting up server cert @ 05/12/25 16:32:51.084
  E0512 16:32:51.340762      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/12/25 16:32:51.618
  STEP: Deploying the custom resource conversion webhook pod @ 05/12/25 16:32:51.635
  STEP: Wait for the deployment to be ready @ 05/12/25 16:32:51.657
  I0512 16:32:51.686688 23 deployment.go:222] deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0512 16:32:52.341649      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:53.342097      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:32:53.718
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:32:53.75
  E0512 16:32:54.342378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:32:54.751591 23 util.go:420] Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  I0512 16:32:54.764817 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:32:55.342778      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:56.344007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:57.344415      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:58.344906      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:32:59.345279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:00.346194      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:01.346380      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:02.347370      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/12/25 16:33:02.445
  STEP: Create a v2 custom resource @ 05/12/25 16:33:02.478
  STEP: List CRs in v1 @ 05/12/25 16:33:02.575
  STEP: List CRs in v2 @ 05/12/25 16:33:02.59
  I0512 16:33:03.324397 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 16:33:03.347857      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-6139" for this suite. @ 05/12/25 16:33:03.351
• [12.349 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:160
  STEP: Creating a kubernetes client @ 05/12/25 16:33:03.383
  I0512 16:33:03.383292 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 16:33:03.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:33:03.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:33:03.448
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/12/25 16:33:03.465
  E0512 16:33:04.348113      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:05.348329      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:06.349454      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:07.350002      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:33:07.527
  I0512 16:33:07.537501 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-cc0586b7-8fe6-4b07-bcec-f72eaad219c3 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 16:33:07.554
  I0512 16:33:07.589231 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6748" for this suite. @ 05/12/25 16:33:07.6
• [4.231 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance] [sig-node, Slow, Conformance]
k8s.io/kubernetes/test/e2e/common/node/expansion.go:301
  STEP: Creating a kubernetes client @ 05/12/25 16:33:07.615
  I0512 16:33:07.615719 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename var-expansion @ 05/12/25 16:33:07.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:33:07.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:33:07.678
  STEP: creating the pod @ 05/12/25 16:33:07.687
  STEP: waiting for pod running @ 05/12/25 16:33:07.715
  E0512 16:33:08.350302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:09.350477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 05/12/25 16:33:09.749
  I0512 16:33:09.757720 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1651 PodName:var-expansion-e40eb728-0db6-4caf-ae4f-f6ca6a7a5aa3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:33:09.758110 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:33:09.759375 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:33:09.759751 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1651/pods/var-expansion-e40eb728-0db6-4caf-ae4f-f6ca6a7a5aa3/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/12/25 16:33:09.898
  I0512 16:33:09.907665 23 exec_util.go:59] ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1651 PodName:var-expansion-e40eb728-0db6-4caf-ae4f-f6ca6a7a5aa3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  I0512 16:33:09.907804 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  I0512 16:33:09.909139 23 exec_util.go:66] ExecWithOptions: Clientset creation
  I0512 16:33:09.909283 23 exec_util.go:83] ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1651/pods/var-expansion-e40eb728-0db6-4caf-ae4f-f6ca6a7a5aa3/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/12/25 16:33:10.039
  E0512 16:33:10.350620      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:33:10.570771 23 pod_client.go:173] Successfully updated pod "var-expansion-e40eb728-0db6-4caf-ae4f-f6ca6a7a5aa3"
  STEP: waiting for annotated pod running @ 05/12/25 16:33:10.57
  STEP: deleting the pod gracefully @ 05/12/25 16:33:10.578
  I0512 16:33:10.579004 23 delete.go:62] Deleting pod "var-expansion-e40eb728-0db6-4caf-ae4f-f6ca6a7a5aa3" in namespace "var-expansion-1651"
  I0512 16:33:10.594790 23 delete.go:70] Wait up to 5m0s for pod "var-expansion-e40eb728-0db6-4caf-ae4f-f6ca6a7a5aa3" to be fully deleted
  E0512 16:33:11.351230      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:12.351682      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:13.352757      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:14.353202      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:15.353430      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:16.354690      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:17.354783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:18.355217      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:19.355462      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:20.355747      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:21.356883      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:22.356311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:23.357430      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:24.357694      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:25.357732      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:26.358304      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:27.358621      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:28.359056      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:29.359211      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:30.360181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:31.360338      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:32.360446      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:33.361395      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:34.362122      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:35.363135      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:36.363428      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:37.364351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:38.364727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:39.365746      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:40.366641      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:41.366733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:42.366953      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:33:42.761504 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1651" for this suite. @ 05/12/25 16:33:42.774
• [35.179 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1791
  STEP: Creating a kubernetes client @ 05/12/25 16:33:42.801
  I0512 16:33:42.802401 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:33:42.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:33:42.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:33:42.851
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/25 16:33:42.869
  I0512 16:33:42.869831 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-9715 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  I0512 16:33:43.072430 23 builder.go:146] stderr: ""
  I0512 16:33:43.072579 23 builder.go:147] stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/12/25 16:33:43.072
  E0512 16:33:43.367451      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:44.367755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:45.368031      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:46.368381      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:47.368494      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/12/25 16:33:48.124
  I0512 16:33:48.124206 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-9715 get pod e2e-test-httpd-pod -o json'
  I0512 16:33:48.295651 23 builder.go:146] stderr: ""
  I0512 16:33:48.295977 23 builder.go:147] stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2025-05-12T16:33:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9715\",\n        \"resourceVersion\": \"65134\",\n        \"uid\": \"66a19309-1dc8-40ba-867d-e06db9eb3821\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-8d585\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"opscontrol-jaku1-worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-8d585\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2025-05-12T16:33:44Z\",\n                \"status\": \"True\",\n                \"type\": \"PodReadyToStartContainers\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2025-05-12T16:33:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2025-05-12T16:33:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2025-05-12T16:33:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2025-05-12T16:33:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://3c5e8c501718aacd92a64205d90126cf68b168adec61592494676f173b580bc4\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2025-05-12T16:33:44Z\"\n                    }\n                },\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-8d585\",\n                        \"readOnly\": true,\n                        \"recursiveReadOnly\": \"Disabled\"\n                    }\n                ]\n            }\n        ],\n        \"hostIP\": \"10.62.16.76\",\n        \"hostIPs\": [\n            {\n                \"ip\": \"10.62.16.76\"\n            }\n        ],\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.69.115\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.69.115\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2025-05-12T16:33:43Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/12/25 16:33:48.296
  I0512 16:33:48.296161 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-9715 replace -f -'
  E0512 16:33:48.368626      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:33:48.608751 23 builder.go:146] stderr: ""
  I0512 16:33:48.608822 23 builder.go:147] stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.36.1-1 @ 05/12/25 16:33:48.608
  I0512 16:33:48.616633 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-9715 delete pods e2e-test-httpd-pod'
  E0512 16:33:49.368788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:50.369693      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:33:51.089121 23 builder.go:146] stderr: ""
  I0512 16:33:51.089186 23 builder.go:147] stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  I0512 16:33:51.089359 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9715" for this suite. @ 05/12/25 16:33:51.098
• [8.316 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:223
  STEP: Creating a kubernetes client @ 05/12/25 16:33:51.117
  I0512 16:33:51.117590 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:33:51.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:33:51.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:33:51.162
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 16:33:51.17
  E0512 16:33:51.369937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:52.370683      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:53.371807      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:54.372170      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:33:55.224
  I0512 16:33:55.234605 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-ce27813d-8d44-4271-b891-a600a185bd23 container client-container: <nil>
  STEP: delete the pod @ 05/12/25 16:33:55.25
  I0512 16:33:55.282301 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3457" for this suite. @ 05/12/25 16:33:55.294
• [4.191 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance] [sig-node, Serial, Disruptive, Conformance]
k8s.io/kubernetes/test/e2e/node/taints.go:284
  STEP: Creating a kubernetes client @ 05/12/25 16:33:55.308
  I0512 16:33:55.308590 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename taint-single-pod @ 05/12/25 16:33:55.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:33:55.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:33:55.361
  I0512 16:33:55.367536 23 wait.go:50] Waiting up to 1m0s for all nodes to be ready
  E0512 16:33:55.372752      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:56.373822      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:57.374158      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:58.374820      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:33:59.375258      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:00.376198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:01.376620      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:02.377666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:03.378183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:04.379156      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:05.379938      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:06.380156      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:07.380383      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:08.380828      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:09.381737      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:10.382260      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:11.382385      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:12.382623      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:13.383034      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:14.383999      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:15.384175      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:16.384565      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:17.385257      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:18.385924      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:19.386688      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:20.386884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:21.387046      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:22.387299      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:23.387805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:24.388068      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:25.388781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:26.389895      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:27.390165      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:28.390401      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:29.390783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:30.391491      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:31.391675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:32.391934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:33.392502      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:34.392423      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:35.392688      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:36.392849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:37.393130      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:38.394174      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:39.394613      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:40.395522      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:41.396305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:42.397040      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:43.397915      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:44.398820      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:45.399498      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:46.399928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:47.400863      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:48.401107      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:49.401666      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:50.402410      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:51.402975      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:52.404167      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:53.404790      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:54.405615      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:34:55.368356 23 util.go:393] Waiting for terminating namespaces to be deleted...
  I0512 16:34:55.379086 23 taints.go:144] Starting informer...
  STEP: Starting pod... @ 05/12/25 16:34:55.379
  E0512 16:34:55.406657      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:34:55.620634 23 taints.go:294] Pod is running on opscontrol-jaku1-worker-1. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/12/25 16:34:55.62
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/25 16:34:55.657
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/12/25 16:34:55.688
  I0512 16:34:55.689039 23 taints.go:313] Pod wasn't evicted. Proceeding
  I0512 16:34:55.689079 23 taints.go:320] Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/25 16:34:55.727
  STEP: Waiting some time to make sure that toleration time passed. @ 05/12/25 16:34:55.739
  E0512 16:34:56.407653      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:57.408335      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:58.408798      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:34:59.409180      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:00.410117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:01.410433      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:02.410880      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:03.411442      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:04.412270      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:05.412365      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:06.412877      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:07.413715      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:08.413972      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:09.414226      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:10.415093      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:11.415957      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:12.416163      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:13.416676      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:14.416855      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:15.417159      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:16.417361      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:17.417566      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:18.417813      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:19.418003      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:20.418293      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:21.418432      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:22.418672      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:23.418873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:24.419096      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:25.419387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:26.420208      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:27.420480      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:28.421095      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:29.422062      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:30.422830      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:31.423351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:32.423970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:33.424999      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:34.425567      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:35.425680      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:36.426067      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:37.426387      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:38.427250      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:39.427771      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:40.427782      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:41.428628      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:42.429409      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:43.429904      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:44.430719      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:45.430997      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:46.431806      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:47.432981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:48.433813      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:49.434493      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:50.434857      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:51.435727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:52.436316      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:53.436626      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:54.436798      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:55.437606      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:56.438286      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:57.439150      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:58.439966      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:35:59.440598      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:00.441334      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:01.442085      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:02.442721      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:03.442915      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:04.443440      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:05.443637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:06.444322      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:07.444482      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:08.445263      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:09.445913      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:10.446665      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:10.740415 23 taints.go:329] Pod wasn't evicted. Test successful
  I0512 16:36:10.741225 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-299" for this suite. @ 05/12/25 16:36:10.754
• [135.459 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/rc.go:104
  STEP: Creating a kubernetes client @ 05/12/25 16:36:10.767
  I0512 16:36:10.767828 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replication-controller @ 05/12/25 16:36:10.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:36:10.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:36:10.809
  STEP: Given a ReplicationController is created @ 05/12/25 16:36:10.814
  STEP: When the matched label of one of its pods change @ 05/12/25 16:36:10.826
  I0512 16:36:10.835699 23 resource.go:87] Pod name pod-release: Found 0 pods out of 1
  E0512 16:36:11.447182      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:12.447412      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:13.448076      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:14.448465      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:15.448785      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:15.848808 23 resource.go:87] Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/12/25 16:36:15.886
  E0512 16:36:16.448829      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:16.909283 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-82" for this suite. @ 05/12/25 16:36:16.923
• [6.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/container_probe.go:528
  STEP: Creating a kubernetes client @ 05/12/25 16:36:16.96
  I0512 16:36:16.960133 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-probe @ 05/12/25 16:36:16.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:36:17.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:36:17.021
  STEP: Creating pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839 @ 05/12/25 16:36:17.027
  E0512 16:36:17.452943      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:18.452945      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/25 16:36:19.068
  I0512 16:36:19.076125 23 container_probe.go:1749] Initial restart count of pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 is 0
  I0512 16:36:19.083216 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:19.453673      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:20.454744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:21.095314 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:21.455189      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:22.455691      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:23.112948 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:23.456777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:24.456759      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:25.121599 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:25.456808      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:26.457253      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:27.133480 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:27.457993      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:28.458609      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:29.142649 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:29.459350      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:30.459485      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:31.152853 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:31.460240      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:32.460512      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:33.163557 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:33.460905      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:34.461219      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:35.173216 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:35.461264      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:36.461846      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:37.184967 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:37.462166      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:38.462628      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:39.194699 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:39.462899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:40.463299      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:41.205065 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:41.463468      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:42.464311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:43.214984 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:43.465333      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:44.465559      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:45.225150 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:45.466030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:46.466294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:47.234097 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:47.467186      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:48.466797      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:49.242433 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:49.467335      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:50.468093      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:51.250739 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:51.468452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:52.468829      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:53.262119 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:53.470125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:54.470692      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:55.271624 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:55.471223      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:56.471391      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:57.287475 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:57.472876      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:36:58.473483      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:36:59.296559 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:36:59.473835      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:00.474850      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:01.307048 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:01.475330      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:02.475569      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:03.314639 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:03.476055      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:04.476551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:05.325002 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:05.477530      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:06.477902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:07.340603 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:07.479003      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:08.479531      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:09.349240 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:09.479977      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:10.480756      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:11.356824 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:11.481367      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:12.481587      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:13.367719 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:13.482546      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:14.482817      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:15.377554 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:15.483386      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:16.483728      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:17.388976 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:17.484084      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:18.484450      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:19.398127 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:19.485078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:20.486088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:21.407617 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:21.486306      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:22.486326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:23.416392 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:23.487021      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:24.487240      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:25.425050 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:25.487300      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:26.487659      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:27.436316 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:27.488311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:28.488679      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:29.444941 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:29.489477      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:30.490327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:31.456420 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:31.490809      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:32.491230      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:33.467262 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:33.491472      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:34.491565      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:35.478041 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:35.492326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:36.492665      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:37.487563 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:37.493013      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:38.493383      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:39.493956      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:39.497775 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:40.494530      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:41.494889      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:41.506605 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:42.495464      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:43.495995      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:43.517697 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:44.496662      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:45.496911      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:45.527246 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:46.497208      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:47.497409      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:47.537388 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:48.497777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:49.498351      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:49.546301 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:50.499232      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:51.499712      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:51.556291 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:52.499985      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:53.500305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:53.565813 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:54.500870      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:55.500980      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:55.575539 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:56.501302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:57.502072      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:57.585909 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:37:58.502392      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:37:59.502545      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:37:59.595318 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:00.502946      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:01.503136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:01.603958 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:02.503327      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:03.504420      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:03.612376 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:04.504738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:05.504975      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:05.626970 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:06.505112      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:07.505416      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:07.633994 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:08.505646      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:09.505847      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:09.641181 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:10.506084      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:11.506325      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:11.649881 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:12.506438      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:13.506702      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:13.659542 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:14.506864      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:15.507110      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:15.670966 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:16.507302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:17.507523      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:17.682975 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:18.507955      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:19.508057      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:19.693565 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:20.508944      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:21.509980      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:21.704421 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:22.510267      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:23.510615      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:23.717286 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:24.511125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:25.511388      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:25.731407 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:26.511547      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:27.511901      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:27.740625 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:28.512244      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:29.512472      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:29.748877 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:30.512873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:31.512918      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:31.756014 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:32.513131      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:33.513685      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:33.765427 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:34.513819      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:35.514128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:35.774418 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:36.515015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:37.515510      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:37.785768 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:38.515716      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:39.516183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:39.793255 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:40.516966      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:41.517446      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:41.802267 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:42.518133      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:43.518731      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:43.809869 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:44.518928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:45.519716      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:45.819454 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:46.519995      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:47.520601      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:47.827956 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:48.521067      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:49.520978      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:49.838553 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:50.522064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:51.522353      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:51.850828 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:52.523527      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:53.523898      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:53.862612 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:54.524349      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:55.524608      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:55.874974 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:56.525582      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:57.525981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:57.886101 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:38:58.527099      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:38:59.527711      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:38:59.896450 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:00.528359      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:01.528279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:01.905236 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:02.528831      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:03.529948      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:03.915932 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:04.530576      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:05.531259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:05.923980 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:06.531570      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:07.531733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:07.936172 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:08.531968      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:09.532398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:09.946854 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:10.533485      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:11.533936      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:11.957023 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:12.534085      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:13.534478      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:13.964400 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:14.535610      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:15.536464      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:15.974789 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:16.537614      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:17.538064      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:17.987889 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:18.538853      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:19.539296      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:19.999136 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:20.539576      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:21.539993      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:22.009710 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:22.540313      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:23.540799      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:24.020241 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:24.541970      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:25.542311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:26.030069 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:26.542692      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:27.543409      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:28.040348 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:28.543865      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:29.544297      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:30.049119 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:30.544208      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:31.544687      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:32.058347 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:32.544929      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:33.545458      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:34.070816 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:34.545491      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:35.546086      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:36.080962 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:36.546215      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:37.546481      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:38.092238 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:38.546893      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:39.547390      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:40.105012 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:40.547921      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:41.548647      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:42.117582 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:42.549647      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:43.550241      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:44.127292 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:44.550312      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:45.550896      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:46.140904 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:46.551030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:47.551229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:48.151839 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:48.552434      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:49.553337      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:50.165685 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:50.554382      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:51.554862      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:52.175860 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:52.555281      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:53.555988      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:54.185088 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:54.557186      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:55.558067      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:56.196470 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:56.558133      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:57.558350      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:39:58.205913 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:39:58.559305      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:39:59.559693      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:00.214654 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:00.560576      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:01.560914      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:02.222240 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:02.561715      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:03.561888      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:04.228213 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:04.562688      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:05.563392      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:06.236751 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:06.563501      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:07.563849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:08.245362 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:08.564896      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:09.565545      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:10.255997 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:10.566581      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:11.566700      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:12.264574 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:12.566964      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:13.567186      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:14.274561 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:14.567810      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:15.568087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:16.287623 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:16.568402      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:17.569128      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:18.296616 23 container_probe.go:1759] Get pod test-grpc-3eef8dac-4bac-4dc5-ba42-1ecf02b778e6 in namespace container-probe-5839
  E0512 16:40:18.569974      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:19.570771      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting the pod @ 05/12/25 16:40:20.296
  I0512 16:40:20.337392 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5839" for this suite. @ 05/12/25 16:40:20.351
• [243.424 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/field_validation.go:620
  STEP: Creating a kubernetes client @ 05/12/25 16:40:20.384
  I0512 16:40:20.384725 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename field-validation @ 05/12/25 16:40:20.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:40:20.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:40:20.438
  I0512 16:40:20.443147 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:40:20.571625      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:21.572178      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:22.573089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:23.573896      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:24.573877      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:25.574826      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:26.574759      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:27.574973      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  W0512 16:40:28.089949      23 warnings.go:70] unknown field "alpha"
  W0512 16:40:28.090047      23 warnings.go:70] unknown field "beta"
  W0512 16:40:28.090067      23 warnings.go:70] unknown field "delta"
  W0512 16:40:28.090085      23 warnings.go:70] unknown field "epsilon"
  W0512 16:40:28.090103      23 warnings.go:70] unknown field "gamma"
  E0512 16:40:28.575923      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:28.695264 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7683" for this suite. @ 05/12/25 16:40:28.71
• [8.341 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ValidatingAdmissionPolicy [Privileged:ClusterAdmin] should allow expressions to refer variables. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/validatingadmissionpolicy.go:221
  STEP: Creating a kubernetes client @ 05/12/25 16:40:28.726
  I0512 16:40:28.726276 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename validating-admission-policy @ 05/12/25 16:40:28.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:40:28.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:40:28.783
  STEP: creating a policy with variables @ 05/12/25 16:40:28.803
  STEP: waiting until the marker is denied @ 05/12/25 16:40:28.892
  STEP: testing a replicated Deployment to be allowed @ 05/12/25 16:40:29.309
  STEP: testing a non-replicated ReplicaSet not to be denied @ 05/12/25 16:40:29.344
  I0512 16:40:29.441388 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "validating-admission-policy-1450" for this suite. @ 05/12/25 16:40:29.471
• [0.773 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1075
  STEP: Creating a kubernetes client @ 05/12/25 16:40:29.499
  I0512 16:40:29.499928 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:40:29.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:40:29.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:40:29.567
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/25 16:40:29.574
  I0512 16:40:29.575156 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6533 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  E0512 16:40:29.576379      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:29.797110 23 builder.go:146] stderr: ""
  I0512 16:40:29.797191 23 builder.go:147] stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/12/25 16:40:29.797
  I0512 16:40:29.797411 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6533 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.36.1-1"}]}} --dry-run=server'
  I0512 16:40:30.004101 23 builder.go:146] stderr: ""
  I0512 16:40:30.004247 23 builder.go:147] stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/25 16:40:30.004
  I0512 16:40:30.012328 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-6533 delete pods e2e-test-httpd-pod'
  E0512 16:40:30.577126      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:31.578015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:32.578424      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:32.730518 23 builder.go:146] stderr: ""
  I0512 16:40:32.730607 23 builder.go:147] stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  I0512 16:40:32.730837 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6533" for this suite. @ 05/12/25 16:40:32.745
• [3.273 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/garbage_collector.go:480
  STEP: Creating a kubernetes client @ 05/12/25 16:40:32.772
  I0512 16:40:32.772323 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename gc @ 05/12/25 16:40:32.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:40:32.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:40:32.822
  STEP: create the deployment @ 05/12/25 16:40:32.827
  W0512 16:40:32.841132      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/12/25 16:40:32.841
  STEP: delete the deployment @ 05/12/25 16:40:33.355
  STEP: wait for all rs to be garbage collected @ 05/12/25 16:40:33.371
  STEP: expected 0 pods, got 2 pods @ 05/12/25 16:40:33.4
  E0512 16:40:33.582712      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/25 16:40:33.898
  I0512 16:40:34.140984 23 garbage_collector.go:265] For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  I0512 16:40:34.141306 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8658" for this suite. @ 05/12/25 16:40:34.15
• [1.393 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for services [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:139
  STEP: Creating a kubernetes client @ 05/12/25 16:40:34.165
  I0512 16:40:34.165556 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 16:40:34.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:40:34.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:40:34.204
  STEP: Creating a test headless service @ 05/12/25 16:40:34.209
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 210.6.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.6.210_udp@PTR;check="$$(dig +tcp +noall +answer +search 210.6.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.6.210_tcp@PTR;sleep 1; done
   @ 05/12/25 16:40:34.248
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 210.6.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.6.210_udp@PTR;check="$$(dig +tcp +noall +answer +search 210.6.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.6.210_tcp@PTR;sleep 1; done
   @ 05/12/25 16:40:34.248
  STEP: creating a pod to probe DNS @ 05/12/25 16:40:34.249
  STEP: submitting the pod to kubernetes @ 05/12/25 16:40:34.25
  E0512 16:40:34.583184      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:35.585044      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/25 16:40:36.295
  STEP: looking for the results for each expected name from probers @ 05/12/25 16:40:36.303
  I0512 16:40:36.313475 23 dns_common.go:478] Unable to read wheezy_udp@dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.325860 23 dns_common.go:478] Unable to read wheezy_tcp@dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.333781 23 dns_common.go:478] Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.345145 23 dns_common.go:478] Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.416159 23 dns_common.go:478] Unable to read jessie_udp@dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.428092 23 dns_common.go:478] Unable to read jessie_tcp@dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.436134 23 dns_common.go:478] Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.444410 23 dns_common.go:478] Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:36.486777 23 dns_common.go:489] Lookups using dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764 failed for: [wheezy_udp@dns-test-service.dns-2011.svc.cluster.local wheezy_tcp@dns-test-service.dns-2011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local jessie_udp@dns-test-service.dns-2011.svc.cluster.local jessie_tcp@dns-test-service.dns-2011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local]

  I0512 16:40:36.521363 23 dns_common.go:495] Pod client logs for webserver: 
  I0512 16:40:36.536481 23 dns_common.go:495] Pod client logs for querier: 
  I0512 16:40:36.548680 23 dns_common.go:495] Pod client logs for jessie-querier: 
  E0512 16:40:36.585143      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:37.585547      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:38.585852      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:39.586643      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:40.586988      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:41.395864 23 dns_common.go:478] Unable to read jessie_udp@dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:41.405383 23 dns_common.go:478] Unable to read jessie_tcp@dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:41.414401 23 dns_common.go:478] Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:41.423868 23 dns_common.go:478] Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local from pod dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764: the server could not find the requested resource (get pods dns-test-04860649-9097-4734-9025-9e4673626764)
  I0512 16:40:41.457990 23 dns_common.go:489] Lookups using dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764 failed for: [jessie_udp@dns-test-service.dns-2011.svc.cluster.local jessie_tcp@dns-test-service.dns-2011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2011.svc.cluster.local]

  I0512 16:40:41.473769 23 dns_common.go:495] Pod client logs for webserver: 
  I0512 16:40:41.487722 23 dns_common.go:495] Pod client logs for querier: 
  I0512 16:40:41.501585 23 dns_common.go:495] Pod client logs for jessie-querier: 
  E0512 16:40:41.587749      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:42.588083      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:43.589088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:44.589437      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:45.589994      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:46.464760 23 dns_common.go:527] DNS probes using dns-2011/dns-test-04860649-9097-4734-9025-9e4673626764 succeeded

  STEP: deleting the pod @ 05/12/25 16:40:46.466
  STEP: deleting the test service @ 05/12/25 16:40:46.517
  STEP: deleting the test headless service @ 05/12/25 16:40:46.551
  I0512 16:40:46.580048 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 16:40:46.590512      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-2011" for this suite. @ 05/12/25 16:40:46.594
• [12.452 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:154
  STEP: Creating a kubernetes client @ 05/12/25 16:40:46.624
  I0512 16:40:46.624659 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 16:40:46.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:40:46.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:40:46.702
  I0512 16:40:46.712181 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:40:47.591191      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:48.592347      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:49.592769      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:50.592974      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:51.593203      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:52.593389      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:53.593699      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:54.594538      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/12/25 16:40:55.466
  I0512 16:40:55.466912 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-6694 --namespace=crd-publish-openapi-6694 create -f -'
  E0512 16:40:55.595030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:55.694608 23 builder.go:146] stderr: ""
  I0512 16:40:55.694735 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-5228-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  I0512 16:40:55.694902 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-6694 --namespace=crd-publish-openapi-6694 delete e2e-test-crd-publish-openapi-5228-crds test-cr'
  I0512 16:40:55.911190 23 builder.go:146] stderr: ""
  I0512 16:40:55.911258 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-5228-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  I0512 16:40:55.911350 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-6694 --namespace=crd-publish-openapi-6694 apply -f -'
  I0512 16:40:56.084748 23 builder.go:146] stderr: ""
  I0512 16:40:56.084830 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-5228-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  I0512 16:40:56.084963 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-6694 --namespace=crd-publish-openapi-6694 delete e2e-test-crd-publish-openapi-5228-crds test-cr'
  I0512 16:40:56.260076 23 builder.go:146] stderr: ""
  I0512 16:40:56.260149 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-5228-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/12/25 16:40:56.26
  I0512 16:40:56.260312 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-6694 explain e2e-test-crd-publish-openapi-5228-crds'
  I0512 16:40:56.433408 23 builder.go:146] stderr: ""
  I0512 16:40:56.433517 23 builder.go:147] stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-5228-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0512 16:40:56.595878      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:57.596601      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:58.596996      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:40:59.597547      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:40:59.618934 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6694" for this suite. @ 05/12/25 16:40:59.64
• [13.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/12/25 16:40:59.655
  I0512 16:40:59.656013 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:40:59.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:40:59.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:40:59.7
  STEP: Creating configMap with name configmap-projected-all-test-volume-84c59a2b-1a8e-491f-a2b5-a3b3ec6c035a @ 05/12/25 16:40:59.707
  STEP: Creating secret with name secret-projected-all-test-volume-c324e84b-b37f-4c29-a472-7792c5cd32fd @ 05/12/25 16:40:59.723
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/12/25 16:40:59.735
  E0512 16:41:00.598386      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:01.599183      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:02.599362      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:03.599938      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:41:03.787
  I0512 16:41:03.795489 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod projected-volume-48facef7-5bd0-4b2f-84bc-c53d2314e955 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 16:41:03.812
  I0512 16:41:03.841447 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6252" for this suite. @ 05/12/25 16:41:03.854
• [4.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/job.go:490
  STEP: Creating a kubernetes client @ 05/12/25 16:41:03.876
  I0512 16:41:03.876479 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename job @ 05/12/25 16:41:03.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:03.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:03.931
  STEP: Creating Indexed job @ 05/12/25 16:41:03.939
  STEP: Ensuring job reaches completions @ 05/12/25 16:41:03.954
  E0512 16:41:04.601011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:05.601337      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:06.602479      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:07.603026      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:08.603236      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:09.603618      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:10.604215      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:11.604108      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:12.604609      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:13.604934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:14.605790      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:15.605955      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 05/12/25 16:41:15.971
  I0512 16:41:15.978415 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1697" for this suite. @ 05/12/25 16:41:15.989
• [12.127 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
  STEP: Creating a kubernetes client @ 05/12/25 16:41:16.004
  I0512 16:41:16.004216 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:41:16.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:16.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:16.046
  STEP: creating Agnhost RC @ 05/12/25 16:41:16.051
  I0512 16:41:16.051591 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-1342 create -f -'
  I0512 16:41:16.342582 23 builder.go:146] stderr: ""
  I0512 16:41:16.342661 23 builder.go:147] stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/12/25 16:41:16.342
  E0512 16:41:16.606118      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:41:17.353869 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:41:17.353960 23 framework.go:733] Found 0 / 1
  E0512 16:41:17.607177      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:41:18.353676 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:41:18.353757 23 framework.go:733] Found 1 / 1
  I0512 16:41:18.353801 23 framework.go:742] WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/12/25 16:41:18.353
  I0512 16:41:18.362502 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:41:18.363119 23 framework.go:765] ForEach: Found 1 pods from the filter.  Now looping through them.
  I0512 16:41:18.363254 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-1342 patch pod agnhost-primary-cthfz -p {"metadata":{"annotations":{"x":"y"}}}'
  I0512 16:41:18.507597 23 builder.go:146] stderr: ""
  I0512 16:41:18.507663 23 builder.go:147] stdout: "pod/agnhost-primary-cthfz patched\n"
  STEP: checking annotations @ 05/12/25 16:41:18.507
  I0512 16:41:18.515410 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:41:18.515513 23 framework.go:765] ForEach: Found 1 pods from the filter.  Now looping through them.
  I0512 16:41:18.515824 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1342" for this suite. @ 05/12/25 16:41:18.526
• [2.532 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_configmap.go:90
  STEP: Creating a kubernetes client @ 05/12/25 16:41:18.536
  I0512 16:41:18.536408 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:41:18.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:18.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:18.586
  STEP: Creating configMap with name projected-configmap-test-volume-map-9be17be1-7a53-43b1-8741-99500685fffd @ 05/12/25 16:41:18.592
  STEP: Creating a pod to test consume configMaps @ 05/12/25 16:41:18.603
  E0512 16:41:18.607498      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:19.608076      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:20.608642      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:21.608952      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:22.609985      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:41:22.661
  I0512 16:41:22.669155 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-configmaps-5e745e19-70d9-4828-bb40-cf9f219ee444 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/25 16:41:22.686
  I0512 16:41:22.730600 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9774" for this suite. @ 05/12/25 16:41:22.744
• [4.226 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:140
  STEP: Creating a kubernetes client @ 05/12/25 16:41:22.767
  I0512 16:41:22.767755 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 16:41:22.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:22.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:22.816
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/12/25 16:41:22.822
  E0512 16:41:23.610092      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:24.610205      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:25.610599      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:26.610788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:41:26.894
  I0512 16:41:26.903194 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-747b7d02-7b81-405d-ba70-e7dce8954baa container test-container: <nil>
  STEP: delete the pod @ 05/12/25 16:41:26.918
  I0512 16:41:26.956877 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1640" for this suite. @ 05/12/25 16:41:26.967
• [4.219 seconds]
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/init_container.go:178
  STEP: Creating a kubernetes client @ 05/12/25 16:41:26.986
  I0512 16:41:26.986477 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename init-container @ 05/12/25 16:41:26.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:27.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:27.046
  STEP: creating the pod @ 05/12/25 16:41:27.052
  I0512 16:41:27.052391 23 init_container.go:213] PodSpec: initContainers in spec.initContainers
  E0512 16:41:27.611821      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:28.612882      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:29.613748      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:30.614951      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:41:31.336849 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-582" for this suite. @ 05/12/25 16:41:31.35
• [4.378 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/12/25 16:41:31.365
  I0512 16:41:31.365649 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/25 16:41:31.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:31.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:31.415
  STEP: Waiting for pod completion @ 05/12/25 16:41:31.438
  E0512 16:41:31.615595      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:32.616558      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:33.617518      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:34.618249      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:41:35.478365 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6367" for this suite. @ 05/12/25 16:41:35.487
• [4.133 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:859
  STEP: Creating a kubernetes client @ 05/12/25 16:41:35.498
  I0512 16:41:35.498830 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 16:41:35.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:35.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:35.54
  STEP: Creating a ResourceQuota with best effort scope @ 05/12/25 16:41:35.545
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/25 16:41:35.557
  E0512 16:41:35.619011      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:36.619488      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 05/12/25 16:41:37.567
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/25 16:41:37.58
  E0512 16:41:37.619892      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:38.621155      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 05/12/25 16:41:39.587
  E0512 16:41:39.621198      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/12/25 16:41:39.627
  E0512 16:41:40.621373      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:41.621906      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/12/25 16:41:41.635
  E0512 16:41:42.623046      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:43.623969      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/12/25 16:41:43.645
  STEP: Ensuring resource quota status released the pod usage @ 05/12/25 16:41:43.681
  E0512 16:41:44.623966      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:45.624599      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 05/12/25 16:41:45.689
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/12/25 16:41:45.713
  E0512 16:41:46.625019      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:47.625417      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/12/25 16:41:47.722
  E0512 16:41:48.626521      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:49.626877      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/12/25 16:41:49.731
  STEP: Ensuring resource quota status released the pod usage @ 05/12/25 16:41:49.755
  E0512 16:41:50.627655      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:51.628117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:41:51.763144 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3397" for this suite. @ 05/12/25 16:41:51.777
• [16.291 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/rc.go:86
  STEP: Creating a kubernetes client @ 05/12/25 16:41:51.793
  I0512 16:41:51.794042 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename replication-controller @ 05/12/25 16:41:51.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:51.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:51.845
  I0512 16:41:51.851573 23 rc.go:544] Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/12/25 16:41:51.877
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/12/25 16:41:51.888
  E0512 16:41:52.628822      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/12/25 16:41:52.91
  I0512 16:41:52.931915 23 rc.go:730] Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/12/25 16:41:52.932
  E0512 16:41:53.629016      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:41:53.962563 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6590" for this suite. @ 05/12/25 16:41:53.976
• [2.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/12/25 16:41:53.999
  I0512 16:41:53.999555 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 16:41:54.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:54.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:54.046
  I0512 16:41:54.053312 23 deployment.go:1645] Creating simple deployment test-new-deployment
  I0512 16:41:54.128306 23 deployment.go:222] deployment "test-new-deployment" doesn't have the required revision set
  E0512 16:41:54.630078      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:55.630140      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 05/12/25 16:41:56.166
  STEP: updating a scale subresource @ 05/12/25 16:41:56.172
  STEP: verifying the deployment Spec.Replicas was modified @ 05/12/25 16:41:56.186
  STEP: Patch a scale subresource @ 05/12/25 16:41:56.198
  I0512 16:41:56.256734 23 deployment.go:633] Deployment "test-new-deployment":
  (v1.Deployment) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=19) "test-new-deployment",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-3672",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "cf91b66c-2796-4f0b-b2d9-512b75ea5f01",
      ResourceVersion: (string) (len=5) "68064",
      Generation: (int64) 3,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664914,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=1) {
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) (len=1) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) <nil>,
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=3) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)(<nil>),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=28) {
              00000000  7b 22 66 3a 73 70 65 63  22 3a 7b 22 66 3a 72 65  |{"f:spec":{"f:re|
              00000010  70 6c 69 63 61 73 22 3a  7b 7d 7d 7d              |plicas":{}}}|
            }
          }),
          Subresource: (string) (len=5) "scale"
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664914,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=619) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 7d 7d 2c  |},"f:name":{}}},|
              00000030  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 70 72 6f  |"f:spec":{"f:pro|
              00000040  67 72 65 73 73 44 65 61  64 6c 69 6e 65 53 65 63  |gressDeadlineSec|
              00000050  6f 6e 64 73 22 3a 7b 7d  2c 22 66 3a 72 65 76 69  |onds":{},"f:revi|
              00000060  73 69 6f 6e 48 69 73 74  6f 72 79 4c 69 6d 69 74  |sionHistoryLimit|
              00000070  22 3a 7b 7d 2c 22 66 3a  73 65 6c 65 63 74 6f 72  |":{},"f:selector|
              00000080  22 3a 7b 7d 2c 22 66 3a  73 74 72 61 74 65 67 79  |":{},"f:strategy|
              00000090  22 3a 7b 22 66 3a 72 6f  6c 6c 69 6e 67 55 70 64  |":{"f:rollingUpd|
              000000a0  61 74 65 22 3a 7b 22 2e  22 3a 7b 7d 2c 22 66 3a  |ate":{".":{},"f:|
              000000b0  6d 61 78 53 75 72 67 65  22 3a 7b 7d 2c 22 66 3a  |maxSurge":{},"f:|
              000000c0  6d 61 78 55 6e 61 76 61  69 6c 61 62 6c 65 22 3a  |maxUnavailable":|
              000000d0  7b 7d 7d 2c 22 66 3a 74  79 70 65 22 3a 7b 7d 7d  |{}},"f:type":{}}|
              000000e0  2c 22 66 3a 74 65 6d 70  6c 61 74 65 22 3a 7b 22  |,"f:template":{"|
              000000f0  66 3a 6d 65 74 61 64 61  74 61 22 3a 7b 22 66 3a  |f:metadata":{"f:|
              00000100  6c 61 62 65 6c 73 22 3a  7b 22 2e 22 3a 7b 7d 2c  |labels":{".":{},|
              00000110  22 66 3a 6e 61 6d 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:name":{}}},"f|
              00000120  3a 73 70 65 63 22 3a 7b  22 66 3a 63 6f 6e 74 61  |:spec":{"f:conta|
              00000130  69 6e 65 72 73 22 3a 7b  22 6b 3a 7b 5c 22 6e 61  |iners":{"k:{\"na|
              00000140  6d 65 5c 22 3a 5c 22 68  74 74 70 64 5c 22 7d 22  |me\":\"httpd\"}"|
              00000150  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 69 6d 61 67  |:{".":{},"f:imag|
              00000160  65 22 3a 7b 7d 2c 22 66  3a 69 6d 61 67 65 50 75  |e":{},"f:imagePu|
              00000170  6c 6c 50 6f 6c 69 63 79  22 3a 7b 7d 2c 22 66 3a  |llPolicy":{},"f:|
              00000180  6e 61 6d 65 22 3a 7b 7d  2c 22 66 3a 72 65 73 6f  |name":{},"f:reso|
              00000190  75 72 63 65 73 22 3a 7b  7d 2c 22 66 3a 73 65 63  |urces":{},"f:sec|
              000001a0  75 72 69 74 79 43 6f 6e  74 65 78 74 22 3a 7b 7d  |urityContext":{}|
              000001b0  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              000001c0  65 73 73 61 67 65 50 61  74 68 22 3a 7b 7d 2c 22  |essagePath":{},"|
              000001d0  66 3a 74 65 72 6d 69 6e  61 74 69 6f 6e 4d 65 73  |f:terminationMes|
              000001e0  73 61 67 65 50 6f 6c 69  63 79 22 3a 7b 7d 7d 7d  |sagePolicy":{}}}|
              000001f0  2c 22 66 3a 64 6e 73 50  6f 6c 69 63 79 22 3a 7b  |,"f:dnsPolicy":{|
              00000200  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              00000210  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              00000220  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              00000230  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000240  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000250  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000260  6e 64 73 22 3a 7b 7d 7d  7d 7d 7d                 |nds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=514) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 72 65 76 69 73  69 6f 6e 22 3a 7b 7d 7d  |io/revision":{}}|
              00000050  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 22 66  |},"f:status":{"f|
              00000060  3a 61 76 61 69 6c 61 62  6c 65 52 65 70 6c 69 63  |:availableReplic|
              00000070  61 73 22 3a 7b 7d 2c 22  66 3a 63 6f 6e 64 69 74  |as":{},"f:condit|
              00000080  69 6f 6e 73 22 3a 7b 22  2e 22 3a 7b 7d 2c 22 6b  |ions":{".":{},"k|
              00000090  3a 7b 5c 22 74 79 70 65  5c 22 3a 5c 22 41 76 61  |:{\"type\":\"Ava|
              000000a0  69 6c 61 62 6c 65 5c 22  7d 22 3a 7b 22 2e 22 3a  |ilable\"}":{".":|
              000000b0  7b 7d 2c 22 66 3a 6c 61  73 74 54 72 61 6e 73 69  |{},"f:lastTransi|
              000000c0  74 69 6f 6e 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |tionTime":{},"f:|
              000000d0  6c 61 73 74 55 70 64 61  74 65 54 69 6d 65 22 3a  |lastUpdateTime":|
              000000e0  7b 7d 2c 22 66 3a 6d 65  73 73 61 67 65 22 3a 7b  |{},"f:message":{|
              000000f0  7d 2c 22 66 3a 72 65 61  73 6f 6e 22 3a 7b 7d 2c  |},"f:reason":{},|
              00000100  22 66 3a 73 74 61 74 75  73 22 3a 7b 7d 2c 22 66  |"f:status":{},"f|
              00000110  3a 74 79 70 65 22 3a 7b  7d 7d 2c 22 6b 3a 7b 5c  |:type":{}},"k:{\|
              00000120  22 74 79 70 65 5c 22 3a  5c 22 50 72 6f 67 72 65  |"type\":\"Progre|
              00000130  73 73 69 6e 67 5c 22 7d  22 3a 7b 22 2e 22 3a 7b  |ssing\"}":{".":{|
              00000140  7d 2c 22 66 3a 6c 61 73  74 54 72 61 6e 73 69 74  |},"f:lastTransit|
              00000150  69 6f 6e 54 69 6d 65 22  3a 7b 7d 2c 22 66 3a 6c  |ionTime":{},"f:l|
              00000160  61 73 74 55 70 64 61 74  65 54 69 6d 65 22 3a 7b  |astUpdateTime":{|
              00000170  7d 2c 22 66 3a 6d 65 73  73 61 67 65 22 3a 7b 7d  |},"f:message":{}|
              00000180  2c 22 66 3a 72 65 61 73  6f 6e 22 3a 7b 7d 2c 22  |,"f:reason":{},"|
              00000190  66 3a 73 74 61 74 75 73  22 3a 7b 7d 2c 22 66 3a  |f:status":{},"f:|
              000001a0  74 79 70 65 22 3a 7b 7d  7d 7d 2c 22 66 3a 6f 62  |type":{}}},"f:ob|
              000001b0  73 65 72 76 65 64 47 65  6e 65 72 61 74 69 6f 6e  |servedGeneration|
              000001c0  22 3a 7b 7d 2c 22 66 3a  72 65 61 64 79 52 65 70  |":{},"f:readyRep|
              000001d0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 72 65 70  |licas":{},"f:rep|
              000001e0  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 75 70 64  |licas":{},"f:upd|
              000001f0  61 74 65 64 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |atedReplicas":{}|
              00000200  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.DeploymentSpec) {
      Replicas: (*int32)(4),
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=1) {
          (string) (len=4) "name": (string) (len=5) "httpd"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=1) {
            (string) (len=4) "name": (string) (len=5) "httpd"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      },
      Strategy: (v1.DeploymentStrategy) {
        Type: (v1.DeploymentStrategyType) (len=13) "RollingUpdate",
        RollingUpdate: (*v1.RollingUpdateDeployment)({
          MaxUnavailable: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          }),
          MaxSurge: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          })
        })
      },
      MinReadySeconds: (int32) 0,
      RevisionHistoryLimit: (*int32)(10),
      Paused: (bool) false,
      ProgressDeadlineSeconds: (*int32)(600)
    },
    Status: (v1.DeploymentStatus) {
      ObservedGeneration: (int64) 1,
      Replicas: (int32) 1,
      UpdatedReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      UnavailableReplicas: (int32) 0,
      Conditions: ([]v1.DeploymentCondition) (len=2) {
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=9) "Available",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=24) "MinimumReplicasAvailable",
          Message: (string) (len=36) "Deployment has minimum availability."
        },
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=11) "Progressing",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664914,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=22) "NewReplicaSetAvailable",
          Message: (string) (len=72) "ReplicaSet \"test-new-deployment-64bcfc6446\" has successfully progressed."
        }
      },
      CollisionCount: (*int32)(<nil>)
    }
  }


  I0512 16:41:56.292262 23 deployment.go:39] New ReplicaSet "test-new-deployment-64bcfc6446" of Deployment "test-new-deployment":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=30) "test-new-deployment-64bcfc6446",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-3672",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "658e9521-f26e-4abe-afa3-1b904dedfef1",
      ResourceVersion: (string) (len=5) "68070",
      Generation: (int64) 2,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664914,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "2",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "3",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=19) "test-new-deployment",
          UID: (types.UID) (len=36) "cf91b66c-2796-4f0b-b2d9-512b75ea5f01",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=781) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 6e 61 6d  65 22 3a 7b 7d 2c 22 66  |},"f:name":{},"f|
              000000d0  3a 70 6f 64 2d 74 65 6d  70 6c 61 74 65 2d 68 61  |:pod-template-ha|
              000000e0  73 68 22 3a 7b 7d 7d 2c  22 66 3a 6f 77 6e 65 72  |sh":{}},"f:owner|
              000000f0  52 65 66 65 72 65 6e 63  65 73 22 3a 7b 22 2e 22  |References":{"."|
              00000100  3a 7b 7d 2c 22 6b 3a 7b  5c 22 75 69 64 5c 22 3a  |:{},"k:{\"uid\":|
              00000110  5c 22 63 66 39 31 62 36  36 63 2d 32 37 39 36 2d  |\"cf91b66c-2796-|
              00000120  34 66 30 62 2d 62 32 64  39 2d 35 31 32 62 37 35  |4f0b-b2d9-512b75|
              00000130  65 61 35 66 30 31 5c 22  7d 22 3a 7b 7d 7d 7d 2c  |ea5f01\"}":{}}},|
              00000140  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 72 65 70  |"f:spec":{"f:rep|
              00000150  6c 69 63 61 73 22 3a 7b  7d 2c 22 66 3a 73 65 6c  |licas":{},"f:sel|
              00000160  65 63 74 6f 72 22 3a 7b  7d 2c 22 66 3a 74 65 6d  |ector":{},"f:tem|
              00000170  70 6c 61 74 65 22 3a 7b  22 66 3a 6d 65 74 61 64  |plate":{"f:metad|
              00000180  61 74 61 22 3a 7b 22 66  3a 6c 61 62 65 6c 73 22  |ata":{"f:labels"|
              00000190  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6e 61 6d 65  |:{".":{},"f:name|
              000001a0  22 3a 7b 7d 2c 22 66 3a  70 6f 64 2d 74 65 6d 70  |":{},"f:pod-temp|
              000001b0  6c 61 74 65 2d 68 61 73  68 22 3a 7b 7d 7d 7d 2c  |late-hash":{}}},|
              000001c0  22 66 3a 73 70 65 63 22  3a 7b 22 66 3a 63 6f 6e  |"f:spec":{"f:con|
              000001d0  74 61 69 6e 65 72 73 22  3a 7b 22 6b 3a 7b 5c 22  |tainers":{"k:{\"|
              000001e0  6e 61 6d 65 5c 22 3a 5c  22 68 74 74 70 64 5c 22  |name\":\"httpd\"|
              000001f0  7d 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 69 6d  |}":{".":{},"f:im|
              00000200  61 67 65 22 3a 7b 7d 2c  22 66 3a 69 6d 61 67 65  |age":{},"f:image|
              00000210  50 75 6c 6c 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |PullPolicy":{},"|
              00000220  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 72 65  |f:name":{},"f:re|
              00000230  73 6f 75 72 63 65 73 22  3a 7b 7d 2c 22 66 3a 73  |sources":{},"f:s|
              00000240  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              00000250  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000260  6e 4d 65 73 73 61 67 65  50 61 74 68 22 3a 7b 7d  |nMessagePath":{}|
              00000270  2c 22 66 3a 74 65 72 6d  69 6e 61 74 69 6f 6e 4d  |,"f:terminationM|
              00000280  65 73 73 61 67 65 50 6f  6c 69 63 79 22 3a 7b 7d  |essagePolicy":{}|
              00000290  7d 7d 2c 22 66 3a 64 6e  73 50 6f 6c 69 63 79 22  |}},"f:dnsPolicy"|
              000002a0  3a 7b 7d 2c 22 66 3a 72  65 73 74 61 72 74 50 6f  |:{},"f:restartPo|
              000002b0  6c 69 63 79 22 3a 7b 7d  2c 22 66 3a 73 63 68 65  |licy":{},"f:sche|
              000002c0  64 75 6c 65 72 4e 61 6d  65 22 3a 7b 7d 2c 22 66  |dulerName":{},"f|
              000002d0  3a 73 65 63 75 72 69 74  79 43 6f 6e 74 65 78 74  |:securityContext|
              000002e0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              000002f0  69 6f 6e 47 72 61 63 65  50 65 72 69 6f 64 53 65  |ionGracePeriodSe|
              00000300  63 6f 6e 64 73 22 3a 7b  7d 7d 7d 7d 7d           |conds":{}}}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=130) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  61 76 61 69 6c 61 62 6c  65 52 65 70 6c 69 63 61  |availableReplica|
              00000020  73 22 3a 7b 7d 2c 22 66  3a 66 75 6c 6c 79 4c 61  |s":{},"f:fullyLa|
              00000030  62 65 6c 65 64 52 65 70  6c 69 63 61 73 22 3a 7b  |beledReplicas":{|
              00000040  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              00000050  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              00000060  65 61 64 79 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |eadyReplicas":{}|
              00000070  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000080  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(2),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=4) "name": (string) (len=5) "httpd",
          (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=4) "name": (string) (len=5) "httpd",
            (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 2,
      FullyLabeledReplicas: (int32) 2,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      ObservedGeneration: (int64) 2,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 16:41:56.307990 23 deployment.go:67] Pod "test-new-deployment-64bcfc6446-qzs4b" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=36) "test-new-deployment-64bcfc6446-qzs4b",
      GenerateName: (string) (len=31) "test-new-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-3672",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "bfe7aa2d-2474-4d4e-a1ce-88466da1bb35",
      ResourceVersion: (string) (len=5) "68056",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664914,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=30) "test-new-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "658e9521-f26e-4abe-afa3-1b904dedfef1",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664914,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 36 35  38 65 39 35 32 31 2d 66  |d\":\"658e9521-f|
              00000090  32 36 65 2d 34 61 62 65  2d 61 66 61 33 2d 31 62  |26e-4abe-afa3-1b|
              000000a0  39 30 34 64 65 64 66 65  66 31 5c 22 7d 22 3a 7b  |904dedfef1\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 39 2e  31 33 36 5c 22 7d 22 3a  |.233.69.136\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-kn6cd",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-kn6cd",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664914,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664915,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664914,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) (len=13) "10.233.69.136",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.69.136"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664914,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882664915,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://84b2aa40f5f69a48e4745cf20aee91b227f63bd11cb7987b736dfeb0372458e4",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-kn6cd",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:41:56.316242 23 deployment.go:67] Pod "test-new-deployment-64bcfc6446-v87k8" is not available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=36) "test-new-deployment-64bcfc6446-v87k8",
      GenerateName: (string) (len=31) "test-new-deployment-64bcfc6446-",
      Namespace: (string) (len=15) "deployment-3672",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "59c938a4-147c-483c-a86b-29811e962334",
      ResourceVersion: (string) (len=5) "68071",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664916,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=10) "64bcfc6446"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=30) "test-new-deployment-64bcfc6446",
          UID: (types.UID) (len=36) "658e9521-f26e-4abe-afa3-1b904dedfef1",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=537) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |.":{},"f:name":{|
              00000040  7d 2c 22 66 3a 70 6f 64  2d 74 65 6d 70 6c 61 74  |},"f:pod-templat|
              00000050  65 2d 68 61 73 68 22 3a  7b 7d 7d 2c 22 66 3a 6f  |e-hash":{}},"f:o|
              00000060  77 6e 65 72 52 65 66 65  72 65 6e 63 65 73 22 3a  |wnerReferences":|
              00000070  7b 22 2e 22 3a 7b 7d 2c  22 6b 3a 7b 5c 22 75 69  |{".":{},"k:{\"ui|
              00000080  64 5c 22 3a 5c 22 36 35  38 65 39 35 32 31 2d 66  |d\":\"658e9521-f|
              00000090  32 36 65 2d 34 61 62 65  2d 61 66 61 33 2d 31 62  |26e-4abe-afa3-1b|
              000000a0  39 30 34 64 65 64 66 65  66 31 5c 22 7d 22 3a 7b  |904dedfef1\"}":{|
              000000b0  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              000000c0  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              000000d0  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              000000e0  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              000000f0  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000100  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              00000110  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              00000120  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              00000130  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              00000140  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              00000150  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              00000160  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000170  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000180  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000190  69 63 79 22 3a 7b 7d 2c  22 66 3a 65 6e 61 62 6c  |icy":{},"f:enabl|
              000001a0  65 53 65 72 76 69 63 65  4c 69 6e 6b 73 22 3a 7b  |eServiceLinks":{|
              000001b0  7d 2c 22 66 3a 72 65 73  74 61 72 74 50 6f 6c 69  |},"f:restartPoli|
              000001c0  63 79 22 3a 7b 7d 2c 22  66 3a 73 63 68 65 64 75  |cy":{},"f:schedu|
              000001d0  6c 65 72 4e 61 6d 65 22  3a 7b 7d 2c 22 66 3a 73  |lerName":{},"f:s|
              000001e0  65 63 75 72 69 74 79 43  6f 6e 74 65 78 74 22 3a  |ecurityContext":|
              000001f0  7b 7d 2c 22 66 3a 74 65  72 6d 69 6e 61 74 69 6f  |{},"f:terminatio|
              00000200  6e 47 72 61 63 65 50 65  72 69 6f 64 53 65 63 6f  |nGracePeriodSeco|
              00000210  6e 64 73 22 3a 7b 7d 7d  7d                       |nds":{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=624) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 6d 65 73 73 61 67  |me":{},"f:messag|
              00000080  65 22 3a 7b 7d 2c 22 66  3a 72 65 61 73 6f 6e 22  |e":{},"f:reason"|
              00000090  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              000000a0  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000000b0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 49 6e  |k:{\"type\":\"In|
              000000c0  69 74 69 61 6c 69 7a 65  64 5c 22 7d 22 3a 7b 22  |itialized\"}":{"|
              000000d0  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 50 72 6f  |.":{},"f:lastPro|
              000000e0  62 65 54 69 6d 65 22 3a  7b 7d 2c 22 66 3a 6c 61  |beTime":{},"f:la|
              000000f0  73 74 54 72 61 6e 73 69  74 69 6f 6e 54 69 6d 65  |stTransitionTime|
              00000100  22 3a 7b 7d 2c 22 66 3a  73 74 61 74 75 73 22 3a  |":{},"f:status":|
              00000110  7b 7d 2c 22 66 3a 74 79  70 65 22 3a 7b 7d 7d 2c  |{},"f:type":{}},|
              00000120  22 6b 3a 7b 5c 22 74 79  70 65 5c 22 3a 5c 22 50  |"k:{\"type\":\"P|
              00000130  6f 64 52 65 61 64 79 54  6f 53 74 61 72 74 43 6f  |odReadyToStartCo|
              00000140  6e 74 61 69 6e 65 72 73  5c 22 7d 22 3a 7b 22 2e  |ntainers\"}":{".|
              00000150  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 50 72 6f 62  |":{},"f:lastProb|
              00000160  65 54 69 6d 65 22 3a 7b  7d 2c 22 66 3a 6c 61 73  |eTime":{},"f:las|
              00000170  74 54 72 61 6e 73 69 74  69 6f 6e 54 69 6d 65 22  |tTransitionTime"|
              00000180  3a 7b 7d 2c 22 66 3a 73  74 61 74 75 73 22 3a 7b  |:{},"f:status":{|
              00000190  7d 2c 22 66 3a 74 79 70  65 22 3a 7b 7d 7d 2c 22  |},"f:type":{}},"|
              000001a0  6b 3a 7b 5c 22 74 79 70  65 5c 22 3a 5c 22 52 65  |k:{\"type\":\"Re|
              000001b0  61 64 79 5c 22 7d 22 3a  7b 22 2e 22 3a 7b 7d 2c  |ady\"}":{".":{},|
              000001c0  22 66 3a 6c 61 73 74 50  72 6f 62 65 54 69 6d 65  |"f:lastProbeTime|
              000001d0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000001e0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000001f0  66 3a 6d 65 73 73 61 67  65 22 3a 7b 7d 2c 22 66  |f:message":{},"f|
              00000200  3a 72 65 61 73 6f 6e 22  3a 7b 7d 2c 22 66 3a 73  |:reason":{},"f:s|
              00000210  74 61 74 75 73 22 3a 7b  7d 2c 22 66 3a 74 79 70  |tatus":{},"f:typ|
              00000220  65 22 3a 7b 7d 7d 7d 2c  22 66 3a 63 6f 6e 74 61  |e":{}}},"f:conta|
              00000230  69 6e 65 72 53 74 61 74  75 73 65 73 22 3a 7b 7d  |inerStatuses":{}|
              00000240  2c 22 66 3a 68 6f 73 74  49 50 22 3a 7b 7d 2c 22  |,"f:hostIP":{},"|
              00000250  66 3a 68 6f 73 74 49 50  73 22 3a 7b 7d 2c 22 66  |f:hostIPs":{},"f|
              00000260  3a 73 74 61 72 74 54 69  6d 65 22 3a 7b 7d 7d 7d  |:startTime":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-zxhzd",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-zxhzd",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-0",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Pending",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=5) "False",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "ContainersNotReady",
          Message: (string) (len=39) "containers with unready status: [httpd]"
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882664916,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.75",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.75"
        }
      },
      PodIP: (string) "",
      PodIPs: ([]v1.PodIP) <nil>,
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882664916,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)({
              Reason: (string) (len=17) "ContainerCreating",
              Message: (string) ""
            }),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) false,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) "",
          ContainerID: (string) "",
          Started: (*bool)(false),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-zxhzd",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:41:56.322600 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3672" for this suite. @ 05/12/25 16:41:56.348
• [2.386 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance] [sig-scheduling, Conformance]
k8s.io/kubernetes/test/e2e/scheduling/limit_range.go:253
  STEP: Creating a kubernetes client @ 05/12/25 16:41:56.386
  I0512 16:41:56.386494 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename limitrange @ 05/12/25 16:41:56.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:56.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:56.608
  STEP: Creating LimitRange "e2e-limitrange-9htrd" in namespace "limitrange-7664" @ 05/12/25 16:41:56.615
  STEP: Creating another limitRange in another namespace @ 05/12/25 16:41:56.628
  E0512 16:41:56.630417      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:41:56.705119 23 limit_range.go:299] Namespace "e2e-limitrange-9htrd-8787" created
  I0512 16:41:56.705188 23 limit_range.go:300] Creating LimitRange "e2e-limitrange-9htrd" in namespace "e2e-limitrange-9htrd-8787"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-9htrd" @ 05/12/25 16:41:56.717
  I0512 16:41:56.726385 23 limit_range.go:309] Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-9htrd" in "limitrange-7664" namespace @ 05/12/25 16:41:56.726
  I0512 16:41:56.742960 23 limit_range.go:335] LimitRange "e2e-limitrange-9htrd" has been patched
  STEP: Delete LimitRange "e2e-limitrange-9htrd" by Collection with labelSelector: "e2e-limitrange-9htrd=patched" @ 05/12/25 16:41:56.743
  STEP: Confirm that the limitRange "e2e-limitrange-9htrd" has been deleted @ 05/12/25 16:41:56.764
  I0512 16:41:56.764499 23 limit_range.go:443] Requesting list of LimitRange to confirm quantity
  I0512 16:41:56.775329 23 limit_range.go:453] Found 0 LimitRange with label "e2e-limitrange-9htrd=patched"
  I0512 16:41:56.775406 23 limit_range.go:344] LimitRange "e2e-limitrange-9htrd" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-9htrd" @ 05/12/25 16:41:56.775
  I0512 16:41:56.786074 23 limit_range.go:350] Found 1 limitRange
  I0512 16:41:56.786318 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7664" for this suite. @ 05/12/25 16:41:56.798
  STEP: Destroying namespace "e2e-limitrange-9htrd-8787" for this suite. @ 05/12/25 16:41:56.815
• [0.445 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/12/25 16:41:56.832
  I0512 16:41:56.832348 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename container-runtime @ 05/12/25 16:41:56.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:41:56.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:41:56.888
  STEP: create the container @ 05/12/25 16:41:56.895
  W0512 16:41:56.919450      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/12/25 16:41:56.919
  E0512 16:41:57.634746      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:58.634952      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:41:59.635101      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/12/25 16:42:00.027
  STEP: the container should be terminated @ 05/12/25 16:42:00.032
  STEP: the termination message should be set @ 05/12/25 16:42:00.032
  I0512 16:42:00.032979 23 runtime.go:167] Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/12/25 16:42:00.033
  I0512 16:42:00.067246 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-591" for this suite. @ 05/12/25 16:42:00.085
• [3.279 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:258
  STEP: Creating a kubernetes client @ 05/12/25 16:42:00.113
  I0512 16:42:00.113350 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename webhook @ 05/12/25 16:42:00.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:00.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:00.157
  STEP: Setting up server cert @ 05/12/25 16:42:00.323
  E0512 16:42:00.635397      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:01.635423      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/25 16:42:02.076
  STEP: Deploying the webhook pod @ 05/12/25 16:42:02.108
  STEP: Wait for the deployment to be ready @ 05/12/25 16:42:02.144
  I0512 16:42:02.159644 23 deployment.go:222] deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 16:42:02.635488      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:03.635906      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/25 16:42:04.181
  STEP: Verifying the service has paired with the endpoint @ 05/12/25 16:42:04.213
  E0512 16:42:04.636023      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:05.214238 23 util.go:420] Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/12/25 16:42:05.233
  STEP: create a pod that should be updated by the webhook @ 05/12/25 16:42:05.27
  I0512 16:42:05.472506 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4641" for this suite. @ 05/12/25 16:42:05.507
  STEP: Destroying namespace "webhook-markers-8227" for this suite. @ 05/12/25 16:42:05.545
• [5.454 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:750
  STEP: Creating a kubernetes client @ 05/12/25 16:42:05.573
  I0512 16:42:05.574314 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 16:42:05.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:05.629
  E0512 16:42:05.636572      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:05.637
  I0512 16:42:05.663794 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9585" for this suite. @ 05/12/25 16:42:05.679
• [0.124 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/12/25 16:42:05.698
  I0512 16:42:05.698258 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 16:42:05.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:05.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:05.788
  STEP: Creating secret with name secret-test-f06d0fe3-7212-4e35-9902-819ffb15aeba @ 05/12/25 16:42:05.799
  STEP: Creating a pod to test consume secrets @ 05/12/25 16:42:05.811
  E0512 16:42:06.639513      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:07.640556      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:08.641556      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:09.642777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:42:09.869
  I0512 16:42:09.884762 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-secrets-4d06c78d-472a-4728-80e7-4c9c4c144a8c container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 16:42:09.907
  I0512 16:42:09.939722 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2876" for this suite. @ 05/12/25 16:42:09.946
• [4.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/resource_quota.go:999
  STEP: Creating a kubernetes client @ 05/12/25 16:42:09.958
  I0512 16:42:09.958732 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename resourcequota @ 05/12/25 16:42:09.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:09.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:10.003
  STEP: Creating a ResourceQuota @ 05/12/25 16:42:10.008
  STEP: Getting a ResourceQuota @ 05/12/25 16:42:10.017
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/12/25 16:42:10.023
  STEP: Patching the ResourceQuota @ 05/12/25 16:42:10.032
  STEP: Deleting a Collection of ResourceQuotas @ 05/12/25 16:42:10.041
  STEP: Verifying the deleted ResourceQuota @ 05/12/25 16:42:10.056
  I0512 16:42:10.061385 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-880" for this suite. @ 05/12/25 16:42:10.07
• [0.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/field_validation.go:166
  STEP: Creating a kubernetes client @ 05/12/25 16:42:10.094
  I0512 16:42:10.094466 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename field-validation @ 05/12/25 16:42:10.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:10.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:10.138
  I0512 16:42:10.151174 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:42:10.643754      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:11.644294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:12.644745      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:13.645709      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:14.646974      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:15.648110      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:16.648958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:17.649665      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  W0512 16:42:17.763715      23 warnings.go:70] unknown field "alpha"
  W0512 16:42:17.763805      23 warnings.go:70] unknown field "beta"
  W0512 16:42:17.763827      23 warnings.go:70] unknown field "delta"
  W0512 16:42:17.763845      23 warnings.go:70] unknown field "epsilon"
  W0512 16:42:17.763862      23 warnings.go:70] unknown field "gamma"
  I0512 16:42:18.380773 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1024" for this suite. @ 05/12/25 16:42:18.389
• [8.314 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance] [sig-instrumentation, Conformance]
k8s.io/kubernetes/test/e2e/instrumentation/core_events.go:176
  STEP: Creating a kubernetes client @ 05/12/25 16:42:18.409
  I0512 16:42:18.409386 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename events @ 05/12/25 16:42:18.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:18.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:18.462
  STEP: Create set of events @ 05/12/25 16:42:18.469
  I0512 16:42:18.479354 23 core_events.go:198] created test-event-1
  I0512 16:42:18.488999 23 core_events.go:198] created test-event-2
  I0512 16:42:18.497595 23 core_events.go:198] created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/12/25 16:42:18.497
  STEP: delete collection of events @ 05/12/25 16:42:18.504
  I0512 16:42:18.504086 23 core_events.go:213] requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/12/25 16:42:18.546
  I0512 16:42:18.546809 23 core_events.go:230] requesting list of events to confirm quantity
  I0512 16:42:18.551940 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7728" for this suite. @ 05/12/25 16:42:18.562
• [0.168 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/security_context.go:489
  STEP: Creating a kubernetes client @ 05/12/25 16:42:18.578
  I0512 16:42:18.578184 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename security-context-test @ 05/12/25 16:42:18.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:18.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:18.627
  E0512 16:42:18.650691      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:19.651279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:20.651490      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:21.651738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:22.651948      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:22.698494 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6072" for this suite. @ 05/12/25 16:42:22.708
• [4.146 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/empty_dir.go:100
  STEP: Creating a kubernetes client @ 05/12/25 16:42:22.725
  I0512 16:42:22.725555 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename emptydir @ 05/12/25 16:42:22.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:22.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:22.827
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/12/25 16:42:22.835
  E0512 16:42:23.652294      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:24.653047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:25.653268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:26.654020      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:42:26.899
  I0512 16:42:26.908757 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-79d4782e-1986-484f-83df-5a4850f86db8 container test-container: <nil>
  STEP: delete the pod @ 05/12/25 16:42:26.922
  I0512 16:42:26.953301 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3189" for this suite. @ 05/12/25 16:42:26.964
• [4.257 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/dns.go:288
  STEP: Creating a kubernetes client @ 05/12/25 16:42:26.983
  I0512 16:42:26.983233 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename dns @ 05/12/25 16:42:26.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:27.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:27.03
  STEP: Creating a test headless service @ 05/12/25 16:42:27.04
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-269.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-269.svc.cluster.local;sleep 1; done
   @ 05/12/25 16:42:27.054
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-269.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-269.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-269.svc.cluster.local;sleep 1; done
   @ 05/12/25 16:42:27.054
  STEP: creating a pod to probe DNS @ 05/12/25 16:42:27.054
  STEP: submitting the pod to kubernetes @ 05/12/25 16:42:27.054
  E0512 16:42:27.655079      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:28.656227      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/25 16:42:29.103
  STEP: looking for the results for each expected name from probers @ 05/12/25 16:42:29.111
  I0512 16:42:29.125484 23 dns_common.go:478] Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.133663 23 dns_common.go:478] Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.141729 23 dns_common.go:478] Unable to read wheezy_udp@dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.150947 23 dns_common.go:478] Unable to read wheezy_tcp@dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.159605 23 dns_common.go:478] Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.167897 23 dns_common.go:478] Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.175578 23 dns_common.go:478] Unable to read jessie_udp@dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.184770 23 dns_common.go:478] Unable to read jessie_tcp@dns-test-service-2.dns-269.svc.cluster.local from pod dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd: the server could not find the requested resource (get pods dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd)
  I0512 16:42:29.185199 23 dns_common.go:489] Lookups using dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local wheezy_udp@dns-test-service-2.dns-269.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-269.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-269.svc.cluster.local jessie_udp@dns-test-service-2.dns-269.svc.cluster.local jessie_tcp@dns-test-service-2.dns-269.svc.cluster.local]

  I0512 16:42:29.198476 23 dns_common.go:495] Pod client logs for webserver: 
  I0512 16:42:29.212280 23 dns_common.go:495] Pod client logs for querier: 
  I0512 16:42:29.226780 23 dns_common.go:495] Pod client logs for jessie-querier: 
  E0512 16:42:29.656900      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:30.657293      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:31.657460      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:32.658388      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:33.658744      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:34.176729 23 dns_common.go:527] DNS probes using dns-269/dns-test-928fb72d-d849-4548-9a32-9fcce2a40fbd succeeded

  STEP: deleting the pod @ 05/12/25 16:42:34.177
  STEP: deleting the test headless service @ 05/12/25 16:42:34.218
  I0512 16:42:34.247908 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "dns-269" for this suite. @ 05/12/25 16:42:34.26
• [7.302 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/garbage_collector.go:818
  STEP: Creating a kubernetes client @ 05/12/25 16:42:34.293
  I0512 16:42:34.293499 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename gc @ 05/12/25 16:42:34.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:34.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:34.368
  I0512 16:42:34.491917 23 garbage_collector.go:840] pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"060338ca-c391-49b3-aefa-90ddbd2eac1f", Controller:(*bool)(0xc00605c0b6), BlockOwnerDeletion:(*bool)(0xc00605c0b7)}}
  I0512 16:42:34.507751 23 garbage_collector.go:844] pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"490d372b-5f9d-48ba-9d49-49c88470c444", Controller:(*bool)(0xc00605cafe), BlockOwnerDeletion:(*bool)(0xc00605caff)}}
  I0512 16:42:34.528232 23 garbage_collector.go:848] pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d64b28e5-f485-45d1-bbe4-b93293b3380c", Controller:(*bool)(0xc005c7ee66), BlockOwnerDeletion:(*bool)(0xc005c7ee67)}}
  E0512 16:42:34.659796      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:35.660383      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:36.660831      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:37.661956      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:38.662928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:39.565818 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-963" for this suite. @ 05/12/25 16:42:39.582
• [5.306 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/service.go:1473
  STEP: Creating a kubernetes client @ 05/12/25 16:42:39.599
  I0512 16:42:39.599981 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename services @ 05/12/25 16:42:39.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:39.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:39.649
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6677 @ 05/12/25 16:42:39.656
  E0512 16:42:39.663811      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/12/25 16:42:39.679
  STEP: creating service externalsvc in namespace services-6677 @ 05/12/25 16:42:39.68
  STEP: creating replication controller externalsvc in namespace services-6677 @ 05/12/25 16:42:39.732
  I0512 16:42:39.747930      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6677, replica count: 2
  E0512 16:42:40.664120      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:41.664997      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:42.665651      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:42.799415      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/12/25 16:42:42.809
  I0512 16:42:42.842021 23 resource.go:361] Creating new exec pod
  E0512 16:42:43.668860      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:44.669763      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:44.892294 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=services-6677 exec execpodmh4cn -- /bin/sh -x -c nslookup clusterip-service.services-6677.svc.cluster.local'
  I0512 16:42:45.253010 23 builder.go:146] stderr: "+ nslookup clusterip-service.services-6677.svc.cluster.local\n"
  I0512 16:42:45.253098 23 builder.go:147] stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-6677.svc.cluster.local\tcanonical name = externalsvc.services-6677.svc.cluster.local.\nName:\texternalsvc.services-6677.svc.cluster.local\nAddress: 10.233.21.155\n\n"
  STEP: deleting ReplicationController externalsvc in namespace services-6677, will wait for the garbage collector to delete the pods @ 05/12/25 16:42:45.253
  I0512 16:42:45.334657 23 resources.go:139] Deleting ReplicationController externalsvc took: 15.233746ms
  I0512 16:42:45.435919 23 resources.go:163] Terminating ReplicationController externalsvc pods took: 101.258764ms
  E0512 16:42:45.671921      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:46.671988      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:47.672020      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:48.672858      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:48.699404 23 service.go:1482] Cleaning up the ClusterIP to ExternalName test service
  I0512 16:42:48.757859 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6677" for this suite. @ 05/12/25 16:42:48.777
• [9.194 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/statefulset.go:965
  STEP: Creating a kubernetes client @ 05/12/25 16:42:48.793
  I0512 16:42:48.793812 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename statefulset @ 05/12/25 16:42:48.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:42:48.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:42:48.848
  STEP: Creating service test in namespace statefulset-3995 @ 05/12/25 16:42:48.857
  I0512 16:42:48.897000 23 wait.go:40] Found 0 stateful pods, waiting for 1
  E0512 16:42:49.673059      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:50.673554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:51.673911      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:52.674290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:53.674821      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:54.675600      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:55.675933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:56.676309      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:57.676696      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:42:58.677393      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:42:58.900771 23 wait.go:50] Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/12/25 16:42:58.915
  I0512 16:42:58.959548 23 wait.go:50] Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 16:42:58.960334 23 wait.go:50] Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
  E0512 16:42:59.677824      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:00.678645      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:01.679141      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:02.679890      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:03.680241      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:04.681007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:05.681961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:06.682331      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:07.682903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:08.683280      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:43:08.956198 23 wait.go:50] Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  I0512 16:43:08.956267 23 wait.go:50] Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/12/25 16:43:08.97
  STEP: Delete all of the StatefulSets @ 05/12/25 16:43:08.977
  STEP: Verify that StatefulSets have been deleted @ 05/12/25 16:43:08.999
  I0512 16:43:09.029440 23 statefulset.go:138] Deleting all statefulset in ns statefulset-3995
  I0512 16:43:09.107346 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3995" for this suite. @ 05/12/25 16:43:09.119
• [20.345 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/12/25 16:43:09.14
  I0512 16:43:09.140851 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename pods @ 05/12/25 16:43:09.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:43:09.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:43:09.209
  STEP: creating the pod @ 05/12/25 16:43:09.216
  STEP: submitting the pod to kubernetes @ 05/12/25 16:43:09.216
  E0512 16:43:09.683630      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:10.683915      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/12/25 16:43:11.267
  STEP: updating the pod @ 05/12/25 16:43:11.276
  E0512 16:43:11.684455      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:43:11.806446 23 pod_client.go:173] Successfully updated pod "pod-update-258d0624-020e-4e5f-8b01-8bef1fdee355"
  STEP: verifying the updated pod is in kubernetes @ 05/12/25 16:43:11.817
  I0512 16:43:11.824638 23 pods.go:391] Pod update OK
  I0512 16:43:11.824899 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7496" for this suite. @ 05/12/25 16:43:11.834
• [2.708 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/crd_publish_openapi.go:195
  STEP: Creating a kubernetes client @ 05/12/25 16:43:11.851
  I0512 16:43:11.851127 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/25 16:43:11.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:43:11.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:43:11.932
  I0512 16:43:11.939116 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  E0512 16:43:12.684354      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:13.685394      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:14.686585      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:15.687006      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:16.687839      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:17.687848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:18.689031      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:19.688993      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/12/25 16:43:20.379
  I0512 16:43:20.379754 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-8056 --namespace=crd-publish-openapi-8056 create -f -'
  I0512 16:43:20.604021 23 builder.go:146] stderr: ""
  I0512 16:43:20.604134 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-8201-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  I0512 16:43:20.604247 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-8056 --namespace=crd-publish-openapi-8056 delete e2e-test-crd-publish-openapi-8201-crds test-cr'
  E0512 16:43:20.690041      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:43:20.846313 23 builder.go:146] stderr: ""
  I0512 16:43:20.846407 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-8201-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  I0512 16:43:20.846500 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-8056 --namespace=crd-publish-openapi-8056 apply -f -'
  I0512 16:43:21.077200 23 builder.go:146] stderr: ""
  I0512 16:43:21.077300 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-8201-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  I0512 16:43:21.077461 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-8056 --namespace=crd-publish-openapi-8056 delete e2e-test-crd-publish-openapi-8201-crds test-cr'
  I0512 16:43:21.255934 23 builder.go:146] stderr: ""
  I0512 16:43:21.256089 23 builder.go:147] stdout: "e2e-test-crd-publish-openapi-8201-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/12/25 16:43:21.256
  I0512 16:43:21.256770 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=crd-publish-openapi-8056 explain e2e-test-crd-publish-openapi-8201-crds'
  I0512 16:43:21.416208 23 builder.go:146] stderr: ""
  I0512 16:43:21.416305 23 builder.go:147] stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-8201-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0512 16:43:21.690652      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:22.691076      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:23.691749      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:43:24.684098 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  E0512 16:43:24.693460      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-publish-openapi-8056" for this suite. @ 05/12/25 16:43:24.701
• [12.861 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/downwardapi.go:168
  STEP: Creating a kubernetes client @ 05/12/25 16:43:24.713
  I0512 16:43:24.713469 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:43:24.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:43:24.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:43:24.841
  STEP: Creating a pod to test downward api env vars @ 05/12/25 16:43:24.849
  E0512 16:43:25.693859      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:26.694038      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:27.694144      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:28.694435      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:43:28.906
  I0512 16:43:28.914533 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downward-api-ed80f9c1-3b05-45ab-bd2b-09369e958874 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/25 16:43:28.956
  I0512 16:43:29.007136 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1146" for this suite. @ 05/12/25 16:43:29.022
• [4.327 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/cronjob.go:168
  STEP: Creating a kubernetes client @ 05/12/25 16:43:29.043
  I0512 16:43:29.043270 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename cronjob @ 05/12/25 16:43:29.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:43:29.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:43:29.106
  STEP: Creating a ReplaceConcurrent cronjob @ 05/12/25 16:43:29.117
  STEP: Ensuring a job is scheduled @ 05/12/25 16:43:29.136
  E0512 16:43:29.694961      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:30.695331      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:31.695534      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:32.695684      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:33.696055      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:34.696988      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:35.697185      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:36.698051      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:37.698118      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:38.698611      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:39.699333      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:40.699830      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:41.700132      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:42.701217      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:43.701619      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:44.701788      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:45.702140      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:46.702943      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:47.703117      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:48.703692      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:49.704781      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:50.705946      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:51.706212      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:52.706499      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:53.707142      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:54.707346      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:55.707412      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:56.707739      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:57.708038      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:58.708769      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:43:59.710113      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:00.711216      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/12/25 16:44:01.144
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/12/25 16:44:01.155
  STEP: Ensuring the job is replaced with a new one @ 05/12/25 16:44:01.165
  E0512 16:44:01.711777      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:02.712055      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:03.712285      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:04.713136      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:05.713541      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:06.713969      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:07.714648      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:08.715072      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:09.715758      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:10.716200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:11.716835      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:12.716943      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:13.717509      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:14.717668      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:15.717962      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:16.718711      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:17.719551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:18.720051      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:19.720957      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:20.721291      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:21.722033      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:22.722311      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:23.723185      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:24.724007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:25.724884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:26.725497      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:27.725937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:28.726062      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:29.726937      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:30.727126      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:31.728391      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:32.728460      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:33.728760      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:34.728963      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:35.730068      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:36.730091      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:37.732596      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:38.733365      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:39.733904      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:40.734157      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:41.734419      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:42.734844      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:43.735757      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:44.738089      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:45.738541      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:46.739034      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:47.739710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:48.740440      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:49.740902      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:50.741554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:51.742008      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:52.742246      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:53.742736      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:54.743241      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:55.744026      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:56.744471      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:57.744733      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:58.745207      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:44:59.745412      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:00.746681      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/12/25 16:45:01.174
  I0512 16:45:01.193732 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4677" for this suite. @ 05/12/25 16:45:01.228
• [92.237 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:237
  STEP: Creating a kubernetes client @ 05/12/25 16:45:01.281
  I0512 16:45:01.282014 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:45:01.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:45:01.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:45:01.334
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 16:45:01.341
  E0512 16:45:01.748269      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:02.748726      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:03.749367      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:04.749934      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:45:05.403
  I0512 16:45:05.411061 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-176e955d-d4e7-4d4c-ba19-54a6ff907a6f container client-container: <nil>
  STEP: delete the pod @ 05/12/25 16:45:05.44
  I0512 16:45:05.473781 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9492" for this suite. @ 05/12/25 16:45:05.483
• [4.218 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] API priority and fairness should support FlowSchema API operations [Conformance] [sig-api-machinery, Conformance]
k8s.io/kubernetes/test/e2e/apimachinery/flowcontrol.go:270
  STEP: Creating a kubernetes client @ 05/12/25 16:45:05.5
  I0512 16:45:05.500965 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename apf @ 05/12/25 16:45:05.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:45:05.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:45:05.551
  STEP: getting /apis @ 05/12/25 16:45:05.557
  STEP: getting /apis/flowcontrol.apiserver.k8s.io @ 05/12/25 16:45:05.569
  STEP: getting /apis/flowcontrol.apiserver.k8s.io/v1 @ 05/12/25 16:45:05.572
  STEP: creating @ 05/12/25 16:45:05.574
  STEP: getting @ 05/12/25 16:45:05.624
  STEP: listing @ 05/12/25 16:45:05.638
  STEP: watching @ 05/12/25 16:45:05.647
  I0512 16:45:05.647839 23 flowcontrol.go:394] starting watch
  STEP: patching @ 05/12/25 16:45:05.651
  STEP: updating @ 05/12/25 16:45:05.668
  I0512 16:45:05.684960 23 flowcontrol.go:422] waiting for watch events with expected annotations
  STEP: getting /status @ 05/12/25 16:45:05.685
  STEP: patching /status @ 05/12/25 16:45:05.693
  STEP: updating /status @ 05/12/25 16:45:05.711
  STEP: deleting @ 05/12/25 16:45:05.74
  E0512 16:45:05.750952      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: deleting a collection @ 05/12/25 16:45:05.771
  I0512 16:45:05.825068 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "apf-8854" for this suite. @ 05/12/25 16:45:05.833
• [0.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance] [sig-apps, Slow, Conformance]
k8s.io/kubernetes/test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/12/25 16:45:05.851
  I0512 16:45:05.851342 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename cronjob @ 05/12/25 16:45:05.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:45:05.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:45:05.886
  STEP: Creating a suspended cronjob @ 05/12/25 16:45:05.891
  STEP: Ensuring no jobs are scheduled @ 05/12/25 16:45:05.901
  E0512 16:45:06.751650      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:07.752427      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:08.752893      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:09.753784      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:10.754031      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:11.754933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:12.754985      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:13.755378      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:14.756074      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:15.756899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:16.757146      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:17.757404      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:18.757639      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:19.757697      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:20.758415      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:21.759185      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:22.759933      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:23.772506      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:24.760989      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:25.762006      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:26.762791      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:27.763832      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:28.764245      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:29.765344      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:30.765531      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:31.765946      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:32.766446      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:33.767188      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:34.766966      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:35.767510      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:36.767706      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:37.768597      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:38.769300      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:39.770152      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:40.771111      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:41.771941      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:42.772220      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:43.772870      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:44.773302      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:45.773675      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:46.774773      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:47.775392      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:48.775693      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:49.775899      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:50.776283      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:51.776727      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:52.777082      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:53.777765      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:54.778848      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:55.779239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:56.780565      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:57.781111      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:58.781259      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:45:59.782366      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:00.782500      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:01.783015      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:02.783052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:03.783286      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:04.783956      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:05.784799      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:06.784865      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:07.785520      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:08.786511      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:09.787163      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:10.787409      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:11.787849      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:12.788131      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:13.788637      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:14.788684      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:15.788997      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:16.789125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:17.789352      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:18.789438      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:19.790390      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:20.790326      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:21.790780      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:22.790912      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:23.791486      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:24.792831      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:25.792707      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:26.792905      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:27.793348      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:28.794343      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:29.795264      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:30.795989      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:31.796790      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:32.797680      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:33.797811      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:34.798738      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:35.799148      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:36.799928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:37.800258      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:38.801306      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:39.801863      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:40.802862      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:41.803303      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:42.803895      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:43.804413      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:44.805201      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:45.805362      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:46.806505      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:47.806884      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:48.807454      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:49.807908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:50.808234      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:51.808427      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:52.809441      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:53.809715      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:54.809908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:55.810087      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:56.810239      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:57.810392      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:58.810774      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:46:59.810710      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:00.811012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:01.811298      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:02.811506      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:03.811957      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:04.812179      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:05.812469      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:06.813690      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:07.813950      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:08.814470      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:09.815436      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:10.815682      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:11.816047      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:12.816507      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:13.816740      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:14.817056      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:15.817256      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:16.817218      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:17.817596      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:18.818613      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:19.819129      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:20.819380      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:21.820030      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:22.820442      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:23.821224      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:24.821359      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:25.822331      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:26.822455      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:27.823868      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:28.824613      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:29.825554      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:30.826390      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:31.826981      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:32.827196      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:33.827439      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:34.828335      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:35.829520      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:36.829954      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:37.830200      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:38.830585      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:39.830866      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:40.830989      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:41.831303      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:42.831442      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:43.832120      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:44.833184      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:45.833578      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:46.834222      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:47.834418      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:48.835318      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:49.836418      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:50.836979      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:51.837891      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:52.839013      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:53.839443      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:54.839965      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:55.840484      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:56.841134      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:57.841889      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:58.842318      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:47:59.842870      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:00.843783      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:01.843983      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:02.844143      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:03.845445      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:04.845333      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:05.846155      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:06.846231      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:07.846636      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:08.847404      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:09.848229      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:10.849126      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:11.849917      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:12.850300      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:13.850407      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:14.851353      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:15.851786      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:16.852284      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:17.852624      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:18.852814      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:19.853958      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:20.854337      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:21.855035      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:22.855181      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:23.855355      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:24.856218      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:25.856629      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:26.856760      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:27.857014      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:28.857659      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:29.857621      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:30.858009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:31.858310      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:32.858906      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:33.859022      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:34.859930      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:35.860863      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:36.861144      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:37.861935      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:38.862561      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:39.863533      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:40.864168      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:41.864800      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:42.865691      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:43.865916      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:44.866992      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:45.867392      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:46.867508      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:47.868041      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:48.868480      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:49.868699      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:50.869085      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:51.869268      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:52.869764      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:53.870066      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:54.870169      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:55.870604      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:56.870827      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:57.871114      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:58.871339      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:48:59.871856      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:00.872172      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:01.872456      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:02.873012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:03.873942      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:04.875012      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:05.875411      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:06.876007      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:07.876204      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:08.876457      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:09.877517      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:10.877606      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:11.878421      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:12.878901      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:13.879156      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:14.880191      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:15.880914      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:16.881400      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:17.882389      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:18.882882      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:19.882908      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:20.883348      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:21.884437      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:22.884653      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:23.884947      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:24.892755      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:25.885279      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:26.885747      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:27.886551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:28.887057      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:29.887973      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:30.888551      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:31.888890      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:32.889433      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:33.889897      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:34.889885      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:35.890063      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:36.890659      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:37.890895      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:38.891692      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:39.891873      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:40.892352      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:41.892841      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:42.893461      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:43.893600      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:44.894588      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:45.894466      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:46.895039      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:47.895219      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:48.895712      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:49.896639      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:50.896905      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:51.897125      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:52.897325      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:53.898412      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:54.899481      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:55.899452      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:56.899914      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:57.900224      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:58.900440      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:49:59.901342      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:00.901604      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:01.902188      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:02.902346      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:03.902805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:04.902903      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/12/25 16:50:05.901
  E0512 16:50:05.902942      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/12/25 16:50:05.908
  I0512 16:50:05.920932 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-318" for this suite. @ 05/12/25 16:50:05.933
• [300.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:195
  STEP: Creating a kubernetes client @ 05/12/25 16:50:05.947
  I0512 16:50:05.947512 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:50:05.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:05.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:05.998
  STEP: Creating a pod to test downward API volume plugin @ 05/12/25 16:50:06.007
  E0512 16:50:06.903255      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:07.904214      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:08.904579      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:09.905354      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:50:10.067
  I0512 16:50:10.074760 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod downwardapi-volume-e52fb926-e0eb-4ddb-b8ee-a3ce8f4ad53d container client-container: <nil>
  STEP: delete the pod @ 05/12/25 16:50:10.119
  I0512 16:50:10.162327 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7405" for this suite. @ 05/12/25 16:50:10.174
• [4.242 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods [Conformance] [sig-cli, Conformance]
k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
  STEP: Creating a kubernetes client @ 05/12/25 16:50:10.189
  I0512 16:50:10.189383 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename kubectl @ 05/12/25 16:50:10.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:10.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:10.243
  I0512 16:50:10.249435 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7618 create -f -'
  I0512 16:50:10.557181 23 builder.go:146] stderr: ""
  I0512 16:50:10.557268 23 builder.go:147] stdout: "replicationcontroller/agnhost-primary created\n"
  I0512 16:50:10.557375 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7618 create -f -'
  E0512 16:50:10.905539      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:50:10.918737 23 builder.go:146] stderr: ""
  I0512 16:50:10.919577 23 builder.go:147] stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/12/25 16:50:10.919
  E0512 16:50:11.906085      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:50:11.944326 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:50:11.944410 23 framework.go:733] Found 0 / 1
  E0512 16:50:12.906373      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:50:12.933656 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:50:12.933738 23 framework.go:733] Found 1 / 1
  I0512 16:50:12.933774 23 framework.go:742] WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  I0512 16:50:12.944635 23 framework.go:692] Selector matched 1 pods for map[app:agnhost]
  I0512 16:50:12.944706 23 framework.go:765] ForEach: Found 1 pods from the filter.  Now looping through them.
  I0512 16:50:12.944817 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7618 describe pod agnhost-primary-j4x4z'
  I0512 16:50:13.132431 23 builder.go:146] stderr: ""
  I0512 16:50:13.132600 23 builder.go:147] stdout: "Name:             agnhost-primary-j4x4z\nNamespace:        kubectl-7618\nPriority:         0\nService Account:  default\nNode:             opscontrol-jaku1-worker-1/10.62.16.76\nStart Time:       Mon, 12 May 2025 16:50:10 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.69.154\nIPs:\n  IP:           10.233.69.154\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://dafc4a0c1fd4ac1dcc1518ecd4203b0ff3817560fee5a5ffdfdedeadd30d3361\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.52\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:b173c7d0ffe3d805d49f4dfe48375169b7b8d2e1feb81783efd61eb9d08042e6\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 12 May 2025 16:50:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z5ns6 (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True \n  Initialized                 True \n  Ready                       True \n  ContainersReady             True \n  PodScheduled                True \nVolumes:\n  kube-api-access-z5ns6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7618/agnhost-primary-j4x4z to opscontrol-jaku1-worker-1\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.52\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  I0512 16:50:13.132752 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7618 describe rc agnhost-primary'
  I0512 16:50:13.342610 23 builder.go:146] stderr: ""
  I0512 16:50:13.342733 23 builder.go:147] stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7618\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:         registry.k8s.io/e2e-test-images/agnhost:2.52\n    Port:          6379/TCP\n    Host Port:     0/TCP\n    Environment:   <none>\n    Mounts:        <none>\n  Volumes:         <none>\n  Node-Selectors:  <none>\n  Tolerations:     <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-j4x4z\n"
  I0512 16:50:13.343355 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7618 describe service agnhost-primary'
  I0512 16:50:13.547383 23 builder.go:146] stderr: ""
  I0512 16:50:13.547490 23 builder.go:147] stdout: "Name:                     agnhost-primary\nNamespace:                kubectl-7618\nLabels:                   app=agnhost\n                          role=primary\nAnnotations:              <none>\nSelector:                 app=agnhost,role=primary\nType:                     ClusterIP\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.233.2.6\nIPs:                      10.233.2.6\nPort:                     <unset>  6379/TCP\nTargetPort:               agnhost-server/TCP\nEndpoints:                10.233.69.154:6379\nSession Affinity:         None\nInternal Traffic Policy:  Cluster\nEvents:                   <none>\n"
  I0512 16:50:13.565058 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7618 describe node opscontrol-jaku1-master-0'
  I0512 16:50:13.837843 23 builder.go:146] stderr: ""
  I0512 16:50:13.838353 23 builder.go:147] stdout: "Name:               opscontrol-jaku1-master-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-ubuntu\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=opscontrol-jaku1-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.62.16.69\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"422b85f1-5a17-caf6-2ab1-7ecc9e5cea26\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":4096,\"VtepMAC\":\"7a:b3:78:08:98:00\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.62.16.69\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 12 May 2025 14:11:43 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  opscontrol-jaku1-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 12 May 2025 16:50:12 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 12 May 2025 14:14:31 +0000   Mon, 12 May 2025 14:14:31 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 12 May 2025 16:50:10 +0000   Mon, 12 May 2025 14:11:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 12 May 2025 16:50:10 +0000   Mon, 12 May 2025 14:11:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 12 May 2025 16:50:10 +0000   Mon, 12 May 2025 14:11:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 12 May 2025 16:50:10 +0000   Mon, 12 May 2025 14:14:38 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.62.16.69\n  ExternalIP:  10.62.16.69\n  Hostname:    opscontrol-jaku1-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  60795672Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8153188Ki\n  pods:               110\nAllocatable:\n  cpu:                1400m\n  ephemeral-storage:  54980715223\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7264356Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4284f3e04f9c47a4b59e54ce697ed9a6\n  System UUID:                422b85f1-5a17-caf6-2ab1-7ecc9e5cea26\n  Boot ID:                    51590a5e-404b-402c-bc05-e50f87755a8a\n  Kernel Version:             5.4.0-67-generic\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.24\n  Kubelet Version:            v1.31.4\n  Kube-Proxy Version:         v1.31.4\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   vsphere://422b85f1-5a17-caf6-2ab1-7ecc9e5cea26\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-d665d669-967ld                                     100m (7%)     0 (0%)      70Mi (0%)        300Mi (4%)     155m\n  kube-system                 dns-autoscaler-5cb4578f5f-zfbzq                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         155m\n  kube-system                 kube-apiserver-opscontrol-jaku1-master-0                   250m (17%)    0 (0%)      0 (0%)           0 (0%)         158m\n  kube-system                 kube-controller-manager-opscontrol-jaku1-master-0          200m (14%)    0 (0%)      0 (0%)           0 (0%)         158m\n  kube-system                 kube-flannel-2rnmf                                         150m (10%)    300m (21%)  64M (0%)         500M (6%)      155m\n  kube-system                 kube-proxy-scd2q                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         157m\n  kube-system                 kube-scheduler-opscontrol-jaku1-master-0                   100m (7%)     0 (0%)      0 (0%)           0 (0%)         158m\n  kube-system                 nodelocaldns-tb4vq                                         100m (7%)     0 (0%)      70Mi (0%)        200Mi (2%)     155m\n  kube-system                 vsphere-cloud-controller-manager-vglq9                     200m (14%)    0 (0%)      0 (0%)           0 (0%)         96m\n  kube-system                 vsphere-csi-controller-6c995c5d65-x2456                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\n  kube-system                 vsphere-csi-node-kxmg5                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8d6233f41a954275-v9pvt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                1120m (80%)    300m (21%)\n  memory             216100Ki (2%)  1024288000 (13%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
  I0512 16:50:13.838518 23 builder.go:121] Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3959303637 --namespace=kubectl-7618 describe namespace kubectl-7618'
  E0512 16:50:13.907427      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:50:14.043791 23 builder.go:146] stderr: ""
  I0512 16:50:14.043907 23 builder.go:147] stdout: "Name:         kubectl-7618\nLabels:       e2e-framework=kubectl\n              e2e-run=c7668d51-6bbe-406a-be04-bad37eb1c076\n              kubernetes.io/metadata.name=kubectl-7618\n              pod-security.kubernetes.io/audit=baseline\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  I0512 16:50:14.044135 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7618" for this suite. @ 05/12/25 16:50:14.057
• [3.886 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance] [sig-network, Conformance]
k8s.io/kubernetes/test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/12/25 16:50:14.076
  I0512 16:50:14.076943 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename proxy @ 05/12/25 16:50:14.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:14.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:14.172
  I0512 16:50:14.189185 23 proxy.go:387] Creating pod...
  E0512 16:50:14.908052      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:15.908453      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:50:16.246157 23 proxy.go:411] Creating service...
  I0512 16:50:16.271625 23 proxy.go:448] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/pods/agnhost/proxy?method=DELETE
  I0512 16:50:16.289434 23 proxy.go:530] http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  I0512 16:50:16.289558 23 proxy.go:448] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/pods/agnhost/proxy?method=OPTIONS
  I0512 16:50:16.297380 23 proxy.go:530] http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  I0512 16:50:16.297467 23 proxy.go:448] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/pods/agnhost/proxy?method=PATCH
  I0512 16:50:16.305583 23 proxy.go:530] http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  I0512 16:50:16.305669 23 proxy.go:448] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/pods/agnhost/proxy?method=POST
  I0512 16:50:16.313139 23 proxy.go:530] http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  I0512 16:50:16.313209 23 proxy.go:448] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/pods/agnhost/proxy?method=PUT
  I0512 16:50:16.321497 23 proxy.go:530] http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  I0512 16:50:16.321575 23 proxy.go:459] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/services/e2e-proxy-test-service/proxy?method=DELETE
  I0512 16:50:16.339712 23 proxy.go:530] http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  I0512 16:50:16.339795 23 proxy.go:459] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/services/e2e-proxy-test-service/proxy?method=OPTIONS
  I0512 16:50:16.351837 23 proxy.go:530] http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  I0512 16:50:16.351916 23 proxy.go:459] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/services/e2e-proxy-test-service/proxy?method=PATCH
  I0512 16:50:16.366767 23 proxy.go:530] http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  I0512 16:50:16.367093 23 proxy.go:459] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/services/e2e-proxy-test-service/proxy?method=POST
  I0512 16:50:16.389822 23 proxy.go:530] http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  I0512 16:50:16.389906 23 proxy.go:459] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/services/e2e-proxy-test-service/proxy?method=PUT
  I0512 16:50:16.402975 23 proxy.go:530] http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  I0512 16:50:16.403174 23 proxy.go:479] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/pods/agnhost/proxy?method=GET
  I0512 16:50:16.409260 23 proxy.go:487] http.Client request:GET StatusCode:301
  I0512 16:50:16.409333 23 proxy.go:479] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/services/e2e-proxy-test-service/proxy?method=GET
  I0512 16:50:16.427338 23 proxy.go:487] http.Client request:GET StatusCode:301
  I0512 16:50:16.427413 23 proxy.go:479] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/pods/agnhost/proxy?method=HEAD
  I0512 16:50:16.435231 23 proxy.go:487] http.Client request:HEAD StatusCode:301
  I0512 16:50:16.435299 23 proxy.go:479] Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7777/services/e2e-proxy-test-service/proxy?method=HEAD
  I0512 16:50:16.450685 23 proxy.go:487] http.Client request:HEAD StatusCode:301
  I0512 16:50:16.451452 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7777" for this suite. @ 05/12/25 16:50:16.463
• [2.405 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/12/25 16:50:16.483
  I0512 16:50:16.483760 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename projected @ 05/12/25 16:50:16.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:16.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:16.527
  STEP: Creating projection with secret that has name projected-secret-test-5c082dae-80b4-4073-900d-e064a7884813 @ 05/12/25 16:50:16.533
  STEP: Creating a pod to test consume secrets @ 05/12/25 16:50:16.545
  E0512 16:50:16.908928      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:17.909917      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:18.910008      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:19.911009      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:50:20.589
  I0512 16:50:20.594475 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-projected-secrets-f50c05b6-bbd6-4b82-b667-729ecaf4c21a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/25 16:50:20.606
  I0512 16:50:20.633621 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5852" for this suite. @ 05/12/25 16:50:20.641
• [4.167 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance] [sig-node, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/node/secrets.go:48
  STEP: Creating a kubernetes client @ 05/12/25 16:50:20.65
  I0512 16:50:20.650982 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename secrets @ 05/12/25 16:50:20.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:20.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:20.684
  STEP: Creating secret with name secret-test-1d17730a-1d7e-4957-8be7-f59348e6650e @ 05/12/25 16:50:20.69
  STEP: Creating a pod to test consume secrets @ 05/12/25 16:50:20.697
  E0512 16:50:20.912180      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:21.912805      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:22.912979      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:23.913398      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/25 16:50:24.74
  I0512 16:50:24.748780 23 output.go:196] Trying to get logs from node opscontrol-jaku1-worker-1 pod pod-secrets-e692d9df-aa91-47bc-aebd-58fb6d109ba5 container secret-env-test: <nil>
  STEP: delete the pod @ 05/12/25 16:50:24.763
  I0512 16:50:24.803256 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7600" for this suite. @ 05/12/25 16:50:24.813
• [4.181 seconds]
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/disruption.go:111
  STEP: Creating a kubernetes client @ 05/12/25 16:50:24.831
  I0512 16:50:24.831734 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename disruption @ 05/12/25 16:50:24.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:24.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:24.874
  STEP: creating the pdb @ 05/12/25 16:50:24.881
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:50:24.902
  E0512 16:50:24.913646      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 05/12/25 16:50:24.917
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:50:24.959
  E0512 16:50:25.914299      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:26.914861      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 05/12/25 16:50:26.973
  STEP: Waiting for the pdb to be processed @ 05/12/25 16:50:26.997
  STEP: Waiting for the pdb to be deleted @ 05/12/25 16:50:27.033
  I0512 16:50:27.041079 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7573" for this suite. @ 05/12/25 16:50:27.051
• [2.234 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance] [sig-storage, Conformance]
k8s.io/kubernetes/test/e2e/storage/csi_inline.go:50
  STEP: Creating a kubernetes client @ 05/12/25 16:50:27.066
  I0512 16:50:27.066578 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/12/25 16:50:27.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:27.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:27.115
  STEP: creating @ 05/12/25 16:50:27.121
  STEP: getting @ 05/12/25 16:50:27.163
  STEP: listing in namespace @ 05/12/25 16:50:27.17
  STEP: patching @ 05/12/25 16:50:27.179
  STEP: deleting @ 05/12/25 16:50:27.218
  I0512 16:50:27.245484 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1561" for this suite. @ 05/12/25 16:50:27.254
• [0.205 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance] [sig-storage, NodeConformance, Conformance]
k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:164
  STEP: Creating a kubernetes client @ 05/12/25 16:50:27.271
  I0512 16:50:27.271654 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename downward-api @ 05/12/25 16:50:27.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:27.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:27.334
  STEP: Creating the pod @ 05/12/25 16:50:27.344
  E0512 16:50:27.916384      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:28.917088      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:29.917290      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:50:29.956937 23 pod_client.go:173] Successfully updated pod "annotationupdatef45d97e3-3f1b-43dd-98c9-fa874a11cdc4"
  E0512 16:50:30.917579      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:31.918446      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  I0512 16:50:31.995333 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4941" for this suite. @ 05/12/25 16:50:32.006
• [4.753 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance] [sig-apps, Conformance]
k8s.io/kubernetes/test/e2e/apps/deployment.go:489
  STEP: Creating a kubernetes client @ 05/12/25 16:50:32.025
  I0512 16:50:32.026050 23 util.go:499] >>> kubeConfig: /tmp/kubeconfig-3959303637
  STEP: Building a namespace api object, basename deployment @ 05/12/25 16:50:32.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/25 16:50:32.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/25 16:50:32.073
  STEP: creating a Deployment @ 05/12/25 16:50:32.1
  I0512 16:50:32.100156 23 deployment.go:507] Creating simple deployment test-deployment-zg4jv
  I0512 16:50:32.136455 23 deployment.go:222] new replicaset for deployment "test-deployment-zg4jv" is yet to be created
  E0512 16:50:32.918721      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  E0512 16:50:33.919315      23 retrywatcher.go:131] "Watch failed" err="context canceled"
  STEP: Getting /status @ 05/12/25 16:50:34.174
  I0512 16:50:34.188297 23 deployment.go:532] Deployment test-deployment-zg4jv has Conditions: [{Available True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zg4jv-f4dbbbf74" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/12/25 16:50:34.193
  I0512 16:50:34.219925 23 deployment.go:552] updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2025, time.May, 12, 16, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2025, time.May, 12, 16, 50, 32, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-zg4jv-f4dbbbf74\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/12/25 16:50:34.22
  I0512 16:50:34.227483 23 deployment.go:579] Observed &Deployment event: ADDED
  I0512 16:50:34.227635 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zg4jv-f4dbbbf74"}
  I0512 16:50:34.228106 23 deployment.go:579] Observed &Deployment event: MODIFIED
  I0512 16:50:34.228194 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zg4jv-f4dbbbf74"}
  I0512 16:50:34.228250 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  I0512 16:50:34.228597 23 deployment.go:579] Observed &Deployment event: MODIFIED
  I0512 16:50:34.228678 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  I0512 16:50:34.228740 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zg4jv-f4dbbbf74" is progressing.}
  I0512 16:50:34.229041 23 deployment.go:579] Observed &Deployment event: MODIFIED
  I0512 16:50:34.229116 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  I0512 16:50:34.229146 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zg4jv-f4dbbbf74" has successfully progressed.}
  I0512 16:50:34.229430 23 deployment.go:579] Observed &Deployment event: MODIFIED
  I0512 16:50:34.229517 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  I0512 16:50:34.229614 23 deployment.go:575] Observed Deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zg4jv-f4dbbbf74" has successfully progressed.}
  I0512 16:50:34.229670 23 deployment.go:572] Found Deployment test-deployment-zg4jv in namespace deployment-4657 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  I0512 16:50:34.229702 23 deployment.go:583] Deployment test-deployment-zg4jv has an updated status
  STEP: patching the Statefulset Status @ 05/12/25 16:50:34.229
  I0512 16:50:34.229788 23 deployment.go:587] Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  I0512 16:50:34.244407 23 deployment.go:591] Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/12/25 16:50:34.244
  I0512 16:50:34.250515 23 deployment.go:616] Observed &Deployment event: ADDED
  I0512 16:50:34.250606 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zg4jv-f4dbbbf74"}
  I0512 16:50:34.250916 23 deployment.go:616] Observed &Deployment event: MODIFIED
  I0512 16:50:34.251011 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zg4jv-f4dbbbf74"}
  I0512 16:50:34.251075 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  I0512 16:50:34.251358 23 deployment.go:616] Observed &Deployment event: MODIFIED
  I0512 16:50:34.251873 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  I0512 16:50:34.251981 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:32 +0000 UTC 2025-05-12 16:50:32 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zg4jv-f4dbbbf74" is progressing.}
  I0512 16:50:34.252488 23 deployment.go:616] Observed &Deployment event: MODIFIED
  I0512 16:50:34.252693 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  I0512 16:50:34.252777 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zg4jv-f4dbbbf74" has successfully progressed.}
  I0512 16:50:34.253151 23 deployment.go:616] Observed &Deployment event: MODIFIED
  I0512 16:50:34.253261 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  I0512 16:50:34.253371 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2025-05-12 16:50:33 +0000 UTC 2025-05-12 16:50:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zg4jv-f4dbbbf74" has successfully progressed.}
  I0512 16:50:34.253445 23 deployment.go:612] Observed deployment test-deployment-zg4jv in namespace deployment-4657 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  I0512 16:50:34.253959 23 deployment.go:616] Observed &Deployment event: MODIFIED
  I0512 16:50:34.254150 23 deployment.go:609] Found deployment test-deployment-zg4jv in namespace deployment-4657 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  I0512 16:50:34.254189 23 deployment.go:620] Deployment test-deployment-zg4jv has a patched status
  I0512 16:50:34.264993 23 deployment.go:633] Deployment "test-deployment-zg4jv":
  (v1.Deployment) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=21) "test-deployment-zg4jv",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-4657",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "dfb0fe6c-8895-48b3-82a3-7fa5ec672ec3",
      ResourceVersion: (string) (len=5) "71298",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882665432,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=2) {
        (string) (len=3) "e2e": (string) (len=7) "testing",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) (len=1) {
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) <nil>,
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=3) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665432,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=657) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              00000020  7d 2c 22 66 3a 65 32 65  22 3a 7b 7d 2c 22 66 3a  |},"f:e2e":{},"f:|
              00000030  6e 61 6d 65 22 3a 7b 7d  7d 7d 2c 22 66 3a 73 70  |name":{}}},"f:sp|
              00000040  65 63 22 3a 7b 22 66 3a  70 72 6f 67 72 65 73 73  |ec":{"f:progress|
              00000050  44 65 61 64 6c 69 6e 65  53 65 63 6f 6e 64 73 22  |DeadlineSeconds"|
              00000060  3a 7b 7d 2c 22 66 3a 72  65 70 6c 69 63 61 73 22  |:{},"f:replicas"|
              00000070  3a 7b 7d 2c 22 66 3a 72  65 76 69 73 69 6f 6e 48  |:{},"f:revisionH|
              00000080  69 73 74 6f 72 79 4c 69  6d 69 74 22 3a 7b 7d 2c  |istoryLimit":{},|
              00000090  22 66 3a 73 65 6c 65 63  74 6f 72 22 3a 7b 7d 2c  |"f:selector":{},|
              000000a0  22 66 3a 73 74 72 61 74  65 67 79 22 3a 7b 22 66  |"f:strategy":{"f|
              000000b0  3a 72 6f 6c 6c 69 6e 67  55 70 64 61 74 65 22 3a  |:rollingUpdate":|
              000000c0  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6d 61 78 53 75  |{".":{},"f:maxSu|
              000000d0  72 67 65 22 3a 7b 7d 2c  22 66 3a 6d 61 78 55 6e  |rge":{},"f:maxUn|
              000000e0  61 76 61 69 6c 61 62 6c  65 22 3a 7b 7d 7d 2c 22  |available":{}},"|
              000000f0  66 3a 74 79 70 65 22 3a  7b 7d 7d 2c 22 66 3a 74  |f:type":{}},"f:t|
              00000100  65 6d 70 6c 61 74 65 22  3a 7b 22 66 3a 6d 65 74  |emplate":{"f:met|
              00000110  61 64 61 74 61 22 3a 7b  22 66 3a 6c 61 62 65 6c  |adata":{"f:label|
              00000120  73 22 3a 7b 22 2e 22 3a  7b 7d 2c 22 66 3a 65 32  |s":{".":{},"f:e2|
              00000130  65 22 3a 7b 7d 2c 22 66  3a 6e 61 6d 65 22 3a 7b  |e":{},"f:name":{|
              00000140  7d 7d 7d 2c 22 66 3a 73  70 65 63 22 3a 7b 22 66  |}}},"f:spec":{"f|
              00000150  3a 63 6f 6e 74 61 69 6e  65 72 73 22 3a 7b 22 6b  |:containers":{"k|
              00000160  3a 7b 5c 22 6e 61 6d 65  5c 22 3a 5c 22 68 74 74  |:{\"name\":\"htt|
              00000170  70 64 5c 22 7d 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |pd\"}":{".":{},"|
              00000180  66 3a 69 6d 61 67 65 22  3a 7b 7d 2c 22 66 3a 69  |f:image":{},"f:i|
              00000190  6d 61 67 65 50 75 6c 6c  50 6f 6c 69 63 79 22 3a  |magePullPolicy":|
              000001a0  7b 7d 2c 22 66 3a 6e 61  6d 65 22 3a 7b 7d 2c 22  |{},"f:name":{},"|
              000001b0  66 3a 72 65 73 6f 75 72  63 65 73 22 3a 7b 7d 2c  |f:resources":{},|
              000001c0  22 66 3a 73 65 63 75 72  69 74 79 43 6f 6e 74 65  |"f:securityConte|
              000001d0  78 74 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |xt":{},"f:termin|
              000001e0  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 61 74 68  |ationMessagePath|
              000001f0  22 3a 7b 7d 2c 22 66 3a  74 65 72 6d 69 6e 61 74  |":{},"f:terminat|
              00000200  69 6f 6e 4d 65 73 73 61  67 65 50 6f 6c 69 63 79  |ionMessagePolicy|
              00000210  22 3a 7b 7d 7d 7d 2c 22  66 3a 64 6e 73 50 6f 6c  |":{}}},"f:dnsPol|
              00000220  69 63 79 22 3a 7b 7d 2c  22 66 3a 72 65 73 74 61  |icy":{},"f:resta|
              00000230  72 74 50 6f 6c 69 63 79  22 3a 7b 7d 2c 22 66 3a  |rtPolicy":{},"f:|
              00000240  73 63 68 65 64 75 6c 65  72 4e 61 6d 65 22 3a 7b  |schedulerName":{|
              00000250  7d 2c 22 66 3a 73 65 63  75 72 69 74 79 43 6f 6e  |},"f:securityCon|
              00000260  74 65 78 74 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |text":{},"f:term|
              00000270  69 6e 61 74 69 6f 6e 47  72 61 63 65 50 65 72 69  |inationGracePeri|
              00000280  6f 64 53 65 63 6f 6e 64  73 22 3a 7b 7d 7d 7d 7d  |odSeconds":{}}}}|
              00000290  7d                                                |}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=8) "e2e.test",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665434,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=147) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 53 74 61 74  |{\"type\":\"Stat|
              00000030  75 73 50 61 74 63 68 65  64 5c 22 7d 22 3a 7b 22  |usPatched\"}":{"|
              00000040  2e 22 3a 7b 7d 2c 22 66  3a 6c 61 73 74 54 72 61  |.":{},"f:lastTra|
              00000050  6e 73 69 74 69 6f 6e 54  69 6d 65 22 3a 7b 7d 2c  |nsitionTime":{},|
              00000060  22 66 3a 6c 61 73 74 55  70 64 61 74 65 54 69 6d  |"f:lastUpdateTim|
              00000070  65 22 3a 7b 7d 2c 22 66  3a 73 74 61 74 75 73 22  |e":{},"f:status"|
              00000080  3a 7b 7d 2c 22 66 3a 74  79 70 65 22 3a 7b 7d 7d  |:{},"f:type":{}}|
              00000090  7d 7d 7d                                          |}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665434,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=373) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 72 65 76 69 73  69 6f 6e 22 3a 7b 7d 7d  |io/revision":{}}|
              00000050  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 22 66  |},"f:status":{"f|
              00000060  3a 61 76 61 69 6c 61 62  6c 65 52 65 70 6c 69 63  |:availableReplic|
              00000070  61 73 22 3a 7b 7d 2c 22  66 3a 63 6f 6e 64 69 74  |as":{},"f:condit|
              00000080  69 6f 6e 73 22 3a 7b 22  2e 22 3a 7b 7d 2c 22 6b  |ions":{".":{},"k|
              00000090  3a 7b 5c 22 74 79 70 65  5c 22 3a 5c 22 50 72 6f  |:{\"type\":\"Pro|
              000000a0  67 72 65 73 73 69 6e 67  5c 22 7d 22 3a 7b 22 2e  |gressing\"}":{".|
              000000b0  22 3a 7b 7d 2c 22 66 3a  6c 61 73 74 54 72 61 6e  |":{},"f:lastTran|
              000000c0  73 69 74 69 6f 6e 54 69  6d 65 22 3a 7b 7d 2c 22  |sitionTime":{},"|
              000000d0  66 3a 6c 61 73 74 55 70  64 61 74 65 54 69 6d 65  |f:lastUpdateTime|
              000000e0  22 3a 7b 7d 2c 22 66 3a  6d 65 73 73 61 67 65 22  |":{},"f:message"|
              000000f0  3a 7b 7d 2c 22 66 3a 72  65 61 73 6f 6e 22 3a 7b  |:{},"f:reason":{|
              00000100  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              00000110  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              00000120  3a 6f 62 73 65 72 76 65  64 47 65 6e 65 72 61 74  |:observedGenerat|
              00000130  69 6f 6e 22 3a 7b 7d 2c  22 66 3a 72 65 61 64 79  |ion":{},"f:ready|
              00000140  52 65 70 6c 69 63 61 73  22 3a 7b 7d 2c 22 66 3a  |Replicas":{},"f:|
              00000150  72 65 70 6c 69 63 61 73  22 3a 7b 7d 2c 22 66 3a  |replicas":{},"f:|
              00000160  75 70 64 61 74 65 64 52  65 70 6c 69 63 61 73 22  |updatedReplicas"|
              00000170  3a 7b 7d 7d 7d                                    |:{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.DeploymentSpec) {
      Replicas: (*int32)(1),
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=2) {
          (string) (len=3) "e2e": (string) (len=7) "testing",
          (string) (len=4) "name": (string) (len=5) "httpd"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=2) {
            (string) (len=3) "e2e": (string) (len=7) "testing",
            (string) (len=4) "name": (string) (len=5) "httpd"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      },
      Strategy: (v1.DeploymentStrategy) {
        Type: (v1.DeploymentStrategyType) (len=13) "RollingUpdate",
        RollingUpdate: (*v1.RollingUpdateDeployment)({
          MaxUnavailable: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          }),
          MaxSurge: (*intstr.IntOrString)({
            Type: (intstr.Type) 1,
            IntVal: (int32) 0,
            StrVal: (string) (len=3) "25%"
          })
        })
      },
      MinReadySeconds: (int32) 0,
      RevisionHistoryLimit: (*int32)(10),
      Paused: (bool) false,
      ProgressDeadlineSeconds: (*int32)(600)
    },
    Status: (v1.DeploymentStatus) {
      ObservedGeneration: (int64) 1,
      Replicas: (int32) 1,
      UpdatedReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      UnavailableReplicas: (int32) 0,
      Conditions: ([]v1.DeploymentCondition) (len=2) {
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=13) "StatusPatched",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.DeploymentCondition) {
          Type: (v1.DeploymentConditionType) (len=11) "Progressing",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastUpdateTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665434,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665434,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) (len=18) "FoundNewReplicaSet",
          Message: (string) (len=55) "Found new replica set \"test-deployment-zg4jv-f4dbbbf74\""
        }
      },
      CollisionCount: (*int32)(<nil>)
    }
  }


  I0512 16:50:34.294201 23 deployment.go:39] New ReplicaSet "test-deployment-zg4jv-f4dbbbf74" of Deployment "test-deployment-zg4jv":
  (v1.ReplicaSet) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=31) "test-deployment-zg4jv-f4dbbbf74",
      GenerateName: (string) "",
      Namespace: (string) (len=15) "deployment-4657",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "6a8b3d93-5eeb-4e87-851a-1bac31e4019b",
      ResourceVersion: (string) (len=5) "71289",
      Generation: (int64) 1,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882665432,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=3) {
        (string) (len=17) "pod-template-hash": (string) (len=9) "f4dbbbf74",
        (string) (len=3) "e2e": (string) (len=7) "testing",
        (string) (len=4) "name": (string) (len=5) "httpd"
      },
      Annotations: (map[string]string) (len=3) {
        (string) (len=41) "deployment.kubernetes.io/desired-replicas": (string) (len=1) "1",
        (string) (len=37) "deployment.kubernetes.io/max-replicas": (string) (len=1) "2",
        (string) (len=33) "deployment.kubernetes.io/revision": (string) (len=1) "1"
      },
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "Deployment",
          Name: (string) (len=21) "test-deployment-zg4jv",
          UID: (types.UID) (len=36) "dfb0fe6c-8895-48b3-82a3-7fa5ec672ec3",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665432,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=803) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 61 6e 6e 6f 74 61  74 69 6f 6e 73 22 3a 7b  |f:annotations":{|
              00000020  22 2e 22 3a 7b 7d 2c 22  66 3a 64 65 70 6c 6f 79  |".":{},"f:deploy|
              00000030  6d 65 6e 74 2e 6b 75 62  65 72 6e 65 74 65 73 2e  |ment.kubernetes.|
              00000040  69 6f 2f 64 65 73 69 72  65 64 2d 72 65 70 6c 69  |io/desired-repli|
              00000050  63 61 73 22 3a 7b 7d 2c  22 66 3a 64 65 70 6c 6f  |cas":{},"f:deplo|
              00000060  79 6d 65 6e 74 2e 6b 75  62 65 72 6e 65 74 65 73  |yment.kubernetes|
              00000070  2e 69 6f 2f 6d 61 78 2d  72 65 70 6c 69 63 61 73  |.io/max-replicas|
              00000080  22 3a 7b 7d 2c 22 66 3a  64 65 70 6c 6f 79 6d 65  |":{},"f:deployme|
              00000090  6e 74 2e 6b 75 62 65 72  6e 65 74 65 73 2e 69 6f  |nt.kubernetes.io|
              000000a0  2f 72 65 76 69 73 69 6f  6e 22 3a 7b 7d 7d 2c 22  |/revision":{}},"|
              000000b0  66 3a 6c 61 62 65 6c 73  22 3a 7b 22 2e 22 3a 7b  |f:labels":{".":{|
              000000c0  7d 2c 22 66 3a 65 32 65  22 3a 7b 7d 2c 22 66 3a  |},"f:e2e":{},"f:|
              000000d0  6e 61 6d 65 22 3a 7b 7d  2c 22 66 3a 70 6f 64 2d  |name":{},"f:pod-|
              000000e0  74 65 6d 70 6c 61 74 65  2d 68 61 73 68 22 3a 7b  |template-hash":{|
              000000f0  7d 7d 2c 22 66 3a 6f 77  6e 65 72 52 65 66 65 72  |}},"f:ownerRefer|
              00000100  65 6e 63 65 73 22 3a 7b  22 2e 22 3a 7b 7d 2c 22  |ences":{".":{},"|
              00000110  6b 3a 7b 5c 22 75 69 64  5c 22 3a 5c 22 64 66 62  |k:{\"uid\":\"dfb|
              00000120  30 66 65 36 63 2d 38 38  39 35 2d 34 38 62 33 2d  |0fe6c-8895-48b3-|
              00000130  38 32 61 33 2d 37 66 61  35 65 63 36 37 32 65 63  |82a3-7fa5ec672ec|
              00000140  33 5c 22 7d 22 3a 7b 7d  7d 7d 2c 22 66 3a 73 70  |3\"}":{}}},"f:sp|
              00000150  65 63 22 3a 7b 22 66 3a  72 65 70 6c 69 63 61 73  |ec":{"f:replicas|
              00000160  22 3a 7b 7d 2c 22 66 3a  73 65 6c 65 63 74 6f 72  |":{},"f:selector|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 65 6d 70 6c 61 74 65  |":{},"f:template|
              00000180  22 3a 7b 22 66 3a 6d 65  74 61 64 61 74 61 22 3a  |":{"f:metadata":|
              00000190  7b 22 66 3a 6c 61 62 65  6c 73 22 3a 7b 22 2e 22  |{"f:labels":{"."|
              000001a0  3a 7b 7d 2c 22 66 3a 65  32 65 22 3a 7b 7d 2c 22  |:{},"f:e2e":{},"|
              000001b0  66 3a 6e 61 6d 65 22 3a  7b 7d 2c 22 66 3a 70 6f  |f:name":{},"f:po|
              000001c0  64 2d 74 65 6d 70 6c 61  74 65 2d 68 61 73 68 22  |d-template-hash"|
              000001d0  3a 7b 7d 7d 7d 2c 22 66  3a 73 70 65 63 22 3a 7b  |:{}}},"f:spec":{|
              000001e0  22 66 3a 63 6f 6e 74 61  69 6e 65 72 73 22 3a 7b  |"f:containers":{|
              000001f0  22 6b 3a 7b 5c 22 6e 61  6d 65 5c 22 3a 5c 22 68  |"k:{\"name\":\"h|
              00000200  74 74 70 64 5c 22 7d 22  3a 7b 22 2e 22 3a 7b 7d  |ttpd\"}":{".":{}|
              00000210  2c 22 66 3a 69 6d 61 67  65 22 3a 7b 7d 2c 22 66  |,"f:image":{},"f|
              00000220  3a 69 6d 61 67 65 50 75  6c 6c 50 6f 6c 69 63 79  |:imagePullPolicy|
              00000230  22 3a 7b 7d 2c 22 66 3a  6e 61 6d 65 22 3a 7b 7d  |":{},"f:name":{}|
              00000240  2c 22 66 3a 72 65 73 6f  75 72 63 65 73 22 3a 7b  |,"f:resources":{|
              00000250  7d 2c 22 66 3a 73 65 63  75 72 69 74 79 43 6f 6e  |},"f:securityCon|
              00000260  74 65 78 74 22 3a 7b 7d  2c 22 66 3a 74 65 72 6d  |text":{},"f:term|
              00000270  69 6e 61 74 69 6f 6e 4d  65 73 73 61 67 65 50 61  |inationMessagePa|
              00000280  74 68 22 3a 7b 7d 2c 22  66 3a 74 65 72 6d 69 6e  |th":{},"f:termin|
              00000290  61 74 69 6f 6e 4d 65 73  73 61 67 65 50 6f 6c 69  |ationMessagePoli|
              000002a0  63 79 22 3a 7b 7d 7d 7d  2c 22 66 3a 64 6e 73 50  |cy":{}}},"f:dnsP|
              000002b0  6f 6c 69 63 79 22 3a 7b  7d 2c 22 66 3a 72 65 73  |olicy":{},"f:res|
              000002c0  74 61 72 74 50 6f 6c 69  63 79 22 3a 7b 7d 2c 22  |tartPolicy":{},"|
              000002d0  66 3a 73 63 68 65 64 75  6c 65 72 4e 61 6d 65 22  |f:schedulerName"|
              000002e0  3a 7b 7d 2c 22 66 3a 73  65 63 75 72 69 74 79 43  |:{},"f:securityC|
              000002f0  6f 6e 74 65 78 74 22 3a  7b 7d 2c 22 66 3a 74 65  |ontext":{},"f:te|
              00000300  72 6d 69 6e 61 74 69 6f  6e 47 72 61 63 65 50 65  |rminationGracePe|
              00000310  72 69 6f 64 53 65 63 6f  6e 64 73 22 3a 7b 7d 7d  |riodSeconds":{}}|
              00000320  7d 7d 7d                                          |}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=7) "apps/v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665433,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=130) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  61 76 61 69 6c 61 62 6c  65 52 65 70 6c 69 63 61  |availableReplica|
              00000020  73 22 3a 7b 7d 2c 22 66  3a 66 75 6c 6c 79 4c 61  |s":{},"f:fullyLa|
              00000030  62 65 6c 65 64 52 65 70  6c 69 63 61 73 22 3a 7b  |beledReplicas":{|
              00000040  7d 2c 22 66 3a 6f 62 73  65 72 76 65 64 47 65 6e  |},"f:observedGen|
              00000050  65 72 61 74 69 6f 6e 22  3a 7b 7d 2c 22 66 3a 72  |eration":{},"f:r|
              00000060  65 61 64 79 52 65 70 6c  69 63 61 73 22 3a 7b 7d  |eadyReplicas":{}|
              00000070  2c 22 66 3a 72 65 70 6c  69 63 61 73 22 3a 7b 7d  |,"f:replicas":{}|
              00000080  7d 7d                                             |}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.ReplicaSetSpec) {
      Replicas: (*int32)(1),
      MinReadySeconds: (int32) 0,
      Selector: (*v1.LabelSelector)({
        MatchLabels: (map[string]string) (len=3) {
          (string) (len=3) "e2e": (string) (len=7) "testing",
          (string) (len=4) "name": (string) (len=5) "httpd",
          (string) (len=17) "pod-template-hash": (string) (len=9) "f4dbbbf74"
        },
        MatchExpressions: ([]v1.LabelSelectorRequirement) <nil>
      }),
      Template: (v1.PodTemplateSpec) {
        ObjectMeta: (v1.ObjectMeta) {
          Name: (string) "",
          GenerateName: (string) "",
          Namespace: (string) "",
          SelfLink: (string) "",
          UID: (types.UID) "",
          ResourceVersion: (string) "",
          Generation: (int64) 0,
          CreationTimestamp: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          DeletionTimestamp: (*v1.Time)(<nil>),
          DeletionGracePeriodSeconds: (*int64)(<nil>),
          Labels: (map[string]string) (len=3) {
            (string) (len=3) "e2e": (string) (len=7) "testing",
            (string) (len=4) "name": (string) (len=5) "httpd",
            (string) (len=17) "pod-template-hash": (string) (len=9) "f4dbbbf74"
          },
          Annotations: (map[string]string) <nil>,
          OwnerReferences: ([]v1.OwnerReference) <nil>,
          Finalizers: ([]string) <nil>,
          ManagedFields: ([]v1.ManagedFieldsEntry) <nil>
        },
        Spec: (v1.PodSpec) {
          Volumes: ([]v1.Volume) <nil>,
          InitContainers: ([]v1.Container) <nil>,
          Containers: ([]v1.Container) (len=1) {
            (v1.Container) {
              Name: (string) (len=5) "httpd",
              Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
              Command: ([]string) <nil>,
              Args: ([]string) <nil>,
              WorkingDir: (string) "",
              Ports: ([]v1.ContainerPort) <nil>,
              EnvFrom: ([]v1.EnvFromSource) <nil>,
              Env: ([]v1.EnvVar) <nil>,
              Resources: (v1.ResourceRequirements) {
                Limits: (v1.ResourceList) <nil>,
                Requests: (v1.ResourceList) <nil>,
                Claims: ([]v1.ResourceClaim) <nil>
              },
              ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
              RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
              VolumeMounts: ([]v1.VolumeMount) <nil>,
              VolumeDevices: ([]v1.VolumeDevice) <nil>,
              LivenessProbe: (*v1.Probe)(<nil>),
              ReadinessProbe: (*v1.Probe)(<nil>),
              StartupProbe: (*v1.Probe)(<nil>),
              Lifecycle: (*v1.Lifecycle)(<nil>),
              TerminationMessagePath: (string) (len=20) "/dev/termination-log",
              TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
              ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
              SecurityContext: (*v1.SecurityContext)({
                Capabilities: (*v1.Capabilities)(<nil>),
                Privileged: (*bool)(<nil>),
                SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
                WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
                RunAsUser: (*int64)(<nil>),
                RunAsGroup: (*int64)(<nil>),
                RunAsNonRoot: (*bool)(<nil>),
                ReadOnlyRootFilesystem: (*bool)(<nil>),
                AllowPrivilegeEscalation: (*bool)(<nil>),
                ProcMount: (*v1.ProcMountType)(<nil>),
                SeccompProfile: (*v1.SeccompProfile)(<nil>),
                AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
              }),
              Stdin: (bool) false,
              StdinOnce: (bool) false,
              TTY: (bool) false
            }
          },
          EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
          RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
          TerminationGracePeriodSeconds: (*int64)(0),
          ActiveDeadlineSeconds: (*int64)(<nil>),
          DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
          NodeSelector: (map[string]string) <nil>,
          ServiceAccountName: (string) "",
          DeprecatedServiceAccount: (string) "",
          AutomountServiceAccountToken: (*bool)(<nil>),
          NodeName: (string) "",
          HostNetwork: (bool) false,
          HostPID: (bool) false,
          HostIPC: (bool) false,
          ShareProcessNamespace: (*bool)(<nil>),
          SecurityContext: (*v1.PodSecurityContext)({
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            SupplementalGroups: ([]int64) <nil>,
            SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
            FSGroup: (*int64)(<nil>),
            Sysctls: ([]v1.Sysctl) <nil>,
            FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
          Hostname: (string) "",
          Subdomain: (string) "",
          Affinity: (*v1.Affinity)(<nil>),
          SchedulerName: (string) (len=17) "default-scheduler",
          Tolerations: ([]v1.Toleration) <nil>,
          HostAliases: ([]v1.HostAlias) <nil>,
          PriorityClassName: (string) "",
          Priority: (*int32)(<nil>),
          DNSConfig: (*v1.PodDNSConfig)(<nil>),
          ReadinessGates: ([]v1.PodReadinessGate) <nil>,
          RuntimeClassName: (*string)(<nil>),
          EnableServiceLinks: (*bool)(<nil>),
          PreemptionPolicy: (*v1.PreemptionPolicy)(<nil>),
          Overhead: (v1.ResourceList) <nil>,
          TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
          SetHostnameAsFQDN: (*bool)(<nil>),
          OS: (*v1.PodOS)(<nil>),
          HostUsers: (*bool)(<nil>),
          SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
          ResourceClaims: ([]v1.PodResourceClaim) <nil>
        }
      }
    },
    Status: (v1.ReplicaSetStatus) {
      Replicas: (int32) 1,
      FullyLabeledReplicas: (int32) 1,
      ReadyReplicas: (int32) 1,
      AvailableReplicas: (int32) 1,
      ObservedGeneration: (int64) 1,
      Conditions: ([]v1.ReplicaSetCondition) <nil>
    }
  }

  I0512 16:50:34.306386 23 deployment.go:67] Pod "test-deployment-zg4jv-f4dbbbf74-6hvhv" is available:
  (v1.Pod) {
    TypeMeta: (v1.TypeMeta) {
      Kind: (string) "",
      APIVersion: (string) ""
    },
    ObjectMeta: (v1.ObjectMeta) {
      Name: (string) (len=37) "test-deployment-zg4jv-f4dbbbf74-6hvhv",
      GenerateName: (string) (len=32) "test-deployment-zg4jv-f4dbbbf74-",
      Namespace: (string) (len=15) "deployment-4657",
      SelfLink: (string) "",
      UID: (types.UID) (len=36) "6b7dc464-a9bf-4de1-bc46-e7690d66a05b",
      ResourceVersion: (string) (len=5) "71288",
      Generation: (int64) 0,
      CreationTimestamp: (v1.Time) {
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882665432,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      },
      DeletionTimestamp: (*v1.Time)(<nil>),
      DeletionGracePeriodSeconds: (*int64)(<nil>),
      Labels: (map[string]string) (len=3) {
        (string) (len=3) "e2e": (string) (len=7) "testing",
        (string) (len=4) "name": (string) (len=5) "httpd",
        (string) (len=17) "pod-template-hash": (string) (len=9) "f4dbbbf74"
      },
      Annotations: (map[string]string) <nil>,
      OwnerReferences: ([]v1.OwnerReference) (len=1) {
        (v1.OwnerReference) {
          APIVersion: (string) (len=7) "apps/v1",
          Kind: (string) (len=10) "ReplicaSet",
          Name: (string) (len=31) "test-deployment-zg4jv-f4dbbbf74",
          UID: (types.UID) (len=36) "6a8b3d93-5eeb-4e87-851a-1bac31e4019b",
          Controller: (*bool)(true),
          BlockOwnerDeletion: (*bool)(true)
        }
      },
      Finalizers: ([]string) <nil>,
      ManagedFields: ([]v1.ManagedFieldsEntry) (len=2) {
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=23) "kube-controller-manager",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665432,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=548) {
              00000000  7b 22 66 3a 6d 65 74 61  64 61 74 61 22 3a 7b 22  |{"f:metadata":{"|
              00000010  66 3a 67 65 6e 65 72 61  74 65 4e 61 6d 65 22 3a  |f:generateName":|
              00000020  7b 7d 2c 22 66 3a 6c 61  62 65 6c 73 22 3a 7b 22  |{},"f:labels":{"|
              00000030  2e 22 3a 7b 7d 2c 22 66  3a 65 32 65 22 3a 7b 7d  |.":{},"f:e2e":{}|
              00000040  2c 22 66 3a 6e 61 6d 65  22 3a 7b 7d 2c 22 66 3a  |,"f:name":{},"f:|
              00000050  70 6f 64 2d 74 65 6d 70  6c 61 74 65 2d 68 61 73  |pod-template-has|
              00000060  68 22 3a 7b 7d 7d 2c 22  66 3a 6f 77 6e 65 72 52  |h":{}},"f:ownerR|
              00000070  65 66 65 72 65 6e 63 65  73 22 3a 7b 22 2e 22 3a  |eferences":{".":|
              00000080  7b 7d 2c 22 6b 3a 7b 5c  22 75 69 64 5c 22 3a 5c  |{},"k:{\"uid\":\|
              00000090  22 36 61 38 62 33 64 39  33 2d 35 65 65 62 2d 34  |"6a8b3d93-5eeb-4|
              000000a0  65 38 37 2d 38 35 31 61  2d 31 62 61 63 33 31 65  |e87-851a-1bac31e|
              000000b0  34 30 31 39 62 5c 22 7d  22 3a 7b 7d 7d 7d 2c 22  |4019b\"}":{}}},"|
              000000c0  66 3a 73 70 65 63 22 3a  7b 22 66 3a 63 6f 6e 74  |f:spec":{"f:cont|
              000000d0  61 69 6e 65 72 73 22 3a  7b 22 6b 3a 7b 5c 22 6e  |ainers":{"k:{\"n|
              000000e0  61 6d 65 5c 22 3a 5c 22  68 74 74 70 64 5c 22 7d  |ame\":\"httpd\"}|
              000000f0  22 3a 7b 22 2e 22 3a 7b  7d 2c 22 66 3a 69 6d 61  |":{".":{},"f:ima|
              00000100  67 65 22 3a 7b 7d 2c 22  66 3a 69 6d 61 67 65 50  |ge":{},"f:imageP|
              00000110  75 6c 6c 50 6f 6c 69 63  79 22 3a 7b 7d 2c 22 66  |ullPolicy":{},"f|
              00000120  3a 6e 61 6d 65 22 3a 7b  7d 2c 22 66 3a 72 65 73  |:name":{},"f:res|
              00000130  6f 75 72 63 65 73 22 3a  7b 7d 2c 22 66 3a 73 65  |ources":{},"f:se|
              00000140  63 75 72 69 74 79 43 6f  6e 74 65 78 74 22 3a 7b  |curityContext":{|
              00000150  7d 2c 22 66 3a 74 65 72  6d 69 6e 61 74 69 6f 6e  |},"f:termination|
              00000160  4d 65 73 73 61 67 65 50  61 74 68 22 3a 7b 7d 2c  |MessagePath":{},|
              00000170  22 66 3a 74 65 72 6d 69  6e 61 74 69 6f 6e 4d 65  |"f:terminationMe|
              00000180  73 73 61 67 65 50 6f 6c  69 63 79 22 3a 7b 7d 7d  |ssagePolicy":{}}|
              00000190  7d 2c 22 66 3a 64 6e 73  50 6f 6c 69 63 79 22 3a  |},"f:dnsPolicy":|
              000001a0  7b 7d 2c 22 66 3a 65 6e  61 62 6c 65 53 65 72 76  |{},"f:enableServ|
              000001b0  69 63 65 4c 69 6e 6b 73  22 3a 7b 7d 2c 22 66 3a  |iceLinks":{},"f:|
              000001c0  72 65 73 74 61 72 74 50  6f 6c 69 63 79 22 3a 7b  |restartPolicy":{|
              000001d0  7d 2c 22 66 3a 73 63 68  65 64 75 6c 65 72 4e 61  |},"f:schedulerNa|
              000001e0  6d 65 22 3a 7b 7d 2c 22  66 3a 73 65 63 75 72 69  |me":{},"f:securi|
              000001f0  74 79 43 6f 6e 74 65 78  74 22 3a 7b 7d 2c 22 66  |tyContext":{},"f|
              00000200  3a 74 65 72 6d 69 6e 61  74 69 6f 6e 47 72 61 63  |:terminationGrac|
              00000210  65 50 65 72 69 6f 64 53  65 63 6f 6e 64 73 22 3a  |ePeriodSeconds":|
              00000220  7b 7d 7d 7d                                       |{}}}|
            }
          }),
          Subresource: (string) ""
        },
        (v1.ManagedFieldsEntry) {
          Manager: (string) (len=7) "kubelet",
          Operation: (v1.ManagedFieldsOperationType) (len=6) "Update",
          APIVersion: (string) (len=2) "v1",
          Time: (*v1.Time)({
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665433,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          }),
          FieldsType: (string) (len=8) "FieldsV1",
          FieldsV1: (*v1.FieldsV1)({
            Raw: ([]uint8) (len=662) {
              00000000  7b 22 66 3a 73 74 61 74  75 73 22 3a 7b 22 66 3a  |{"f:status":{"f:|
              00000010  63 6f 6e 64 69 74 69 6f  6e 73 22 3a 7b 22 6b 3a  |conditions":{"k:|
              00000020  7b 5c 22 74 79 70 65 5c  22 3a 5c 22 43 6f 6e 74  |{\"type\":\"Cont|
              00000030  61 69 6e 65 72 73 52 65  61 64 79 5c 22 7d 22 3a  |ainersReady\"}":|
              00000040  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000050  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000060  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000070  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000080  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000090  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              000000a0  22 49 6e 69 74 69 61 6c  69 7a 65 64 5c 22 7d 22  |"Initialized\"}"|
              000000b0  3a 7b 22 2e 22 3a 7b 7d  2c 22 66 3a 6c 61 73 74  |:{".":{},"f:last|
              000000c0  50 72 6f 62 65 54 69 6d  65 22 3a 7b 7d 2c 22 66  |ProbeTime":{},"f|
              000000d0  3a 6c 61 73 74 54 72 61  6e 73 69 74 69 6f 6e 54  |:lastTransitionT|
              000000e0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 73 74 61 74 75  |ime":{},"f:statu|
              000000f0  73 22 3a 7b 7d 2c 22 66  3a 74 79 70 65 22 3a 7b  |s":{},"f:type":{|
              00000100  7d 7d 2c 22 6b 3a 7b 5c  22 74 79 70 65 5c 22 3a  |}},"k:{\"type\":|
              00000110  5c 22 50 6f 64 52 65 61  64 79 54 6f 53 74 61 72  |\"PodReadyToStar|
              00000120  74 43 6f 6e 74 61 69 6e  65 72 73 5c 22 7d 22 3a  |tContainers\"}":|
              00000130  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 50  |{".":{},"f:lastP|
              00000140  72 6f 62 65 54 69 6d 65  22 3a 7b 7d 2c 22 66 3a  |robeTime":{},"f:|
              00000150  6c 61 73 74 54 72 61 6e  73 69 74 69 6f 6e 54 69  |lastTransitionTi|
              00000160  6d 65 22 3a 7b 7d 2c 22  66 3a 73 74 61 74 75 73  |me":{},"f:status|
              00000170  22 3a 7b 7d 2c 22 66 3a  74 79 70 65 22 3a 7b 7d  |":{},"f:type":{}|
              00000180  7d 2c 22 6b 3a 7b 5c 22  74 79 70 65 5c 22 3a 5c  |},"k:{\"type\":\|
              00000190  22 52 65 61 64 79 5c 22  7d 22 3a 7b 22 2e 22 3a  |"Ready\"}":{".":|
              000001a0  7b 7d 2c 22 66 3a 6c 61  73 74 50 72 6f 62 65 54  |{},"f:lastProbeT|
              000001b0  69 6d 65 22 3a 7b 7d 2c  22 66 3a 6c 61 73 74 54  |ime":{},"f:lastT|
              000001c0  72 61 6e 73 69 74 69 6f  6e 54 69 6d 65 22 3a 7b  |ransitionTime":{|
              000001d0  7d 2c 22 66 3a 73 74 61  74 75 73 22 3a 7b 7d 2c  |},"f:status":{},|
              000001e0  22 66 3a 74 79 70 65 22  3a 7b 7d 7d 7d 2c 22 66  |"f:type":{}}},"f|
              000001f0  3a 63 6f 6e 74 61 69 6e  65 72 53 74 61 74 75 73  |:containerStatus|
              00000200  65 73 22 3a 7b 7d 2c 22  66 3a 68 6f 73 74 49 50  |es":{},"f:hostIP|
              00000210  22 3a 7b 7d 2c 22 66 3a  68 6f 73 74 49 50 73 22  |":{},"f:hostIPs"|
              00000220  3a 7b 7d 2c 22 66 3a 70  68 61 73 65 22 3a 7b 7d  |:{},"f:phase":{}|
              00000230  2c 22 66 3a 70 6f 64 49  50 22 3a 7b 7d 2c 22 66  |,"f:podIP":{},"f|
              00000240  3a 70 6f 64 49 50 73 22  3a 7b 22 2e 22 3a 7b 7d  |:podIPs":{".":{}|
              00000250  2c 22 6b 3a 7b 5c 22 69  70 5c 22 3a 5c 22 31 30  |,"k:{\"ip\":\"10|
              00000260  2e 32 33 33 2e 36 39 2e  31 35 39 5c 22 7d 22 3a  |.233.69.159\"}":|
              00000270  7b 22 2e 22 3a 7b 7d 2c  22 66 3a 69 70 22 3a 7b  |{".":{},"f:ip":{|
              00000280  7d 7d 7d 2c 22 66 3a 73  74 61 72 74 54 69 6d 65  |}}},"f:startTime|
              00000290  22 3a 7b 7d 7d 7d                                 |":{}}}|
            }
          }),
          Subresource: (string) (len=6) "status"
        }
      }
    },
    Spec: (v1.PodSpec) {
      Volumes: ([]v1.Volume) (len=1) {
        (v1.Volume) {
          Name: (string) (len=21) "kube-api-access-mtd6k",
          VolumeSource: (v1.VolumeSource) {
            HostPath: (*v1.HostPathVolumeSource)(<nil>),
            EmptyDir: (*v1.EmptyDirVolumeSource)(<nil>),
            GCEPersistentDisk: (*v1.GCEPersistentDiskVolumeSource)(<nil>),
            AWSElasticBlockStore: (*v1.AWSElasticBlockStoreVolumeSource)(<nil>),
            GitRepo: (*v1.GitRepoVolumeSource)(<nil>),
            Secret: (*v1.SecretVolumeSource)(<nil>),
            NFS: (*v1.NFSVolumeSource)(<nil>),
            ISCSI: (*v1.ISCSIVolumeSource)(<nil>),
            Glusterfs: (*v1.GlusterfsVolumeSource)(<nil>),
            PersistentVolumeClaim: (*v1.PersistentVolumeClaimVolumeSource)(<nil>),
            RBD: (*v1.RBDVolumeSource)(<nil>),
            FlexVolume: (*v1.FlexVolumeSource)(<nil>),
            Cinder: (*v1.CinderVolumeSource)(<nil>),
            CephFS: (*v1.CephFSVolumeSource)(<nil>),
            Flocker: (*v1.FlockerVolumeSource)(<nil>),
            DownwardAPI: (*v1.DownwardAPIVolumeSource)(<nil>),
            FC: (*v1.FCVolumeSource)(<nil>),
            AzureFile: (*v1.AzureFileVolumeSource)(<nil>),
            ConfigMap: (*v1.ConfigMapVolumeSource)(<nil>),
            VsphereVolume: (*v1.VsphereVirtualDiskVolumeSource)(<nil>),
            Quobyte: (*v1.QuobyteVolumeSource)(<nil>),
            AzureDisk: (*v1.AzureDiskVolumeSource)(<nil>),
            PhotonPersistentDisk: (*v1.PhotonPersistentDiskVolumeSource)(<nil>),
            Projected: (*v1.ProjectedVolumeSource)({
              Sources: ([]v1.VolumeProjection) (len=3) {
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)({
                    Audience: (string) "",
                    ExpirationSeconds: (*int64)(3607),
                    Path: (string) (len=5) "token"
                  }),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)(<nil>),
                  ConfigMap: (*v1.ConfigMapProjection)({
                    LocalObjectReference: (v1.LocalObjectReference) {
                      Name: (string) (len=16) "kube-root-ca.crt"
                    },
                    Items: ([]v1.KeyToPath) (len=1) {
                      (v1.KeyToPath) {
                        Key: (string) (len=6) "ca.crt",
                        Path: (string) (len=6) "ca.crt",
                        Mode: (*int32)(<nil>)
                      }
                    },
                    Optional: (*bool)(<nil>)
                  }),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                },
                (v1.VolumeProjection) {
                  Secret: (*v1.SecretProjection)(<nil>),
                  DownwardAPI: (*v1.DownwardAPIProjection)({
                    Items: ([]v1.DownwardAPIVolumeFile) (len=1) {
                      (v1.DownwardAPIVolumeFile) {
                        Path: (string) (len=9) "namespace",
                        FieldRef: (*v1.ObjectFieldSelector)({
                          APIVersion: (string) (len=2) "v1",
                          FieldPath: (string) (len=18) "metadata.namespace"
                        }),
                        ResourceFieldRef: (*v1.ResourceFieldSelector)(<nil>),
                        Mode: (*int32)(<nil>)
                      }
                    }
                  }),
                  ConfigMap: (*v1.ConfigMapProjection)(<nil>),
                  ServiceAccountToken: (*v1.ServiceAccountTokenProjection)(<nil>),
                  ClusterTrustBundle: (*v1.ClusterTrustBundleProjection)(<nil>)
                }
              },
              DefaultMode: (*int32)(420)
            }),
            PortworxVolume: (*v1.PortworxVolumeSource)(<nil>),
            ScaleIO: (*v1.ScaleIOVolumeSource)(<nil>),
            StorageOS: (*v1.StorageOSVolumeSource)(<nil>),
            CSI: (*v1.CSIVolumeSource)(<nil>),
            Ephemeral: (*v1.EphemeralVolumeSource)(<nil>),
            Image: (*v1.ImageVolumeSource)(<nil>)
          }
        }
      },
      InitContainers: ([]v1.Container) <nil>,
      Containers: ([]v1.Container) (len=1) {
        (v1.Container) {
          Name: (string) (len=5) "httpd",
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          Command: ([]string) <nil>,
          Args: ([]string) <nil>,
          WorkingDir: (string) "",
          Ports: ([]v1.ContainerPort) <nil>,
          EnvFrom: ([]v1.EnvFromSource) <nil>,
          Env: ([]v1.EnvVar) <nil>,
          Resources: (v1.ResourceRequirements) {
            Limits: (v1.ResourceList) <nil>,
            Requests: (v1.ResourceList) <nil>,
            Claims: ([]v1.ResourceClaim) <nil>
          },
          ResizePolicy: ([]v1.ContainerResizePolicy) <nil>,
          RestartPolicy: (*v1.ContainerRestartPolicy)(<nil>),
          VolumeMounts: ([]v1.VolumeMount) (len=1) {
            (v1.VolumeMount) {
              Name: (string) (len=21) "kube-api-access-mtd6k",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)(<nil>),
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              SubPath: (string) "",
              MountPropagation: (*v1.MountPropagationMode)(<nil>),
              SubPathExpr: (string) ""
            }
          },
          VolumeDevices: ([]v1.VolumeDevice) <nil>,
          LivenessProbe: (*v1.Probe)(<nil>),
          ReadinessProbe: (*v1.Probe)(<nil>),
          StartupProbe: (*v1.Probe)(<nil>),
          Lifecycle: (*v1.Lifecycle)(<nil>),
          TerminationMessagePath: (string) (len=20) "/dev/termination-log",
          TerminationMessagePolicy: (v1.TerminationMessagePolicy) (len=4) "File",
          ImagePullPolicy: (v1.PullPolicy) (len=12) "IfNotPresent",
          SecurityContext: (*v1.SecurityContext)({
            Capabilities: (*v1.Capabilities)(<nil>),
            Privileged: (*bool)(<nil>),
            SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
            WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
            RunAsUser: (*int64)(<nil>),
            RunAsGroup: (*int64)(<nil>),
            RunAsNonRoot: (*bool)(<nil>),
            ReadOnlyRootFilesystem: (*bool)(<nil>),
            AllowPrivilegeEscalation: (*bool)(<nil>),
            ProcMount: (*v1.ProcMountType)(<nil>),
            SeccompProfile: (*v1.SeccompProfile)(<nil>),
            AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
          }),
          Stdin: (bool) false,
          StdinOnce: (bool) false,
          TTY: (bool) false
        }
      },
      EphemeralContainers: ([]v1.EphemeralContainer) <nil>,
      RestartPolicy: (v1.RestartPolicy) (len=6) "Always",
      TerminationGracePeriodSeconds: (*int64)(0),
      ActiveDeadlineSeconds: (*int64)(<nil>),
      DNSPolicy: (v1.DNSPolicy) (len=12) "ClusterFirst",
      NodeSelector: (map[string]string) <nil>,
      ServiceAccountName: (string) (len=7) "default",
      DeprecatedServiceAccount: (string) (len=7) "default",
      AutomountServiceAccountToken: (*bool)(<nil>),
      NodeName: (string) (len=25) "opscontrol-jaku1-worker-1",
      HostNetwork: (bool) false,
      HostPID: (bool) false,
      HostIPC: (bool) false,
      ShareProcessNamespace: (*bool)(<nil>),
      SecurityContext: (*v1.PodSecurityContext)({
        SELinuxOptions: (*v1.SELinuxOptions)(<nil>),
        WindowsOptions: (*v1.WindowsSecurityContextOptions)(<nil>),
        RunAsUser: (*int64)(<nil>),
        RunAsGroup: (*int64)(<nil>),
        RunAsNonRoot: (*bool)(<nil>),
        SupplementalGroups: ([]int64) <nil>,
        SupplementalGroupsPolicy: (*v1.SupplementalGroupsPolicy)(<nil>),
        FSGroup: (*int64)(<nil>),
        Sysctls: ([]v1.Sysctl) <nil>,
        FSGroupChangePolicy: (*v1.PodFSGroupChangePolicy)(<nil>),
        SeccompProfile: (*v1.SeccompProfile)(<nil>),
        AppArmorProfile: (*v1.AppArmorProfile)(<nil>)
      }),
      ImagePullSecrets: ([]v1.LocalObjectReference) <nil>,
      Hostname: (string) "",
      Subdomain: (string) "",
      Affinity: (*v1.Affinity)(<nil>),
      SchedulerName: (string) (len=17) "default-scheduler",
      Tolerations: ([]v1.Toleration) (len=2) {
        (v1.Toleration) {
          Key: (string) (len=28) "node.kubernetes.io/not-ready",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        },
        (v1.Toleration) {
          Key: (string) (len=30) "node.kubernetes.io/unreachable",
          Operator: (v1.TolerationOperator) (len=6) "Exists",
          Value: (string) "",
          Effect: (v1.TaintEffect) (len=9) "NoExecute",
          TolerationSeconds: (*int64)(300)
        }
      },
      HostAliases: ([]v1.HostAlias) <nil>,
      PriorityClassName: (string) "",
      Priority: (*int32)(0),
      DNSConfig: (*v1.PodDNSConfig)(<nil>),
      ReadinessGates: ([]v1.PodReadinessGate) <nil>,
      RuntimeClassName: (*string)(<nil>),
      EnableServiceLinks: (*bool)(true),
      PreemptionPolicy: (*v1.PreemptionPolicy)((len=20) "PreemptLowerPriority"),
      Overhead: (v1.ResourceList) <nil>,
      TopologySpreadConstraints: ([]v1.TopologySpreadConstraint) <nil>,
      SetHostnameAsFQDN: (*bool)(<nil>),
      OS: (*v1.PodOS)(<nil>),
      HostUsers: (*bool)(<nil>),
      SchedulingGates: ([]v1.PodSchedulingGate) <nil>,
      ResourceClaims: ([]v1.PodResourceClaim) <nil>
    },
    Status: (v1.PodStatus) {
      Phase: (v1.PodPhase) (len=7) "Running",
      Conditions: ([]v1.PodCondition) (len=5) {
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=25) "PodReadyToStartContainers",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665433,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=11) "Initialized",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665432,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=5) "Ready",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665433,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=15) "ContainersReady",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665433,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        },
        (v1.PodCondition) {
          Type: (v1.PodConditionType) (len=12) "PodScheduled",
          Status: (v1.ConditionStatus) (len=4) "True",
          LastProbeTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 0,
              loc: (*time.Location)(<nil>)
            }
          },
          LastTransitionTime: (v1.Time) {
            Time: (time.Time) {
              wall: (uint64) 0,
              ext: (int64) 63882665432,
              loc: (*time.Location)({
                name: (string) (len=3) "UTC",
                zone: ([]time.zone) <nil>,
                tx: ([]time.zoneTrans) <nil>,
                extend: (string) "",
                cacheStart: (int64) 0,
                cacheEnd: (int64) 0,
                cacheZone: (*time.zone)(<nil>)
              })
            }
          },
          Reason: (string) "",
          Message: (string) ""
        }
      },
      Message: (string) "",
      Reason: (string) "",
      NominatedNodeName: (string) "",
      HostIP: (string) (len=11) "10.62.16.76",
      HostIPs: ([]v1.HostIP) (len=1) {
        (v1.HostIP) {
          IP: (string) (len=11) "10.62.16.76"
        }
      },
      PodIP: (string) (len=13) "10.233.69.159",
      PodIPs: ([]v1.PodIP) (len=1) {
        (v1.PodIP) {
          IP: (string) (len=13) "10.233.69.159"
        }
      },
      StartTime: (*v1.Time)({
        Time: (time.Time) {
          wall: (uint64) 0,
          ext: (int64) 63882665432,
          loc: (*time.Location)({
            name: (string) (len=3) "UTC",
            zone: ([]time.zone) <nil>,
            tx: ([]time.zoneTrans) <nil>,
            extend: (string) "",
            cacheStart: (int64) 0,
            cacheEnd: (int64) 0,
            cacheZone: (*time.zone)(<nil>)
          })
        }
      }),
      InitContainerStatuses: ([]v1.ContainerStatus) <nil>,
      ContainerStatuses: ([]v1.ContainerStatus) (len=1) {
        (v1.ContainerStatus) {
          Name: (string) (len=5) "httpd",
          State: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)({
              StartedAt: (v1.Time) {
                Time: (time.Time) {
                  wall: (uint64) 0,
                  ext: (int64) 63882665433,
                  loc: (*time.Location)(<already shown>)
                }
              }
            }),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          LastTerminationState: (v1.ContainerState) {
            Waiting: (*v1.ContainerStateWaiting)(<nil>),
            Running: (*v1.ContainerStateRunning)(<nil>),
            Terminated: (*v1.ContainerStateTerminated)(<nil>)
          },
          Ready: (bool) true,
          RestartCount: (int32) 0,
          Image: (string) (len=46) "registry.k8s.io/e2e-test-images/httpd:2.4.38-4",
          ImageID: (string) (len=109) "registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22",
          ContainerID: (string) (len=77) "containerd://eddacccc0ce376b1980b21f3678360021bc43e0cee9dd4761d06838f596dab50",
          Started: (*bool)(true),
          AllocatedResources: (v1.ResourceList) <nil>,
          Resources: (*v1.ResourceRequirements)(<nil>),
          VolumeMounts: ([]v1.VolumeMountStatus) (len=1) {
            (v1.VolumeMountStatus) {
              Name: (string) (len=21) "kube-api-access-mtd6k",
              MountPath: (string) (len=45) "/var/run/secrets/kubernetes.io/serviceaccount",
              ReadOnly: (bool) true,
              RecursiveReadOnly: (*v1.RecursiveReadOnlyMode)((len=8) "Disabled")
            }
          },
          User: (*v1.ContainerUser)(<nil>),
          AllocatedResourcesStatus: ([]v1.ResourceStatus) <nil>
        }
      },
      QOSClass: (v1.PodQOSClass) (len=10) "BestEffort",
      EphemeralContainerStatuses: ([]v1.ContainerStatus) <nil>,
      Resize: (v1.PodResizeStatus) "",
      ResourceClaimStatuses: ([]v1.PodResourceClaimStatus) <nil>
    }
  }

  I0512 16:50:34.318663 23 helper.go:122] Waiting up to 7m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4657" for this suite. @ 05/12/25 16:50:34.336
• [2.326 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
k8s.io/kubernetes/test/e2e/e2e.go:80
  I0512 16:50:34.353413 23 suites.go:34] Running AfterSuite actions on node 1
  I0512 16:50:34.353468 23 util.go:607] Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
k8s.io/kubernetes/test/e2e/e2e_test.go:158
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
k8s.io/kubernetes/test/e2e/framework/test_context.go:612
[ReportAfterSuite] PASSED [0.131 seconds]
------------------------------

Ran 404 of 6605 Specs in 7029.312 seconds
SUCCESS! -- 404 Passed | 0 Failed | 0 Pending | 6201 Skipped
PASS

Ginkgo ran 1 suite in 1h57m11.023998991s
Test Suite Passed
