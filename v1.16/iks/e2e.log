I0622 15:14:26.498272      24 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-114803800
I0622 15:14:26.498577      24 e2e.go:92] Starting e2e run "1fc36ad5-94e2-42ee-ae39-b6b9c6769d50" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1592838864 - Will randomize all specs
Will run 276 of 4847 specs

Jun 22 15:14:26.523: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:14:26.526: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 22 15:14:26.584: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 22 15:14:26.666: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 22 15:14:26.666: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Jun 22 15:14:26.666: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 22 15:14:26.688: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 22 15:14:26.688: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jun 22 15:14:26.688: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'ibm-master-proxy' (0 seconds elapsed)
Jun 22 15:14:26.688: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jun 22 15:14:26.688: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Jun 22 15:14:26.688: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Jun 22 15:14:26.688: INFO: e2e test version: v1.16.11
Jun 22 15:14:26.692: INFO: kube-apiserver version: v1.16.11+IKS
Jun 22 15:14:26.692: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:14:26.710: INFO: Cluster IP family: ipv4
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:14:26.710: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
Jun 22 15:14:26.827: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 22 15:14:26.865: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6172/configmap-test-5a6492d8-c31a-401f-9584-36a6c5805602
STEP: Creating a pod to test consume configMaps
Jun 22 15:14:27.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd" in namespace "configmap-6172" to be "success or failure"
Jun 22 15:14:27.061: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.106206ms
Jun 22 15:14:29.072: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024177639s
Jun 22 15:14:31.083: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034692119s
Jun 22 15:14:33.094: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046153364s
Jun 22 15:14:35.106: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058015654s
Jun 22 15:14:37.118: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.069849878s
Jun 22 15:14:39.129: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.080891262s
Jun 22 15:14:41.147: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.09822626s
Jun 22 15:14:43.158: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.109982131s
Jun 22 15:14:45.169: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.120836838s
Jun 22 15:14:47.182: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.13355922s
Jun 22 15:14:49.193: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.144560248s
Jun 22 15:14:51.205: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.156527992s
Jun 22 15:14:53.222: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.173351742s
Jun 22 15:14:55.233: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 28.184492829s
Jun 22 15:14:57.244: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 30.195624071s
Jun 22 15:14:59.255: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.20641012s
Jun 22 15:15:01.266: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 34.218128694s
Jun 22 15:15:03.278: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 36.229713863s
Jun 22 15:15:05.290: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 38.241736104s
Jun 22 15:15:07.306: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 40.258095874s
Jun 22 15:15:09.317: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.268837793s
Jun 22 15:15:11.328: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 44.279607019s
Jun 22 15:15:13.341: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 46.292365889s
Jun 22 15:15:15.355: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 48.307081028s
STEP: Saw pod success
Jun 22 15:15:15.355: INFO: Pod "pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd" satisfied condition "success or failure"
Jun 22 15:15:15.366: INFO: Trying to get logs from node 10.45.191.149 pod pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd container env-test: <nil>
STEP: delete the pod
Jun 22 15:15:15.487: INFO: Waiting for pod pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd to disappear
Jun 22 15:15:15.496: INFO: Pod pod-configmaps-45bc3986-bb5d-4787-b4a9-d3c88a96f9bd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:15:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6172" for this suite.
Jun 22 15:15:21.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:15:22.013: INFO: namespace configmap-6172 deletion completed in 6.50368713s

• [SLOW TEST:55.303 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:15:22.014: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 15:15:30.838: INFO: Successfully updated pod "pod-update-393cb48b-6489-48ae-9b1b-2f7c3ecf1dec"
STEP: verifying the updated pod is in kubernetes
Jun 22 15:15:30.863: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:15:30.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4230" for this suite.
Jun 22 15:16:00.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:16:01.466: INFO: namespace pods-4230 deletion completed in 30.5881737s

• [SLOW TEST:39.453 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:16:01.467: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:16:27.848: INFO: Container started at 2020-06-22 15:16:04 +0000 UTC, pod became ready at 2020-06-22 15:16:26 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:16:27.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6961" for this suite.
Jun 22 15:16:57.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:16:58.399: INFO: namespace container-probe-6961 deletion completed in 30.538002854s

• [SLOW TEST:56.933 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:16:58.401: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-8017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8017 to expose endpoints map[]
Jun 22 15:16:58.700: INFO: Get endpoints failed (15.036864ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 22 15:16:59.715: INFO: successfully validated that service endpoint-test2 in namespace services-8017 exposes endpoints map[] (1.030275035s elapsed)
STEP: Creating pod pod1 in namespace services-8017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8017 to expose endpoints map[pod1:[80]]
Jun 22 15:17:03.866: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.128993182s elapsed, will retry)
Jun 22 15:17:04.893: INFO: successfully validated that service endpoint-test2 in namespace services-8017 exposes endpoints map[pod1:[80]] (5.155975731s elapsed)
STEP: Creating pod pod2 in namespace services-8017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8017 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 22 15:17:09.118: INFO: Unexpected endpoints: found map[c79d71ef-28b2-4102-95c4-d311be45c20f:[80]], expected map[pod1:[80] pod2:[80]] (4.211654902s elapsed, will retry)
Jun 22 15:17:10.153: INFO: successfully validated that service endpoint-test2 in namespace services-8017 exposes endpoints map[pod1:[80] pod2:[80]] (5.247147351s elapsed)
STEP: Deleting pod pod1 in namespace services-8017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8017 to expose endpoints map[pod2:[80]]
Jun 22 15:17:10.191: INFO: successfully validated that service endpoint-test2 in namespace services-8017 exposes endpoints map[pod2:[80]] (22.453403ms elapsed)
STEP: Deleting pod pod2 in namespace services-8017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8017 to expose endpoints map[]
Jun 22 15:17:10.223: INFO: successfully validated that service endpoint-test2 in namespace services-8017 exposes endpoints map[] (12.046794ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:17:10.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8017" for this suite.
Jun 22 15:17:40.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:17:41.175: INFO: namespace services-8017 deletion completed in 30.850412187s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:42.775 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:17:41.176: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-a70dd789-d635-48bc-86fa-3fbf0d77e0d8
STEP: Creating a pod to test consume configMaps
Jun 22 15:17:41.432: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc" in namespace "projected-7385" to be "success or failure"
Jun 22 15:17:41.442: INFO: Pod "pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.842507ms
Jun 22 15:17:43.452: INFO: Pod "pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020235533s
Jun 22 15:17:45.463: INFO: Pod "pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030851895s
STEP: Saw pod success
Jun 22 15:17:45.463: INFO: Pod "pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc" satisfied condition "success or failure"
Jun 22 15:17:45.473: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 15:17:45.585: INFO: Waiting for pod pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc to disappear
Jun 22 15:17:45.594: INFO: Pod pod-projected-configmaps-06de5507-afad-4b3c-b605-17f6c6e1dfbc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:17:45.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7385" for this suite.
Jun 22 15:17:51.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:17:52.111: INFO: namespace projected-7385 deletion completed in 6.498031539s

• [SLOW TEST:10.935 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:17:52.111: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:17:52.390: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 22 15:17:57.402: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 15:18:47.425: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 22 15:18:49.437: INFO: Creating deployment "test-rollover-deployment"
Jun 22 15:18:49.470: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 22 15:18:51.490: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 22 15:18:51.517: INFO: Ensure that both replica sets have 1 created replica
Jun 22 15:18:51.537: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 22 15:18:51.562: INFO: Updating deployment test-rollover-deployment
Jun 22 15:18:51.562: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 22 15:18:53.587: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 22 15:18:53.608: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 22 15:18:53.631: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:18:53.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435931, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:18:55.654: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:18:55.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435931, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:18:57.655: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:18:57.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435931, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:18:59.654: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:18:59.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435938, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:01.654: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:19:01.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435938, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:03.659: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:19:03.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435938, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:05.655: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:19:05.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435938, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:07.654: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 15:19:07.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435938, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435929, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:09.653: INFO: 
Jun 22 15:19:09.654: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 15:19:09.696: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-177 /apis/apps/v1/namespaces/deployment-177/deployments/test-rollover-deployment f429843a-2488-4bef-9c6e-5b68c4b62af3 20401 2 2020-06-22 15:18:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023e4f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-22 15:18:49 +0000 UTC,LastTransitionTime:2020-06-22 15:18:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-06-22 15:19:08 +0000 UTC,LastTransitionTime:2020-06-22 15:18:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 15:19:09.706: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-177 /apis/apps/v1/namespaces/deployment-177/replicasets/test-rollover-deployment-7d7dc6548c b89fd969-2da1-41a3-badf-580f1cc1bb42 20391 2 2020-06-22 15:18:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f429843a-2488-4bef-9c6e-5b68c4b62af3 0xc0023e5427 0xc0023e5428}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023e5488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 15:19:09.706: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 22 15:19:09.706: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-177 /apis/apps/v1/namespaces/deployment-177/replicasets/test-rollover-controller c90cc588-5369-4839-8e88-e03e83f22559 20400 2 2020-06-22 15:17:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f429843a-2488-4bef-9c6e-5b68c4b62af3 0xc0023e5347 0xc0023e5348}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0023e53a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 15:19:09.707: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-177 /apis/apps/v1/namespaces/deployment-177/replicasets/test-rollover-deployment-f6c94f66c a9deaa99-21db-4238-a5d7-199293e5656c 20353 2 2020-06-22 15:18:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f429843a-2488-4bef-9c6e-5b68c4b62af3 0xc0023e54f0 0xc0023e54f1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0023e5568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 15:19:09.719: INFO: Pod "test-rollover-deployment-7d7dc6548c-l65wv" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-l65wv test-rollover-deployment-7d7dc6548c- deployment-177 /api/v1/namespaces/deployment-177/pods/test-rollover-deployment-7d7dc6548c-l65wv 3b17b2fb-18c7-4876-ac3a-35dc8900b1dc 20376 0 2020-06-22 15:18:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c b89fd969-2da1-41a3-badf-580f1cc1bb42 0xc0023e5d67 0xc0023e5d68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-54ddf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-54ddf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-54ddf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:18:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:18:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:18:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:18:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:172.30.44.39,StartTime:2020-06-22 15:18:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://d4ebc7f3059de5f91698e937673c7ce4e45cb459e69ee314cca7b18d278e1374,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.44.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:19:09.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-177" for this suite.
Jun 22 15:19:17.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:19:18.273: INFO: namespace deployment-177 deletion completed in 8.539107163s

• [SLOW TEST:86.162 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:19:18.273: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:19:30.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6385" for this suite.
Jun 22 15:19:38.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:19:39.092: INFO: namespace job-6385 deletion completed in 8.522695322s

• [SLOW TEST:20.819 seconds]
[sig-apps] Job
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:19:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:19:39.333: INFO: Creating deployment "test-recreate-deployment"
Jun 22 15:19:39.350: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 22 15:19:39.374: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 22 15:19:39.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-68fc85c7bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:41.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:43.397: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:45.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:47.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:49.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728435979, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:19:51.393: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 22 15:19:51.417: INFO: Updating deployment test-recreate-deployment
Jun 22 15:19:51.417: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 15:19:51.527: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3201 /apis/apps/v1/namespaces/deployment-3201/deployments/test-recreate-deployment 0d0ea345-84d7-4334-ad5b-8522d3e89f94 20724 2 2020-06-22 15:19:39 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000e73048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-22 15:19:51 +0000 UTC,LastTransitionTime:2020-06-22 15:19:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-06-22 15:19:51 +0000 UTC,LastTransitionTime:2020-06-22 15:19:39 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 15:19:51.537: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-3201 /apis/apps/v1/namespaces/deployment-3201/replicasets/test-recreate-deployment-5f94c574ff c2f5b748-c720-4632-b8ad-b9bd5318fb03 20722 1 2020-06-22 15:19:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0d0ea345-84d7-4334-ad5b-8522d3e89f94 0xc000e73427 0xc000e73428}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000e73488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 15:19:51.537: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 22 15:19:51.538: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-3201 /apis/apps/v1/namespaces/deployment-3201/replicasets/test-recreate-deployment-68fc85c7bb 761e1536-754c-422e-81ee-96b42d6f10e1 20714 2 2020-06-22 15:19:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0d0ea345-84d7-4334-ad5b-8522d3e89f94 0xc000e73527 0xc000e73528}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000e73588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 15:19:51.550: INFO: Pod "test-recreate-deployment-5f94c574ff-n7xth" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-n7xth test-recreate-deployment-5f94c574ff- deployment-3201 /api/v1/namespaces/deployment-3201/pods/test-recreate-deployment-5f94c574ff-n7xth a859e647-a7f2-4d41-811a-9b79e0cde5c1 20726 0 2020-06-22 15:19:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff c2f5b748-c720-4632-b8ad-b9bd5318fb03 0xc000e73a07 0xc000e73a08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pkd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pkd7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pkd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:19:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:19:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:19:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:19:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:,StartTime:2020-06-22 15:19:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:19:51.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3201" for this suite.
Jun 22 15:19:59.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:20:00.091: INFO: namespace deployment-3201 deletion completed in 8.525806034s

• [SLOW TEST:20.998 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:20:00.092: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-b1e38830-ff81-4ace-8df7-d4ab1fcbecfd
STEP: Creating a pod to test consume secrets
Jun 22 15:20:00.360: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3be2df5c-2117-44be-b39c-041b62edf9c5" in namespace "projected-6772" to be "success or failure"
Jun 22 15:20:00.381: INFO: Pod "pod-projected-secrets-3be2df5c-2117-44be-b39c-041b62edf9c5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.515212ms
Jun 22 15:20:02.394: INFO: Pod "pod-projected-secrets-3be2df5c-2117-44be-b39c-041b62edf9c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033726656s
STEP: Saw pod success
Jun 22 15:20:02.394: INFO: Pod "pod-projected-secrets-3be2df5c-2117-44be-b39c-041b62edf9c5" satisfied condition "success or failure"
Jun 22 15:20:02.405: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-secrets-3be2df5c-2117-44be-b39c-041b62edf9c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 15:20:02.505: INFO: Waiting for pod pod-projected-secrets-3be2df5c-2117-44be-b39c-041b62edf9c5 to disappear
Jun 22 15:20:02.516: INFO: Pod pod-projected-secrets-3be2df5c-2117-44be-b39c-041b62edf9c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:20:02.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6772" for this suite.
Jun 22 15:20:08.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:20:09.064: INFO: namespace projected-6772 deletion completed in 6.534421238s

• [SLOW TEST:8.972 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:20:09.065: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 22 15:20:09.455: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:20:14.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6050" for this suite.
Jun 22 15:20:44.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:20:44.650: INFO: namespace init-container-6050 deletion completed in 30.516927492s

• [SLOW TEST:35.585 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:20:44.650: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Jun 22 15:20:44.899: INFO: Waiting up to 5m0s for pod "client-containers-741f0735-e935-4080-9c9d-aac68172fcd3" in namespace "containers-2922" to be "success or failure"
Jun 22 15:20:44.913: INFO: Pod "client-containers-741f0735-e935-4080-9c9d-aac68172fcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.160317ms
Jun 22 15:20:46.925: INFO: Pod "client-containers-741f0735-e935-4080-9c9d-aac68172fcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026292057s
Jun 22 15:20:48.936: INFO: Pod "client-containers-741f0735-e935-4080-9c9d-aac68172fcd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037351246s
STEP: Saw pod success
Jun 22 15:20:48.936: INFO: Pod "client-containers-741f0735-e935-4080-9c9d-aac68172fcd3" satisfied condition "success or failure"
Jun 22 15:20:48.950: INFO: Trying to get logs from node 10.45.191.149 pod client-containers-741f0735-e935-4080-9c9d-aac68172fcd3 container test-container: <nil>
STEP: delete the pod
Jun 22 15:20:49.056: INFO: Waiting for pod client-containers-741f0735-e935-4080-9c9d-aac68172fcd3 to disappear
Jun 22 15:20:49.066: INFO: Pod client-containers-741f0735-e935-4080-9c9d-aac68172fcd3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:20:49.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2922" for this suite.
Jun 22 15:20:57.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:20:57.565: INFO: namespace containers-2922 deletion completed in 8.483148864s

• [SLOW TEST:12.915 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:20:57.566: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-93
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 15:20:57.817: INFO: Waiting up to 5m0s for pod "pod-4a9157c5-fe19-4cb5-85d0-d0c47555fc13" in namespace "emptydir-93" to be "success or failure"
Jun 22 15:20:57.826: INFO: Pod "pod-4a9157c5-fe19-4cb5-85d0-d0c47555fc13": Phase="Pending", Reason="", readiness=false. Elapsed: 8.843937ms
Jun 22 15:20:59.837: INFO: Pod "pod-4a9157c5-fe19-4cb5-85d0-d0c47555fc13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019476074s
STEP: Saw pod success
Jun 22 15:20:59.837: INFO: Pod "pod-4a9157c5-fe19-4cb5-85d0-d0c47555fc13" satisfied condition "success or failure"
Jun 22 15:20:59.855: INFO: Trying to get logs from node 10.45.191.150 pod pod-4a9157c5-fe19-4cb5-85d0-d0c47555fc13 container test-container: <nil>
STEP: delete the pod
Jun 22 15:20:59.924: INFO: Waiting for pod pod-4a9157c5-fe19-4cb5-85d0-d0c47555fc13 to disappear
Jun 22 15:20:59.934: INFO: Pod pod-4a9157c5-fe19-4cb5-85d0-d0c47555fc13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:20:59.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-93" for this suite.
Jun 22 15:21:08.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:21:08.474: INFO: namespace emptydir-93 deletion completed in 8.52319036s

• [SLOW TEST:10.908 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:21:08.474: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 15:21:09.327: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 15:21:11.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:21:13.373: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436069, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 15:21:16.412: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:21:26.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3222" for this suite.
Jun 22 15:21:34.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:21:35.340: INFO: namespace webhook-3222 deletion completed in 8.505686124s
STEP: Destroying namespace "webhook-3222-markers" for this suite.
Jun 22 15:21:41.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:21:41.836: INFO: namespace webhook-3222-markers deletion completed in 6.495943191s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.428 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:21:41.902: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7122
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:21:42.147: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 15:21:46.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-7122 create -f -'
Jun 22 15:21:46.665: INFO: stderr: ""
Jun 22 15:21:46.665: INFO: stdout: "e2e-test-crd-publish-openapi-3927-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 15:21:46.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-7122 delete e2e-test-crd-publish-openapi-3927-crds test-cr'
Jun 22 15:21:46.823: INFO: stderr: ""
Jun 22 15:21:46.823: INFO: stdout: "e2e-test-crd-publish-openapi-3927-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 22 15:21:46.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-7122 apply -f -'
Jun 22 15:21:47.484: INFO: stderr: ""
Jun 22 15:21:47.484: INFO: stdout: "e2e-test-crd-publish-openapi-3927-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 15:21:47.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-7122 delete e2e-test-crd-publish-openapi-3927-crds test-cr'
Jun 22 15:21:47.644: INFO: stderr: ""
Jun 22 15:21:47.644: INFO: stdout: "e2e-test-crd-publish-openapi-3927-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 22 15:21:47.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-3927-crds'
Jun 22 15:21:48.050: INFO: stderr: ""
Jun 22 15:21:48.050: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3927-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:21:52.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7122" for this suite.
Jun 22 15:21:58.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:21:59.331: INFO: namespace crd-publish-openapi-7122 deletion completed in 6.783198682s

• [SLOW TEST:17.428 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:21:59.331: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Jun 22 15:21:59.621: INFO: Waiting up to 5m0s for pod "var-expansion-f142fb84-a2a4-4d0e-a75b-385d388ef83c" in namespace "var-expansion-8260" to be "success or failure"
Jun 22 15:21:59.632: INFO: Pod "var-expansion-f142fb84-a2a4-4d0e-a75b-385d388ef83c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.407034ms
Jun 22 15:22:01.654: INFO: Pod "var-expansion-f142fb84-a2a4-4d0e-a75b-385d388ef83c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03282901s
STEP: Saw pod success
Jun 22 15:22:01.654: INFO: Pod "var-expansion-f142fb84-a2a4-4d0e-a75b-385d388ef83c" satisfied condition "success or failure"
Jun 22 15:22:01.666: INFO: Trying to get logs from node 10.45.191.149 pod var-expansion-f142fb84-a2a4-4d0e-a75b-385d388ef83c container dapi-container: <nil>
STEP: delete the pod
Jun 22 15:22:01.736: INFO: Waiting for pod var-expansion-f142fb84-a2a4-4d0e-a75b-385d388ef83c to disappear
Jun 22 15:22:01.752: INFO: Pod var-expansion-f142fb84-a2a4-4d0e-a75b-385d388ef83c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:22:01.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8260" for this suite.
Jun 22 15:22:07.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:22:08.393: INFO: namespace var-expansion-8260 deletion completed in 6.622620027s

• [SLOW TEST:9.062 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:22:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-100
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:22:19.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-100" for this suite.
Jun 22 15:22:25.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:22:26.434: INFO: namespace resourcequota-100 deletion completed in 6.614065222s

• [SLOW TEST:18.041 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:22:26.435: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:22:26.687: INFO: Creating deployment "webserver-deployment"
Jun 22 15:22:26.701: INFO: Waiting for observed generation 1
Jun 22 15:22:28.727: INFO: Waiting for all required pods to come up
Jun 22 15:22:28.744: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 22 15:23:54.777: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 22 15:23:54.803: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 22 15:23:54.831: INFO: Updating deployment webserver-deployment
Jun 22 15:23:54.831: INFO: Waiting for observed generation 2
Jun 22 15:23:56.855: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 22 15:23:56.866: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 22 15:23:56.884: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 15:23:56.929: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 22 15:23:56.929: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 22 15:23:56.946: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 15:23:56.973: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 22 15:23:56.973: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 22 15:23:57.003: INFO: Updating deployment webserver-deployment
Jun 22 15:23:57.003: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 22 15:23:57.028: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 22 15:23:59.060: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 15:23:59.101: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7264 /apis/apps/v1/namespaces/deployment-7264/deployments/webserver-deployment c5fcb381-4831-4b8d-b4e8-018ba2b23077 21870 3 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001e63bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-22 15:23:57 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-06-22 15:23:58 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,},},ReadyReplicas:10,CollisionCount:nil,},}

Jun 22 15:23:59.125: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-7264 /apis/apps/v1/namespaces/deployment-7264/replicasets/webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 21763 3 2020-06-22 15:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c5fcb381-4831-4b8d-b4e8-018ba2b23077 0xc004a12987 0xc004a12988}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a129f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 15:23:59.125: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 22 15:23:59.125: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-7264 /apis/apps/v1/namespaces/deployment-7264/replicasets/webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 21869 3 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c5fcb381-4831-4b8d-b4e8-018ba2b23077 0xc004a128c7 0xc004a128c8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a12928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[]ReplicaSetCondition{},},}
Jun 22 15:23:59.156: INFO: Pod "webserver-deployment-595b5b9587-2qx66" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2qx66 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-2qx66 957eaf70-12fd-4b3a-be07-2360f64216ee 21860 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a12ea7 0xc004a12ea8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:172.30.44.49,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:23:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://0b978e4707981e8746f4233d00c34f5c66682db8d449daef10e72ad4f950737c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.44.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.156: INFO: Pod "webserver-deployment-595b5b9587-2zzhw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2zzhw webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-2zzhw e5079b19-2b1e-43cb-bcea-c86c7996543d 21787 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13027 0xc004a13028}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.156: INFO: Pod "webserver-deployment-595b5b9587-54m9j" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-54m9j webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-54m9j d5830789-df56-44e1-bd7a-352e42ec68c3 21580 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13187 0xc004a13188}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:172.30.131.75,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:23:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c96fa834337ab6a9910cac45d75dd5975f53198a6e84dbcc50dda5721891ec61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.131.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.157: INFO: Pod "webserver-deployment-595b5b9587-7bndx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7bndx webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-7bndx 216ffbea-bb4e-4b65-8252-610122409108 21445 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13307 0xc004a13308}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:172.30.226.79,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:22:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://140c45ed498a49e4b96fc5a767ffee2ffc59d028708a68346c0d338f83b58b71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.226.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.157: INFO: Pod "webserver-deployment-595b5b9587-7tgx8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7tgx8 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-7tgx8 93be6b8b-fb8f-4c44-a8e1-64eb51538f3f 21867 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13487 0xc004a13488}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:172.30.131.86,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:23:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9168f3c2d3a279b450202b8c38987602920a027d8c08115bfd659da33d5e9fa3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.131.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.157: INFO: Pod "webserver-deployment-595b5b9587-8prsq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8prsq webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-8prsq 7d5a747e-8ed5-4e80-bb1b-84298608f7e5 21442 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13607 0xc004a13608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:172.30.226.78,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:22:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://51043e4f0330c6ef4464efe7b25693e1aa6a80a1f51172d23e207bd92f1fd536,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.226.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.157: INFO: Pod "webserver-deployment-595b5b9587-99wwj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-99wwj webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-99wwj 0f6e5a1d-b66a-4fbe-8b03-d0ea5a48556f 21570 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13787 0xc004a13788}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:172.30.131.76,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:23:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://4a47ed269af232aea123f961b3d75dfcc89784ebec2f42aa3fd4463a2311de5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.131.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.157: INFO: Pod "webserver-deployment-595b5b9587-cmfk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cmfk2 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-cmfk2 48bacf79-9518-47a7-9bc3-ea295c824417 21771 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13907 0xc004a13908}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.157: INFO: Pod "webserver-deployment-595b5b9587-cnmp5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cnmp5 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-cnmp5 d5e1387b-67ed-476c-a4b2-7a72d6d4afaf 21778 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13a67 0xc004a13a68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.157: INFO: Pod "webserver-deployment-595b5b9587-f75kd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f75kd webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-f75kd 52fae40b-91ba-4d82-b5f4-a2ef60c27d59 21437 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13bc7 0xc004a13bc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:172.30.44.44,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:22:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://94d57b0ab2b0167efda7306fbb90de6da8b6eea65b15126d9c2cc6307e2a71aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.44.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.158: INFO: Pod "webserver-deployment-595b5b9587-fjdq8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fjdq8 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-fjdq8 27faa8e4-6d28-4598-83a3-ed09433c8e0c 21453 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13d47 0xc004a13d48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:172.30.44.45,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:22:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://60d5134c571ae979b482fe83314931140c6e05643c29fadd5d0d02078b89e965,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.44.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.158: INFO: Pod "webserver-deployment-595b5b9587-jjdgz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jjdgz webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-jjdgz 99d5b79c-e49c-40d8-8575-9c414bf528df 21785 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc004a13ed7 0xc004a13ed8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.158: INFO: Pod "webserver-deployment-595b5b9587-kkt57" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kkt57 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-kkt57 6c91a759-2090-487f-a7a3-099dd4c300a6 21772 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9a0c7 0xc001c9a0c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.158: INFO: Pod "webserver-deployment-595b5b9587-mngq4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mngq4 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-mngq4 2fff4bbe-1378-4424-8fb3-4ca8a73d6bf6 21757 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9a397 0xc001c9a398}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.158: INFO: Pod "webserver-deployment-595b5b9587-sqnt2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sqnt2 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-sqnt2 43018e6d-0825-47da-ab7c-a44fbba198b6 21779 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9a6e7 0xc001c9a6e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.158: INFO: Pod "webserver-deployment-595b5b9587-swz4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-swz4k webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-swz4k 2ea3c19d-0dbb-4410-97a3-daecbfd87a3f 21725 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9aa27 0xc001c9aa28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.159: INFO: Pod "webserver-deployment-595b5b9587-t7cnz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-t7cnz webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-t7cnz 0259cb03-fdb5-4ca4-8250-25fa28d9163d 21583 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9ad37 0xc001c9ad38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:172.30.131.80,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:23:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://18131704cad96d7e9951c7cca8701eaea258e864ff60b327f2621b6a1f04cf20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.131.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.159: INFO: Pod "webserver-deployment-595b5b9587-trzfq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-trzfq webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-trzfq 301245e0-9649-4b1a-bbb9-390875ed7d51 21434 0 2020-06-22 15:22:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9b0e7 0xc001c9b0e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:22:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:172.30.44.43,StartTime:2020-06-22 15:22:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 15:22:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2f6e306d9e067f5593bd3d4f837810ad2f3b75d2e7efb96c45affcb107250fec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.44.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.159: INFO: Pod "webserver-deployment-595b5b9587-zp92f" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zp92f webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-zp92f 714c2083-885e-4a83-8350-b0e0040f3481 21783 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9b407 0xc001c9b408}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.159: INFO: Pod "webserver-deployment-595b5b9587-zw2g9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zw2g9 webserver-deployment-595b5b9587- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-595b5b9587-zw2g9 882acf4c-42be-4c45-80fa-f283af073f3b 21762 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cdbb0d7f-3641-4b0e-a934-ebdb1b1e0947 0xc001c9b757 0xc001c9b758}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.159: INFO: Pod "webserver-deployment-c7997dcc8-4fk8w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4fk8w webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-4fk8w 5d14d531-0a1c-48ca-96b4-44e54b883ea0 21777 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001c9ba47 0xc001c9ba48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.159: INFO: Pod "webserver-deployment-c7997dcc8-68qkh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-68qkh webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-68qkh e5e259f9-70dd-4af3-8a9a-e118a201c1e2 21769 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001c9bca7 0xc001c9bca8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.159: INFO: Pod "webserver-deployment-c7997dcc8-9p25b" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9p25b webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-9p25b 339c1c91-a51a-4e8e-a3d4-919b95846b37 21805 0 2020-06-22 15:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001c9be27 0xc001c9be28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:172.30.131.83,StartTime:2020-06-22 15:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.131.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.160: INFO: Pod "webserver-deployment-c7997dcc8-bx67n" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bx67n webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-bx67n 0e12cb14-4258-462c-ac12-2acef000f606 21726 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001c9bfd7 0xc001c9bfd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.160: INFO: Pod "webserver-deployment-c7997dcc8-f89v6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-f89v6 webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-f89v6 7454ceb8-8cac-4ec2-bcb2-acb99ac455f4 21774 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b52167 0xc001b52168}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.160: INFO: Pod "webserver-deployment-c7997dcc8-j2gfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-j2gfs webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-j2gfs a493a648-32e4-44ec-8253-7c905f775c4c 21741 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b522e7 0xc001b522e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.160: INFO: Pod "webserver-deployment-c7997dcc8-jd4hp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jd4hp webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-jd4hp bd8e1681-f169-4b80-8ac9-48e8f1bee75a 21871 0 2020-06-22 15:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b52487 0xc001b52488}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:172.30.226.80,StartTime:2020-06-22 15:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.226.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.160: INFO: Pod "webserver-deployment-c7997dcc8-ll2f5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ll2f5 webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-ll2f5 e3fe322c-7197-44a2-b34a-38c128fcc42d 21775 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b52637 0xc001b52638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.160: INFO: Pod "webserver-deployment-c7997dcc8-nggws" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nggws webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-nggws e6564975-1ce6-45b2-aab8-adc476642e24 21802 0 2020-06-22 15:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b527b7 0xc001b527b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:172.30.44.46,StartTime:2020-06-22 15:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.44.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.160: INFO: Pod "webserver-deployment-c7997dcc8-s6hvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-s6hvj webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-s6hvj e9051aee-12ce-45bb-ac8c-1161dd83fe6f 21790 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b52967 0xc001b52968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.165: INFO: Pod "webserver-deployment-c7997dcc8-w4jq9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w4jq9 webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-w4jq9 3b80e87d-a6a5-4814-afdb-55f380e26835 21654 0 2020-06-22 15:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b52ae7 0xc001b52ae8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.165: INFO: Pod "webserver-deployment-c7997dcc8-xgq95" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xgq95 webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-xgq95 d4b0f41b-37eb-46dd-81a8-bd812400ab3b 21653 0 2020-06-22 15:23:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b52c67 0xc001b52c68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.149,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.149,PodIP:,StartTime:2020-06-22 15:23:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 15:23:59.166: INFO: Pod "webserver-deployment-c7997dcc8-xtkhn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xtkhn webserver-deployment-c7997dcc8- deployment-7264 /api/v1/namespaces/deployment-7264/pods/webserver-deployment-c7997dcc8-xtkhn a91e206f-0b0d-4253-b4c5-fa25baeba94c 21781 0 2020-06-22 15:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 4f211558-c37b-4c38-849e-0a6dea8d7f21 0xc001b52de7 0xc001b52de8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8597d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8597d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8597d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 15:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:,StartTime:2020-06-22 15:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:23:59.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7264" for this suite.
Jun 22 15:24:11.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:24:11.873: INFO: namespace deployment-7264 deletion completed in 12.685262094s

• [SLOW TEST:105.439 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:24:11.874: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 15:24:12.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6" in namespace "projected-4526" to be "success or failure"
Jun 22 15:24:12.170: INFO: Pod "downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.890139ms
Jun 22 15:24:14.184: INFO: Pod "downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028374514s
Jun 22 15:24:16.198: INFO: Pod "downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042415118s
STEP: Saw pod success
Jun 22 15:24:16.198: INFO: Pod "downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6" satisfied condition "success or failure"
Jun 22 15:24:16.210: INFO: Trying to get logs from node 10.45.191.149 pod downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6 container client-container: <nil>
STEP: delete the pod
Jun 22 15:24:16.311: INFO: Waiting for pod downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6 to disappear
Jun 22 15:24:16.328: INFO: Pod downwardapi-volume-8c3732b9-71aa-4c88-9e8e-acfcbd7fdef6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:24:16.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4526" for this suite.
Jun 22 15:24:22.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:24:22.974: INFO: namespace projected-4526 deletion completed in 6.626622953s

• [SLOW TEST:11.101 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:24:22.978: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8752
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-19997125-5511-4f38-a5ef-f1a1a33fee9f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-19997125-5511-4f38-a5ef-f1a1a33fee9f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:24:27.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8752" for this suite.
Jun 22 15:24:47.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:24:48.234: INFO: namespace projected-8752 deletion completed in 20.628590187s

• [SLOW TEST:25.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:24:48.234: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jun 22 15:24:48.481: INFO: namespace kubectl-4235
Jun 22 15:24:48.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-4235'
Jun 22 15:24:48.905: INFO: stderr: ""
Jun 22 15:24:48.905: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 15:24:49.921: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:24:49.921: INFO: Found 0 / 1
Jun 22 15:24:50.919: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:24:50.919: INFO: Found 1 / 1
Jun 22 15:24:50.919: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 15:24:50.931: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:24:50.931: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 15:24:50.931: INFO: wait on redis-master startup in kubectl-4235 
Jun 22 15:24:50.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs redis-master-d2t7d redis-master --namespace=kubectl-4235'
Jun 22 15:24:51.189: INFO: stderr: ""
Jun 22 15:24:51.189: INFO: stdout: "1:C 22 Jun 2020 15:24:50.259 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 22 Jun 2020 15:24:50.259 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 22 Jun 2020 15:24:50.259 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 22 Jun 2020 15:24:50.261 * Running mode=standalone, port=6379.\n1:M 22 Jun 2020 15:24:50.261 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jun 2020 15:24:50.261 # Server initialized\n1:M 22 Jun 2020 15:24:50.261 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jun 2020 15:24:50.261 * Ready to accept connections\n"
STEP: exposing RC
Jun 22 15:24:51.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4235'
Jun 22 15:24:51.392: INFO: stderr: ""
Jun 22 15:24:51.392: INFO: stdout: "service/rm2 exposed\n"
Jun 22 15:24:51.408: INFO: Service rm2 in namespace kubectl-4235 found.
STEP: exposing service
Jun 22 15:24:53.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4235'
Jun 22 15:24:53.609: INFO: stderr: ""
Jun 22 15:24:53.610: INFO: stdout: "service/rm3 exposed\n"
Jun 22 15:24:53.627: INFO: Service rm3 in namespace kubectl-4235 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:24:55.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4235" for this suite.
Jun 22 15:25:07.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:25:08.414: INFO: namespace kubectl-4235 deletion completed in 12.730419732s

• [SLOW TEST:20.180 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:25:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:25:08.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-9033'
Jun 22 15:25:08.933: INFO: stderr: ""
Jun 22 15:25:08.933: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 22 15:25:08.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-9033'
Jun 22 15:25:09.305: INFO: stderr: ""
Jun 22 15:25:09.305: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 15:25:10.319: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:10.319: INFO: Found 0 / 1
Jun 22 15:25:11.318: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:11.318: INFO: Found 0 / 1
Jun 22 15:25:12.318: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:12.318: INFO: Found 0 / 1
Jun 22 15:25:13.321: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:13.321: INFO: Found 0 / 1
Jun 22 15:25:14.318: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:14.318: INFO: Found 0 / 1
Jun 22 15:25:15.322: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:15.322: INFO: Found 0 / 1
Jun 22 15:25:16.318: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:16.318: INFO: Found 1 / 1
Jun 22 15:25:16.319: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 15:25:16.331: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 15:25:16.331: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 15:25:16.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 describe pod redis-master-bqnwg --namespace=kubectl-9033'
Jun 22 15:25:16.520: INFO: stderr: ""
Jun 22 15:25:16.520: INFO: stdout: "Name:         redis-master-bqnwg\nNamespace:    kubectl-9033\nPriority:     0\nNode:         10.45.191.131/10.45.191.131\nStart Time:   Mon, 22 Jun 2020 15:25:08 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.226.87\nIPs:\n  IP:           172.30.226.87\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://01b60ea00625f6392682187f025693b86f3d98145e6d30f6d423b0be4e954f12\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 22 Jun 2020 15:25:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ddvvr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ddvvr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ddvvr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 600s\n                 node.kubernetes.io/unreachable:NoExecute for 600s\nEvents:\n  Type    Reason     Age        From                    Message\n  ----    ------     ----       ----                    -------\n  Normal  Scheduled  <unknown>  default-scheduler       Successfully assigned kubectl-9033/redis-master-bqnwg to 10.45.191.131\n  Normal  Pulling    6s         kubelet, 10.45.191.131  Pulling image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Pulled     2s         kubelet, 10.45.191.131  Successfully pulled image \"docker.io/library/redis:5.0.5-alpine\"\n  Normal  Created    2s         kubelet, 10.45.191.131  Created container redis-master\n  Normal  Started    1s         kubelet, 10.45.191.131  Started container redis-master\n"
Jun 22 15:25:16.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 describe rc redis-master --namespace=kubectl-9033'
Jun 22 15:25:16.711: INFO: stderr: ""
Jun 22 15:25:16.712: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9033\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: redis-master-bqnwg\n"
Jun 22 15:25:16.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 describe service redis-master --namespace=kubectl-9033'
Jun 22 15:25:16.894: INFO: stderr: ""
Jun 22 15:25:16.894: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9033\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.58.109\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.226.87:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 22 15:25:16.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 describe node 10.45.191.131'
Jun 22 15:25:17.399: INFO: stderr: ""
Jun 22 15:25:17.399: INFO: stdout: "Name:               10.45.191.131\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.175.96.162\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.45.191.131\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-broaq0bl0dps8fkp34o0-kubee2epvgp-default-00000110\n                    ibm-cloud.kubernetes.io/worker-pool-id=broaq0bl0dps8fkp34o0-a13ee6d\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.16.10_1534\n                    ibm-cloud.kubernetes.io/zone=lon04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.45.191.131\n                    kubernetes.io/os=linux\n                    privateVLAN=2722966\n                    publicVLAN=2722964\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 22 Jun 2020 13:23:40 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 22 Jun 2020 15:24:43 +0000   Mon, 22 Jun 2020 13:23:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 22 Jun 2020 15:24:43 +0000   Mon, 22 Jun 2020 13:23:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 22 Jun 2020 15:24:43 +0000   Mon, 22 Jun 2020 13:23:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 22 Jun 2020 15:24:43 +0000   Mon, 22 Jun 2020 13:23:50 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.45.191.131\n  ExternalIP:  158.175.96.162\n  Hostname:    10.45.191.131\nCapacity:\n cpu:                4\n ephemeral-storage:  102685624Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16419684Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  99892574949\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13627236Ki\n pods:               110\nSystem Info:\n Machine ID:                 b9e1e0c1492c447283f7eca98c317a70\n System UUID:                FA8594D5-704F-DA4B-CA88-7FDD3F308F6C\n Boot ID:                    5ef6f93b-bedf-47c1-bb95-34a6e30d30af\n Kernel Version:             4.15.0-101-generic\n OS Image:                   Ubuntu 18.04.4 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.3.4\n Kubelet Version:            v1.16.10+IKS\n Kube-Proxy Version:         v1.16.10+IKS\nProviderID:                  ibm://fee034388aa6435883a1f720010ab3a2///broaq0bl0dps8fkp34o0/kube-broaq0bl0dps8fkp34o0-kubee2epvgp-default-00000110\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  ibm-system                 ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-qhll5      5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         119m\n  kube-system                calico-node-s8lh4                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         121m\n  kube-system                coredns-697bf86f8c-h8ckn                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     97m\n  kube-system                ibm-keepalived-watcher-jmnzb                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         121m\n  kube-system                ibm-master-proxy-static-10.45.191.131                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      121m\n  kube-system                public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-7kzrx         10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         117m\n  kubectl-9033               redis-master-bqnwg                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         9s\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m\n  sonobuoy                   sonobuoy-e2e-job-d71e65d5f08d4631                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-jfbn9    0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                395m (10%)     300m (7%)\n  memory             307730Ki (2%)  909600Ki (6%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Jun 22 15:25:17.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 describe namespace kubectl-9033'
Jun 22 15:25:17.599: INFO: stderr: ""
Jun 22 15:25:17.600: INFO: stdout: "Name:         kubectl-9033\nLabels:       e2e-framework=kubectl\n              e2e-run=1fc36ad5-94e2-42ee-ae39-b6b9c6769d50\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:25:17.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9033" for this suite.
Jun 22 15:25:29.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:25:30.209: INFO: namespace kubectl-9033 deletion completed in 12.58971891s

• [SLOW TEST:21.791 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:25:30.209: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 15:25:30.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7e4d8b0-9c30-44f4-b1f3-705796b2d443" in namespace "downward-api-4286" to be "success or failure"
Jun 22 15:25:30.485: INFO: Pod "downwardapi-volume-c7e4d8b0-9c30-44f4-b1f3-705796b2d443": Phase="Pending", Reason="", readiness=false. Elapsed: 13.177718ms
Jun 22 15:25:32.498: INFO: Pod "downwardapi-volume-c7e4d8b0-9c30-44f4-b1f3-705796b2d443": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025429223s
STEP: Saw pod success
Jun 22 15:25:32.498: INFO: Pod "downwardapi-volume-c7e4d8b0-9c30-44f4-b1f3-705796b2d443" satisfied condition "success or failure"
Jun 22 15:25:32.510: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-c7e4d8b0-9c30-44f4-b1f3-705796b2d443 container client-container: <nil>
STEP: delete the pod
Jun 22 15:25:32.619: INFO: Waiting for pod downwardapi-volume-c7e4d8b0-9c30-44f4-b1f3-705796b2d443 to disappear
Jun 22 15:25:32.631: INFO: Pod downwardapi-volume-c7e4d8b0-9c30-44f4-b1f3-705796b2d443 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:25:32.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4286" for this suite.
Jun 22 15:25:40.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:25:41.265: INFO: namespace downward-api-4286 deletion completed in 8.615144044s

• [SLOW TEST:11.056 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:25:41.265: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:25:41.597: INFO: (0) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 68.61204ms)
Jun 22 15:25:41.620: INFO: (1) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.794976ms)
Jun 22 15:25:41.645: INFO: (2) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.548295ms)
Jun 22 15:25:41.668: INFO: (3) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.154014ms)
Jun 22 15:25:41.695: INFO: (4) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 27.010564ms)
Jun 22 15:25:41.718: INFO: (5) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.623201ms)
Jun 22 15:25:41.741: INFO: (6) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.027209ms)
Jun 22 15:25:41.764: INFO: (7) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.730057ms)
Jun 22 15:25:41.789: INFO: (8) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.851081ms)
Jun 22 15:25:41.812: INFO: (9) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.608271ms)
Jun 22 15:25:41.837: INFO: (10) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.085616ms)
Jun 22 15:25:41.864: INFO: (11) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 26.910658ms)
Jun 22 15:25:41.888: INFO: (12) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.976693ms)
Jun 22 15:25:41.913: INFO: (13) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 24.512693ms)
Jun 22 15:25:41.947: INFO: (14) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 33.699092ms)
Jun 22 15:25:41.971: INFO: (15) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.971656ms)
Jun 22 15:25:41.993: INFO: (16) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.650671ms)
Jun 22 15:25:42.020: INFO: (17) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 26.065248ms)
Jun 22 15:25:42.042: INFO: (18) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 22.655174ms)
Jun 22 15:25:42.066: INFO: (19) /api/v1/nodes/10.45.191.131/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 23.180105ms)
[AfterEach] version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:25:42.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7990" for this suite.
Jun 22 15:25:48.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:25:48.686: INFO: namespace proxy-7990 deletion completed in 6.602254618s

• [SLOW TEST:7.421 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:25:48.687: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 15:25:48.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5189'
Jun 22 15:25:49.099: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 15:25:49.099: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Jun 22 15:25:51.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5189'
Jun 22 15:25:51.318: INFO: stderr: ""
Jun 22 15:25:51.318: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:25:51.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5189" for this suite.
Jun 22 15:26:03.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:26:03.978: INFO: namespace kubectl-5189 deletion completed in 12.638036148s

• [SLOW TEST:15.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:26:03.978: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Jun 22 15:26:04.263: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6099" to be "success or failure"
Jun 22 15:26:04.280: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 17.336634ms
Jun 22 15:26:06.294: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031287501s
STEP: Saw pod success
Jun 22 15:26:06.294: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 22 15:26:06.306: INFO: Trying to get logs from node 10.45.191.150 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 22 15:26:06.376: INFO: Waiting for pod pod-host-path-test to disappear
Jun 22 15:26:06.390: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:26:06.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6099" for this suite.
Jun 22 15:26:14.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:26:15.032: INFO: namespace hostpath-6099 deletion completed in 8.621350165s

• [SLOW TEST:11.054 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:26:15.033: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-qfvq
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 15:26:15.334: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qfvq" in namespace "subpath-9255" to be "success or failure"
Jun 22 15:26:15.348: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Pending", Reason="", readiness=false. Elapsed: 13.631164ms
Jun 22 15:26:17.360: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026342798s
Jun 22 15:26:19.375: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 4.041013805s
Jun 22 15:26:21.388: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 6.05444904s
Jun 22 15:26:23.403: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.069355376s
Jun 22 15:26:25.418: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 10.083815805s
Jun 22 15:26:27.433: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 12.098908936s
Jun 22 15:26:29.446: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 14.112304488s
Jun 22 15:26:31.459: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 16.125105125s
Jun 22 15:26:33.473: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 18.139442041s
Jun 22 15:26:35.487: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 20.153099646s
Jun 22 15:26:37.499: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Running", Reason="", readiness=true. Elapsed: 22.165132259s
Jun 22 15:26:39.515: INFO: Pod "pod-subpath-test-projected-qfvq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.18142108s
STEP: Saw pod success
Jun 22 15:26:39.516: INFO: Pod "pod-subpath-test-projected-qfvq" satisfied condition "success or failure"
Jun 22 15:26:39.528: INFO: Trying to get logs from node 10.45.191.131 pod pod-subpath-test-projected-qfvq container test-container-subpath-projected-qfvq: <nil>
STEP: delete the pod
Jun 22 15:26:39.604: INFO: Waiting for pod pod-subpath-test-projected-qfvq to disappear
Jun 22 15:26:39.615: INFO: Pod pod-subpath-test-projected-qfvq no longer exists
STEP: Deleting pod pod-subpath-test-projected-qfvq
Jun 22 15:26:39.616: INFO: Deleting pod "pod-subpath-test-projected-qfvq" in namespace "subpath-9255"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:26:39.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9255" for this suite.
Jun 22 15:26:45.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:26:46.227: INFO: namespace subpath-9255 deletion completed in 6.582072497s

• [SLOW TEST:31.195 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:26:46.228: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 15:26:47.194: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 15:26:49.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436407, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436407, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436407, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436407, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 15:26:52.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:26:53.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7230" for this suite.
Jun 22 15:27:01.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:27:01.774: INFO: namespace webhook-7230 deletion completed in 8.65519114s
STEP: Destroying namespace "webhook-7230-markers" for this suite.
Jun 22 15:27:07.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:27:08.391: INFO: namespace webhook-7230-markers deletion completed in 6.617088219s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.240 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:27:08.468: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 15:27:08.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673" in namespace "downward-api-9976" to be "success or failure"
Jun 22 15:27:08.765: INFO: Pod "downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673": Phase="Pending", Reason="", readiness=false. Elapsed: 17.161267ms
Jun 22 15:27:10.778: INFO: Pod "downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673": Phase="Running", Reason="", readiness=true. Elapsed: 2.030264597s
Jun 22 15:27:12.791: INFO: Pod "downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042699021s
STEP: Saw pod success
Jun 22 15:27:12.791: INFO: Pod "downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673" satisfied condition "success or failure"
Jun 22 15:27:12.808: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673 container client-container: <nil>
STEP: delete the pod
Jun 22 15:27:12.872: INFO: Waiting for pod downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673 to disappear
Jun 22 15:27:12.883: INFO: Pod downwardapi-volume-e06f9489-312c-410c-ad08-bcb4c13b9673 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:27:12.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9976" for this suite.
Jun 22 15:27:18.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:27:19.602: INFO: namespace downward-api-9976 deletion completed in 6.700118841s

• [SLOW TEST:11.134 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:27:19.602: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6855
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:27:23.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6855" for this suite.
Jun 22 15:27:34.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:27:34.585: INFO: namespace containers-6855 deletion completed in 10.623168169s

• [SLOW TEST:14.983 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:27:34.586: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Jun 22 15:27:34.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-7096 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 22 15:27:34.990: INFO: stderr: ""
Jun 22 15:27:34.990: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Jun 22 15:27:34.990: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 22 15:27:34.990: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7096" to be "running and ready, or succeeded"
Jun 22 15:27:35.001: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.353348ms
Jun 22 15:27:37.239: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.249370804s
Jun 22 15:27:37.239: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 22 15:27:37.239: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 22 15:27:37.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs logs-generator logs-generator --namespace=kubectl-7096'
Jun 22 15:27:37.506: INFO: stderr: ""
Jun 22 15:27:37.506: INFO: stdout: "I0622 15:27:36.250659       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/n5l 261\nI0622 15:27:36.450887       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/xdf 411\nI0622 15:27:36.651026       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/5pr 581\nI0622 15:27:36.850993       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/nbc 294\nI0622 15:27:37.050981       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/rknw 522\nI0622 15:27:37.250923       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/7zz9 355\nI0622 15:27:37.450937       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/7g7q 587\n"
STEP: limiting log lines
Jun 22 15:27:37.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs logs-generator logs-generator --namespace=kubectl-7096 --tail=1'
Jun 22 15:27:37.678: INFO: stderr: ""
Jun 22 15:27:37.678: INFO: stdout: "I0622 15:27:37.650911       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/66kl 420\n"
STEP: limiting log bytes
Jun 22 15:27:37.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs logs-generator logs-generator --namespace=kubectl-7096 --limit-bytes=1'
Jun 22 15:27:37.883: INFO: stderr: ""
Jun 22 15:27:37.884: INFO: stdout: "I"
STEP: exposing timestamps
Jun 22 15:27:37.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs logs-generator logs-generator --namespace=kubectl-7096 --tail=1 --timestamps'
Jun 22 15:27:38.070: INFO: stderr: ""
Jun 22 15:27:38.070: INFO: stdout: "2020-06-22T15:27:38.051125264Z I0622 15:27:38.050929       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/zdpb 566\n"
STEP: restricting to a time range
Jun 22 15:27:40.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs logs-generator logs-generator --namespace=kubectl-7096 --since=1s'
Jun 22 15:27:40.746: INFO: stderr: ""
Jun 22 15:27:40.746: INFO: stdout: "I0622 15:27:39.850904       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/d6rp 274\nI0622 15:27:40.050964       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/pv22 422\nI0622 15:27:40.251054       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rmj 521\nI0622 15:27:40.451055       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/s2lj 277\nI0622 15:27:40.650877       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/cxzg 539\n"
Jun 22 15:27:40.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs logs-generator logs-generator --namespace=kubectl-7096 --since=24h'
Jun 22 15:27:40.936: INFO: stderr: ""
Jun 22 15:27:40.936: INFO: stdout: "I0622 15:27:36.250659       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/n5l 261\nI0622 15:27:36.450887       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/xdf 411\nI0622 15:27:36.651026       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/5pr 581\nI0622 15:27:36.850993       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/nbc 294\nI0622 15:27:37.050981       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/rknw 522\nI0622 15:27:37.250923       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/7zz9 355\nI0622 15:27:37.450937       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/7g7q 587\nI0622 15:27:37.650911       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/66kl 420\nI0622 15:27:37.851011       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/qqg 410\nI0622 15:27:38.050929       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/zdpb 566\nI0622 15:27:38.250874       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/c99 429\nI0622 15:27:38.451049       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/zsrr 259\nI0622 15:27:38.650839       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/4wf 461\nI0622 15:27:38.850965       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/4dff 441\nI0622 15:27:39.050896       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/xdqv 468\nI0622 15:27:39.250911       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/rgs 401\nI0622 15:27:39.451025       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/q4t 260\nI0622 15:27:39.650922       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/dpz 343\nI0622 15:27:39.850904       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/d6rp 274\nI0622 15:27:40.050964       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/pv22 422\nI0622 15:27:40.251054       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rmj 521\nI0622 15:27:40.451055       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/s2lj 277\nI0622 15:27:40.650877       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/cxzg 539\nI0622 15:27:40.850859       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/t6xc 455\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Jun 22 15:27:40.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete pod logs-generator --namespace=kubectl-7096'
Jun 22 15:27:43.265: INFO: stderr: ""
Jun 22 15:27:43.265: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:27:43.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7096" for this suite.
Jun 22 15:27:51.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:27:51.863: INFO: namespace kubectl-7096 deletion completed in 8.577678234s

• [SLOW TEST:17.278 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:27:51.864: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 15:27:52.156: INFO: Waiting up to 5m0s for pod "pod-3bb7fa9d-199b-451f-9346-77a2979b815b" in namespace "emptydir-7470" to be "success or failure"
Jun 22 15:27:52.175: INFO: Pod "pod-3bb7fa9d-199b-451f-9346-77a2979b815b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.535325ms
Jun 22 15:27:54.191: INFO: Pod "pod-3bb7fa9d-199b-451f-9346-77a2979b815b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03472215s
Jun 22 15:27:56.204: INFO: Pod "pod-3bb7fa9d-199b-451f-9346-77a2979b815b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047938286s
STEP: Saw pod success
Jun 22 15:27:56.204: INFO: Pod "pod-3bb7fa9d-199b-451f-9346-77a2979b815b" satisfied condition "success or failure"
Jun 22 15:27:56.216: INFO: Trying to get logs from node 10.45.191.131 pod pod-3bb7fa9d-199b-451f-9346-77a2979b815b container test-container: <nil>
STEP: delete the pod
Jun 22 15:27:56.292: INFO: Waiting for pod pod-3bb7fa9d-199b-451f-9346-77a2979b815b to disappear
Jun 22 15:27:56.304: INFO: Pod pod-3bb7fa9d-199b-451f-9346-77a2979b815b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:27:56.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7470" for this suite.
Jun 22 15:28:04.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:28:04.943: INFO: namespace emptydir-7470 deletion completed in 8.613577736s

• [SLOW TEST:13.080 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:28:04.944: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-43
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:28:05.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-43" for this suite.
Jun 22 15:28:11.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:28:11.899: INFO: namespace kubelet-test-43 deletion completed in 6.619016958s

• [SLOW TEST:6.955 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:28:11.902: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Jun 22 15:28:12.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-7068'
Jun 22 15:28:12.429: INFO: stderr: ""
Jun 22 15:28:12.429: INFO: stdout: "pod/pause created\n"
Jun 22 15:28:12.429: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 22 15:28:12.429: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7068" to be "running and ready"
Jun 22 15:28:12.443: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010347ms
Jun 22 15:28:14.455: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.026544029s
Jun 22 15:28:14.455: INFO: Pod "pause" satisfied condition "running and ready"
Jun 22 15:28:14.455: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 22 15:28:14.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 label pods pause testing-label=testing-label-value --namespace=kubectl-7068'
Jun 22 15:28:14.579: INFO: stderr: ""
Jun 22 15:28:14.579: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 22 15:28:14.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pod pause -L testing-label --namespace=kubectl-7068'
Jun 22 15:28:14.725: INFO: stderr: ""
Jun 22 15:28:14.725: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 22 15:28:14.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 label pods pause testing-label- --namespace=kubectl-7068'
Jun 22 15:28:14.862: INFO: stderr: ""
Jun 22 15:28:14.862: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 22 15:28:14.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pod pause -L testing-label --namespace=kubectl-7068'
Jun 22 15:28:14.977: INFO: stderr: ""
Jun 22 15:28:14.977: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Jun 22 15:28:14.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-7068'
Jun 22 15:28:15.136: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 15:28:15.136: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 22 15:28:15.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get rc,svc -l name=pause --no-headers --namespace=kubectl-7068'
Jun 22 15:28:15.288: INFO: stderr: "No resources found in kubectl-7068 namespace.\n"
Jun 22 15:28:15.288: INFO: stdout: ""
Jun 22 15:28:15.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -l name=pause --namespace=kubectl-7068 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 15:28:15.430: INFO: stderr: ""
Jun 22 15:28:15.430: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:28:15.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7068" for this suite.
Jun 22 15:28:21.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:28:22.037: INFO: namespace kubectl-7068 deletion completed in 6.586736638s

• [SLOW TEST:10.136 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:28:22.038: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Jun 22 15:28:22.274: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-114803800 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:28:22.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2628" for this suite.
Jun 22 15:28:28.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:28:29.025: INFO: namespace kubectl-2628 deletion completed in 6.597034701s

• [SLOW TEST:6.988 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:28:29.025: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:28:33.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2670" for this suite.
Jun 22 15:29:19.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:29:20.044: INFO: namespace kubelet-test-2670 deletion completed in 46.656027056s

• [SLOW TEST:51.019 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:29:20.045: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-80b83ec9-ecfe-410f-a13c-7c02e7b09ef2 in namespace container-probe-5969
Jun 22 15:29:24.348: INFO: Started pod busybox-80b83ec9-ecfe-410f-a13c-7c02e7b09ef2 in namespace container-probe-5969
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 15:29:24.360: INFO: Initial restart count of pod busybox-80b83ec9-ecfe-410f-a13c-7c02e7b09ef2 is 0
Jun 22 15:30:08.680: INFO: Restart count of pod container-probe-5969/busybox-80b83ec9-ecfe-410f-a13c-7c02e7b09ef2 is now 1 (44.319936545s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:30:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5969" for this suite.
Jun 22 15:30:14.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:30:15.340: INFO: namespace container-probe-5969 deletion completed in 6.608448222s

• [SLOW TEST:55.296 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:30:15.341: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-bdf54124-568b-4ebd-af54-8d1a63e78f5b
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:30:15.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1553" for this suite.
Jun 22 15:30:21.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:30:22.220: INFO: namespace secrets-1553 deletion completed in 6.607881732s

• [SLOW TEST:6.879 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:30:22.220: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 15:30:23.225: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 15:30:25.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436623, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436623, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436623, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436623, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 15:30:28.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:30:28.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8022" for this suite.
Jun 22 15:30:36.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:30:37.246: INFO: namespace webhook-8022 deletion completed in 8.604957869s
STEP: Destroying namespace "webhook-8022-markers" for this suite.
Jun 22 15:30:43.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:30:43.845: INFO: namespace webhook-8022-markers deletion completed in 6.598133599s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.704 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:30:43.924: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jun 22 15:30:44.178: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Jun 22 15:30:45.023: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 22 15:30:47.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:30:49.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:30:51.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:30:53.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728436645, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 15:30:58.883: INFO: Waited 3.680823116s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:30:59.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2498" for this suite.
Jun 22 15:31:07.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:31:08.257: INFO: namespace aggregator-2498 deletion completed in 8.659674762s

• [SLOW TEST:24.332 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:31:08.257: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4541
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:31:08.504: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:31:08.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4541" for this suite.
Jun 22 15:31:14.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:31:15.359: INFO: namespace custom-resource-definition-4541 deletion completed in 6.657382576s

• [SLOW TEST:7.102 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:31:15.360: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-5da39235-3110-4cc4-a9fb-7aa265c2c32f
STEP: Creating a pod to test consume configMaps
Jun 22 15:31:15.660: INFO: Waiting up to 5m0s for pod "pod-configmaps-2686ce18-7481-4e57-9c79-67ee8e335955" in namespace "configmap-6063" to be "success or failure"
Jun 22 15:31:15.678: INFO: Pod "pod-configmaps-2686ce18-7481-4e57-9c79-67ee8e335955": Phase="Pending", Reason="", readiness=false. Elapsed: 17.241835ms
Jun 22 15:31:17.692: INFO: Pod "pod-configmaps-2686ce18-7481-4e57-9c79-67ee8e335955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031283849s
STEP: Saw pod success
Jun 22 15:31:17.692: INFO: Pod "pod-configmaps-2686ce18-7481-4e57-9c79-67ee8e335955" satisfied condition "success or failure"
Jun 22 15:31:17.704: INFO: Trying to get logs from node 10.45.191.150 pod pod-configmaps-2686ce18-7481-4e57-9c79-67ee8e335955 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 15:31:17.820: INFO: Waiting for pod pod-configmaps-2686ce18-7481-4e57-9c79-67ee8e335955 to disappear
Jun 22 15:31:17.832: INFO: Pod pod-configmaps-2686ce18-7481-4e57-9c79-67ee8e335955 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:31:17.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6063" for this suite.
Jun 22 15:31:23.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:31:24.610: INFO: namespace configmap-6063 deletion completed in 6.757184607s

• [SLOW TEST:9.250 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:31:24.611: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 22 15:31:24.896: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 15:31:24.954: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 15:31:24.971: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.131 before test
Jun 22 15:31:25.067: INFO: sonobuoy-e2e-job-d71e65d5f08d4631 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.067: INFO: 	Container e2e ready: true, restart count 0
Jun 22 15:31:25.067: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:31:25.067: INFO: ibm-master-proxy-static-10.45.191.131 from kube-system started at 2020-06-22 13:23:38 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.067: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:31:25.067: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:31:25.067: INFO: ibm-keepalived-watcher-jmnzb from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.067: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:31:25.067: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-qhll5 from ibm-system started at 2020-06-22 13:25:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.068: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 15:31:25.068: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-7kzrx from kube-system started at 2020-06-22 13:27:37 +0000 UTC (4 container statuses recorded)
Jun 22 15:31:25.068: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 15:31:25.068: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 15:31:25.068: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 15:31:25.068: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 15:31:25.068: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-jfbn9 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.068: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:31:25.068: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:31:25.068: INFO: coredns-697bf86f8c-h8ckn from kube-system started at 2020-06-22 13:47:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.068: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:31:25.068: INFO: calico-node-s8lh4 from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.068: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:31:25.068: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-06-22 13:27:29 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.068: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 22 15:31:25.068: INFO: sonobuoy from sonobuoy started at 2020-06-22 15:11:50 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.068: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 15:31:25.068: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.149 before test
Jun 22 15:31:25.187: INFO: ibm-keepalived-watcher-n8x49 from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.187: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:31:25.187: INFO: calico-node-k8xbh from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.187: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:31:25.187: INFO: dashboard-metrics-scraper-576c46d9bd-8tnxf from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.187: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun 22 15:31:25.187: INFO: calico-kube-controllers-7c7d954b58-zwmf2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.187: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 15:31:25.187: INFO: kubernetes-dashboard-c6b4b9d77-wmvpw from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 22 15:31:25.188: INFO: coredns-autoscaler-6b97b7f9b-bldh2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container autoscaler ready: true, restart count 0
Jun 22 15:31:25.188: INFO: ibm-master-proxy-static-10.45.191.149 from kube-system started at 2020-06-22 13:22:54 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:31:25.188: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:31:25.188: INFO: ibm-file-plugin-7c85454984-k87nb from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 22 15:31:25.188: INFO: ibm-storage-watcher-7b49c697c8-fmvjk from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 22 15:31:25.188: INFO: olm-operator-557b484679-lvl7g from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container olm-operator ready: true, restart count 0
Jun 22 15:31:25.188: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-7gntq from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:31:25.188: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:31:25.188: INFO: metrics-server-759d6d9f96-q5lgr from kube-system started at 2020-06-22 13:23:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 15:31:25.188: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun 22 15:31:25.188: INFO: catalog-operator-57d89fd5c4-bdgnq from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 22 15:31:25.188: INFO: vpn-58b48cdc7c-z2bv5 from kube-system started at 2020-06-22 13:47:07 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container vpn ready: true, restart count 0
Jun 22 15:31:25.188: INFO: coredns-697bf86f8c-2z94m from kube-system started at 2020-06-22 13:47:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.188: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:31:25.188: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.150 before test
Jun 22 15:31:25.536: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-vrttn from ibm-system started at 2020-06-22 13:25:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 15:31:25.536: INFO: ibm-keepalived-watcher-wkknt from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:31:25.536: INFO: coredns-697bf86f8c-xcrdh from kube-system started at 2020-06-22 13:47:53 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:31:25.536: INFO: addon-catalog-source-fh2f8 from ibm-system started at 2020-06-22 13:27:43 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jun 22 15:31:25.536: INFO: ibm-master-proxy-static-10.45.191.150 from kube-system started at 2020-06-22 13:23:20 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:31:25.536: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:31:25.536: INFO: calico-node-gvtlw from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:31:25.536: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-rg86f from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:31:25.536: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:31:25.536: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-wqw5d from kube-system started at 2020-06-22 13:27:37 +0000 UTC (4 container statuses recorded)
Jun 22 15:31:25.536: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 15:31:25.536: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 15:31:25.536: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 15:31:25.536: INFO: 	Container nginx-ingress ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6080a5c5-0c91-4147-aee7-9a224937df15 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6080a5c5-0c91-4147-aee7-9a224937df15 off the node 10.45.191.149
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6080a5c5-0c91-4147-aee7-9a224937df15
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:36:31.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2551" for this suite.
Jun 22 15:36:45.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:36:46.695: INFO: namespace sched-pred-2551 deletion completed in 14.848778659s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:322.084 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:36:46.696: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 15:36:47.005: INFO: Waiting up to 5m0s for pod "pod-96fc7483-d08a-4930-91c4-cb955e486fda" in namespace "emptydir-1000" to be "success or failure"
Jun 22 15:36:47.025: INFO: Pod "pod-96fc7483-d08a-4930-91c4-cb955e486fda": Phase="Pending", Reason="", readiness=false. Elapsed: 20.173074ms
Jun 22 15:36:49.038: INFO: Pod "pod-96fc7483-d08a-4930-91c4-cb955e486fda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032371381s
Jun 22 15:36:51.051: INFO: Pod "pod-96fc7483-d08a-4930-91c4-cb955e486fda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045293322s
STEP: Saw pod success
Jun 22 15:36:51.051: INFO: Pod "pod-96fc7483-d08a-4930-91c4-cb955e486fda" satisfied condition "success or failure"
Jun 22 15:36:51.064: INFO: Trying to get logs from node 10.45.191.150 pod pod-96fc7483-d08a-4930-91c4-cb955e486fda container test-container: <nil>
STEP: delete the pod
Jun 22 15:36:51.182: INFO: Waiting for pod pod-96fc7483-d08a-4930-91c4-cb955e486fda to disappear
Jun 22 15:36:51.194: INFO: Pod pod-96fc7483-d08a-4930-91c4-cb955e486fda no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:36:51.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1000" for this suite.
Jun 22 15:36:57.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:36:57.850: INFO: namespace emptydir-1000 deletion completed in 6.631540723s

• [SLOW TEST:11.154 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:36:57.850: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 22 15:36:58.132: INFO: Waiting up to 5m0s for pod "downward-api-22a65cd6-a213-4a54-a56f-0b22a30ca2be" in namespace "downward-api-1915" to be "success or failure"
Jun 22 15:36:58.148: INFO: Pod "downward-api-22a65cd6-a213-4a54-a56f-0b22a30ca2be": Phase="Pending", Reason="", readiness=false. Elapsed: 15.481392ms
Jun 22 15:37:00.160: INFO: Pod "downward-api-22a65cd6-a213-4a54-a56f-0b22a30ca2be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027909736s
STEP: Saw pod success
Jun 22 15:37:00.160: INFO: Pod "downward-api-22a65cd6-a213-4a54-a56f-0b22a30ca2be" satisfied condition "success or failure"
Jun 22 15:37:00.175: INFO: Trying to get logs from node 10.45.191.149 pod downward-api-22a65cd6-a213-4a54-a56f-0b22a30ca2be container dapi-container: <nil>
STEP: delete the pod
Jun 22 15:37:00.298: INFO: Waiting for pod downward-api-22a65cd6-a213-4a54-a56f-0b22a30ca2be to disappear
Jun 22 15:37:00.315: INFO: Pod downward-api-22a65cd6-a213-4a54-a56f-0b22a30ca2be no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:37:00.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1915" for this suite.
Jun 22 15:37:06.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:37:06.954: INFO: namespace downward-api-1915 deletion completed in 6.619362249s

• [SLOW TEST:9.104 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:37:06.954: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 15:37:08.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 15:37:11.232: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:37:11.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3832" for this suite.
Jun 22 15:37:19.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:37:20.248: INFO: namespace webhook-3832 deletion completed in 8.601882667s
STEP: Destroying namespace "webhook-3832-markers" for this suite.
Jun 22 15:37:26.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:37:26.864: INFO: namespace webhook-3832-markers deletion completed in 6.615464485s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.984 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:37:26.938: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 15:37:27.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a" in namespace "projected-2737" to be "success or failure"
Jun 22 15:37:27.237: INFO: Pod "downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.323451ms
Jun 22 15:37:29.251: INFO: Pod "downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03028972s
Jun 22 15:37:31.265: INFO: Pod "downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043836387s
STEP: Saw pod success
Jun 22 15:37:31.265: INFO: Pod "downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a" satisfied condition "success or failure"
Jun 22 15:37:31.277: INFO: Trying to get logs from node 10.45.191.131 pod downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a container client-container: <nil>
STEP: delete the pod
Jun 22 15:37:31.396: INFO: Waiting for pod downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a to disappear
Jun 22 15:37:31.407: INFO: Pod downwardapi-volume-951d4f1a-e3b6-48b1-a0b8-9960c15b954a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:37:31.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2737" for this suite.
Jun 22 15:37:39.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:37:40.057: INFO: namespace projected-2737 deletion completed in 8.630473255s

• [SLOW TEST:13.119 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:37:40.058: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Jun 22 15:37:40.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=kubectl-4617 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 22 15:37:43.171: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 22 15:37:43.171: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:37:45.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4617" for this suite.
Jun 22 15:37:51.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:37:51.869: INFO: namespace kubectl-4617 deletion completed in 6.642835111s

• [SLOW TEST:11.812 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:37:51.870: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-dhmfj in namespace proxy-7044
I0622 15:37:52.170878      24 runners.go:184] Created replication controller with name: proxy-service-dhmfj, namespace: proxy-7044, replica count: 1
I0622 15:37:53.221488      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 15:37:54.221869      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 15:37:55.222257      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 15:37:56.222545      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 15:37:57.222860      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 15:37:58.223171      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 15:37:59.223422      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 15:38:00.223723      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 15:38:01.224072      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 15:38:02.224370      24 runners.go:184] proxy-service-dhmfj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 15:38:02.242: INFO: setup took 10.127485317s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 22 15:38:02.286: INFO: (0) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 43.368944ms)
Jun 22 15:38:02.286: INFO: (0) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 44.014813ms)
Jun 22 15:38:02.286: INFO: (0) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 43.941671ms)
Jun 22 15:38:02.286: INFO: (0) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 44.182447ms)
Jun 22 15:38:02.286: INFO: (0) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 43.811266ms)
Jun 22 15:38:02.287: INFO: (0) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 44.594658ms)
Jun 22 15:38:02.296: INFO: (0) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 53.826038ms)
Jun 22 15:38:02.296: INFO: (0) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 54.191ms)
Jun 22 15:38:02.297: INFO: (0) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 53.855835ms)
Jun 22 15:38:02.299: INFO: (0) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 55.965065ms)
Jun 22 15:38:02.310: INFO: (0) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 66.87346ms)
Jun 22 15:38:02.318: INFO: (0) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 75.784209ms)
Jun 22 15:38:02.319: INFO: (0) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 75.097991ms)
Jun 22 15:38:02.319: INFO: (0) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 76.13457ms)
Jun 22 15:38:02.319: INFO: (0) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 76.252671ms)
Jun 22 15:38:02.505: INFO: (0) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 262.890987ms)
Jun 22 15:38:02.528: INFO: (1) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 22.211784ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 28.171442ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 28.30679ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 28.231432ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 28.632841ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 28.432065ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 28.35895ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 28.885637ms)
Jun 22 15:38:02.534: INFO: (1) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 28.496448ms)
Jun 22 15:38:02.535: INFO: (1) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 28.509815ms)
Jun 22 15:38:02.539: INFO: (1) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 33.241597ms)
Jun 22 15:38:02.539: INFO: (1) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 33.07283ms)
Jun 22 15:38:02.545: INFO: (1) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 39.260285ms)
Jun 22 15:38:02.545: INFO: (1) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 39.420183ms)
Jun 22 15:38:02.545: INFO: (1) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 39.208939ms)
Jun 22 15:38:02.545: INFO: (1) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 39.02863ms)
Jun 22 15:38:02.567: INFO: (2) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 21.655874ms)
Jun 22 15:38:02.575: INFO: (2) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 29.673276ms)
Jun 22 15:38:02.575: INFO: (2) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 29.992585ms)
Jun 22 15:38:02.576: INFO: (2) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 30.069355ms)
Jun 22 15:38:02.576: INFO: (2) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 30.678394ms)
Jun 22 15:38:02.576: INFO: (2) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 30.376507ms)
Jun 22 15:38:02.576: INFO: (2) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 30.22099ms)
Jun 22 15:38:02.576: INFO: (2) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 30.373747ms)
Jun 22 15:38:02.576: INFO: (2) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 30.439473ms)
Jun 22 15:38:02.577: INFO: (2) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 30.893131ms)
Jun 22 15:38:02.582: INFO: (2) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 36.201668ms)
Jun 22 15:38:02.585: INFO: (2) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 39.636887ms)
Jun 22 15:38:02.587: INFO: (2) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 41.167095ms)
Jun 22 15:38:02.587: INFO: (2) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 40.981688ms)
Jun 22 15:38:02.587: INFO: (2) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 41.180773ms)
Jun 22 15:38:02.587: INFO: (2) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 41.601154ms)
Jun 22 15:38:02.609: INFO: (3) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 22.007574ms)
Jun 22 15:38:02.613: INFO: (3) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 25.613157ms)
Jun 22 15:38:02.613: INFO: (3) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 25.816328ms)
Jun 22 15:38:02.613: INFO: (3) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 25.448193ms)
Jun 22 15:38:02.613: INFO: (3) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 25.753914ms)
Jun 22 15:38:02.613: INFO: (3) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 25.754415ms)
Jun 22 15:38:02.614: INFO: (3) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 27.16539ms)
Jun 22 15:38:02.614: INFO: (3) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 27.168868ms)
Jun 22 15:38:02.614: INFO: (3) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 26.511796ms)
Jun 22 15:38:02.614: INFO: (3) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 26.686246ms)
Jun 22 15:38:02.622: INFO: (3) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 34.84648ms)
Jun 22 15:38:02.623: INFO: (3) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 35.205878ms)
Jun 22 15:38:02.627: INFO: (3) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 39.032871ms)
Jun 22 15:38:02.627: INFO: (3) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 39.199691ms)
Jun 22 15:38:02.627: INFO: (3) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 39.061353ms)
Jun 22 15:38:02.627: INFO: (3) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 39.286229ms)
Jun 22 15:38:02.646: INFO: (4) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 19.578251ms)
Jun 22 15:38:02.655: INFO: (4) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 27.935854ms)
Jun 22 15:38:02.655: INFO: (4) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 28.250392ms)
Jun 22 15:38:02.655: INFO: (4) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 27.906527ms)
Jun 22 15:38:02.655: INFO: (4) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 27.694287ms)
Jun 22 15:38:02.655: INFO: (4) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 28.526904ms)
Jun 22 15:38:02.655: INFO: (4) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 27.886902ms)
Jun 22 15:38:02.655: INFO: (4) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 28.186514ms)
Jun 22 15:38:02.656: INFO: (4) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 28.320895ms)
Jun 22 15:38:02.656: INFO: (4) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 27.599774ms)
Jun 22 15:38:02.659: INFO: (4) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 31.447351ms)
Jun 22 15:38:02.665: INFO: (4) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 37.336922ms)
Jun 22 15:38:02.669: INFO: (4) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 40.744713ms)
Jun 22 15:38:02.669: INFO: (4) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 41.060709ms)
Jun 22 15:38:02.669: INFO: (4) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 41.083205ms)
Jun 22 15:38:02.677: INFO: (4) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 48.649061ms)
Jun 22 15:38:02.697: INFO: (5) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 20.503035ms)
Jun 22 15:38:02.701: INFO: (5) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 23.505418ms)
Jun 22 15:38:02.701: INFO: (5) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 23.342432ms)
Jun 22 15:38:02.701: INFO: (5) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 23.455032ms)
Jun 22 15:38:02.701: INFO: (5) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 23.963606ms)
Jun 22 15:38:02.701: INFO: (5) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 23.734676ms)
Jun 22 15:38:02.701: INFO: (5) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.205993ms)
Jun 22 15:38:02.702: INFO: (5) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.52438ms)
Jun 22 15:38:02.702: INFO: (5) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 24.472363ms)
Jun 22 15:38:02.702: INFO: (5) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 24.53422ms)
Jun 22 15:38:02.712: INFO: (5) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 34.322747ms)
Jun 22 15:38:02.716: INFO: (5) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 38.625154ms)
Jun 22 15:38:02.720: INFO: (5) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 42.642857ms)
Jun 22 15:38:02.720: INFO: (5) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 42.366055ms)
Jun 22 15:38:02.720: INFO: (5) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 42.556499ms)
Jun 22 15:38:02.720: INFO: (5) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 42.839293ms)
Jun 22 15:38:02.741: INFO: (6) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 20.664467ms)
Jun 22 15:38:02.745: INFO: (6) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 24.170426ms)
Jun 22 15:38:02.745: INFO: (6) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 23.924852ms)
Jun 22 15:38:02.745: INFO: (6) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 24.353477ms)
Jun 22 15:38:02.746: INFO: (6) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 25.608465ms)
Jun 22 15:38:02.746: INFO: (6) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 25.603092ms)
Jun 22 15:38:02.746: INFO: (6) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 25.732459ms)
Jun 22 15:38:02.746: INFO: (6) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 26.034743ms)
Jun 22 15:38:02.746: INFO: (6) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 25.613497ms)
Jun 22 15:38:02.747: INFO: (6) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 26.135463ms)
Jun 22 15:38:02.753: INFO: (6) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 32.601782ms)
Jun 22 15:38:02.763: INFO: (6) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 42.438778ms)
Jun 22 15:38:02.763: INFO: (6) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 42.719712ms)
Jun 22 15:38:02.763: INFO: (6) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 42.151515ms)
Jun 22 15:38:02.764: INFO: (6) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 43.959197ms)
Jun 22 15:38:02.764: INFO: (6) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 43.479696ms)
Jun 22 15:38:02.785: INFO: (7) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 21.046616ms)
Jun 22 15:38:02.789: INFO: (7) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.288059ms)
Jun 22 15:38:02.789: INFO: (7) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 24.405641ms)
Jun 22 15:38:02.789: INFO: (7) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.627909ms)
Jun 22 15:38:02.789: INFO: (7) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 24.691671ms)
Jun 22 15:38:02.790: INFO: (7) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 25.051416ms)
Jun 22 15:38:02.790: INFO: (7) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 25.503907ms)
Jun 22 15:38:02.790: INFO: (7) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 25.808863ms)
Jun 22 15:38:02.790: INFO: (7) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 25.744658ms)
Jun 22 15:38:02.790: INFO: (7) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 25.563241ms)
Jun 22 15:38:02.799: INFO: (7) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 33.96041ms)
Jun 22 15:38:02.800: INFO: (7) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 35.18611ms)
Jun 22 15:38:02.806: INFO: (7) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 41.592849ms)
Jun 22 15:38:02.806: INFO: (7) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 41.499394ms)
Jun 22 15:38:02.806: INFO: (7) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 41.556637ms)
Jun 22 15:38:02.806: INFO: (7) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 41.832746ms)
Jun 22 15:38:02.829: INFO: (8) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 21.69964ms)
Jun 22 15:38:02.831: INFO: (8) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 23.886608ms)
Jun 22 15:38:02.831: INFO: (8) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 24.046278ms)
Jun 22 15:38:02.831: INFO: (8) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 24.202671ms)
Jun 22 15:38:02.831: INFO: (8) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 24.512492ms)
Jun 22 15:38:02.831: INFO: (8) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 24.617943ms)
Jun 22 15:38:02.832: INFO: (8) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 24.981071ms)
Jun 22 15:38:02.833: INFO: (8) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 26.320196ms)
Jun 22 15:38:02.833: INFO: (8) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 26.496652ms)
Jun 22 15:38:02.837: INFO: (8) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 30.334685ms)
Jun 22 15:38:02.840: INFO: (8) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 33.098573ms)
Jun 22 15:38:02.842: INFO: (8) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 34.522763ms)
Jun 22 15:38:02.844: INFO: (8) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 37.440178ms)
Jun 22 15:38:02.844: INFO: (8) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 37.792144ms)
Jun 22 15:38:02.845: INFO: (8) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 37.978948ms)
Jun 22 15:38:02.846: INFO: (8) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 38.736073ms)
Jun 22 15:38:02.868: INFO: (9) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 21.644587ms)
Jun 22 15:38:02.869: INFO: (9) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 22.761635ms)
Jun 22 15:38:02.869: INFO: (9) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 22.920725ms)
Jun 22 15:38:02.869: INFO: (9) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 23.049751ms)
Jun 22 15:38:02.877: INFO: (9) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 31.277729ms)
Jun 22 15:38:02.877: INFO: (9) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 31.350712ms)
Jun 22 15:38:02.877: INFO: (9) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 31.288914ms)
Jun 22 15:38:02.877: INFO: (9) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 31.853219ms)
Jun 22 15:38:02.878: INFO: (9) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 31.416115ms)
Jun 22 15:38:02.879: INFO: (9) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 32.785196ms)
Jun 22 15:38:02.881: INFO: (9) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 34.743494ms)
Jun 22 15:38:02.881: INFO: (9) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 35.013812ms)
Jun 22 15:38:02.882: INFO: (9) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 36.251022ms)
Jun 22 15:38:02.883: INFO: (9) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 36.58983ms)
Jun 22 15:38:02.883: INFO: (9) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 36.89169ms)
Jun 22 15:38:02.892: INFO: (9) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 45.856874ms)
Jun 22 15:38:02.915: INFO: (10) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 23.207815ms)
Jun 22 15:38:02.915: INFO: (10) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 23.132209ms)
Jun 22 15:38:02.916: INFO: (10) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 23.648508ms)
Jun 22 15:38:02.916: INFO: (10) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 23.659728ms)
Jun 22 15:38:02.917: INFO: (10) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 24.617702ms)
Jun 22 15:38:02.917: INFO: (10) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 24.802873ms)
Jun 22 15:38:02.917: INFO: (10) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 25.112367ms)
Jun 22 15:38:02.917: INFO: (10) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.80436ms)
Jun 22 15:38:02.917: INFO: (10) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 24.671849ms)
Jun 22 15:38:02.918: INFO: (10) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 25.721383ms)
Jun 22 15:38:02.923: INFO: (10) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 31.115428ms)
Jun 22 15:38:02.933: INFO: (10) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 40.841703ms)
Jun 22 15:38:02.934: INFO: (10) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 41.260053ms)
Jun 22 15:38:02.934: INFO: (10) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 41.5358ms)
Jun 22 15:38:02.934: INFO: (10) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 41.49597ms)
Jun 22 15:38:02.936: INFO: (10) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 43.966375ms)
Jun 22 15:38:02.958: INFO: (11) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 21.552028ms)
Jun 22 15:38:02.960: INFO: (11) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 23.428657ms)
Jun 22 15:38:02.960: INFO: (11) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 23.583051ms)
Jun 22 15:38:02.960: INFO: (11) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 24.001007ms)
Jun 22 15:38:02.960: INFO: (11) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 23.679017ms)
Jun 22 15:38:02.960: INFO: (11) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 23.769669ms)
Jun 22 15:38:02.961: INFO: (11) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 23.825485ms)
Jun 22 15:38:02.961: INFO: (11) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.254997ms)
Jun 22 15:38:02.961: INFO: (11) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 23.822225ms)
Jun 22 15:38:02.964: INFO: (11) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 27.272585ms)
Jun 22 15:38:02.969: INFO: (11) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 33.003327ms)
Jun 22 15:38:02.977: INFO: (11) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 40.953536ms)
Jun 22 15:38:02.981: INFO: (11) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 44.351726ms)
Jun 22 15:38:02.981: INFO: (11) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 44.605479ms)
Jun 22 15:38:02.981: INFO: (11) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 44.497692ms)
Jun 22 15:38:02.982: INFO: (11) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 45.061285ms)
Jun 22 15:38:03.004: INFO: (12) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 21.687365ms)
Jun 22 15:38:03.005: INFO: (12) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 23.115801ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 24.273721ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 24.389029ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.30548ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 24.308597ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 24.522223ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.410774ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 24.576872ms)
Jun 22 15:38:03.007: INFO: (12) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 24.565819ms)
Jun 22 15:38:03.014: INFO: (12) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 32.426666ms)
Jun 22 15:38:03.017: INFO: (12) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 34.525907ms)
Jun 22 15:38:03.020: INFO: (12) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 38.225376ms)
Jun 22 15:38:03.020: INFO: (12) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 38.10139ms)
Jun 22 15:38:03.020: INFO: (12) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 38.308118ms)
Jun 22 15:38:03.021: INFO: (12) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 38.517707ms)
Jun 22 15:38:03.044: INFO: (13) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 23.235111ms)
Jun 22 15:38:03.044: INFO: (13) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 23.521072ms)
Jun 22 15:38:03.045: INFO: (13) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 23.40132ms)
Jun 22 15:38:03.045: INFO: (13) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 23.345879ms)
Jun 22 15:38:03.045: INFO: (13) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 23.459751ms)
Jun 22 15:38:03.047: INFO: (13) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 26.050146ms)
Jun 22 15:38:03.047: INFO: (13) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 25.752466ms)
Jun 22 15:38:03.047: INFO: (13) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 25.857851ms)
Jun 22 15:38:03.058: INFO: (13) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 37.064498ms)
Jun 22 15:38:03.058: INFO: (13) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 37.381403ms)
Jun 22 15:38:03.058: INFO: (13) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 37.203233ms)
Jun 22 15:38:03.058: INFO: (13) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 37.179603ms)
Jun 22 15:38:03.059: INFO: (13) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 37.437072ms)
Jun 22 15:38:03.061: INFO: (13) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 40.124455ms)
Jun 22 15:38:03.061: INFO: (13) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 40.288316ms)
Jun 22 15:38:03.064: INFO: (13) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 42.44548ms)
Jun 22 15:38:03.088: INFO: (14) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 23.873787ms)
Jun 22 15:38:03.088: INFO: (14) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 23.797646ms)
Jun 22 15:38:03.090: INFO: (14) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 26.232886ms)
Jun 22 15:38:03.095: INFO: (14) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 30.539431ms)
Jun 22 15:38:03.096: INFO: (14) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 31.656801ms)
Jun 22 15:38:03.096: INFO: (14) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 32.05916ms)
Jun 22 15:38:03.096: INFO: (14) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 32.140945ms)
Jun 22 15:38:03.097: INFO: (14) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 32.376053ms)
Jun 22 15:38:03.097: INFO: (14) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 32.672367ms)
Jun 22 15:38:03.099: INFO: (14) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 34.942443ms)
Jun 22 15:38:03.103: INFO: (14) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 38.786429ms)
Jun 22 15:38:03.105: INFO: (14) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 40.821099ms)
Jun 22 15:38:03.105: INFO: (14) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 40.618758ms)
Jun 22 15:38:03.109: INFO: (14) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 44.493393ms)
Jun 22 15:38:03.109: INFO: (14) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 45.135025ms)
Jun 22 15:38:03.110: INFO: (14) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 45.89689ms)
Jun 22 15:38:03.134: INFO: (15) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 22.795645ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 24.03991ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.454945ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 24.724298ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 24.55181ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 24.889897ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 24.706224ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 25.252276ms)
Jun 22 15:38:03.135: INFO: (15) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 25.125447ms)
Jun 22 15:38:03.136: INFO: (15) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 25.417269ms)
Jun 22 15:38:03.139: INFO: (15) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 28.860129ms)
Jun 22 15:38:03.151: INFO: (15) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 40.462223ms)
Jun 22 15:38:03.151: INFO: (15) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 39.98188ms)
Jun 22 15:38:03.151: INFO: (15) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 39.827858ms)
Jun 22 15:38:03.151: INFO: (15) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 40.355607ms)
Jun 22 15:38:03.151: INFO: (15) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 40.784721ms)
Jun 22 15:38:03.173: INFO: (16) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 21.626274ms)
Jun 22 15:38:03.178: INFO: (16) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 25.988039ms)
Jun 22 15:38:03.178: INFO: (16) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 26.100853ms)
Jun 22 15:38:03.178: INFO: (16) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 26.110544ms)
Jun 22 15:38:03.178: INFO: (16) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 26.009853ms)
Jun 22 15:38:03.178: INFO: (16) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 26.109197ms)
Jun 22 15:38:03.178: INFO: (16) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 26.001158ms)
Jun 22 15:38:03.182: INFO: (16) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 30.107277ms)
Jun 22 15:38:03.182: INFO: (16) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 29.917857ms)
Jun 22 15:38:03.182: INFO: (16) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 30.0064ms)
Jun 22 15:38:03.187: INFO: (16) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 35.169608ms)
Jun 22 15:38:03.193: INFO: (16) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 40.720455ms)
Jun 22 15:38:03.196: INFO: (16) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 44.427964ms)
Jun 22 15:38:03.196: INFO: (16) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 44.345247ms)
Jun 22 15:38:03.196: INFO: (16) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 44.840367ms)
Jun 22 15:38:03.197: INFO: (16) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 45.244968ms)
Jun 22 15:38:03.237: INFO: (17) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 40.210527ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 40.351464ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 40.335816ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 40.728312ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 40.579504ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 40.925837ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 40.767883ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 40.955571ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 41.001146ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 41.167431ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 40.993415ms)
Jun 22 15:38:03.238: INFO: (17) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 41.116337ms)
Jun 22 15:38:03.244: INFO: (17) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 47.078049ms)
Jun 22 15:38:03.244: INFO: (17) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 47.144702ms)
Jun 22 15:38:03.244: INFO: (17) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 47.125464ms)
Jun 22 15:38:03.244: INFO: (17) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 47.391077ms)
Jun 22 15:38:03.272: INFO: (18) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 27.745228ms)
Jun 22 15:38:03.273: INFO: (18) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 28.816849ms)
Jun 22 15:38:03.273: INFO: (18) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 28.198633ms)
Jun 22 15:38:03.273: INFO: (18) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 28.627176ms)
Jun 22 15:38:03.273: INFO: (18) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 28.758687ms)
Jun 22 15:38:03.273: INFO: (18) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 28.617491ms)
Jun 22 15:38:03.273: INFO: (18) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 29.151581ms)
Jun 22 15:38:03.273: INFO: (18) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 29.006858ms)
Jun 22 15:38:03.274: INFO: (18) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 28.665634ms)
Jun 22 15:38:03.274: INFO: (18) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 28.695136ms)
Jun 22 15:38:03.281: INFO: (18) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 36.093598ms)
Jun 22 15:38:03.282: INFO: (18) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 37.730687ms)
Jun 22 15:38:03.284: INFO: (18) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 39.244562ms)
Jun 22 15:38:03.284: INFO: (18) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 39.021255ms)
Jun 22 15:38:03.284: INFO: (18) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 38.921039ms)
Jun 22 15:38:03.284: INFO: (18) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 39.268132ms)
Jun 22 15:38:03.309: INFO: (19) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:462/proxy/: tls qux (200; 24.673536ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:443/proxy/tlsrewritem... (200; 39.766687ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">test<... (200; 39.949716ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 40.043212ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 39.818745ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/https:proxy-service-dhmfj-c8p7x:460/proxy/: tls baz (200; 39.918512ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname1/proxy/: foo (200; 40.26063ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname2/proxy/: tls qux (200; 40.222389ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname1/proxy/: foo (200; 39.897169ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:1080/proxy/rewriteme">... (200; 40.127573ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:162/proxy/: bar (200; 40.025377ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/: <a href="/api/v1/namespaces/proxy-7044/pods/proxy-service-dhmfj-c8p7x/proxy/rewriteme">test</a> (200; 40.387486ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/services/http:proxy-service-dhmfj:portname2/proxy/: bar (200; 39.955601ms)
Jun 22 15:38:03.324: INFO: (19) /api/v1/namespaces/proxy-7044/pods/http:proxy-service-dhmfj-c8p7x:160/proxy/: foo (200; 40.095566ms)
Jun 22 15:38:03.335: INFO: (19) /api/v1/namespaces/proxy-7044/services/proxy-service-dhmfj:portname2/proxy/: bar (200; 50.54671ms)
Jun 22 15:38:03.335: INFO: (19) /api/v1/namespaces/proxy-7044/services/https:proxy-service-dhmfj:tlsportname1/proxy/: tls baz (200; 50.6817ms)
STEP: deleting ReplicationController proxy-service-dhmfj in namespace proxy-7044, will wait for the garbage collector to delete the pods
Jun 22 15:38:03.442: INFO: Deleting ReplicationController proxy-service-dhmfj took: 39.594171ms
Jun 22 15:38:03.642: INFO: Terminating ReplicationController proxy-service-dhmfj pods took: 200.325428ms
[AfterEach] version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:38:10.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7044" for this suite.
Jun 22 15:38:18.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:38:19.297: INFO: namespace proxy-7044 deletion completed in 8.629870583s

• [SLOW TEST:27.428 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:38:19.298: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Jun 22 15:38:19.561: INFO: Waiting up to 5m0s for pod "var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557" in namespace "var-expansion-1810" to be "success or failure"
Jun 22 15:38:19.576: INFO: Pod "var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557": Phase="Pending", Reason="", readiness=false. Elapsed: 14.680169ms
Jun 22 15:38:21.589: INFO: Pod "var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028386842s
Jun 22 15:38:23.603: INFO: Pod "var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042034452s
STEP: Saw pod success
Jun 22 15:38:23.603: INFO: Pod "var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557" satisfied condition "success or failure"
Jun 22 15:38:23.617: INFO: Trying to get logs from node 10.45.191.131 pod var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557 container dapi-container: <nil>
STEP: delete the pod
Jun 22 15:38:23.682: INFO: Waiting for pod var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557 to disappear
Jun 22 15:38:23.696: INFO: Pod var-expansion-0415c824-baed-4fc0-8a9e-f8a6ca6f0557 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:38:23.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1810" for this suite.
Jun 22 15:38:29.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:38:30.569: INFO: namespace var-expansion-1810 deletion completed in 6.853956688s

• [SLOW TEST:11.271 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:38:30.569: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:38:32.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7198" for this suite.
Jun 22 15:38:39.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:38:39.591: INFO: namespace emptydir-wrapper-7198 deletion completed in 6.578297173s

• [SLOW TEST:9.022 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:38:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-287
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 22 15:38:39.845: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:38:43.789: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:38:59.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-287" for this suite.
Jun 22 15:39:05.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:39:06.501: INFO: namespace crd-publish-openapi-287 deletion completed in 6.688925343s

• [SLOW TEST:26.907 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:39:06.501: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:39:06.786: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:39:09.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-902" for this suite.
Jun 22 15:39:55.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:39:55.958: INFO: namespace pods-902 deletion completed in 46.86249724s

• [SLOW TEST:49.458 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:39:55.959: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-lkjz
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 15:39:56.301: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lkjz" in namespace "subpath-5814" to be "success or failure"
Jun 22 15:39:56.317: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Pending", Reason="", readiness=false. Elapsed: 15.960159ms
Jun 22 15:39:58.360: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 2.058242969s
Jun 22 15:40:00.386: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 4.084992532s
Jun 22 15:40:02.405: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 6.103327381s
Jun 22 15:40:04.425: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 8.123871651s
Jun 22 15:40:06.444: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 10.142838412s
Jun 22 15:40:08.464: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 12.162165172s
Jun 22 15:40:10.482: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 14.180397081s
Jun 22 15:40:12.498: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 16.196511167s
Jun 22 15:40:14.518: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 18.216043256s
Jun 22 15:40:16.537: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Running", Reason="", readiness=true. Elapsed: 20.235369707s
Jun 22 15:40:18.555: INFO: Pod "pod-subpath-test-downwardapi-lkjz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.25375207s
STEP: Saw pod success
Jun 22 15:40:18.555: INFO: Pod "pod-subpath-test-downwardapi-lkjz" satisfied condition "success or failure"
Jun 22 15:40:18.571: INFO: Trying to get logs from node 10.45.191.149 pod pod-subpath-test-downwardapi-lkjz container test-container-subpath-downwardapi-lkjz: <nil>
STEP: delete the pod
Jun 22 15:40:18.722: INFO: Waiting for pod pod-subpath-test-downwardapi-lkjz to disappear
Jun 22 15:40:18.744: INFO: Pod pod-subpath-test-downwardapi-lkjz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lkjz
Jun 22 15:40:18.744: INFO: Deleting pod "pod-subpath-test-downwardapi-lkjz" in namespace "subpath-5814"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:40:18.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5814" for this suite.
Jun 22 15:40:24.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:40:25.488: INFO: namespace subpath-5814 deletion completed in 6.698949533s

• [SLOW TEST:29.529 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:40:25.488: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-8llt
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 15:40:25.944: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8llt" in namespace "subpath-8912" to be "success or failure"
Jun 22 15:40:25.964: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Pending", Reason="", readiness=false. Elapsed: 19.765045ms
Jun 22 15:40:27.982: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038043394s
Jun 22 15:40:30.001: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 4.057302148s
Jun 22 15:40:32.025: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 6.080562598s
Jun 22 15:40:34.042: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 8.098058223s
Jun 22 15:40:36.063: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 10.118671163s
Jun 22 15:40:38.081: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 12.136617723s
Jun 22 15:40:40.100: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 14.156069484s
Jun 22 15:40:42.121: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 16.177130519s
Jun 22 15:40:44.138: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 18.194207343s
Jun 22 15:40:46.162: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 20.218280489s
Jun 22 15:40:48.185: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Running", Reason="", readiness=true. Elapsed: 22.240615719s
Jun 22 15:40:50.203: INFO: Pod "pod-subpath-test-configmap-8llt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.259116879s
STEP: Saw pod success
Jun 22 15:40:50.203: INFO: Pod "pod-subpath-test-configmap-8llt" satisfied condition "success or failure"
Jun 22 15:40:50.218: INFO: Trying to get logs from node 10.45.191.149 pod pod-subpath-test-configmap-8llt container test-container-subpath-configmap-8llt: <nil>
STEP: delete the pod
Jun 22 15:40:50.310: INFO: Waiting for pod pod-subpath-test-configmap-8llt to disappear
Jun 22 15:40:50.325: INFO: Pod pod-subpath-test-configmap-8llt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8llt
Jun 22 15:40:50.325: INFO: Deleting pod "pod-subpath-test-configmap-8llt" in namespace "subpath-8912"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:40:50.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8912" for this suite.
Jun 22 15:40:58.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:40:59.094: INFO: namespace subpath-8912 deletion completed in 8.704621036s

• [SLOW TEST:33.606 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:40:59.095: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 15:40:59.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2177'
Jun 22 15:40:59.502: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 15:40:59.502: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Jun 22 15:40:59.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete jobs e2e-test-httpd-job --namespace=kubectl-2177'
Jun 22 15:40:59.740: INFO: stderr: ""
Jun 22 15:40:59.740: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:40:59.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2177" for this suite.
Jun 22 15:41:05.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:41:06.494: INFO: namespace kubectl-2177 deletion completed in 6.728155207s

• [SLOW TEST:7.399 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:41:06.494: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-9869/configmap-test-b3510439-fb79-4ddf-b6c2-b36298dda10c
STEP: Creating a pod to test consume configMaps
Jun 22 15:41:06.852: INFO: Waiting up to 5m0s for pod "pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53" in namespace "configmap-9869" to be "success or failure"
Jun 22 15:41:06.869: INFO: Pod "pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53": Phase="Pending", Reason="", readiness=false. Elapsed: 16.719714ms
Jun 22 15:41:08.887: INFO: Pod "pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035035397s
Jun 22 15:41:10.904: INFO: Pod "pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05181073s
STEP: Saw pod success
Jun 22 15:41:10.904: INFO: Pod "pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53" satisfied condition "success or failure"
Jun 22 15:41:10.924: INFO: Trying to get logs from node 10.45.191.131 pod pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53 container env-test: <nil>
STEP: delete the pod
Jun 22 15:41:11.066: INFO: Waiting for pod pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53 to disappear
Jun 22 15:41:11.084: INFO: Pod pod-configmaps-0dd7eab8-87a7-4e5e-92ef-58c0cedc7c53 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:41:11.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9869" for this suite.
Jun 22 15:41:19.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:41:19.779: INFO: namespace configmap-9869 deletion completed in 8.670630122s

• [SLOW TEST:13.285 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:41:19.781: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-5012dc8e-cee7-4a9a-8e44-62cdf9a5318b
STEP: Creating a pod to test consume secrets
Jun 22 15:41:20.094: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-deb19ed4-ecea-4485-87df-dae70aa878af" in namespace "projected-5847" to be "success or failure"
Jun 22 15:41:20.113: INFO: Pod "pod-projected-secrets-deb19ed4-ecea-4485-87df-dae70aa878af": Phase="Pending", Reason="", readiness=false. Elapsed: 18.637523ms
Jun 22 15:41:22.130: INFO: Pod "pod-projected-secrets-deb19ed4-ecea-4485-87df-dae70aa878af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035025945s
STEP: Saw pod success
Jun 22 15:41:22.130: INFO: Pod "pod-projected-secrets-deb19ed4-ecea-4485-87df-dae70aa878af" satisfied condition "success or failure"
Jun 22 15:41:22.145: INFO: Trying to get logs from node 10.45.191.131 pod pod-projected-secrets-deb19ed4-ecea-4485-87df-dae70aa878af container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 15:41:22.233: INFO: Waiting for pod pod-projected-secrets-deb19ed4-ecea-4485-87df-dae70aa878af to disappear
Jun 22 15:41:22.254: INFO: Pod pod-projected-secrets-deb19ed4-ecea-4485-87df-dae70aa878af no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:41:22.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5847" for this suite.
Jun 22 15:41:28.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:41:28.958: INFO: namespace projected-5847 deletion completed in 6.670979594s

• [SLOW TEST:9.178 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:41:28.959: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9607
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-d8a3309f-68e7-4302-987f-4f9491664778
STEP: Creating a pod to test consume configMaps
Jun 22 15:41:29.271: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ae14b4c-6526-4f25-ba9f-06e6d27503d3" in namespace "configmap-9607" to be "success or failure"
Jun 22 15:41:29.289: INFO: Pod "pod-configmaps-4ae14b4c-6526-4f25-ba9f-06e6d27503d3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.407816ms
Jun 22 15:41:31.310: INFO: Pod "pod-configmaps-4ae14b4c-6526-4f25-ba9f-06e6d27503d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03874839s
STEP: Saw pod success
Jun 22 15:41:31.310: INFO: Pod "pod-configmaps-4ae14b4c-6526-4f25-ba9f-06e6d27503d3" satisfied condition "success or failure"
Jun 22 15:41:31.327: INFO: Trying to get logs from node 10.45.191.150 pod pod-configmaps-4ae14b4c-6526-4f25-ba9f-06e6d27503d3 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 15:41:31.471: INFO: Waiting for pod pod-configmaps-4ae14b4c-6526-4f25-ba9f-06e6d27503d3 to disappear
Jun 22 15:41:31.488: INFO: Pod pod-configmaps-4ae14b4c-6526-4f25-ba9f-06e6d27503d3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:41:31.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9607" for this suite.
Jun 22 15:41:39.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:41:40.164: INFO: namespace configmap-9607 deletion completed in 8.653570824s

• [SLOW TEST:11.205 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:41:40.164: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 15:41:40.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8356'
Jun 22 15:41:40.607: INFO: stderr: ""
Jun 22 15:41:40.607: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Jun 22 15:41:40.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete pods e2e-test-httpd-pod --namespace=kubectl-8356'
Jun 22 15:41:54.539: INFO: stderr: ""
Jun 22 15:41:54.539: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:41:54.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8356" for this suite.
Jun 22 15:42:00.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:42:01.289: INFO: namespace kubectl-8356 deletion completed in 6.724128858s

• [SLOW TEST:21.125 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:42:01.295: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 15:42:01.609: INFO: Waiting up to 5m0s for pod "downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83" in namespace "projected-3542" to be "success or failure"
Jun 22 15:42:01.627: INFO: Pod "downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83": Phase="Pending", Reason="", readiness=false. Elapsed: 18.067988ms
Jun 22 15:42:03.645: INFO: Pod "downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035600908s
Jun 22 15:42:05.665: INFO: Pod "downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055369385s
STEP: Saw pod success
Jun 22 15:42:05.665: INFO: Pod "downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83" satisfied condition "success or failure"
Jun 22 15:42:05.681: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83 container client-container: <nil>
STEP: delete the pod
Jun 22 15:42:05.770: INFO: Waiting for pod downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83 to disappear
Jun 22 15:42:05.787: INFO: Pod downwardapi-volume-599cd216-9fe7-4119-b945-23fe60361e83 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:42:05.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3542" for this suite.
Jun 22 15:42:11.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:42:12.465: INFO: namespace projected-3542 deletion completed in 6.648813779s

• [SLOW TEST:11.170 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:42:12.465: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 15:42:14.839: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:42:14.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4355" for this suite.
Jun 22 15:42:20.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:42:21.617: INFO: namespace container-runtime-4355 deletion completed in 6.688502103s

• [SLOW TEST:9.152 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:42:21.618: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jun 22 15:42:21.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-7229'
Jun 22 15:42:22.220: INFO: stderr: ""
Jun 22 15:42:22.220: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 15:42:22.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7229'
Jun 22 15:42:22.377: INFO: stderr: ""
Jun 22 15:42:22.377: INFO: stdout: "update-demo-nautilus-jc6hc update-demo-nautilus-r6h8r "
Jun 22 15:42:22.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-jc6hc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7229'
Jun 22 15:42:22.500: INFO: stderr: ""
Jun 22 15:42:22.500: INFO: stdout: ""
Jun 22 15:42:22.500: INFO: update-demo-nautilus-jc6hc is created but not running
Jun 22 15:42:27.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7229'
Jun 22 15:42:27.625: INFO: stderr: ""
Jun 22 15:42:27.625: INFO: stdout: "update-demo-nautilus-jc6hc update-demo-nautilus-r6h8r "
Jun 22 15:42:27.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-jc6hc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7229'
Jun 22 15:42:27.753: INFO: stderr: ""
Jun 22 15:42:27.753: INFO: stdout: "true"
Jun 22 15:42:27.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-jc6hc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7229'
Jun 22 15:42:27.866: INFO: stderr: ""
Jun 22 15:42:27.866: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 15:42:27.866: INFO: validating pod update-demo-nautilus-jc6hc
Jun 22 15:42:27.903: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 15:42:27.903: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 15:42:27.903: INFO: update-demo-nautilus-jc6hc is verified up and running
Jun 22 15:42:27.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-r6h8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7229'
Jun 22 15:42:28.030: INFO: stderr: ""
Jun 22 15:42:28.030: INFO: stdout: "true"
Jun 22 15:42:28.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-r6h8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7229'
Jun 22 15:42:28.166: INFO: stderr: ""
Jun 22 15:42:28.166: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 15:42:28.166: INFO: validating pod update-demo-nautilus-r6h8r
Jun 22 15:42:28.208: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 15:42:28.208: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 15:42:28.208: INFO: update-demo-nautilus-r6h8r is verified up and running
STEP: using delete to clean up resources
Jun 22 15:42:28.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-7229'
Jun 22 15:42:28.365: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 15:42:28.365: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 15:42:28.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7229'
Jun 22 15:42:28.507: INFO: stderr: "No resources found in kubectl-7229 namespace.\n"
Jun 22 15:42:28.507: INFO: stdout: ""
Jun 22 15:42:28.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -l name=update-demo --namespace=kubectl-7229 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 15:42:28.634: INFO: stderr: ""
Jun 22 15:42:28.634: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:42:28.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7229" for this suite.
Jun 22 15:42:58.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:42:59.888: INFO: namespace kubectl-7229 deletion completed in 31.228964157s

• [SLOW TEST:38.271 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:42:59.889: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 22 15:43:00.203: INFO: Waiting up to 5m0s for pod "downward-api-0c95f988-f20a-4264-a2a3-f99528064e60" in namespace "downward-api-2533" to be "success or failure"
Jun 22 15:43:00.221: INFO: Pod "downward-api-0c95f988-f20a-4264-a2a3-f99528064e60": Phase="Pending", Reason="", readiness=false. Elapsed: 17.509366ms
Jun 22 15:43:02.242: INFO: Pod "downward-api-0c95f988-f20a-4264-a2a3-f99528064e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038513858s
STEP: Saw pod success
Jun 22 15:43:02.242: INFO: Pod "downward-api-0c95f988-f20a-4264-a2a3-f99528064e60" satisfied condition "success or failure"
Jun 22 15:43:02.272: INFO: Trying to get logs from node 10.45.191.149 pod downward-api-0c95f988-f20a-4264-a2a3-f99528064e60 container dapi-container: <nil>
STEP: delete the pod
Jun 22 15:43:02.454: INFO: Waiting for pod downward-api-0c95f988-f20a-4264-a2a3-f99528064e60 to disappear
Jun 22 15:43:02.475: INFO: Pod downward-api-0c95f988-f20a-4264-a2a3-f99528064e60 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:43:02.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2533" for this suite.
Jun 22 15:43:08.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:43:09.192: INFO: namespace downward-api-2533 deletion completed in 6.693753281s

• [SLOW TEST:9.303 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:43:09.192: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Jun 22 15:43:11.591: INFO: Pod pod-hostip-31f3ff54-edeb-462c-97e8-0c831d7dffd0 has hostIP: 10.45.191.131
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:43:11.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2473" for this suite.
Jun 22 15:43:41.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:43:42.354: INFO: namespace pods-2473 deletion completed in 30.71296598s

• [SLOW TEST:33.162 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:43:42.354: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:43:46.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9575" for this suite.
Jun 22 15:43:54.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:43:55.522: INFO: namespace kubelet-test-9575 deletion completed in 8.796306285s

• [SLOW TEST:13.168 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:43:55.522: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Jun 22 15:43:56.427: INFO: created pod pod-service-account-defaultsa
Jun 22 15:43:56.427: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 22 15:43:56.451: INFO: created pod pod-service-account-mountsa
Jun 22 15:43:56.451: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 22 15:43:56.483: INFO: created pod pod-service-account-nomountsa
Jun 22 15:43:56.483: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 22 15:43:56.507: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 22 15:43:56.507: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 22 15:43:56.530: INFO: created pod pod-service-account-mountsa-mountspec
Jun 22 15:43:56.530: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 22 15:43:56.554: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 22 15:43:56.554: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 22 15:43:56.576: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 22 15:43:56.576: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 22 15:43:56.598: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 22 15:43:56.598: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 22 15:43:56.619: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 22 15:43:56.620: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:43:56.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6285" for this suite.
Jun 22 15:44:04.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:44:05.343: INFO: namespace svcaccounts-6285 deletion completed in 8.69522295s

• [SLOW TEST:9.821 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:44:05.343: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 22 15:44:05.617: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 15:44:05.700: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 15:44:05.721: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.131 before test
Jun 22 15:44:05.852: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-jfbn9 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:44:05.852: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:44:05.852: INFO: ibm-master-proxy-static-10.45.191.131 from kube-system started at 2020-06-22 13:23:38 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:44:05.852: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:44:05.852: INFO: ibm-keepalived-watcher-jmnzb from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:44:05.852: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-qhll5 from ibm-system started at 2020-06-22 13:25:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 15:44:05.852: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-7kzrx from kube-system started at 2020-06-22 13:27:37 +0000 UTC (4 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 15:44:05.852: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 15:44:05.852: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 15:44:05.852: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 15:44:05.852: INFO: coredns-697bf86f8c-h8ckn from kube-system started at 2020-06-22 13:47:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:44:05.852: INFO: calico-node-s8lh4 from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:44:05.852: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-06-22 13:27:29 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 22 15:44:05.852: INFO: sonobuoy from sonobuoy started at 2020-06-22 15:11:50 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 15:44:05.852: INFO: sonobuoy-e2e-job-d71e65d5f08d4631 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:05.852: INFO: 	Container e2e ready: true, restart count 0
Jun 22 15:44:05.852: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:44:05.852: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.149 before test
Jun 22 15:44:05.911: INFO: calico-kube-controllers-7c7d954b58-zwmf2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.911: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 15:44:05.911: INFO: kubernetes-dashboard-c6b4b9d77-wmvpw from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.911: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 22 15:44:05.911: INFO: coredns-autoscaler-6b97b7f9b-bldh2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.911: INFO: 	Container autoscaler ready: true, restart count 0
Jun 22 15:44:05.911: INFO: ibm-master-proxy-static-10.45.191.149 from kube-system started at 2020-06-22 13:22:54 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:05.911: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:44:05.911: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:44:05.912: INFO: ibm-keepalived-watcher-n8x49 from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:44:05.912: INFO: calico-node-k8xbh from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:44:05.912: INFO: dashboard-metrics-scraper-576c46d9bd-8tnxf from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun 22 15:44:05.912: INFO: ibm-file-plugin-7c85454984-k87nb from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 22 15:44:05.912: INFO: ibm-storage-watcher-7b49c697c8-fmvjk from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 22 15:44:05.912: INFO: olm-operator-557b484679-lvl7g from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container olm-operator ready: true, restart count 0
Jun 22 15:44:05.912: INFO: metrics-server-759d6d9f96-q5lgr from kube-system started at 2020-06-22 13:23:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 15:44:05.912: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun 22 15:44:05.912: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-7gntq from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:44:05.912: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:44:05.912: INFO: coredns-697bf86f8c-2z94m from kube-system started at 2020-06-22 13:47:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.912: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:44:05.913: INFO: catalog-operator-57d89fd5c4-bdgnq from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.913: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 22 15:44:05.913: INFO: vpn-58b48cdc7c-z2bv5 from kube-system started at 2020-06-22 13:47:07 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:05.913: INFO: 	Container vpn ready: true, restart count 0
Jun 22 15:44:05.913: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.150 before test
Jun 22 15:44:06.025: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-wqw5d from kube-system started at 2020-06-22 13:27:37 +0000 UTC (4 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 15:44:06.025: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 15:44:06.025: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 15:44:06.025: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 15:44:06.025: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-vrttn from ibm-system started at 2020-06-22 13:25:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 15:44:06.025: INFO: ibm-keepalived-watcher-wkknt from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:44:06.025: INFO: coredns-697bf86f8c-xcrdh from kube-system started at 2020-06-22 13:47:53 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:44:06.025: INFO: addon-catalog-source-fh2f8 from ibm-system started at 2020-06-22 13:27:43 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jun 22 15:44:06.025: INFO: ibm-master-proxy-static-10.45.191.150 from kube-system started at 2020-06-22 13:23:20 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:44:06.025: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:44:06.025: INFO: calico-node-gvtlw from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:44:06.025: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-rg86f from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:44:06.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:44:06.025: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.45.191.131
STEP: verifying the node has the label node 10.45.191.149
STEP: verifying the node has the label node 10.45.191.150
Jun 22 15:44:06.222: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.45.191.131
Jun 22 15:44:06.222: INFO: Pod addon-catalog-source-fh2f8 requesting resource cpu=10m on Node 10.45.191.150
Jun 22 15:44:06.222: INFO: Pod catalog-operator-57d89fd5c4-bdgnq requesting resource cpu=10m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-qhll5 requesting resource cpu=5m on Node 10.45.191.131
Jun 22 15:44:06.222: INFO: Pod ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-vrttn requesting resource cpu=5m on Node 10.45.191.150
Jun 22 15:44:06.222: INFO: Pod olm-operator-557b484679-lvl7g requesting resource cpu=10m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod calico-kube-controllers-7c7d954b58-zwmf2 requesting resource cpu=10m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod calico-node-gvtlw requesting resource cpu=250m on Node 10.45.191.150
Jun 22 15:44:06.222: INFO: Pod calico-node-k8xbh requesting resource cpu=250m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod calico-node-s8lh4 requesting resource cpu=250m on Node 10.45.191.131
Jun 22 15:44:06.222: INFO: Pod coredns-697bf86f8c-2z94m requesting resource cpu=100m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod coredns-697bf86f8c-h8ckn requesting resource cpu=100m on Node 10.45.191.131
Jun 22 15:44:06.222: INFO: Pod coredns-697bf86f8c-xcrdh requesting resource cpu=100m on Node 10.45.191.150
Jun 22 15:44:06.222: INFO: Pod coredns-autoscaler-6b97b7f9b-bldh2 requesting resource cpu=20m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod dashboard-metrics-scraper-576c46d9bd-8tnxf requesting resource cpu=1m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod ibm-file-plugin-7c85454984-k87nb requesting resource cpu=50m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod ibm-keepalived-watcher-jmnzb requesting resource cpu=5m on Node 10.45.191.131
Jun 22 15:44:06.222: INFO: Pod ibm-keepalived-watcher-n8x49 requesting resource cpu=5m on Node 10.45.191.149
Jun 22 15:44:06.222: INFO: Pod ibm-keepalived-watcher-wkknt requesting resource cpu=5m on Node 10.45.191.150
Jun 22 15:44:06.222: INFO: Pod ibm-master-proxy-static-10.45.191.131 requesting resource cpu=25m on Node 10.45.191.131
Jun 22 15:44:06.223: INFO: Pod ibm-master-proxy-static-10.45.191.149 requesting resource cpu=25m on Node 10.45.191.149
Jun 22 15:44:06.223: INFO: Pod ibm-master-proxy-static-10.45.191.150 requesting resource cpu=25m on Node 10.45.191.150
Jun 22 15:44:06.223: INFO: Pod ibm-storage-watcher-7b49c697c8-fmvjk requesting resource cpu=50m on Node 10.45.191.149
Jun 22 15:44:06.223: INFO: Pod kubernetes-dashboard-c6b4b9d77-wmvpw requesting resource cpu=50m on Node 10.45.191.149
Jun 22 15:44:06.223: INFO: Pod metrics-server-759d6d9f96-q5lgr requesting resource cpu=121m on Node 10.45.191.149
Jun 22 15:44:06.223: INFO: Pod public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-7kzrx requesting resource cpu=10m on Node 10.45.191.131
Jun 22 15:44:06.223: INFO: Pod public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-wqw5d requesting resource cpu=10m on Node 10.45.191.150
Jun 22 15:44:06.223: INFO: Pod vpn-58b48cdc7c-z2bv5 requesting resource cpu=5m on Node 10.45.191.149
Jun 22 15:44:06.223: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.45.191.131
Jun 22 15:44:06.223: INFO: Pod sonobuoy-e2e-job-d71e65d5f08d4631 requesting resource cpu=0m on Node 10.45.191.131
Jun 22 15:44:06.223: INFO: Pod sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-7gntq requesting resource cpu=0m on Node 10.45.191.149
Jun 22 15:44:06.223: INFO: Pod sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-jfbn9 requesting resource cpu=0m on Node 10.45.191.131
Jun 22 15:44:06.223: INFO: Pod sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-rg86f requesting resource cpu=0m on Node 10.45.191.150
STEP: Starting Pods to consume most of the cluster CPU.
Jun 22 15:44:06.223: INFO: Creating a pod which consumes cpu=2460m on Node 10.45.191.131
Jun 22 15:44:06.253: INFO: Creating a pod which consumes cpu=2242m on Node 10.45.191.149
Jun 22 15:44:06.281: INFO: Creating a pod which consumes cpu=2453m on Node 10.45.191.150
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16025a5e-495c-4a1c-9519-dffa93b28ec0.161ae820ec1423df], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6540/filler-pod-16025a5e-495c-4a1c-9519-dffa93b28ec0 to 10.45.191.150]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16025a5e-495c-4a1c-9519-dffa93b28ec0.161ae8212c53b249], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16025a5e-495c-4a1c-9519-dffa93b28ec0.161ae8212ef571c3], Reason = [Created], Message = [Created container filler-pod-16025a5e-495c-4a1c-9519-dffa93b28ec0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16025a5e-495c-4a1c-9519-dffa93b28ec0.161ae821386be525], Reason = [Started], Message = [Started container filler-pod-16025a5e-495c-4a1c-9519-dffa93b28ec0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b94227f3-419c-4584-9981-ca26e710aa65.161ae820ea2dbf99], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6540/filler-pod-b94227f3-419c-4584-9981-ca26e710aa65 to 10.45.191.149]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b94227f3-419c-4584-9981-ca26e710aa65.161ae821274dd643], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b94227f3-419c-4584-9981-ca26e710aa65.161ae8212a6c04ef], Reason = [Created], Message = [Created container filler-pod-b94227f3-419c-4584-9981-ca26e710aa65]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b94227f3-419c-4584-9981-ca26e710aa65.161ae82132a2b068], Reason = [Started], Message = [Started container filler-pod-b94227f3-419c-4584-9981-ca26e710aa65]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9782d73-1136-4f99-a364-229af94fb743.161ae820e8c9facd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6540/filler-pod-b9782d73-1136-4f99-a364-229af94fb743 to 10.45.191.131]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9782d73-1136-4f99-a364-229af94fb743.161ae8212d7dc63a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9782d73-1136-4f99-a364-229af94fb743.161ae821319762f5], Reason = [Created], Message = [Created container filler-pod-b9782d73-1136-4f99-a364-229af94fb743]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b9782d73-1136-4f99-a364-229af94fb743.161ae8213c466e5a], Reason = [Started], Message = [Started container filler-pod-b9782d73-1136-4f99-a364-229af94fb743]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.161ae821e1a56073], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.161ae821e34b30f9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.45.191.149
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.45.191.150
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.45.191.131
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:44:11.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6540" for this suite.
Jun 22 15:44:19.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:44:20.366: INFO: namespace sched-pred-6540 deletion completed in 8.706834079s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:15.023 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:44:20.367: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-4698624a-bd6c-4d7c-aea9-be76fc602f2c
STEP: Creating a pod to test consume configMaps
Jun 22 15:44:20.686: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aab106a1-333c-459f-bbef-2f26871a845b" in namespace "projected-1995" to be "success or failure"
Jun 22 15:44:20.703: INFO: Pod "pod-projected-configmaps-aab106a1-333c-459f-bbef-2f26871a845b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.522282ms
Jun 22 15:44:22.723: INFO: Pod "pod-projected-configmaps-aab106a1-333c-459f-bbef-2f26871a845b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03660311s
STEP: Saw pod success
Jun 22 15:44:22.723: INFO: Pod "pod-projected-configmaps-aab106a1-333c-459f-bbef-2f26871a845b" satisfied condition "success or failure"
Jun 22 15:44:22.740: INFO: Trying to get logs from node 10.45.191.131 pod pod-projected-configmaps-aab106a1-333c-459f-bbef-2f26871a845b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 15:44:22.835: INFO: Waiting for pod pod-projected-configmaps-aab106a1-333c-459f-bbef-2f26871a845b to disappear
Jun 22 15:44:22.850: INFO: Pod pod-projected-configmaps-aab106a1-333c-459f-bbef-2f26871a845b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:44:22.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1995" for this suite.
Jun 22 15:44:28.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:44:29.545: INFO: namespace projected-1995 deletion completed in 6.669953276s

• [SLOW TEST:9.178 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:44:29.545: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:44:46.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3411" for this suite.
Jun 22 15:44:54.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:44:55.117: INFO: namespace resourcequota-3411 deletion completed in 8.731194851s

• [SLOW TEST:25.571 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:44:55.117: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 15:44:59.620: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 15:44:59.637: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 15:45:01.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 15:45:01.659: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 15:45:03.637: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 15:45:03.654: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:45:03.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1849" for this suite.
Jun 22 15:45:15.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:45:16.366: INFO: namespace container-lifecycle-hook-1849 deletion completed in 12.686940008s

• [SLOW TEST:21.249 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:45:16.367: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-2266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:45:16.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2266" for this suite.
Jun 22 15:45:22.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:45:23.329: INFO: namespace tables-2266 deletion completed in 6.653916751s

• [SLOW TEST:6.962 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:45:23.330: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 22 15:45:28.281: INFO: Successfully updated pod "annotationupdate82f1fad9-0e81-495d-aecb-f8e24080dc47"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:45:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4661" for this suite.
Jun 22 15:45:52.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:45:53.291: INFO: namespace downward-api-4661 deletion completed in 22.915737373s

• [SLOW TEST:29.961 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:45:53.291: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 15:45:54.318: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 22 15:45:56.373: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437554, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437554, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437554, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437554, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 15:45:59.436: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:46:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9396" for this suite.
Jun 22 15:46:20.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:46:20.833: INFO: namespace webhook-9396 deletion completed in 8.695600494s
STEP: Destroying namespace "webhook-9396-markers" for this suite.
Jun 22 15:46:26.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:46:27.568: INFO: namespace webhook-9396-markers deletion completed in 6.734515697s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:34.383 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:46:27.677: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-4488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Jun 22 15:46:27.945: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 15:47:28.054: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:47:28.141: INFO: Starting informer...
STEP: Starting pods...
Jun 22 15:47:28.431: INFO: Pod1 is running on 10.45.191.131. Tainting Node
Jun 22 15:47:30.726: INFO: Pod2 is running on 10.45.191.131. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 22 15:47:48.679: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 22 15:48:08.674: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:48:08.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4488" for this suite.
Jun 22 15:48:16.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:48:17.438: INFO: namespace taint-multiple-pods-4488 deletion completed in 8.685566081s

• [SLOW TEST:109.762 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:48:17.440: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-99fd1678-2dbd-4beb-8ae1-262cf5eff7c3
STEP: Creating a pod to test consume configMaps
Jun 22 15:48:17.788: INFO: Waiting up to 5m0s for pod "pod-configmaps-01280e78-0f6e-4ef7-ab71-7a2366c98646" in namespace "configmap-3196" to be "success or failure"
Jun 22 15:48:17.807: INFO: Pod "pod-configmaps-01280e78-0f6e-4ef7-ab71-7a2366c98646": Phase="Pending", Reason="", readiness=false. Elapsed: 18.936371ms
Jun 22 15:48:19.828: INFO: Pod "pod-configmaps-01280e78-0f6e-4ef7-ab71-7a2366c98646": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03917455s
STEP: Saw pod success
Jun 22 15:48:19.828: INFO: Pod "pod-configmaps-01280e78-0f6e-4ef7-ab71-7a2366c98646" satisfied condition "success or failure"
Jun 22 15:48:19.849: INFO: Trying to get logs from node 10.45.191.149 pod pod-configmaps-01280e78-0f6e-4ef7-ab71-7a2366c98646 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 15:48:19.984: INFO: Waiting for pod pod-configmaps-01280e78-0f6e-4ef7-ab71-7a2366c98646 to disappear
Jun 22 15:48:20.000: INFO: Pod pod-configmaps-01280e78-0f6e-4ef7-ab71-7a2366c98646 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:48:20.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3196" for this suite.
Jun 22 15:48:28.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:48:28.694: INFO: namespace configmap-3196 deletion completed in 8.668469716s

• [SLOW TEST:11.254 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:48:28.695: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-6cd2a331-eab2-4595-a04c-60d932bfd6d1
STEP: Creating secret with name secret-projected-all-test-volume-8ab45426-3e08-413c-bfaa-a9aa9bf0f943
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 22 15:48:29.048: INFO: Waiting up to 5m0s for pod "projected-volume-27f9b1d5-9e31-48dd-a081-3c4451948a3a" in namespace "projected-917" to be "success or failure"
Jun 22 15:48:29.067: INFO: Pod "projected-volume-27f9b1d5-9e31-48dd-a081-3c4451948a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.552376ms
Jun 22 15:48:31.090: INFO: Pod "projected-volume-27f9b1d5-9e31-48dd-a081-3c4451948a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042356452s
STEP: Saw pod success
Jun 22 15:48:31.090: INFO: Pod "projected-volume-27f9b1d5-9e31-48dd-a081-3c4451948a3a" satisfied condition "success or failure"
Jun 22 15:48:31.107: INFO: Trying to get logs from node 10.45.191.149 pod projected-volume-27f9b1d5-9e31-48dd-a081-3c4451948a3a container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 22 15:48:31.188: INFO: Waiting for pod projected-volume-27f9b1d5-9e31-48dd-a081-3c4451948a3a to disappear
Jun 22 15:48:31.204: INFO: Pod projected-volume-27f9b1d5-9e31-48dd-a081-3c4451948a3a no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:48:31.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-917" for this suite.
Jun 22 15:48:37.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:48:37.899: INFO: namespace projected-917 deletion completed in 6.671367736s

• [SLOW TEST:9.204 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:48:37.900: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-2705
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2705 to expose endpoints map[]
Jun 22 15:48:38.222: INFO: successfully validated that service multi-endpoint-test in namespace services-2705 exposes endpoints map[] (16.337624ms elapsed)
STEP: Creating pod pod1 in namespace services-2705
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2705 to expose endpoints map[pod1:[100]]
Jun 22 15:48:40.375: INFO: successfully validated that service multi-endpoint-test in namespace services-2705 exposes endpoints map[pod1:[100]] (2.119654766s elapsed)
STEP: Creating pod pod2 in namespace services-2705
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2705 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 22 15:48:43.637: INFO: successfully validated that service multi-endpoint-test in namespace services-2705 exposes endpoints map[pod1:[100] pod2:[101]] (3.240896166s elapsed)
STEP: Deleting pod pod1 in namespace services-2705
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2705 to expose endpoints map[pod2:[101]]
Jun 22 15:48:43.709: INFO: successfully validated that service multi-endpoint-test in namespace services-2705 exposes endpoints map[pod2:[101]] (41.616328ms elapsed)
STEP: Deleting pod pod2 in namespace services-2705
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2705 to expose endpoints map[]
Jun 22 15:48:43.753: INFO: successfully validated that service multi-endpoint-test in namespace services-2705 exposes endpoints map[] (14.879635ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:48:43.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2705" for this suite.
Jun 22 15:48:49.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:48:50.771: INFO: namespace services-2705 deletion completed in 6.903230138s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.872 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:48:50.772: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 15:48:51.088: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbe5f22e-6a8b-4ba1-9a4b-fffb418883d5" in namespace "projected-1209" to be "success or failure"
Jun 22 15:48:51.106: INFO: Pod "downwardapi-volume-fbe5f22e-6a8b-4ba1-9a4b-fffb418883d5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.366849ms
Jun 22 15:48:53.126: INFO: Pod "downwardapi-volume-fbe5f22e-6a8b-4ba1-9a4b-fffb418883d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037999081s
STEP: Saw pod success
Jun 22 15:48:53.126: INFO: Pod "downwardapi-volume-fbe5f22e-6a8b-4ba1-9a4b-fffb418883d5" satisfied condition "success or failure"
Jun 22 15:48:53.145: INFO: Trying to get logs from node 10.45.191.149 pod downwardapi-volume-fbe5f22e-6a8b-4ba1-9a4b-fffb418883d5 container client-container: <nil>
STEP: delete the pod
Jun 22 15:48:53.232: INFO: Waiting for pod downwardapi-volume-fbe5f22e-6a8b-4ba1-9a4b-fffb418883d5 to disappear
Jun 22 15:48:53.250: INFO: Pod downwardapi-volume-fbe5f22e-6a8b-4ba1-9a4b-fffb418883d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:48:53.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1209" for this suite.
Jun 22 15:48:59.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:49:00.056: INFO: namespace projected-1209 deletion completed in 6.774479361s

• [SLOW TEST:9.284 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:49:00.057: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-12e07884-881b-43a4-82b8-15ca1ebe7736
STEP: Creating a pod to test consume secrets
Jun 22 15:49:00.387: INFO: Waiting up to 5m0s for pod "pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1" in namespace "secrets-1157" to be "success or failure"
Jun 22 15:49:00.411: INFO: Pod "pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.127245ms
Jun 22 15:49:02.429: INFO: Pod "pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041683515s
Jun 22 15:49:04.449: INFO: Pod "pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061671205s
STEP: Saw pod success
Jun 22 15:49:04.449: INFO: Pod "pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1" satisfied condition "success or failure"
Jun 22 15:49:04.465: INFO: Trying to get logs from node 10.45.191.150 pod pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 15:49:04.754: INFO: Waiting for pod pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1 to disappear
Jun 22 15:49:04.782: INFO: Pod pod-secrets-41c38ca6-3c30-46c7-b916-40e7b95327c1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:49:04.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1157" for this suite.
Jun 22 15:49:10.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:49:11.491: INFO: namespace secrets-1157 deletion completed in 6.683384551s

• [SLOW TEST:11.434 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:49:11.492: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6340
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6340
STEP: creating replication controller externalsvc in namespace services-6340
I0622 15:49:11.886035      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-6340, replica count: 2
I0622 15:49:14.936641      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 22 15:49:15.031: INFO: Creating new exec pod
Jun 22 15:49:19.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-6340 execpod26bmc -- /bin/sh -x -c nslookup nodeport-service'
Jun 22 15:49:19.705: INFO: stderr: "+ nslookup nodeport-service\n"
Jun 22 15:49:19.705: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6340.svc.cluster.local\tcanonical name = externalsvc.services-6340.svc.cluster.local.\nName:\texternalsvc.services-6340.svc.cluster.local\nAddress: 172.21.215.201\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6340, will wait for the garbage collector to delete the pods
Jun 22 15:49:19.807: INFO: Deleting ReplicationController externalsvc took: 34.740952ms
Jun 22 15:49:20.009: INFO: Terminating ReplicationController externalsvc pods took: 202.742527ms
Jun 22 15:49:34.694: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:49:34.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6340" for this suite.
Jun 22 15:49:42.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:49:43.464: INFO: namespace services-6340 deletion completed in 8.677333656s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:31.973 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:49:43.465: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5268
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-3b86a2c8-1285-4cb6-908d-5c7db595b8f3
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-3b86a2c8-1285-4cb6-908d-5c7db595b8f3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:49:48.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5268" for this suite.
Jun 22 15:50:08.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:50:08.769: INFO: namespace configmap-5268 deletion completed in 20.732942352s

• [SLOW TEST:25.303 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:50:08.769: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:50:09.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2676" for this suite.
Jun 22 15:50:15.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:50:15.801: INFO: namespace custom-resource-definition-2676 deletion completed in 6.70827714s

• [SLOW TEST:7.032 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:50:15.801: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:50:16.190: INFO: (0) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 96.765311ms)
Jun 22 15:50:16.216: INFO: (1) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.865408ms)
Jun 22 15:50:16.243: INFO: (2) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 27.122502ms)
Jun 22 15:50:16.274: INFO: (3) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 30.451438ms)
Jun 22 15:50:16.303: INFO: (4) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 29.068979ms)
Jun 22 15:50:16.329: INFO: (5) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.922703ms)
Jun 22 15:50:16.365: INFO: (6) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 35.481409ms)
Jun 22 15:50:16.398: INFO: (7) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 32.564046ms)
Jun 22 15:50:16.425: INFO: (8) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 27.800965ms)
Jun 22 15:50:16.454: INFO: (9) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 28.602281ms)
Jun 22 15:50:16.480: INFO: (10) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.707072ms)
Jun 22 15:50:16.506: INFO: (11) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 26.123779ms)
Jun 22 15:50:16.534: INFO: (12) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 27.776078ms)
Jun 22 15:50:16.561: INFO: (13) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 26.867115ms)
Jun 22 15:50:16.592: INFO: (14) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 30.815853ms)
Jun 22 15:50:16.621: INFO: (15) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 28.665137ms)
Jun 22 15:50:16.652: INFO: (16) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 31.134223ms)
Jun 22 15:50:16.678: INFO: (17) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 26.020863ms)
Jun 22 15:50:16.704: INFO: (18) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.953868ms)
Jun 22 15:50:16.729: INFO: (19) /api/v1/nodes/10.45.191.131:10250/proxy/logs/: <pre>
<a href="alb/">alb/</a>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/... (200; 25.203698ms)
[AfterEach] version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:50:16.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7258" for this suite.
Jun 22 15:50:22.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:50:23.463: INFO: namespace proxy-7258 deletion completed in 6.699982019s

• [SLOW TEST:7.662 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:50:23.467: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 22 15:50:23.743: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 15:50:23.802: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 15:50:23.825: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.131 before test
Jun 22 15:50:23.933: INFO: ibm-master-proxy-static-10.45.191.131 from kube-system started at 2020-06-22 13:23:38 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:23.933: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:50:23.933: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:50:23.933: INFO: ibm-keepalived-watcher-jmnzb from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:23.933: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:50:23.933: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-jfbn9 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:23.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:50:23.933: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:50:23.933: INFO: calico-node-s8lh4 from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:23.933: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:50:23.933: INFO: sonobuoy from sonobuoy started at 2020-06-22 15:11:50 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:23.933: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 15:50:23.933: INFO: sonobuoy-e2e-job-d71e65d5f08d4631 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:23.933: INFO: 	Container e2e ready: true, restart count 0
Jun 22 15:50:23.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:50:23.933: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.149 before test
Jun 22 15:50:24.060: INFO: ibm-keepalived-watcher-n8x49 from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:50:24.060: INFO: calico-node-k8xbh from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:50:24.060: INFO: dashboard-metrics-scraper-576c46d9bd-8tnxf from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun 22 15:50:24.060: INFO: calico-kube-controllers-7c7d954b58-zwmf2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 15:50:24.060: INFO: kubernetes-dashboard-c6b4b9d77-wmvpw from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 22 15:50:24.060: INFO: coredns-autoscaler-6b97b7f9b-bldh2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container autoscaler ready: true, restart count 0
Jun 22 15:50:24.060: INFO: ibm-master-proxy-static-10.45.191.149 from kube-system started at 2020-06-22 13:22:54 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:50:24.060: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:50:24.060: INFO: ibm-file-plugin-7c85454984-k87nb from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 22 15:50:24.060: INFO: ibm-storage-watcher-7b49c697c8-fmvjk from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 22 15:50:24.060: INFO: olm-operator-557b484679-lvl7g from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container olm-operator ready: true, restart count 0
Jun 22 15:50:24.060: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-vrs4m from kube-system started at 2020-06-22 15:47:30 +0000 UTC (4 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 15:50:24.060: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 15:50:24.060: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 15:50:24.060: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 15:50:24.060: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-7gntq from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:50:24.060: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:50:24.060: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-7m74w from ibm-system started at 2020-06-22 15:47:30 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 15:50:24.060: INFO: metrics-server-759d6d9f96-q5lgr from kube-system started at 2020-06-22 13:23:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 15:50:24.060: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun 22 15:50:24.060: INFO: catalog-operator-57d89fd5c4-bdgnq from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 22 15:50:24.060: INFO: vpn-58b48cdc7c-z2bv5 from kube-system started at 2020-06-22 13:47:07 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container vpn ready: true, restart count 0
Jun 22 15:50:24.060: INFO: coredns-697bf86f8c-2z94m from kube-system started at 2020-06-22 13:47:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.060: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:50:24.060: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.150 before test
Jun 22 15:50:24.113: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-vrttn from ibm-system started at 2020-06-22 13:25:52 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 15:50:24.113: INFO: coredns-697bf86f8c-xcrdh from kube-system started at 2020-06-22 13:47:53 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:50:24.113: INFO: addon-catalog-source-fh2f8 from ibm-system started at 2020-06-22 13:27:43 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jun 22 15:50:24.113: INFO: ibm-keepalived-watcher-wkknt from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 15:50:24.113: INFO: calico-node-gvtlw from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 15:50:24.113: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-rg86f from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 15:50:24.113: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 15:50:24.113: INFO: ibm-master-proxy-static-10.45.191.150 from kube-system started at 2020-06-22 13:23:20 +0000 UTC (2 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 15:50:24.113: INFO: 	Container pause ready: true, restart count 0
Jun 22 15:50:24.113: INFO: coredns-697bf86f8c-k8tpm from kube-system started at 2020-06-22 15:47:30 +0000 UTC (1 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container coredns ready: true, restart count 0
Jun 22 15:50:24.113: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-wqw5d from kube-system started at 2020-06-22 13:27:37 +0000 UTC (4 container statuses recorded)
Jun 22 15:50:24.113: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 15:50:24.113: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 15:50:24.113: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 15:50:24.113: INFO: 	Container nginx-ingress ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7926d34b-995d-4207-b2f5-615352c0a2a4 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-7926d34b-995d-4207-b2f5-615352c0a2a4 off the node 10.45.191.149
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7926d34b-995d-4207-b2f5-615352c0a2a4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:50:36.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1762" for this suite.
Jun 22 15:50:56.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:50:57.443: INFO: namespace sched-pred-1762 deletion completed in 20.721122794s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:33.976 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:50:57.444: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8775
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 22 15:50:57.719: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 22 15:51:13.309: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:51:16.785: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:51:32.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8775" for this suite.
Jun 22 15:51:40.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:51:41.183: INFO: namespace crd-publish-openapi-8775 deletion completed in 8.690262528s

• [SLOW TEST:43.739 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:51:41.183: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6114
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-c0733ea6-4cb8-48e6-bc92-21b7cefc97b4
STEP: Creating configMap with name cm-test-opt-upd-ed0967a5-7799-4d2c-954b-ccbcdcad8f18
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c0733ea6-4cb8-48e6-bc92-21b7cefc97b4
STEP: Updating configmap cm-test-opt-upd-ed0967a5-7799-4d2c-954b-ccbcdcad8f18
STEP: Creating configMap with name cm-test-opt-create-f715327d-b4a0-48cb-b70f-03d3c1b12d5e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:51:47.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6114" for this suite.
Jun 22 15:52:00.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:52:00.729: INFO: namespace configmap-6114 deletion completed in 12.712482771s

• [SLOW TEST:19.546 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:52:00.736: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-421
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 15:52:01.018: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 15:52:19.431: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.131.119:8080/dial?request=hostName&protocol=http&host=172.30.131.118&port=8080&tries=1'] Namespace:pod-network-test-421 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:52:19.431: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:52:19.649: INFO: Waiting for endpoints: map[]
Jun 22 15:52:19.666: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.131.119:8080/dial?request=hostName&protocol=http&host=172.30.226.106&port=8080&tries=1'] Namespace:pod-network-test-421 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:52:19.666: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:52:19.896: INFO: Waiting for endpoints: map[]
Jun 22 15:52:19.915: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.131.119:8080/dial?request=hostName&protocol=http&host=172.30.44.41&port=8080&tries=1'] Namespace:pod-network-test-421 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:52:19.915: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:52:20.144: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:52:20.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-421" for this suite.
Jun 22 15:52:34.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:52:34.834: INFO: namespace pod-network-test-421 deletion completed in 14.664599296s

• [SLOW TEST:34.098 seconds]
[sig-network] Networking
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:52:34.835: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 15:52:35.601: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 15:52:37.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437955, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437955, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437955, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728437955, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 15:52:40.841: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:52:41.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9512" for this suite.
Jun 22 15:52:49.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:52:50.359: INFO: namespace webhook-9512 deletion completed in 8.719706566s
STEP: Destroying namespace "webhook-9512-markers" for this suite.
Jun 22 15:52:56.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:52:57.055: INFO: namespace webhook-9512-markers deletion completed in 6.695915255s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.315 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:52:57.149: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 15:52:57.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289" in namespace "downward-api-8961" to be "success or failure"
Jun 22 15:52:57.472: INFO: Pod "downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289": Phase="Pending", Reason="", readiness=false. Elapsed: 22.396495ms
Jun 22 15:52:59.491: INFO: Pod "downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041258681s
Jun 22 15:53:01.510: INFO: Pod "downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060823108s
STEP: Saw pod success
Jun 22 15:53:01.510: INFO: Pod "downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289" satisfied condition "success or failure"
Jun 22 15:53:01.528: INFO: Trying to get logs from node 10.45.191.131 pod downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289 container client-container: <nil>
STEP: delete the pod
Jun 22 15:53:01.699: INFO: Waiting for pod downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289 to disappear
Jun 22 15:53:01.716: INFO: Pod downwardapi-volume-ec5f1f9c-9a75-4f8a-a5ac-d49e567e4289 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:53:01.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8961" for this suite.
Jun 22 15:53:07.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:53:08.433: INFO: namespace downward-api-8961 deletion completed in 6.687646042s

• [SLOW TEST:11.284 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:53:08.433: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jun 22 15:53:08.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-9774'
Jun 22 15:53:09.052: INFO: stderr: ""
Jun 22 15:53:09.052: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 15:53:09.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:09.185: INFO: stderr: ""
Jun 22 15:53:09.185: INFO: stdout: "update-demo-nautilus-tz8nf update-demo-nautilus-z2vcd "
Jun 22 15:53:09.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-tz8nf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:09.309: INFO: stderr: ""
Jun 22 15:53:09.309: INFO: stdout: ""
Jun 22 15:53:09.309: INFO: update-demo-nautilus-tz8nf is created but not running
Jun 22 15:53:14.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:14.433: INFO: stderr: ""
Jun 22 15:53:14.433: INFO: stdout: "update-demo-nautilus-tz8nf update-demo-nautilus-z2vcd "
Jun 22 15:53:14.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-tz8nf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:14.550: INFO: stderr: ""
Jun 22 15:53:14.550: INFO: stdout: "true"
Jun 22 15:53:14.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-tz8nf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:14.669: INFO: stderr: ""
Jun 22 15:53:14.669: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 15:53:14.669: INFO: validating pod update-demo-nautilus-tz8nf
Jun 22 15:53:14.711: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 15:53:14.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 15:53:14.711: INFO: update-demo-nautilus-tz8nf is verified up and running
Jun 22 15:53:14.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-z2vcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:14.890: INFO: stderr: ""
Jun 22 15:53:14.890: INFO: stdout: "true"
Jun 22 15:53:14.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-z2vcd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:15.026: INFO: stderr: ""
Jun 22 15:53:15.026: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 15:53:15.026: INFO: validating pod update-demo-nautilus-z2vcd
Jun 22 15:53:15.076: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 15:53:15.076: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 15:53:15.076: INFO: update-demo-nautilus-z2vcd is verified up and running
STEP: scaling down the replication controller
Jun 22 15:53:15.078: INFO: scanned /root for discovery docs: <nil>
Jun 22 15:53:15.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9774'
Jun 22 15:53:16.272: INFO: stderr: ""
Jun 22 15:53:16.272: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 15:53:16.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:16.401: INFO: stderr: ""
Jun 22 15:53:16.401: INFO: stdout: "update-demo-nautilus-tz8nf update-demo-nautilus-z2vcd "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 15:53:21.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:21.565: INFO: stderr: ""
Jun 22 15:53:21.565: INFO: stdout: "update-demo-nautilus-tz8nf update-demo-nautilus-z2vcd "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 15:53:26.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:26.701: INFO: stderr: ""
Jun 22 15:53:26.701: INFO: stdout: "update-demo-nautilus-tz8nf update-demo-nautilus-z2vcd "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 15:53:31.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:31.837: INFO: stderr: ""
Jun 22 15:53:31.837: INFO: stdout: "update-demo-nautilus-z2vcd "
Jun 22 15:53:31.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-z2vcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:31.980: INFO: stderr: ""
Jun 22 15:53:31.980: INFO: stdout: "true"
Jun 22 15:53:31.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-z2vcd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:32.098: INFO: stderr: ""
Jun 22 15:53:32.098: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 15:53:32.098: INFO: validating pod update-demo-nautilus-z2vcd
Jun 22 15:53:32.130: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 15:53:32.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 15:53:32.130: INFO: update-demo-nautilus-z2vcd is verified up and running
STEP: scaling up the replication controller
Jun 22 15:53:32.132: INFO: scanned /root for discovery docs: <nil>
Jun 22 15:53:32.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9774'
Jun 22 15:53:33.371: INFO: stderr: ""
Jun 22 15:53:33.371: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 15:53:33.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:33.495: INFO: stderr: ""
Jun 22 15:53:33.495: INFO: stdout: "update-demo-nautilus-pkt2j update-demo-nautilus-z2vcd "
Jun 22 15:53:33.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-pkt2j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:33.953: INFO: stderr: ""
Jun 22 15:53:33.953: INFO: stdout: ""
Jun 22 15:53:33.953: INFO: update-demo-nautilus-pkt2j is created but not running
Jun 22 15:53:38.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9774'
Jun 22 15:53:39.099: INFO: stderr: ""
Jun 22 15:53:39.099: INFO: stdout: "update-demo-nautilus-pkt2j update-demo-nautilus-z2vcd "
Jun 22 15:53:39.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-pkt2j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:39.238: INFO: stderr: ""
Jun 22 15:53:39.238: INFO: stdout: "true"
Jun 22 15:53:39.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-pkt2j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:39.367: INFO: stderr: ""
Jun 22 15:53:39.367: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 15:53:39.367: INFO: validating pod update-demo-nautilus-pkt2j
Jun 22 15:53:39.408: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 15:53:39.408: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 15:53:39.408: INFO: update-demo-nautilus-pkt2j is verified up and running
Jun 22 15:53:39.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-z2vcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:39.529: INFO: stderr: ""
Jun 22 15:53:39.529: INFO: stdout: "true"
Jun 22 15:53:39.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-z2vcd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9774'
Jun 22 15:53:39.653: INFO: stderr: ""
Jun 22 15:53:39.653: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 15:53:39.653: INFO: validating pod update-demo-nautilus-z2vcd
Jun 22 15:53:39.681: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 15:53:39.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 15:53:39.681: INFO: update-demo-nautilus-z2vcd is verified up and running
STEP: using delete to clean up resources
Jun 22 15:53:39.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-9774'
Jun 22 15:53:39.851: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 15:53:39.851: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 15:53:39.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9774'
Jun 22 15:53:40.012: INFO: stderr: "No resources found in kubectl-9774 namespace.\n"
Jun 22 15:53:40.012: INFO: stdout: ""
Jun 22 15:53:40.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -l name=update-demo --namespace=kubectl-9774 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 15:53:40.764: INFO: stderr: ""
Jun 22 15:53:40.764: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:53:40.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9774" for this suite.
Jun 22 15:53:54.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:53:55.449: INFO: namespace kubectl-9774 deletion completed in 14.659310595s

• [SLOW TEST:47.016 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:53:55.450: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:54:07.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9427" for this suite.
Jun 22 15:54:13.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:54:13.720: INFO: namespace resourcequota-9427 deletion completed in 6.681193832s

• [SLOW TEST:18.270 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:54:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3308
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:54:13.998: INFO: Creating ReplicaSet my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7
Jun 22 15:54:14.036: INFO: Pod name my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7: Found 0 pods out of 1
Jun 22 15:54:19.057: INFO: Pod name my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7: Found 1 pods out of 1
Jun 22 15:54:19.057: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7" is running
Jun 22 15:54:19.074: INFO: Pod "my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7-r8dh6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 15:54:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 15:54:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 15:54:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 15:54:14 +0000 UTC Reason: Message:}])
Jun 22 15:54:19.074: INFO: Trying to dial the pod
Jun 22 15:54:24.149: INFO: Controller my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7: Got expected result from replica 1 [my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7-r8dh6]: "my-hostname-basic-fbc4be2c-f4b8-4942-ad25-83aedb0338a7-r8dh6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:54:24.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3308" for this suite.
Jun 22 15:54:32.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:54:32.867: INFO: namespace replicaset-3308 deletion completed in 8.693949957s

• [SLOW TEST:19.147 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:54:32.869: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9510
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 15:54:33.144: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:54:34.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9510" for this suite.
Jun 22 15:54:40.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:54:41.376: INFO: namespace custom-resource-definition-9510 deletion completed in 7.115926256s

• [SLOW TEST:8.507 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:54:41.376: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 15:54:42.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 15:54:44.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438082, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438082, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438082, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438082, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 15:54:47.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 22 15:54:47.987: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:54:48.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2023" for this suite.
Jun 22 15:54:56.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:54:56.774: INFO: namespace webhook-2023 deletion completed in 8.69849529s
STEP: Destroying namespace "webhook-2023-markers" for this suite.
Jun 22 15:55:02.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:55:03.490: INFO: namespace webhook-2023-markers deletion completed in 6.715271392s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.210 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:55:03.588: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2405
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2405
STEP: creating replication controller externalsvc in namespace services-2405
I0622 15:55:03.957289      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-2405, replica count: 2
I0622 15:55:07.008343      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 22 15:55:07.093: INFO: Creating new exec pod
Jun 22 15:55:09.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-2405 execpodt272z -- /bin/sh -x -c nslookup clusterip-service'
Jun 22 15:55:09.541: INFO: stderr: "+ nslookup clusterip-service\n"
Jun 22 15:55:09.541: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-2405.svc.cluster.local\tcanonical name = externalsvc.services-2405.svc.cluster.local.\nName:\texternalsvc.services-2405.svc.cluster.local\nAddress: 172.21.162.59\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2405, will wait for the garbage collector to delete the pods
Jun 22 15:55:09.649: INFO: Deleting ReplicationController externalsvc took: 41.12143ms
Jun 22 15:55:09.850: INFO: Terminating ReplicationController externalsvc pods took: 200.324165ms
Jun 22 15:55:20.738: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:55:20.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2405" for this suite.
Jun 22 15:55:28.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:55:29.490: INFO: namespace services-2405 deletion completed in 8.655598547s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.903 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:55:29.491: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-72df715f-feee-47e9-9af0-554259001faf
STEP: Creating a pod to test consume secrets
Jun 22 15:55:29.810: INFO: Waiting up to 5m0s for pod "pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068" in namespace "secrets-5954" to be "success or failure"
Jun 22 15:55:29.831: INFO: Pod "pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068": Phase="Pending", Reason="", readiness=false. Elapsed: 21.487993ms
Jun 22 15:55:31.850: INFO: Pod "pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040100502s
Jun 22 15:55:33.879: INFO: Pod "pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069279014s
STEP: Saw pod success
Jun 22 15:55:33.879: INFO: Pod "pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068" satisfied condition "success or failure"
Jun 22 15:55:33.896: INFO: Trying to get logs from node 10.45.191.131 pod pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 15:55:34.044: INFO: Waiting for pod pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068 to disappear
Jun 22 15:55:34.065: INFO: Pod pod-secrets-adaa9bcd-5398-487d-9540-6f375ce22068 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:55:34.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5954" for this suite.
Jun 22 15:55:40.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:55:40.763: INFO: namespace secrets-5954 deletion completed in 6.674417133s

• [SLOW TEST:11.272 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:55:40.766: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-3537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 22 15:55:47.257: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:47.257: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:47.518: INFO: Exec stderr: ""
Jun 22 15:55:47.518: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:47.518: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:47.705: INFO: Exec stderr: ""
Jun 22 15:55:47.705: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:47.706: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:47.903: INFO: Exec stderr: ""
Jun 22 15:55:47.903: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:47.903: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:48.135: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 22 15:55:48.135: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:48.334: INFO: Exec stderr: ""
Jun 22 15:55:48.334: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:48.334: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:48.536: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 22 15:55:48.537: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:48.537: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:48.751: INFO: Exec stderr: ""
Jun 22 15:55:48.751: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:48.751: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:49.028: INFO: Exec stderr: ""
Jun 22 15:55:49.028: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:49.028: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:49.226: INFO: Exec stderr: ""
Jun 22 15:55:49.226: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3537 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 15:55:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 15:55:49.456: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:55:49.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3537" for this suite.
Jun 22 15:56:35.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:56:36.459: INFO: namespace e2e-kubelet-etc-hosts-3537 deletion completed in 46.978078965s

• [SLOW TEST:55.694 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:56:36.460: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 15:56:36.997: INFO: Number of nodes with available pods: 0
Jun 22 15:56:36.997: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:38.041: INFO: Number of nodes with available pods: 0
Jun 22 15:56:38.041: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:39.037: INFO: Number of nodes with available pods: 2
Jun 22 15:56:39.038: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:40.036: INFO: Number of nodes with available pods: 3
Jun 22 15:56:40.036: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 22 15:56:40.127: INFO: Number of nodes with available pods: 2
Jun 22 15:56:40.127: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:41.167: INFO: Number of nodes with available pods: 2
Jun 22 15:56:41.167: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:42.166: INFO: Number of nodes with available pods: 2
Jun 22 15:56:42.166: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:43.165: INFO: Number of nodes with available pods: 2
Jun 22 15:56:43.165: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:44.163: INFO: Number of nodes with available pods: 2
Jun 22 15:56:44.163: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:45.166: INFO: Number of nodes with available pods: 2
Jun 22 15:56:45.166: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 15:56:46.169: INFO: Number of nodes with available pods: 3
Jun 22 15:56:46.169: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-577, will wait for the garbage collector to delete the pods
Jun 22 15:56:46.299: INFO: Deleting DaemonSet.extensions daemon-set took: 39.151853ms
Jun 22 15:56:46.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.395611ms
Jun 22 15:56:54.611: INFO: Number of nodes with available pods: 0
Jun 22 15:56:54.611: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 15:56:54.630: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-577/daemonsets","resourceVersion":"29312"},"items":null}

Jun 22 15:56:54.642: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-577/pods","resourceVersion":"29313"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:56:54.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-577" for this suite.
Jun 22 15:57:02.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:57:03.455: INFO: namespace daemonsets-577 deletion completed in 8.690024186s

• [SLOW TEST:26.996 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:57:03.456: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 15:57:07.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1579" for this suite.
Jun 22 15:57:53.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 15:57:54.491: INFO: namespace kubelet-test-1579 deletion completed in 46.614817757s

• [SLOW TEST:51.035 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 15:57:54.492: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-5795aeb1-3852-433e-a020-86cf060f14ba in namespace container-probe-1376
Jun 22 15:57:56.805: INFO: Started pod busybox-5795aeb1-3852-433e-a020-86cf060f14ba in namespace container-probe-1376
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 15:57:56.816: INFO: Initial restart count of pod busybox-5795aeb1-3852-433e-a020-86cf060f14ba is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:01:58.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1376" for this suite.
Jun 22 16:02:04.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:02:05.395: INFO: namespace container-probe-1376 deletion completed in 6.613143096s

• [SLOW TEST:250.903 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:02:05.396: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:02:05.695: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 22 16:02:10.719: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 16:02:10.719: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 16:02:14.823: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5772 /apis/apps/v1/namespaces/deployment-5772/deployments/test-cleanup-deployment bd538079-9420-43b1-88fc-5ff97de70253 29893 1 2020-06-22 16:02:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0050ee648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-22 16:02:10 +0000 UTC,LastTransitionTime:2020-06-22 16:02:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2020-06-22 16:02:13 +0000 UTC,LastTransitionTime:2020-06-22 16:02:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 16:02:14.841: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-5772 /apis/apps/v1/namespaces/deployment-5772/replicasets/test-cleanup-deployment-65db99849b d75123eb-a25b-4252-bebb-79d841bc3264 29883 1 2020-06-22 16:02:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment bd538079-9420-43b1-88fc-5ff97de70253 0xc001a88997 0xc001a88998}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001a88a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 16:02:14.853: INFO: Pod "test-cleanup-deployment-65db99849b-2s5bp" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-2s5bp test-cleanup-deployment-65db99849b- deployment-5772 /api/v1/namespaces/deployment-5772/pods/test-cleanup-deployment-65db99849b-2s5bp e19eef25-e73e-4b5a-9141-59bbcc1309cc 29882 0 2020-06-22 16:02:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b d75123eb-a25b-4252-bebb-79d841bc3264 0xc001a88e27 0xc001a88e28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zrs7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zrs7q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zrs7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.131,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:02:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:02:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:02:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:02:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.131,PodIP:172.30.226.112,StartTime:2020-06-22 16:02:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 16:02:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://901fc12fb3990181523520338a0e9ada111a83e470dbcbe1994c472ea6619974,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.226.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:02:14.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5772" for this suite.
Jun 22 16:02:22.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:02:23.538: INFO: namespace deployment-5772 deletion completed in 8.659474307s

• [SLOW TEST:18.143 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:02:23.538: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-f906c748-ad1c-43bc-805c-4fde80b49632 in namespace container-probe-8024
Jun 22 16:02:25.856: INFO: Started pod liveness-f906c748-ad1c-43bc-805c-4fde80b49632 in namespace container-probe-8024
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 16:02:25.869: INFO: Initial restart count of pod liveness-f906c748-ad1c-43bc-805c-4fde80b49632 is 0
Jun 22 16:02:39.970: INFO: Restart count of pod container-probe-8024/liveness-f906c748-ad1c-43bc-805c-4fde80b49632 is now 1 (14.100809272s elapsed)
Jun 22 16:03:00.111: INFO: Restart count of pod container-probe-8024/liveness-f906c748-ad1c-43bc-805c-4fde80b49632 is now 2 (34.241699406s elapsed)
Jun 22 16:03:20.248: INFO: Restart count of pod container-probe-8024/liveness-f906c748-ad1c-43bc-805c-4fde80b49632 is now 3 (54.378720025s elapsed)
Jun 22 16:03:40.636: INFO: Restart count of pod container-probe-8024/liveness-f906c748-ad1c-43bc-805c-4fde80b49632 is now 4 (1m14.767463561s elapsed)
Jun 22 16:04:41.016: INFO: Restart count of pod container-probe-8024/liveness-f906c748-ad1c-43bc-805c-4fde80b49632 is now 5 (2m15.146759587s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:04:41.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8024" for this suite.
Jun 22 16:04:49.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:04:49.857: INFO: namespace container-probe-8024 deletion completed in 8.777660942s

• [SLOW TEST:146.319 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:04:49.857: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1918
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jun 22 16:04:50.131: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:05:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1918" for this suite.
Jun 22 16:05:19.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:05:19.784: INFO: namespace crd-publish-openapi-1918 deletion completed in 6.652031891s

• [SLOW TEST:29.926 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:05:19.784: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 16:05:20.072: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b432dd97-d8f3-41ac-a937-70872ee3d607" in namespace "downward-api-3763" to be "success or failure"
Jun 22 16:05:20.086: INFO: Pod "downwardapi-volume-b432dd97-d8f3-41ac-a937-70872ee3d607": Phase="Pending", Reason="", readiness=false. Elapsed: 13.78093ms
Jun 22 16:05:22.098: INFO: Pod "downwardapi-volume-b432dd97-d8f3-41ac-a937-70872ee3d607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025521973s
STEP: Saw pod success
Jun 22 16:05:22.098: INFO: Pod "downwardapi-volume-b432dd97-d8f3-41ac-a937-70872ee3d607" satisfied condition "success or failure"
Jun 22 16:05:22.110: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-b432dd97-d8f3-41ac-a937-70872ee3d607 container client-container: <nil>
STEP: delete the pod
Jun 22 16:05:22.217: INFO: Waiting for pod downwardapi-volume-b432dd97-d8f3-41ac-a937-70872ee3d607 to disappear
Jun 22 16:05:22.227: INFO: Pod downwardapi-volume-b432dd97-d8f3-41ac-a937-70872ee3d607 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:05:22.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3763" for this suite.
Jun 22 16:05:28.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:05:28.886: INFO: namespace downward-api-3763 deletion completed in 6.632894398s

• [SLOW TEST:9.102 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:05:28.887: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3618
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 22 16:05:33.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec pod-sharedvolume-42e94031-3c2b-4c48-bce4-409de58deca8 -c busybox-main-container --namespace=emptydir-3618 -- cat /usr/share/volumeshare/shareddata.txt'
Jun 22 16:05:33.731: INFO: stderr: ""
Jun 22 16:05:33.731: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:05:33.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3618" for this suite.
Jun 22 16:05:41.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:05:42.367: INFO: namespace emptydir-3618 deletion completed in 8.609917724s

• [SLOW TEST:13.481 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:05:42.368: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 16:05:44.696: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:05:44.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6666" for this suite.
Jun 22 16:05:50.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:05:51.399: INFO: namespace container-runtime-6666 deletion completed in 6.63948425s

• [SLOW TEST:9.031 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:05:51.399: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 16:05:52.422: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 16:05:54.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438752, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438752, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438752, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438752, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 16:05:57.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:05:57.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8929" for this suite.
Jun 22 16:06:05.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:06:06.415: INFO: namespace webhook-8929 deletion completed in 8.673214502s
STEP: Destroying namespace "webhook-8929-markers" for this suite.
Jun 22 16:06:12.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:06:13.039: INFO: namespace webhook-8929-markers deletion completed in 6.624744071s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.719 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:06:13.118: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 22 16:06:13.737: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 22 16:06:15.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438773, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438773, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438773, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728438773, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 16:06:18.836: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:06:18.853: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:06:20.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4579" for this suite.
Jun 22 16:06:28.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:06:29.203: INFO: namespace crd-webhook-4579 deletion completed in 8.639268458s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.171 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:06:29.290: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-7019
STEP: creating replication controller nodeport-test in namespace services-7019
I0622 16:06:29.612451      24 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-7019, replica count: 2
I0622 16:06:32.662974      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 16:06:32.663: INFO: Creating new exec pod
Jun 22 16:06:37.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-7019 execpoddgxhk -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 22 16:06:38.120: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 22 16:06:38.120: INFO: stdout: ""
Jun 22 16:06:38.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-7019 execpoddgxhk -- /bin/sh -x -c nc -zv -t -w 2 172.21.150.53 80'
Jun 22 16:06:38.497: INFO: stderr: "+ nc -zv -t -w 2 172.21.150.53 80\nConnection to 172.21.150.53 80 port [tcp/http] succeeded!\n"
Jun 22 16:06:38.497: INFO: stdout: ""
Jun 22 16:06:38.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-7019 execpoddgxhk -- /bin/sh -x -c nc -zv -t -w 2 10.45.191.131 30364'
Jun 22 16:06:38.813: INFO: stderr: "+ nc -zv -t -w 2 10.45.191.131 30364\nConnection to 10.45.191.131 30364 port [tcp/30364] succeeded!\n"
Jun 22 16:06:38.813: INFO: stdout: ""
Jun 22 16:06:38.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-7019 execpoddgxhk -- /bin/sh -x -c nc -zv -t -w 2 10.45.191.149 30364'
Jun 22 16:06:39.151: INFO: stderr: "+ nc -zv -t -w 2 10.45.191.149 30364\nConnection to 10.45.191.149 30364 port [tcp/30364] succeeded!\n"
Jun 22 16:06:39.151: INFO: stdout: ""
Jun 22 16:06:39.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-7019 execpoddgxhk -- /bin/sh -x -c nc -zv -t -w 2 158.175.96.162 30364'
Jun 22 16:06:39.538: INFO: stderr: "+ nc -zv -t -w 2 158.175.96.162 30364\nConnection to 158.175.96.162 30364 port [tcp/30364] succeeded!\n"
Jun 22 16:06:39.538: INFO: stdout: ""
Jun 22 16:06:39.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-7019 execpoddgxhk -- /bin/sh -x -c nc -zv -t -w 2 158.175.96.168 30364'
Jun 22 16:06:39.928: INFO: stderr: "+ nc -zv -t -w 2 158.175.96.168 30364\nConnection to 158.175.96.168 30364 port [tcp/30364] succeeded!\n"
Jun 22 16:06:39.928: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:06:39.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7019" for this suite.
Jun 22 16:06:48.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:06:48.792: INFO: namespace services-7019 deletion completed in 8.834161265s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.502 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:06:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3110
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jun 22 16:06:49.106: INFO: Found 0 stateful pods, waiting for 3
Jun 22 16:06:59.125: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:06:59.125: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:06:59.125: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 22 16:06:59.228: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 22 16:07:09.336: INFO: Updating stateful set ss2
Jun 22 16:07:09.364: INFO: Waiting for Pod statefulset-3110/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 22 16:07:19.466: INFO: Found 1 stateful pods, waiting for 3
Jun 22 16:07:29.481: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:07:29.481: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:07:29.481: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 22 16:07:39.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:07:39.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:07:39.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 22 16:07:39.557: INFO: Updating stateful set ss2
Jun 22 16:07:39.587: INFO: Waiting for Pod statefulset-3110/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 16:07:49.665: INFO: Updating stateful set ss2
Jun 22 16:07:49.694: INFO: Waiting for StatefulSet statefulset-3110/ss2 to complete update
Jun 22 16:07:49.694: INFO: Waiting for Pod statefulset-3110/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 16:07:59.723: INFO: Waiting for StatefulSet statefulset-3110/ss2 to complete update
Jun 22 16:07:59.723: INFO: Waiting for Pod statefulset-3110/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 16:08:09.725: INFO: Waiting for StatefulSet statefulset-3110/ss2 to complete update
Jun 22 16:08:09.725: INFO: Waiting for Pod statefulset-3110/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 16:08:19.731: INFO: Waiting for StatefulSet statefulset-3110/ss2 to complete update
Jun 22 16:08:19.731: INFO: Waiting for Pod statefulset-3110/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 16:08:29.723: INFO: Waiting for StatefulSet statefulset-3110/ss2 to complete update
Jun 22 16:08:29.723: INFO: Waiting for Pod statefulset-3110/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 16:08:39.725: INFO: Waiting for StatefulSet statefulset-3110/ss2 to complete update
Jun 22 16:08:39.725: INFO: Waiting for Pod statefulset-3110/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 22 16:08:49.756: INFO: Deleting all statefulset in ns statefulset-3110
Jun 22 16:08:49.776: INFO: Scaling statefulset ss2 to 0
Jun 22 16:09:19.839: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 16:09:19.855: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:09:19.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3110" for this suite.
Jun 22 16:09:28.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:09:29.123: INFO: namespace statefulset-3110 deletion completed in 9.164399758s

• [SLOW TEST:160.330 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:09:29.124: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:09:42.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1540" for this suite.
Jun 22 16:09:48.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:09:49.273: INFO: namespace resourcequota-1540 deletion completed in 6.622752667s

• [SLOW TEST:20.149 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:09:49.273: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:09:49.791: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 22 16:09:49.821: INFO: Number of nodes with available pods: 0
Jun 22 16:09:49.822: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 22 16:09:49.898: INFO: Number of nodes with available pods: 0
Jun 22 16:09:49.898: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:50.912: INFO: Number of nodes with available pods: 0
Jun 22 16:09:50.912: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:51.910: INFO: Number of nodes with available pods: 0
Jun 22 16:09:51.910: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:52.931: INFO: Number of nodes with available pods: 1
Jun 22 16:09:52.931: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 22 16:09:53.012: INFO: Number of nodes with available pods: 1
Jun 22 16:09:53.012: INFO: Number of running nodes: 0, number of available pods: 1
Jun 22 16:09:54.034: INFO: Number of nodes with available pods: 0
Jun 22 16:09:54.034: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 22 16:09:54.072: INFO: Number of nodes with available pods: 0
Jun 22 16:09:54.073: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:55.085: INFO: Number of nodes with available pods: 0
Jun 22 16:09:55.085: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:56.089: INFO: Number of nodes with available pods: 0
Jun 22 16:09:56.089: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:57.086: INFO: Number of nodes with available pods: 0
Jun 22 16:09:57.086: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:58.087: INFO: Number of nodes with available pods: 0
Jun 22 16:09:58.087: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:09:59.085: INFO: Number of nodes with available pods: 0
Jun 22 16:09:59.086: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:10:00.089: INFO: Number of nodes with available pods: 0
Jun 22 16:10:00.089: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:10:01.087: INFO: Number of nodes with available pods: 1
Jun 22 16:10:01.087: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2369, will wait for the garbage collector to delete the pods
Jun 22 16:10:01.245: INFO: Deleting DaemonSet.extensions daemon-set took: 47.11337ms
Jun 22 16:10:01.446: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.321983ms
Jun 22 16:10:05.059: INFO: Number of nodes with available pods: 0
Jun 22 16:10:05.059: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 16:10:05.075: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2369/daemonsets","resourceVersion":"31448"},"items":null}

Jun 22 16:10:05.086: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2369/pods","resourceVersion":"31448"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:10:05.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2369" for this suite.
Jun 22 16:10:13.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:10:13.979: INFO: namespace daemonsets-2369 deletion completed in 8.760308632s

• [SLOW TEST:24.706 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:10:13.979: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 16:10:14.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-6742'
Jun 22 16:10:14.421: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 16:10:14.421: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Jun 22 16:10:16.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6742'
Jun 22 16:10:16.642: INFO: stderr: ""
Jun 22 16:10:16.642: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:10:16.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6742" for this suite.
Jun 22 16:10:28.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:10:29.301: INFO: namespace kubectl-6742 deletion completed in 12.633877085s

• [SLOW TEST:15.322 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:10:29.302: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9283
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 22 16:10:29.596: INFO: Waiting up to 5m0s for pod "pod-d4da345e-61ac-4522-832f-3a00b2e93a6e" in namespace "emptydir-9283" to be "success or failure"
Jun 22 16:10:29.611: INFO: Pod "pod-d4da345e-61ac-4522-832f-3a00b2e93a6e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.395588ms
Jun 22 16:10:31.624: INFO: Pod "pod-d4da345e-61ac-4522-832f-3a00b2e93a6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027613588s
STEP: Saw pod success
Jun 22 16:10:31.624: INFO: Pod "pod-d4da345e-61ac-4522-832f-3a00b2e93a6e" satisfied condition "success or failure"
Jun 22 16:10:31.636: INFO: Trying to get logs from node 10.45.191.131 pod pod-d4da345e-61ac-4522-832f-3a00b2e93a6e container test-container: <nil>
STEP: delete the pod
Jun 22 16:10:31.742: INFO: Waiting for pod pod-d4da345e-61ac-4522-832f-3a00b2e93a6e to disappear
Jun 22 16:10:31.766: INFO: Pod pod-d4da345e-61ac-4522-832f-3a00b2e93a6e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:10:31.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9283" for this suite.
Jun 22 16:10:37.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:10:38.402: INFO: namespace emptydir-9283 deletion completed in 6.608936414s

• [SLOW TEST:9.101 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:10:38.402: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3008
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:10:38.661: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 16:10:42.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-3008 create -f -'
Jun 22 16:10:43.091: INFO: stderr: ""
Jun 22 16:10:43.091: INFO: stdout: "e2e-test-crd-publish-openapi-3039-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 16:10:43.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-3008 delete e2e-test-crd-publish-openapi-3039-crds test-cr'
Jun 22 16:10:43.348: INFO: stderr: ""
Jun 22 16:10:43.348: INFO: stdout: "e2e-test-crd-publish-openapi-3039-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 22 16:10:43.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-3008 apply -f -'
Jun 22 16:10:43.618: INFO: stderr: ""
Jun 22 16:10:43.619: INFO: stdout: "e2e-test-crd-publish-openapi-3039-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 16:10:43.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-3008 delete e2e-test-crd-publish-openapi-3039-crds test-cr'
Jun 22 16:10:43.788: INFO: stderr: ""
Jun 22 16:10:43.788: INFO: stdout: "e2e-test-crd-publish-openapi-3039-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 22 16:10:43.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-3039-crds'
Jun 22 16:10:44.155: INFO: stderr: ""
Jun 22 16:10:44.155: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3039-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:10:48.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3008" for this suite.
Jun 22 16:10:54.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:10:54.689: INFO: namespace crd-publish-openapi-3008 deletion completed in 6.63848931s

• [SLOW TEST:16.287 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:10:54.690: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 22 16:11:05.231: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 16:11:05.231161      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:11:05.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8450" for this suite.
Jun 22 16:11:13.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:11:13.865: INFO: namespace gc-8450 deletion completed in 8.615126933s

• [SLOW TEST:19.176 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:11:13.866: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:11:18.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4878" for this suite.
Jun 22 16:11:24.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:11:25.456: INFO: namespace watch-4878 deletion completed in 6.771027748s

• [SLOW TEST:11.591 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:11:25.457: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2655
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-310b1d91-60a9-4d5c-9dbf-1f504898322c
STEP: Creating secret with name s-test-opt-upd-a049df2d-586a-4a6b-b802-5d83af000c0d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-310b1d91-60a9-4d5c-9dbf-1f504898322c
STEP: Updating secret s-test-opt-upd-a049df2d-586a-4a6b-b802-5d83af000c0d
STEP: Creating secret with name s-test-opt-create-955d3c1a-c092-43eb-9a99-0e580c58c4b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:12:47.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2655" for this suite.
Jun 22 16:13:17.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:13:18.436: INFO: namespace secrets-2655 deletion completed in 30.741645601s

• [SLOW TEST:112.979 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:13:18.436: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 16:13:18.740: INFO: Waiting up to 5m0s for pod "pod-01e4fa96-4530-46bb-a4a9-aee7ff353ffd" in namespace "emptydir-9085" to be "success or failure"
Jun 22 16:13:18.758: INFO: Pod "pod-01e4fa96-4530-46bb-a4a9-aee7ff353ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.819476ms
Jun 22 16:13:20.770: INFO: Pod "pod-01e4fa96-4530-46bb-a4a9-aee7ff353ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029497687s
STEP: Saw pod success
Jun 22 16:13:20.770: INFO: Pod "pod-01e4fa96-4530-46bb-a4a9-aee7ff353ffd" satisfied condition "success or failure"
Jun 22 16:13:20.781: INFO: Trying to get logs from node 10.45.191.150 pod pod-01e4fa96-4530-46bb-a4a9-aee7ff353ffd container test-container: <nil>
STEP: delete the pod
Jun 22 16:13:20.839: INFO: Waiting for pod pod-01e4fa96-4530-46bb-a4a9-aee7ff353ffd to disappear
Jun 22 16:13:20.851: INFO: Pod pod-01e4fa96-4530-46bb-a4a9-aee7ff353ffd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:13:20.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9085" for this suite.
Jun 22 16:13:26.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:13:27.535: INFO: namespace emptydir-9085 deletion completed in 6.657133494s

• [SLOW TEST:9.099 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:13:27.535: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:13:27.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 version'
Jun 22 16:13:27.895: INFO: stderr: ""
Jun 22 16:13:27.895: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.11\", GitCommit:\"436254b798f772bcb8e67dcfe122e46500eeb254\", GitTreeState:\"clean\", BuildDate:\"2020-06-17T11:49:23Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.11+IKS\", GitCommit:\"c33b093e937d1f9eee340f9bddfa14e1e54ad1a4\", GitTreeState:\"clean\", BuildDate:\"2020-06-17T21:37:54Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:13:27.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4570" for this suite.
Jun 22 16:13:33.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:13:34.581: INFO: namespace kubectl-4570 deletion completed in 6.659310546s

• [SLOW TEST:7.046 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:13:34.584: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:13:34.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3430" for this suite.
Jun 22 16:13:41.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:13:41.600: INFO: namespace resourcequota-3430 deletion completed in 6.604720254s

• [SLOW TEST:7.016 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:13:41.600: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 16:13:42.332: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 16:13:44.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728439222, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728439222, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728439222, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728439222, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 16:13:47.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:13:47.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4813" for this suite.
Jun 22 16:13:55.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:13:56.590: INFO: namespace webhook-4813 deletion completed in 8.970663679s
STEP: Destroying namespace "webhook-4813-markers" for this suite.
Jun 22 16:14:02.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:14:03.243: INFO: namespace webhook-4813-markers deletion completed in 6.653206208s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.727 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:14:03.327: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 16:14:03.624: INFO: Waiting up to 5m0s for pod "pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560" in namespace "emptydir-277" to be "success or failure"
Jun 22 16:14:03.638: INFO: Pod "pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560": Phase="Pending", Reason="", readiness=false. Elapsed: 14.305852ms
Jun 22 16:14:05.678: INFO: Pod "pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054132093s
Jun 22 16:14:07.691: INFO: Pod "pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066594188s
STEP: Saw pod success
Jun 22 16:14:07.691: INFO: Pod "pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560" satisfied condition "success or failure"
Jun 22 16:14:07.703: INFO: Trying to get logs from node 10.45.191.149 pod pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560 container test-container: <nil>
STEP: delete the pod
Jun 22 16:14:07.805: INFO: Waiting for pod pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560 to disappear
Jun 22 16:14:07.823: INFO: Pod pod-ff3a2e7c-e360-4ee2-b0e4-2c5196c40560 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:14:07.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-277" for this suite.
Jun 22 16:14:13.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:14:14.482: INFO: namespace emptydir-277 deletion completed in 6.634207231s

• [SLOW TEST:11.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:14:14.482: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Jun 22 16:14:14.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-3318'
Jun 22 16:14:15.138: INFO: stderr: ""
Jun 22 16:14:15.138: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 16:14:15.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3318'
Jun 22 16:14:15.279: INFO: stderr: ""
Jun 22 16:14:15.279: INFO: stdout: "update-demo-nautilus-8shkg update-demo-nautilus-bvccz "
Jun 22 16:14:15.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-8shkg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:15.415: INFO: stderr: ""
Jun 22 16:14:15.415: INFO: stdout: ""
Jun 22 16:14:15.415: INFO: update-demo-nautilus-8shkg is created but not running
Jun 22 16:14:20.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3318'
Jun 22 16:14:20.539: INFO: stderr: ""
Jun 22 16:14:20.540: INFO: stdout: "update-demo-nautilus-8shkg update-demo-nautilus-bvccz "
Jun 22 16:14:20.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-8shkg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:20.670: INFO: stderr: ""
Jun 22 16:14:20.670: INFO: stdout: "true"
Jun 22 16:14:20.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-8shkg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:20.817: INFO: stderr: ""
Jun 22 16:14:20.817: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 16:14:20.817: INFO: validating pod update-demo-nautilus-8shkg
Jun 22 16:14:20.845: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 16:14:20.845: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 16:14:20.845: INFO: update-demo-nautilus-8shkg is verified up and running
Jun 22 16:14:20.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-bvccz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:20.968: INFO: stderr: ""
Jun 22 16:14:20.968: INFO: stdout: "true"
Jun 22 16:14:20.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-nautilus-bvccz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:21.092: INFO: stderr: ""
Jun 22 16:14:21.092: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 16:14:21.092: INFO: validating pod update-demo-nautilus-bvccz
Jun 22 16:14:21.120: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 16:14:21.120: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 16:14:21.120: INFO: update-demo-nautilus-bvccz is verified up and running
STEP: rolling-update to new replication controller
Jun 22 16:14:21.122: INFO: scanned /root for discovery docs: <nil>
Jun 22 16:14:21.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3318'
Jun 22 16:14:44.252: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 16:14:44.252: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 16:14:44.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3318'
Jun 22 16:14:44.379: INFO: stderr: ""
Jun 22 16:14:44.379: INFO: stdout: "update-demo-kitten-6c5g9 update-demo-kitten-vnsdn "
Jun 22 16:14:44.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-kitten-6c5g9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:44.519: INFO: stderr: ""
Jun 22 16:14:44.519: INFO: stdout: "true"
Jun 22 16:14:44.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-kitten-6c5g9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:44.645: INFO: stderr: ""
Jun 22 16:14:44.645: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 16:14:44.645: INFO: validating pod update-demo-kitten-6c5g9
Jun 22 16:14:44.682: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 16:14:44.682: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 16:14:44.682: INFO: update-demo-kitten-6c5g9 is verified up and running
Jun 22 16:14:44.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-kitten-vnsdn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:44.818: INFO: stderr: ""
Jun 22 16:14:44.818: INFO: stdout: "true"
Jun 22 16:14:44.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods update-demo-kitten-vnsdn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3318'
Jun 22 16:14:44.964: INFO: stderr: ""
Jun 22 16:14:44.964: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 16:14:44.964: INFO: validating pod update-demo-kitten-vnsdn
Jun 22 16:14:44.991: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 16:14:44.991: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 16:14:44.991: INFO: update-demo-kitten-vnsdn is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:14:44.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3318" for this suite.
Jun 22 16:15:09.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:15:09.645: INFO: namespace kubectl-3318 deletion completed in 24.628505476s

• [SLOW TEST:55.163 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:15:09.645: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-344
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 22 16:15:09.982: INFO: Waiting up to 5m0s for pod "pod-27b0d86d-2666-486f-a17c-c266926e474a" in namespace "emptydir-344" to be "success or failure"
Jun 22 16:15:09.994: INFO: Pod "pod-27b0d86d-2666-486f-a17c-c266926e474a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.961118ms
Jun 22 16:15:12.006: INFO: Pod "pod-27b0d86d-2666-486f-a17c-c266926e474a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024549723s
STEP: Saw pod success
Jun 22 16:15:12.006: INFO: Pod "pod-27b0d86d-2666-486f-a17c-c266926e474a" satisfied condition "success or failure"
Jun 22 16:15:12.018: INFO: Trying to get logs from node 10.45.191.150 pod pod-27b0d86d-2666-486f-a17c-c266926e474a container test-container: <nil>
STEP: delete the pod
Jun 22 16:15:12.118: INFO: Waiting for pod pod-27b0d86d-2666-486f-a17c-c266926e474a to disappear
Jun 22 16:15:12.129: INFO: Pod pod-27b0d86d-2666-486f-a17c-c266926e474a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:15:12.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-344" for this suite.
Jun 22 16:15:20.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:15:20.779: INFO: namespace emptydir-344 deletion completed in 8.625072707s

• [SLOW TEST:11.134 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:15:20.780: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7216
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:15:24.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7216" for this suite.
Jun 22 16:15:54.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:15:55.016: INFO: namespace replication-controller-7216 deletion completed in 30.785567865s

• [SLOW TEST:34.237 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:15:55.017: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-9b3aac89-a1dc-4bea-9c83-93c731ae66b9
STEP: Creating a pod to test consume secrets
Jun 22 16:15:55.334: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cafbac2e-35b0-48e0-ac75-ea416c2c2064" in namespace "projected-6837" to be "success or failure"
Jun 22 16:15:55.347: INFO: Pod "pod-projected-secrets-cafbac2e-35b0-48e0-ac75-ea416c2c2064": Phase="Pending", Reason="", readiness=false. Elapsed: 12.221777ms
Jun 22 16:15:57.359: INFO: Pod "pod-projected-secrets-cafbac2e-35b0-48e0-ac75-ea416c2c2064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024510065s
STEP: Saw pod success
Jun 22 16:15:57.359: INFO: Pod "pod-projected-secrets-cafbac2e-35b0-48e0-ac75-ea416c2c2064" satisfied condition "success or failure"
Jun 22 16:15:57.373: INFO: Trying to get logs from node 10.45.191.149 pod pod-projected-secrets-cafbac2e-35b0-48e0-ac75-ea416c2c2064 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 16:15:57.469: INFO: Waiting for pod pod-projected-secrets-cafbac2e-35b0-48e0-ac75-ea416c2c2064 to disappear
Jun 22 16:15:57.480: INFO: Pod pod-projected-secrets-cafbac2e-35b0-48e0-ac75-ea416c2c2064 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:15:57.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6837" for this suite.
Jun 22 16:16:03.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:16:04.127: INFO: namespace projected-6837 deletion completed in 6.620698756s

• [SLOW TEST:9.111 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:16:04.130: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 22 16:16:07.027: INFO: Successfully updated pod "labelsupdate65251c1a-48ce-498b-85e3-d81a98f7ccee"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:16:11.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1740" for this suite.
Jun 22 16:16:23.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:16:24.118: INFO: namespace projected-1740 deletion completed in 12.950681781s

• [SLOW TEST:19.988 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:16:24.121: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5591
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5591
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-5591
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5591
Jun 22 16:16:24.444: INFO: Found 0 stateful pods, waiting for 1
Jun 22 16:16:34.458: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 22 16:16:34.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 16:16:34.870: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 16:16:34.870: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 16:16:34.870: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 16:16:34.882: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 16:16:44.896: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 16:16:44.896: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 16:16:44.963: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:16:44.963: INFO: ss-0  10.45.191.150  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:16:44.963: INFO: ss-1                 Pending         []
Jun 22 16:16:44.963: INFO: 
Jun 22 16:16:44.963: INFO: StatefulSet ss has not reached scale 3, at 2
Jun 22 16:16:45.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98528089s
Jun 22 16:16:46.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.969856295s
Jun 22 16:16:48.009: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956908563s
Jun 22 16:16:49.023: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.939584501s
Jun 22 16:16:50.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.924921385s
Jun 22 16:16:51.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.900188577s
Jun 22 16:16:52.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.887467284s
Jun 22 16:16:53.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.873882251s
Jun 22 16:16:54.105: INFO: Verifying statefulset ss doesn't scale past 3 for another 857.00373ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5591
Jun 22 16:16:55.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:16:55.498: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 16:16:55.498: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 16:16:55.498: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 16:16:55.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:16:55.889: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 16:16:55.889: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 16:16:55.889: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 16:16:55.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:16:56.278: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 16:16:56.278: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 16:16:56.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 16:16:56.292: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:16:56.292: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:16:56.292: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 22 16:16:56.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 16:16:56.674: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 16:16:56.674: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 16:16:56.674: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 16:16:56.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 16:16:57.023: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 16:16:57.023: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 16:16:57.023: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 16:16:57.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 16:16:57.427: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 16:16:57.427: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 16:16:57.427: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 16:16:57.427: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 16:16:57.443: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 22 16:17:07.477: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 16:17:07.477: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 16:17:07.477: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 16:17:07.535: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:07.535: INFO: ss-0  10.45.191.150  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:07.535: INFO: ss-1  10.45.191.131  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:07.536: INFO: ss-2  10.45.191.149  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:07.536: INFO: 
Jun 22 16:17:07.536: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 16:17:08.550: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:08.550: INFO: ss-0  10.45.191.150  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:08.551: INFO: ss-1  10.45.191.131  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:08.551: INFO: ss-2  10.45.191.149  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:08.551: INFO: 
Jun 22 16:17:08.551: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 16:17:09.564: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:09.564: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:09.564: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:09.564: INFO: 
Jun 22 16:17:09.564: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 16:17:10.577: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:10.577: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:10.577: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:10.577: INFO: 
Jun 22 16:17:10.577: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 16:17:11.590: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:11.590: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:11.590: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:11.590: INFO: 
Jun 22 16:17:11.590: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 16:17:12.604: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:12.604: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:12.604: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:12.604: INFO: 
Jun 22 16:17:12.604: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 16:17:13.617: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:13.618: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:13.618: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:13.618: INFO: 
Jun 22 16:17:13.618: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 16:17:14.630: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:14.630: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:14.630: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:14.630: INFO: 
Jun 22 16:17:14.630: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 16:17:15.644: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:15.644: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:15.644: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:15.644: INFO: 
Jun 22 16:17:15.644: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 16:17:16.657: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 22 16:17:16.657: INFO: ss-0  10.45.191.150  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:24 +0000 UTC  }]
Jun 22 16:17:16.657: INFO: ss-1  10.45.191.131  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-22 16:16:44 +0000 UTC  }]
Jun 22 16:17:16.657: INFO: 
Jun 22 16:17:16.657: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5591
Jun 22 16:17:17.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:17:18.025: INFO: rc: 1
Jun 22 16:17:18.026: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 22 16:17:28.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:17:28.172: INFO: rc: 1
Jun 22 16:17:28.172: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:17:38.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:17:38.320: INFO: rc: 1
Jun 22 16:17:38.320: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:17:48.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:17:48.449: INFO: rc: 1
Jun 22 16:17:48.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:17:58.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:17:58.607: INFO: rc: 1
Jun 22 16:17:58.607: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:18:08.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:18:08.753: INFO: rc: 1
Jun 22 16:18:08.753: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:18:18.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:18:18.880: INFO: rc: 1
Jun 22 16:18:18.880: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:18:28.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:18:29.043: INFO: rc: 1
Jun 22 16:18:29.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:18:39.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:18:39.202: INFO: rc: 1
Jun 22 16:18:39.202: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:18:49.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:18:49.342: INFO: rc: 1
Jun 22 16:18:49.342: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:18:59.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:18:59.503: INFO: rc: 1
Jun 22 16:18:59.503: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:19:09.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:19:09.630: INFO: rc: 1
Jun 22 16:19:09.630: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:19:19.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:19:19.798: INFO: rc: 1
Jun 22 16:19:19.798: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:19:29.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:19:29.943: INFO: rc: 1
Jun 22 16:19:29.943: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:19:39.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:19:40.090: INFO: rc: 1
Jun 22 16:19:40.090: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:19:50.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:19:50.258: INFO: rc: 1
Jun 22 16:19:50.258: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:20:00.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:20:00.400: INFO: rc: 1
Jun 22 16:20:00.400: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:20:10.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:20:10.535: INFO: rc: 1
Jun 22 16:20:10.535: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:20:20.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:20:20.696: INFO: rc: 1
Jun 22 16:20:20.696: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:20:30.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:20:30.823: INFO: rc: 1
Jun 22 16:20:30.823: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:20:40.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:20:40.962: INFO: rc: 1
Jun 22 16:20:40.962: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:20:50.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:20:51.316: INFO: rc: 1
Jun 22 16:20:51.316: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:21:01.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:21:01.499: INFO: rc: 1
Jun 22 16:21:01.499: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:21:11.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:21:11.632: INFO: rc: 1
Jun 22 16:21:11.632: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:21:21.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:21:21.784: INFO: rc: 1
Jun 22 16:21:21.784: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:21:31.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:21:31.914: INFO: rc: 1
Jun 22 16:21:31.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:21:41.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:21:42.053: INFO: rc: 1
Jun 22 16:21:42.053: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:21:52.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:21:52.212: INFO: rc: 1
Jun 22 16:21:52.212: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:22:02.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:22:02.397: INFO: rc: 1
Jun 22 16:22:02.397: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:22:12.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:22:12.550: INFO: rc: 1
Jun 22 16:22:12.550: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 16:22:22.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-5591 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:22:22.702: INFO: rc: 1
Jun 22 16:22:22.702: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Jun 22 16:22:22.702: INFO: Scaling statefulset ss to 0
Jun 22 16:22:22.754: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 22 16:22:22.773: INFO: Deleting all statefulset in ns statefulset-5591
Jun 22 16:22:22.790: INFO: Scaling statefulset ss to 0
Jun 22 16:22:22.841: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 16:22:22.858: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:22:22.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5591" for this suite.
Jun 22 16:22:31.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:22:31.586: INFO: namespace statefulset-5591 deletion completed in 8.636337696s

• [SLOW TEST:367.466 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:22:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:22:35.984: INFO: Waiting up to 5m0s for pod "client-envvars-3246c91a-0690-4acf-84ac-104695d34fe3" in namespace "pods-8251" to be "success or failure"
Jun 22 16:22:35.994: INFO: Pod "client-envvars-3246c91a-0690-4acf-84ac-104695d34fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.679616ms
Jun 22 16:22:38.006: INFO: Pod "client-envvars-3246c91a-0690-4acf-84ac-104695d34fe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022588626s
STEP: Saw pod success
Jun 22 16:22:38.006: INFO: Pod "client-envvars-3246c91a-0690-4acf-84ac-104695d34fe3" satisfied condition "success or failure"
Jun 22 16:22:38.017: INFO: Trying to get logs from node 10.45.191.131 pod client-envvars-3246c91a-0690-4acf-84ac-104695d34fe3 container env3cont: <nil>
STEP: delete the pod
Jun 22 16:22:38.123: INFO: Waiting for pod client-envvars-3246c91a-0690-4acf-84ac-104695d34fe3 to disappear
Jun 22 16:22:38.135: INFO: Pod client-envvars-3246c91a-0690-4acf-84ac-104695d34fe3 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:22:38.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8251" for this suite.
Jun 22 16:22:50.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:22:50.794: INFO: namespace pods-8251 deletion completed in 12.633512161s

• [SLOW TEST:19.207 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:22:50.794: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-85ce777f-f2e2-42b0-9af3-8b27d9ac2736
STEP: Creating a pod to test consume configMaps
Jun 22 16:22:51.099: INFO: Waiting up to 5m0s for pod "pod-configmaps-4942053f-9c62-42b9-9398-62df1606a380" in namespace "configmap-4253" to be "success or failure"
Jun 22 16:22:51.114: INFO: Pod "pod-configmaps-4942053f-9c62-42b9-9398-62df1606a380": Phase="Pending", Reason="", readiness=false. Elapsed: 15.019163ms
Jun 22 16:22:53.126: INFO: Pod "pod-configmaps-4942053f-9c62-42b9-9398-62df1606a380": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0271054s
STEP: Saw pod success
Jun 22 16:22:53.126: INFO: Pod "pod-configmaps-4942053f-9c62-42b9-9398-62df1606a380" satisfied condition "success or failure"
Jun 22 16:22:53.137: INFO: Trying to get logs from node 10.45.191.131 pod pod-configmaps-4942053f-9c62-42b9-9398-62df1606a380 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 16:22:53.203: INFO: Waiting for pod pod-configmaps-4942053f-9c62-42b9-9398-62df1606a380 to disappear
Jun 22 16:22:53.215: INFO: Pod pod-configmaps-4942053f-9c62-42b9-9398-62df1606a380 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:22:53.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4253" for this suite.
Jun 22 16:23:01.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:23:02.164: INFO: namespace configmap-4253 deletion completed in 8.923792349s

• [SLOW TEST:11.370 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:23:02.165: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4718
STEP: Creating secret with name secret-test-49d606e1-f981-43c1-81ba-a4a3bcf19c49
STEP: Creating a pod to test consume secrets
Jun 22 16:23:02.784: INFO: Waiting up to 5m0s for pod "pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013" in namespace "secrets-1506" to be "success or failure"
Jun 22 16:23:02.799: INFO: Pod "pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013": Phase="Pending", Reason="", readiness=false. Elapsed: 14.632699ms
Jun 22 16:23:04.811: INFO: Pod "pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013": Phase="Running", Reason="", readiness=true. Elapsed: 2.026791395s
Jun 22 16:23:06.824: INFO: Pod "pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039709371s
STEP: Saw pod success
Jun 22 16:23:06.824: INFO: Pod "pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013" satisfied condition "success or failure"
Jun 22 16:23:06.836: INFO: Trying to get logs from node 10.45.191.149 pod pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 16:23:06.946: INFO: Waiting for pod pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013 to disappear
Jun 22 16:23:06.957: INFO: Pod pod-secrets-27cdc932-d82d-4e40-a4ad-b4d0df356013 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:23:06.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1506" for this suite.
Jun 22 16:23:13.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:23:13.663: INFO: namespace secrets-1506 deletion completed in 6.679676667s
STEP: Destroying namespace "secret-namespace-4718" for this suite.
Jun 22 16:23:21.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:23:22.293: INFO: namespace secret-namespace-4718 deletion completed in 8.630115536s

• [SLOW TEST:20.128 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:23:22.294: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-1665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Jun 22 16:23:22.552: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 16:24:22.644: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:24:22.663: INFO: Starting informer...
STEP: Starting pod...
Jun 22 16:24:22.921: INFO: Pod is running on 10.45.191.150. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 22 16:24:22.974: INFO: Pod wasn't evicted. Proceeding
Jun 22 16:24:22.974: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 22 16:25:38.041: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:25:38.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1665" for this suite.
Jun 22 16:26:08.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:26:08.702: INFO: namespace taint-single-pod-1665 deletion completed in 30.632598303s

• [SLOW TEST:166.408 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:26:08.702: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:26:09.060: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8e8ad695-6632-4e9d-ab2b-88eee7006acb", Controller:(*bool)(0xc003cd2d1a), BlockOwnerDeletion:(*bool)(0xc003cd2d1b)}}
Jun 22 16:26:09.073: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d1b77922-2f83-4e82-86ce-a2f987abdc60", Controller:(*bool)(0xc003e38826), BlockOwnerDeletion:(*bool)(0xc003e38827)}}
Jun 22 16:26:09.086: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"728ba4a6-84e8-4b23-a0cb-5a8c9b8803be", Controller:(*bool)(0xc0041b6f56), BlockOwnerDeletion:(*bool)(0xc0041b6f57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:26:14.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4163" for this suite.
Jun 22 16:26:20.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:26:20.781: INFO: namespace gc-4163 deletion completed in 6.637977916s

• [SLOW TEST:12.079 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:26:20.782: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6170
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:26:21.040: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 16:26:24.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-6170 create -f -'
Jun 22 16:26:25.699: INFO: stderr: ""
Jun 22 16:26:25.699: INFO: stdout: "e2e-test-crd-publish-openapi-5391-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 16:26:25.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-6170 delete e2e-test-crd-publish-openapi-5391-crds test-cr'
Jun 22 16:26:25.858: INFO: stderr: ""
Jun 22 16:26:25.858: INFO: stdout: "e2e-test-crd-publish-openapi-5391-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 22 16:26:25.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-6170 apply -f -'
Jun 22 16:26:26.190: INFO: stderr: ""
Jun 22 16:26:26.190: INFO: stdout: "e2e-test-crd-publish-openapi-5391-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 16:26:26.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-6170 delete e2e-test-crd-publish-openapi-5391-crds test-cr'
Jun 22 16:26:26.606: INFO: stderr: ""
Jun 22 16:26:26.606: INFO: stdout: "e2e-test-crd-publish-openapi-5391-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 22 16:26:26.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-5391-crds'
Jun 22 16:26:26.862: INFO: stderr: ""
Jun 22 16:26:26.862: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5391-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:26:30.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6170" for this suite.
Jun 22 16:26:36.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:26:37.427: INFO: namespace crd-publish-openapi-6170 deletion completed in 6.637368331s

• [SLOW TEST:16.645 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:26:37.428: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 22 16:26:37.721: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 22 16:26:42.865: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:26:42.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9234" for this suite.
Jun 22 16:26:51.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:26:51.692: INFO: namespace replication-controller-9234 deletion completed in 8.751482618s

• [SLOW TEST:14.265 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:26:51.693: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-262
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-12378a66-cf0c-4934-b89f-155ee6b18efd
STEP: Creating configMap with name cm-test-opt-upd-2c97f82f-2ca4-4fbf-8197-8fcdcd1a1148
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-12378a66-cf0c-4934-b89f-155ee6b18efd
STEP: Updating configmap cm-test-opt-upd-2c97f82f-2ca4-4fbf-8197-8fcdcd1a1148
STEP: Creating configMap with name cm-test-opt-create-9b94064a-de50-46d2-abda-8cdfec25092c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:28:12.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-262" for this suite.
Jun 22 16:28:32.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:28:33.129: INFO: namespace projected-262 deletion completed in 20.610438287s

• [SLOW TEST:101.436 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:28:33.130: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 22 16:28:33.425: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:28:50.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8343" for this suite.
Jun 22 16:28:58.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:28:59.276: INFO: namespace pods-8343 deletion completed in 8.669632535s

• [SLOW TEST:26.146 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:28:59.280: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Jun 22 16:28:59.559: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 22 16:28:59.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-4028'
Jun 22 16:28:59.905: INFO: stderr: ""
Jun 22 16:28:59.905: INFO: stdout: "service/redis-slave created\n"
Jun 22 16:28:59.905: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 22 16:28:59.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-4028'
Jun 22 16:29:00.408: INFO: stderr: ""
Jun 22 16:29:00.408: INFO: stdout: "service/redis-master created\n"
Jun 22 16:29:00.408: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 22 16:29:00.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-4028'
Jun 22 16:29:00.801: INFO: stderr: ""
Jun 22 16:29:00.801: INFO: stdout: "service/frontend created\n"
Jun 22 16:29:00.801: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 22 16:29:00.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-4028'
Jun 22 16:29:01.045: INFO: stderr: ""
Jun 22 16:29:01.046: INFO: stdout: "deployment.apps/frontend created\n"
Jun 22 16:29:01.046: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 16:29:01.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-4028'
Jun 22 16:29:01.510: INFO: stderr: ""
Jun 22 16:29:01.510: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 22 16:29:01.510: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 22 16:29:01.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-4028'
Jun 22 16:29:01.775: INFO: stderr: ""
Jun 22 16:29:01.775: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 22 16:29:01.775: INFO: Waiting for all frontend pods to be Running.
Jun 22 16:29:16.826: INFO: Waiting for frontend to serve content.
Jun 22 16:29:16.870: INFO: Trying to add a new entry to the guestbook.
Jun 22 16:29:16.908: INFO: Verifying that added entry can be retrieved.
Jun 22 16:29:16.947: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:21.996: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:27.043: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:32.092: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:37.290: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:42.339: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:47.389: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:52.433: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:29:57.485: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 16:30:02.526: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jun 22 16:30:07.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-4028'
Jun 22 16:30:07.824: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 16:30:07.824: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 16:30:07.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-4028'
Jun 22 16:30:08.044: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 16:30:08.044: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 16:30:08.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-4028'
Jun 22 16:30:08.281: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 16:30:08.281: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 16:30:08.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-4028'
Jun 22 16:30:08.405: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 16:30:08.405: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 16:30:08.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-4028'
Jun 22 16:30:08.592: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 16:30:08.592: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 16:30:08.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete --grace-period=0 --force -f - --namespace=kubectl-4028'
Jun 22 16:30:08.743: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 16:30:08.744: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:30:08.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4028" for this suite.
Jun 22 16:30:22.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:30:23.768: INFO: namespace kubectl-4028 deletion completed in 14.996586111s

• [SLOW TEST:84.488 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:30:23.768: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-929d4faa-5fb6-4755-ac4b-25fe3c38b044
STEP: Creating a pod to test consume secrets
Jun 22 16:30:24.072: INFO: Waiting up to 5m0s for pod "pod-secrets-4b1f59d5-7de8-4c0c-8cdb-6d84dea7ffde" in namespace "secrets-5733" to be "success or failure"
Jun 22 16:30:24.085: INFO: Pod "pod-secrets-4b1f59d5-7de8-4c0c-8cdb-6d84dea7ffde": Phase="Pending", Reason="", readiness=false. Elapsed: 12.6903ms
Jun 22 16:30:26.098: INFO: Pod "pod-secrets-4b1f59d5-7de8-4c0c-8cdb-6d84dea7ffde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025055249s
STEP: Saw pod success
Jun 22 16:30:26.098: INFO: Pod "pod-secrets-4b1f59d5-7de8-4c0c-8cdb-6d84dea7ffde" satisfied condition "success or failure"
Jun 22 16:30:26.111: INFO: Trying to get logs from node 10.45.191.150 pod pod-secrets-4b1f59d5-7de8-4c0c-8cdb-6d84dea7ffde container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 16:30:26.211: INFO: Waiting for pod pod-secrets-4b1f59d5-7de8-4c0c-8cdb-6d84dea7ffde to disappear
Jun 22 16:30:26.224: INFO: Pod pod-secrets-4b1f59d5-7de8-4c0c-8cdb-6d84dea7ffde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:30:26.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5733" for this suite.
Jun 22 16:30:34.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:30:34.890: INFO: namespace secrets-5733 deletion completed in 8.641301697s

• [SLOW TEST:11.122 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:30:34.891: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-ecc95474-fa27-46af-b291-e71d3014b196
STEP: Creating a pod to test consume configMaps
Jun 22 16:30:35.205: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-24e86488-183f-47bd-a21c-fc0bb7b67498" in namespace "projected-2506" to be "success or failure"
Jun 22 16:30:35.219: INFO: Pod "pod-projected-configmaps-24e86488-183f-47bd-a21c-fc0bb7b67498": Phase="Pending", Reason="", readiness=false. Elapsed: 13.956839ms
Jun 22 16:30:37.234: INFO: Pod "pod-projected-configmaps-24e86488-183f-47bd-a21c-fc0bb7b67498": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028265041s
STEP: Saw pod success
Jun 22 16:30:37.234: INFO: Pod "pod-projected-configmaps-24e86488-183f-47bd-a21c-fc0bb7b67498" satisfied condition "success or failure"
Jun 22 16:30:37.245: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-configmaps-24e86488-183f-47bd-a21c-fc0bb7b67498 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 16:30:37.309: INFO: Waiting for pod pod-projected-configmaps-24e86488-183f-47bd-a21c-fc0bb7b67498 to disappear
Jun 22 16:30:37.320: INFO: Pod pod-projected-configmaps-24e86488-183f-47bd-a21c-fc0bb7b67498 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:30:37.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2506" for this suite.
Jun 22 16:30:43.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:30:43.946: INFO: namespace projected-2506 deletion completed in 6.602833231s

• [SLOW TEST:9.055 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:30:43.947: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-120fe83d-3805-426d-954c-9e8a03be16ea
STEP: Creating a pod to test consume secrets
Jun 22 16:30:44.257: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345" in namespace "projected-6085" to be "success or failure"
Jun 22 16:30:44.272: INFO: Pod "pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345": Phase="Pending", Reason="", readiness=false. Elapsed: 14.951112ms
Jun 22 16:30:46.285: INFO: Pod "pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345": Phase="Running", Reason="", readiness=true. Elapsed: 2.027865563s
Jun 22 16:30:48.297: INFO: Pod "pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040571921s
STEP: Saw pod success
Jun 22 16:30:48.298: INFO: Pod "pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345" satisfied condition "success or failure"
Jun 22 16:30:48.310: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 16:30:48.379: INFO: Waiting for pod pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345 to disappear
Jun 22 16:30:48.390: INFO: Pod pod-projected-secrets-6868c96f-cd68-4a85-a412-8e8f39fb4345 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:30:48.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6085" for this suite.
Jun 22 16:30:56.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:30:57.077: INFO: namespace projected-6085 deletion completed in 8.646996909s

• [SLOW TEST:13.130 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:30:57.081: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 16:30:58.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 16:31:00.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440258, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440258, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440258, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440258, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 16:31:03.244: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:31:03.266: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4510-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:31:04.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2723" for this suite.
Jun 22 16:31:12.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:31:12.844: INFO: namespace webhook-2723 deletion completed in 8.647113098s
STEP: Destroying namespace "webhook-2723-markers" for this suite.
Jun 22 16:31:18.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:31:19.482: INFO: namespace webhook-2723-markers deletion completed in 6.637419349s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:31:19.562: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 22 16:31:20.003: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35529 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 16:31:20.003: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35529 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 22 16:31:30.043: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35545 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 16:31:30.043: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35545 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 22 16:31:40.081: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35561 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 16:31:40.081: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35561 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 22 16:31:50.123: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35574 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 16:31:50.124: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-a 866b5aa2-6cec-47d0-bdea-a7ccbce6ccdc 35574 0 2020-06-22 16:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 22 16:32:00.334: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-b 827ec3b5-a3b0-4d81-9117-4c1848355106 35588 0 2020-06-22 16:32:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 16:32:00.334: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-b 827ec3b5-a3b0-4d81-9117-4c1848355106 35588 0 2020-06-22 16:32:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 22 16:32:10.371: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-b 827ec3b5-a3b0-4d81-9117-4c1848355106 35602 0 2020-06-22 16:32:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 16:32:10.371: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9084 /api/v1/namespaces/watch-9084/configmaps/e2e-watch-test-configmap-b 827ec3b5-a3b0-4d81-9117-4c1848355106 35602 0 2020-06-22 16:32:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:32:20.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9084" for this suite.
Jun 22 16:32:26.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:32:27.069: INFO: namespace watch-9084 deletion completed in 6.664582231s

• [SLOW TEST:67.508 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:32:27.070: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:32:27.361: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-85aa306f-bbbc-482c-b7b5-548c3ac912db" in namespace "security-context-test-5237" to be "success or failure"
Jun 22 16:32:27.375: INFO: Pod "busybox-privileged-false-85aa306f-bbbc-482c-b7b5-548c3ac912db": Phase="Pending", Reason="", readiness=false. Elapsed: 13.735621ms
Jun 22 16:32:29.391: INFO: Pod "busybox-privileged-false-85aa306f-bbbc-482c-b7b5-548c3ac912db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029955247s
Jun 22 16:32:29.391: INFO: Pod "busybox-privileged-false-85aa306f-bbbc-482c-b7b5-548c3ac912db" satisfied condition "success or failure"
Jun 22 16:32:29.463: INFO: Got logs for pod "busybox-privileged-false-85aa306f-bbbc-482c-b7b5-548c3ac912db": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:32:29.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5237" for this suite.
Jun 22 16:32:35.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:32:36.114: INFO: namespace security-context-test-5237 deletion completed in 6.621888257s

• [SLOW TEST:9.044 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:32:36.115: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7560
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 22 16:32:39.024: INFO: Successfully updated pod "annotationupdate7794566f-2c14-448a-8636-bf953427fca9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:32:43.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7560" for this suite.
Jun 22 16:33:13.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:33:13.780: INFO: namespace projected-7560 deletion completed in 30.629838711s

• [SLOW TEST:37.665 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:33:13.780: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-684
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3221
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:33:20.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-142" for this suite.
Jun 22 16:33:26.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:33:27.429: INFO: namespace namespaces-142 deletion completed in 6.725004066s
STEP: Destroying namespace "nsdeletetest-684" for this suite.
Jun 22 16:33:27.448: INFO: Namespace nsdeletetest-684 was already deleted
STEP: Destroying namespace "nsdeletetest-3221" for this suite.
Jun 22 16:33:33.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:33:34.054: INFO: namespace nsdeletetest-3221 deletion completed in 6.605565367s

• [SLOW TEST:20.274 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:33:34.054: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 16:33:36.404: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:33:36.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7930" for this suite.
Jun 22 16:33:42.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:33:43.073: INFO: namespace container-runtime-7930 deletion completed in 6.600995058s

• [SLOW TEST:9.019 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:33:43.073: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:34:09.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2581" for this suite.
Jun 22 16:34:17.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:34:17.741: INFO: namespace container-runtime-2581 deletion completed in 8.628234888s

• [SLOW TEST:34.667 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:34:17.742: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 16:34:18.922: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 22 16:34:20.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440458, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440458, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440459, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728440458, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 16:34:24.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:34:24.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9548" for this suite.
Jun 22 16:34:32.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:34:32.768: INFO: namespace webhook-9548 deletion completed in 8.692359386s
STEP: Destroying namespace "webhook-9548-markers" for this suite.
Jun 22 16:34:38.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:34:39.412: INFO: namespace webhook-9548-markers deletion completed in 6.6437681s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.760 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:34:39.505: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-12206750-027c-4d92-8984-8dbb7e4f300c
STEP: Creating a pod to test consume configMaps
Jun 22 16:34:39.813: INFO: Waiting up to 5m0s for pod "pod-configmaps-05284140-883d-4ee3-b672-6abc346348da" in namespace "configmap-8705" to be "success or failure"
Jun 22 16:34:39.828: INFO: Pod "pod-configmaps-05284140-883d-4ee3-b672-6abc346348da": Phase="Pending", Reason="", readiness=false. Elapsed: 15.400495ms
Jun 22 16:34:41.840: INFO: Pod "pod-configmaps-05284140-883d-4ee3-b672-6abc346348da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027149707s
STEP: Saw pod success
Jun 22 16:34:41.840: INFO: Pod "pod-configmaps-05284140-883d-4ee3-b672-6abc346348da" satisfied condition "success or failure"
Jun 22 16:34:41.853: INFO: Trying to get logs from node 10.45.191.150 pod pod-configmaps-05284140-883d-4ee3-b672-6abc346348da container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 16:34:41.946: INFO: Waiting for pod pod-configmaps-05284140-883d-4ee3-b672-6abc346348da to disappear
Jun 22 16:34:41.958: INFO: Pod pod-configmaps-05284140-883d-4ee3-b672-6abc346348da no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:34:41.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8705" for this suite.
Jun 22 16:34:48.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:34:48.861: INFO: namespace configmap-8705 deletion completed in 6.878094426s

• [SLOW TEST:9.356 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:34:48.861: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:35:05.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8065" for this suite.
Jun 22 16:35:13.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:35:14.441: INFO: namespace resourcequota-8065 deletion completed in 8.772018298s

• [SLOW TEST:25.580 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:35:14.441: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-508e09e2-2e67-49dc-b6ce-e6690370eec5
STEP: Creating a pod to test consume configMaps
Jun 22 16:35:14.756: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b18fa5d5-b1b4-435f-a726-16d5b702883b" in namespace "projected-5425" to be "success or failure"
Jun 22 16:35:14.768: INFO: Pod "pod-projected-configmaps-b18fa5d5-b1b4-435f-a726-16d5b702883b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.38002ms
Jun 22 16:35:16.792: INFO: Pod "pod-projected-configmaps-b18fa5d5-b1b4-435f-a726-16d5b702883b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036191416s
STEP: Saw pod success
Jun 22 16:35:16.792: INFO: Pod "pod-projected-configmaps-b18fa5d5-b1b4-435f-a726-16d5b702883b" satisfied condition "success or failure"
Jun 22 16:35:16.804: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-configmaps-b18fa5d5-b1b4-435f-a726-16d5b702883b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 16:35:16.861: INFO: Waiting for pod pod-projected-configmaps-b18fa5d5-b1b4-435f-a726-16d5b702883b to disappear
Jun 22 16:35:16.872: INFO: Pod pod-projected-configmaps-b18fa5d5-b1b4-435f-a726-16d5b702883b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:35:16.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5425" for this suite.
Jun 22 16:35:22.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:35:23.571: INFO: namespace projected-5425 deletion completed in 6.673450403s

• [SLOW TEST:9.130 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:35:23.572: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 16:35:23.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2" in namespace "downward-api-9814" to be "success or failure"
Jun 22 16:35:23.887: INFO: Pod "downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.835677ms
Jun 22 16:35:25.900: INFO: Pod "downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025794908s
Jun 22 16:35:27.913: INFO: Pod "downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038316036s
STEP: Saw pod success
Jun 22 16:35:27.913: INFO: Pod "downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2" satisfied condition "success or failure"
Jun 22 16:35:27.926: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2 container client-container: <nil>
STEP: delete the pod
Jun 22 16:35:27.985: INFO: Waiting for pod downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2 to disappear
Jun 22 16:35:27.996: INFO: Pod downwardapi-volume-e53e96f7-d11d-4362-b225-69f8c42b71f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:35:27.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9814" for this suite.
Jun 22 16:35:34.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:35:34.756: INFO: namespace downward-api-9814 deletion completed in 6.7305012s

• [SLOW TEST:11.183 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:35:34.756: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 16:35:38.106: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:35:38.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8243" for this suite.
Jun 22 16:35:46.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:35:46.799: INFO: namespace container-runtime-8243 deletion completed in 8.623110919s

• [SLOW TEST:12.042 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:35:46.799: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:35:51.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6127" for this suite.
Jun 22 16:36:37.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:36:38.044: INFO: namespace kubelet-test-6127 deletion completed in 46.847004865s

• [SLOW TEST:51.245 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:36:38.045: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jun 22 16:36:38.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 create -f - --namespace=kubectl-5185'
Jun 22 16:36:38.994: INFO: stderr: ""
Jun 22 16:36:38.994: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 16:36:40.009: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 16:36:40.009: INFO: Found 0 / 1
Jun 22 16:36:41.013: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 16:36:41.013: INFO: Found 1 / 1
Jun 22 16:36:41.014: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 22 16:36:41.025: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 16:36:41.025: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 16:36:41.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 patch pod redis-master-5prgw --namespace=kubectl-5185 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 22 16:36:41.206: INFO: stderr: ""
Jun 22 16:36:41.206: INFO: stdout: "pod/redis-master-5prgw patched\n"
STEP: checking annotations
Jun 22 16:36:41.219: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 16:36:41.219: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:36:41.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5185" for this suite.
Jun 22 16:36:53.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:36:53.917: INFO: namespace kubectl-5185 deletion completed in 12.664219321s

• [SLOW TEST:15.872 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:36:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9418
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:36:54.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9418" for this suite.
Jun 22 16:37:24.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:37:25.169: INFO: namespace pods-9418 deletion completed in 30.905960767s

• [SLOW TEST:31.252 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:37:25.170: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Jun 22 16:37:25.461: INFO: Waiting up to 5m0s for pod "var-expansion-66589f79-0684-44f0-9811-a24eebfd1459" in namespace "var-expansion-2702" to be "success or failure"
Jun 22 16:37:25.474: INFO: Pod "var-expansion-66589f79-0684-44f0-9811-a24eebfd1459": Phase="Pending", Reason="", readiness=false. Elapsed: 12.622237ms
Jun 22 16:37:27.486: INFO: Pod "var-expansion-66589f79-0684-44f0-9811-a24eebfd1459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024823894s
Jun 22 16:37:29.500: INFO: Pod "var-expansion-66589f79-0684-44f0-9811-a24eebfd1459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038735331s
STEP: Saw pod success
Jun 22 16:37:29.500: INFO: Pod "var-expansion-66589f79-0684-44f0-9811-a24eebfd1459" satisfied condition "success or failure"
Jun 22 16:37:29.512: INFO: Trying to get logs from node 10.45.191.150 pod var-expansion-66589f79-0684-44f0-9811-a24eebfd1459 container dapi-container: <nil>
STEP: delete the pod
Jun 22 16:37:29.626: INFO: Waiting for pod var-expansion-66589f79-0684-44f0-9811-a24eebfd1459 to disappear
Jun 22 16:37:29.636: INFO: Pod var-expansion-66589f79-0684-44f0-9811-a24eebfd1459 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:37:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2702" for this suite.
Jun 22 16:37:37.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:37:38.300: INFO: namespace var-expansion-2702 deletion completed in 8.634992207s

• [SLOW TEST:13.130 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:37:38.301: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1432, will wait for the garbage collector to delete the pods
Jun 22 16:37:41.003: INFO: Deleting Job.batch foo took: 39.582906ms
Jun 22 16:37:41.103: INFO: Terminating Job.batch foo pods took: 100.319402ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:38:20.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1432" for this suite.
Jun 22 16:38:28.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:38:29.299: INFO: namespace job-1432 deletion completed in 8.653174092s

• [SLOW TEST:50.998 seconds]
[sig-apps] Job
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:38:29.299: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 22 16:38:33.688: INFO: &Pod{ObjectMeta:{send-events-0bce7b55-2588-454a-bf3a-5975d0054e4c  events-3670 /api/v1/namespaces/events-3670/pods/send-events-0bce7b55-2588-454a-bf3a-5975d0054e4c 08f4420f-e4b0-46e8-9a1e-b5136dff5b98 36857 0 2020-06-22 16:38:29 +0000 UTC <nil> <nil> map[name:foo time:599415468] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-85zxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-85zxf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-85zxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:38:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 16:38:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:172.30.131.115,StartTime:2020-06-22 16:38:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 16:38:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:containerd://bced8cddb9ba6e36571606c6022abd0af32669246004ffb1e769b630be455741,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.131.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 22 16:38:35.708: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 22 16:38:37.725: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:38:37.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3670" for this suite.
Jun 22 16:39:23.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:39:24.472: INFO: namespace events-3670 deletion completed in 46.689197169s

• [SLOW TEST:55.173 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:39:24.473: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 22 16:39:24.761: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:39:28.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-416" for this suite.
Jun 22 16:39:36.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:39:37.441: INFO: namespace init-container-416 deletion completed in 8.632017476s

• [SLOW TEST:12.969 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:39:37.442: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 16:39:42.304: INFO: Successfully updated pod "pod-update-activedeadlineseconds-68e425bd-7f03-4061-a687-8d651924704a"
Jun 22 16:39:42.304: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-68e425bd-7f03-4061-a687-8d651924704a" in namespace "pods-6342" to be "terminated due to deadline exceeded"
Jun 22 16:39:42.318: INFO: Pod "pod-update-activedeadlineseconds-68e425bd-7f03-4061-a687-8d651924704a": Phase="Running", Reason="", readiness=true. Elapsed: 13.458958ms
Jun 22 16:39:44.330: INFO: Pod "pod-update-activedeadlineseconds-68e425bd-7f03-4061-a687-8d651924704a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.025837644s
Jun 22 16:39:44.330: INFO: Pod "pod-update-activedeadlineseconds-68e425bd-7f03-4061-a687-8d651924704a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:39:44.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6342" for this suite.
Jun 22 16:39:52.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:39:53.259: INFO: namespace pods-6342 deletion completed in 8.894924299s

• [SLOW TEST:15.818 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:39:53.260: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4730.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4730.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4730.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4730.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4730.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4730.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 16:40:05.775: INFO: DNS probes using dns-4730/dns-test-297369a8-0234-4fa6-9763-cfbcf23949a3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:40:05.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4730" for this suite.
Jun 22 16:40:13.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:40:14.588: INFO: namespace dns-4730 deletion completed in 8.660061462s

• [SLOW TEST:21.328 seconds]
[sig-network] DNS
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:40:14.588: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 16:40:15.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 16:40:19.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 22 16:40:21.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 attach --namespace=webhook-8363 to-be-attached-pod -i -c=container1'
Jun 22 16:40:21.398: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:40:21.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8363" for this suite.
Jun 22 16:40:51.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:40:52.111: INFO: namespace webhook-8363 deletion completed in 30.657989552s
STEP: Destroying namespace "webhook-8363-markers" for this suite.
Jun 22 16:40:58.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:40:58.751: INFO: namespace webhook-8363-markers deletion completed in 6.640503673s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:44.246 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:40:58.834: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 16:40:59.130: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc6568be-ff1c-404a-a21f-09b35f2b4f2e" in namespace "projected-9558" to be "success or failure"
Jun 22 16:40:59.143: INFO: Pod "downwardapi-volume-fc6568be-ff1c-404a-a21f-09b35f2b4f2e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.873627ms
Jun 22 16:41:01.157: INFO: Pod "downwardapi-volume-fc6568be-ff1c-404a-a21f-09b35f2b4f2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027654527s
STEP: Saw pod success
Jun 22 16:41:01.158: INFO: Pod "downwardapi-volume-fc6568be-ff1c-404a-a21f-09b35f2b4f2e" satisfied condition "success or failure"
Jun 22 16:41:01.173: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-fc6568be-ff1c-404a-a21f-09b35f2b4f2e container client-container: <nil>
STEP: delete the pod
Jun 22 16:41:01.349: INFO: Waiting for pod downwardapi-volume-fc6568be-ff1c-404a-a21f-09b35f2b4f2e to disappear
Jun 22 16:41:01.360: INFO: Pod downwardapi-volume-fc6568be-ff1c-404a-a21f-09b35f2b4f2e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:41:01.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9558" for this suite.
Jun 22 16:41:07.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:41:08.051: INFO: namespace projected-9558 deletion completed in 6.664864068s

• [SLOW TEST:9.217 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:41:08.051: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-016c61a1-96f1-4908-bd5d-75118e87c934
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:41:08.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6117" for this suite.
Jun 22 16:41:16.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:41:16.999: INFO: namespace configmap-6117 deletion completed in 8.665199144s

• [SLOW TEST:8.948 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:41:17.000: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7270
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7270
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jun 22 16:41:17.312: INFO: Found 0 stateful pods, waiting for 3
Jun 22 16:41:27.331: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:41:27.332: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:41:27.332: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 16:41:27.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7270 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 16:41:27.724: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 16:41:27.724: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 16:41:27.724: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 22 16:41:37.826: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 22 16:41:47.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7270 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:41:48.256: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 16:41:48.256: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 16:41:48.256: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 16:41:58.350: INFO: Waiting for StatefulSet statefulset-7270/ss2 to complete update
Jun 22 16:41:58.350: INFO: Waiting for Pod statefulset-7270/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 22 16:42:08.379: INFO: Waiting for StatefulSet statefulset-7270/ss2 to complete update
Jun 22 16:42:08.379: INFO: Waiting for Pod statefulset-7270/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun 22 16:42:18.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7270 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 16:42:18.716: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 16:42:18.716: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 16:42:18.716: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 16:42:28.841: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 22 16:42:38.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7270 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 16:42:39.363: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 16:42:39.363: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 16:42:39.363: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 16:43:09.455: INFO: Waiting for StatefulSet statefulset-7270/ss2 to complete update
Jun 22 16:43:09.455: INFO: Waiting for Pod statefulset-7270/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 22 16:43:19.487: INFO: Deleting all statefulset in ns statefulset-7270
Jun 22 16:43:19.515: INFO: Scaling statefulset ss2 to 0
Jun 22 16:43:49.579: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 16:43:49.596: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:43:49.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7270" for this suite.
Jun 22 16:43:57.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:43:58.350: INFO: namespace statefulset-7270 deletion completed in 8.658956682s

• [SLOW TEST:161.350 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:43:58.350: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-10d52e7a-36d4-4cdc-86a8-2cfca10b8de0
STEP: Creating a pod to test consume configMaps
Jun 22 16:43:58.677: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-56f3c46f-0d9f-4562-924e-a36933abf98b" in namespace "projected-7723" to be "success or failure"
Jun 22 16:43:58.695: INFO: Pod "pod-projected-configmaps-56f3c46f-0d9f-4562-924e-a36933abf98b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.084619ms
Jun 22 16:44:00.708: INFO: Pod "pod-projected-configmaps-56f3c46f-0d9f-4562-924e-a36933abf98b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030435937s
STEP: Saw pod success
Jun 22 16:44:00.708: INFO: Pod "pod-projected-configmaps-56f3c46f-0d9f-4562-924e-a36933abf98b" satisfied condition "success or failure"
Jun 22 16:44:00.723: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-configmaps-56f3c46f-0d9f-4562-924e-a36933abf98b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 16:44:00.857: INFO: Waiting for pod pod-projected-configmaps-56f3c46f-0d9f-4562-924e-a36933abf98b to disappear
Jun 22 16:44:00.868: INFO: Pod pod-projected-configmaps-56f3c46f-0d9f-4562-924e-a36933abf98b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:44:00.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7723" for this suite.
Jun 22 16:44:06.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:44:07.558: INFO: namespace projected-7723 deletion completed in 6.649094588s

• [SLOW TEST:9.207 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:44:07.558: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-tcsg
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 16:44:07.882: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tcsg" in namespace "subpath-306" to be "success or failure"
Jun 22 16:44:07.898: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Pending", Reason="", readiness=false. Elapsed: 15.596303ms
Jun 22 16:44:09.910: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 2.02752825s
Jun 22 16:44:11.923: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 4.041379247s
Jun 22 16:44:14.060: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 6.177796689s
Jun 22 16:44:16.072: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 8.190223278s
Jun 22 16:44:18.084: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 10.201999856s
Jun 22 16:44:20.097: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 12.215338283s
Jun 22 16:44:22.110: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 14.227594895s
Jun 22 16:44:24.125: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 16.24343672s
Jun 22 16:44:26.139: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 18.257008968s
Jun 22 16:44:28.390: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Running", Reason="", readiness=true. Elapsed: 20.507860111s
Jun 22 16:44:30.402: INFO: Pod "pod-subpath-test-configmap-tcsg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.520385771s
STEP: Saw pod success
Jun 22 16:44:30.402: INFO: Pod "pod-subpath-test-configmap-tcsg" satisfied condition "success or failure"
Jun 22 16:44:30.414: INFO: Trying to get logs from node 10.45.191.150 pod pod-subpath-test-configmap-tcsg container test-container-subpath-configmap-tcsg: <nil>
STEP: delete the pod
Jun 22 16:44:30.483: INFO: Waiting for pod pod-subpath-test-configmap-tcsg to disappear
Jun 22 16:44:30.495: INFO: Pod pod-subpath-test-configmap-tcsg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tcsg
Jun 22 16:44:30.495: INFO: Deleting pod "pod-subpath-test-configmap-tcsg" in namespace "subpath-306"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:44:30.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-306" for this suite.
Jun 22 16:44:36.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:44:37.154: INFO: namespace subpath-306 deletion completed in 6.624049894s

• [SLOW TEST:29.596 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:44:37.154: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 16:44:37.452: INFO: Waiting up to 5m0s for pod "downwardapi-volume-776b7f6e-abf9-4b8c-8bf9-83a805725cb9" in namespace "downward-api-1641" to be "success or failure"
Jun 22 16:44:37.465: INFO: Pod "downwardapi-volume-776b7f6e-abf9-4b8c-8bf9-83a805725cb9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.71468ms
Jun 22 16:44:39.478: INFO: Pod "downwardapi-volume-776b7f6e-abf9-4b8c-8bf9-83a805725cb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026337007s
STEP: Saw pod success
Jun 22 16:44:39.478: INFO: Pod "downwardapi-volume-776b7f6e-abf9-4b8c-8bf9-83a805725cb9" satisfied condition "success or failure"
Jun 22 16:44:39.491: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-776b7f6e-abf9-4b8c-8bf9-83a805725cb9 container client-container: <nil>
STEP: delete the pod
Jun 22 16:44:39.843: INFO: Waiting for pod downwardapi-volume-776b7f6e-abf9-4b8c-8bf9-83a805725cb9 to disappear
Jun 22 16:44:39.861: INFO: Pod downwardapi-volume-776b7f6e-abf9-4b8c-8bf9-83a805725cb9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:44:39.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1641" for this suite.
Jun 22 16:44:45.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:44:46.540: INFO: namespace downward-api-1641 deletion completed in 6.644571792s

• [SLOW TEST:9.386 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:44:46.545: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 16:44:47.593: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 16:44:49.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728441087, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728441087, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728441087, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728441087, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 16:44:52.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:44:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2222" for this suite.
Jun 22 16:45:01.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:45:01.689: INFO: namespace webhook-2222 deletion completed in 8.710327685s
STEP: Destroying namespace "webhook-2222-markers" for this suite.
Jun 22 16:45:07.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:45:08.316: INFO: namespace webhook-2222-markers deletion completed in 6.626814853s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.861 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:45:08.406: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 16:45:08.697: INFO: Waiting up to 5m0s for pod "pod-6266565d-4fda-4a81-b9ac-10f7f58f6b28" in namespace "emptydir-5762" to be "success or failure"
Jun 22 16:45:08.709: INFO: Pod "pod-6266565d-4fda-4a81-b9ac-10f7f58f6b28": Phase="Pending", Reason="", readiness=false. Elapsed: 12.128208ms
Jun 22 16:45:10.722: INFO: Pod "pod-6266565d-4fda-4a81-b9ac-10f7f58f6b28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025058288s
STEP: Saw pod success
Jun 22 16:45:10.722: INFO: Pod "pod-6266565d-4fda-4a81-b9ac-10f7f58f6b28" satisfied condition "success or failure"
Jun 22 16:45:10.736: INFO: Trying to get logs from node 10.45.191.150 pod pod-6266565d-4fda-4a81-b9ac-10f7f58f6b28 container test-container: <nil>
STEP: delete the pod
Jun 22 16:45:10.797: INFO: Waiting for pod pod-6266565d-4fda-4a81-b9ac-10f7f58f6b28 to disappear
Jun 22 16:45:10.810: INFO: Pod pod-6266565d-4fda-4a81-b9ac-10f7f58f6b28 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:45:10.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5762" for this suite.
Jun 22 16:45:17.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:45:17.645: INFO: namespace emptydir-5762 deletion completed in 6.810889289s

• [SLOW TEST:9.238 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:45:17.645: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:45:17.910: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 22 16:45:19.066: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:45:19.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9552" for this suite.
Jun 22 16:45:27.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:45:27.725: INFO: namespace replication-controller-9552 deletion completed in 8.614367787s

• [SLOW TEST:10.080 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:45:27.726: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 16:45:28.137: INFO: Number of nodes with available pods: 0
Jun 22 16:45:28.137: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:45:29.181: INFO: Number of nodes with available pods: 0
Jun 22 16:45:29.181: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:45:30.203: INFO: Number of nodes with available pods: 2
Jun 22 16:45:30.203: INFO: Node 10.45.191.150 is running more than one daemon pod
Jun 22 16:45:31.175: INFO: Number of nodes with available pods: 3
Jun 22 16:45:31.175: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 22 16:45:31.250: INFO: Number of nodes with available pods: 2
Jun 22 16:45:31.251: INFO: Node 10.45.191.150 is running more than one daemon pod
Jun 22 16:45:32.287: INFO: Number of nodes with available pods: 2
Jun 22 16:45:32.287: INFO: Node 10.45.191.150 is running more than one daemon pod
Jun 22 16:45:33.291: INFO: Number of nodes with available pods: 3
Jun 22 16:45:33.291: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5125, will wait for the garbage collector to delete the pods
Jun 22 16:45:33.592: INFO: Deleting DaemonSet.extensions daemon-set took: 193.299255ms
Jun 22 16:45:33.793: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.296287ms
Jun 22 16:45:44.605: INFO: Number of nodes with available pods: 0
Jun 22 16:45:44.605: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 16:45:44.629: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5125/daemonsets","resourceVersion":"38605"},"items":null}

Jun 22 16:45:44.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5125/pods","resourceVersion":"38605"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:45:44.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5125" for this suite.
Jun 22 16:45:52.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:45:53.535: INFO: namespace daemonsets-5125 deletion completed in 8.779490372s

• [SLOW TEST:25.809 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:45:53.535: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6353
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 22 16:45:53.820: INFO: Waiting up to 5m0s for pod "downward-api-53088e00-3650-453a-9a98-325876241993" in namespace "downward-api-6353" to be "success or failure"
Jun 22 16:45:53.834: INFO: Pod "downward-api-53088e00-3650-453a-9a98-325876241993": Phase="Pending", Reason="", readiness=false. Elapsed: 13.974402ms
Jun 22 16:45:55.849: INFO: Pod "downward-api-53088e00-3650-453a-9a98-325876241993": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029752853s
STEP: Saw pod success
Jun 22 16:45:55.850: INFO: Pod "downward-api-53088e00-3650-453a-9a98-325876241993" satisfied condition "success or failure"
Jun 22 16:45:55.861: INFO: Trying to get logs from node 10.45.191.150 pod downward-api-53088e00-3650-453a-9a98-325876241993 container dapi-container: <nil>
STEP: delete the pod
Jun 22 16:45:55.937: INFO: Waiting for pod downward-api-53088e00-3650-453a-9a98-325876241993 to disappear
Jun 22 16:45:55.951: INFO: Pod downward-api-53088e00-3650-453a-9a98-325876241993 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:45:55.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6353" for this suite.
Jun 22 16:46:02.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:46:02.673: INFO: namespace downward-api-6353 deletion completed in 6.698241649s

• [SLOW TEST:9.139 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:46:02.676: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Jun 22 16:46:02.983: INFO: Waiting up to 5m0s for pod "client-containers-273e115c-4faf-42be-a774-42860bceede8" in namespace "containers-784" to be "success or failure"
Jun 22 16:46:03.000: INFO: Pod "client-containers-273e115c-4faf-42be-a774-42860bceede8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.066879ms
Jun 22 16:46:05.013: INFO: Pod "client-containers-273e115c-4faf-42be-a774-42860bceede8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029359351s
STEP: Saw pod success
Jun 22 16:46:05.013: INFO: Pod "client-containers-273e115c-4faf-42be-a774-42860bceede8" satisfied condition "success or failure"
Jun 22 16:46:05.030: INFO: Trying to get logs from node 10.45.191.150 pod client-containers-273e115c-4faf-42be-a774-42860bceede8 container test-container: <nil>
STEP: delete the pod
Jun 22 16:46:05.095: INFO: Waiting for pod client-containers-273e115c-4faf-42be-a774-42860bceede8 to disappear
Jun 22 16:46:05.111: INFO: Pod client-containers-273e115c-4faf-42be-a774-42860bceede8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:46:05.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-784" for this suite.
Jun 22 16:46:11.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:46:11.934: INFO: namespace containers-784 deletion completed in 6.798658468s

• [SLOW TEST:9.258 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:46:11.934: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-499
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:46:29.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-499" for this suite.
Jun 22 16:46:35.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:46:36.049: INFO: namespace resourcequota-499 deletion completed in 6.637960154s

• [SLOW TEST:24.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:46:36.049: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-a282ebca-37e3-44cb-94c9-f471d9a27741
STEP: Creating a pod to test consume secrets
Jun 22 16:46:36.351: INFO: Waiting up to 5m0s for pod "pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4" in namespace "secrets-7106" to be "success or failure"
Jun 22 16:46:36.366: INFO: Pod "pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.621925ms
Jun 22 16:46:38.378: INFO: Pod "pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026185498s
Jun 22 16:46:40.389: INFO: Pod "pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037955515s
STEP: Saw pod success
Jun 22 16:46:40.389: INFO: Pod "pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4" satisfied condition "success or failure"
Jun 22 16:46:40.401: INFO: Trying to get logs from node 10.45.191.150 pod pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 16:46:40.463: INFO: Waiting for pod pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4 to disappear
Jun 22 16:46:40.476: INFO: Pod pod-secrets-3c6cc27c-78ca-4e87-a876-f9da537dbfa4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:46:40.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7106" for this suite.
Jun 22 16:46:46.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:46:47.113: INFO: namespace secrets-7106 deletion completed in 6.611746929s

• [SLOW TEST:11.063 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:46:47.114: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6179
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-6179
Jun 22 16:46:47.447: INFO: Found 0 stateful pods, waiting for 1
Jun 22 16:46:57.461: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 22 16:46:57.558: INFO: Deleting all statefulset in ns statefulset-6179
Jun 22 16:46:57.575: INFO: Scaling statefulset ss to 0
Jun 22 16:47:17.638: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 16:47:17.655: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:47:17.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6179" for this suite.
Jun 22 16:47:25.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:47:26.644: INFO: namespace statefulset-6179 deletion completed in 8.893154678s

• [SLOW TEST:39.530 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:47:26.644: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4159
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-ae232b14-18ee-46da-94ad-d9a479f99dc9
STEP: Creating secret with name s-test-opt-upd-ff7158b8-aa6c-4815-95b4-9a24ee89af4d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ae232b14-18ee-46da-94ad-d9a479f99dc9
STEP: Updating secret s-test-opt-upd-ff7158b8-aa6c-4815-95b4-9a24ee89af4d
STEP: Creating secret with name s-test-opt-create-d681516c-3245-4dd1-bfdf-7479562e7a50
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:47:31.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4159" for this suite.
Jun 22 16:47:43.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:47:44.006: INFO: namespace projected-4159 deletion completed in 12.644033227s

• [SLOW TEST:17.362 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:47:44.007: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-5838
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5838
STEP: Deleting pre-stop pod
Jun 22 16:47:53.423: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:47:53.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5838" for this suite.
Jun 22 16:48:39.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:48:40.133: INFO: namespace prestop-5838 deletion completed in 46.662148009s

• [SLOW TEST:56.126 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:48:40.134: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 16:48:40.420: INFO: Waiting up to 5m0s for pod "pod-21230cff-2f5f-4bdb-afc1-1934245518d2" in namespace "emptydir-1639" to be "success or failure"
Jun 22 16:48:40.432: INFO: Pod "pod-21230cff-2f5f-4bdb-afc1-1934245518d2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.852711ms
Jun 22 16:48:42.445: INFO: Pod "pod-21230cff-2f5f-4bdb-afc1-1934245518d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025060098s
STEP: Saw pod success
Jun 22 16:48:42.445: INFO: Pod "pod-21230cff-2f5f-4bdb-afc1-1934245518d2" satisfied condition "success or failure"
Jun 22 16:48:42.457: INFO: Trying to get logs from node 10.45.191.150 pod pod-21230cff-2f5f-4bdb-afc1-1934245518d2 container test-container: <nil>
STEP: delete the pod
Jun 22 16:48:42.521: INFO: Waiting for pod pod-21230cff-2f5f-4bdb-afc1-1934245518d2 to disappear
Jun 22 16:48:42.535: INFO: Pod pod-21230cff-2f5f-4bdb-afc1-1934245518d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:48:42.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1639" for this suite.
Jun 22 16:48:48.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:48:49.418: INFO: namespace emptydir-1639 deletion completed in 6.857960153s

• [SLOW TEST:9.284 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:48:49.419: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 22 16:48:54.256: INFO: Successfully updated pod "adopt-release-428vl"
STEP: Checking that the Job readopts the Pod
Jun 22 16:48:54.256: INFO: Waiting up to 15m0s for pod "adopt-release-428vl" in namespace "job-3632" to be "adopted"
Jun 22 16:48:54.270: INFO: Pod "adopt-release-428vl": Phase="Running", Reason="", readiness=true. Elapsed: 14.096529ms
Jun 22 16:48:56.283: INFO: Pod "adopt-release-428vl": Phase="Running", Reason="", readiness=true. Elapsed: 2.026821375s
Jun 22 16:48:56.283: INFO: Pod "adopt-release-428vl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 22 16:48:56.818: INFO: Successfully updated pod "adopt-release-428vl"
STEP: Checking that the Job releases the Pod
Jun 22 16:48:56.819: INFO: Waiting up to 15m0s for pod "adopt-release-428vl" in namespace "job-3632" to be "released"
Jun 22 16:48:56.837: INFO: Pod "adopt-release-428vl": Phase="Running", Reason="", readiness=true. Elapsed: 18.416727ms
Jun 22 16:48:56.837: INFO: Pod "adopt-release-428vl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:48:56.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3632" for this suite.
Jun 22 16:49:46.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:49:47.601: INFO: namespace job-3632 deletion completed in 50.736480098s

• [SLOW TEST:58.183 seconds]
[sig-apps] Job
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:49:47.604: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 16:49:47.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb" in namespace "projected-3791" to be "success or failure"
Jun 22 16:49:47.898: INFO: Pod "downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.517532ms
Jun 22 16:49:49.914: INFO: Pod "downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028787112s
Jun 22 16:49:51.930: INFO: Pod "downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044301162s
STEP: Saw pod success
Jun 22 16:49:51.930: INFO: Pod "downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb" satisfied condition "success or failure"
Jun 22 16:49:51.942: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb container client-container: <nil>
STEP: delete the pod
Jun 22 16:49:52.010: INFO: Waiting for pod downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb to disappear
Jun 22 16:49:52.021: INFO: Pod downwardapi-volume-044c40b5-0ecf-4368-84e3-db4da4b929fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:49:52.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3791" for this suite.
Jun 22 16:49:58.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:49:58.806: INFO: namespace projected-3791 deletion completed in 6.761109257s

• [SLOW TEST:11.202 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:49:58.806: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:49:59.094: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f0c25229-9792-438b-945f-8678f3785de9" in namespace "security-context-test-676" to be "success or failure"
Jun 22 16:49:59.106: INFO: Pod "alpine-nnp-false-f0c25229-9792-438b-945f-8678f3785de9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.146778ms
Jun 22 16:50:01.122: INFO: Pod "alpine-nnp-false-f0c25229-9792-438b-945f-8678f3785de9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027930938s
Jun 22 16:50:03.136: INFO: Pod "alpine-nnp-false-f0c25229-9792-438b-945f-8678f3785de9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042170038s
Jun 22 16:50:03.137: INFO: Pod "alpine-nnp-false-f0c25229-9792-438b-945f-8678f3785de9" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:50:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-676" for this suite.
Jun 22 16:50:11.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:50:11.950: INFO: namespace security-context-test-676 deletion completed in 8.745616621s

• [SLOW TEST:13.144 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:50:11.954: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 16:50:12.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fbefb80-e4c4-43dd-a2cb-133bd46eed02" in namespace "projected-3944" to be "success or failure"
Jun 22 16:50:12.265: INFO: Pod "downwardapi-volume-6fbefb80-e4c4-43dd-a2cb-133bd46eed02": Phase="Pending", Reason="", readiness=false. Elapsed: 12.184641ms
Jun 22 16:50:14.276: INFO: Pod "downwardapi-volume-6fbefb80-e4c4-43dd-a2cb-133bd46eed02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023718682s
STEP: Saw pod success
Jun 22 16:50:14.277: INFO: Pod "downwardapi-volume-6fbefb80-e4c4-43dd-a2cb-133bd46eed02" satisfied condition "success or failure"
Jun 22 16:50:14.289: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-6fbefb80-e4c4-43dd-a2cb-133bd46eed02 container client-container: <nil>
STEP: delete the pod
Jun 22 16:50:14.527: INFO: Waiting for pod downwardapi-volume-6fbefb80-e4c4-43dd-a2cb-133bd46eed02 to disappear
Jun 22 16:50:14.538: INFO: Pod downwardapi-volume-6fbefb80-e4c4-43dd-a2cb-133bd46eed02 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:50:14.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3944" for this suite.
Jun 22 16:50:22.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:50:23.195: INFO: namespace projected-3944 deletion completed in 8.621219782s

• [SLOW TEST:11.241 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:50:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-87714ea2-d203-4483-ad50-20224fc7a448
STEP: Creating a pod to test consume configMaps
Jun 22 16:50:23.531: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b08d8c9-4cd5-4352-ad32-2fb91fbcda41" in namespace "projected-4374" to be "success or failure"
Jun 22 16:50:23.547: INFO: Pod "pod-projected-configmaps-0b08d8c9-4cd5-4352-ad32-2fb91fbcda41": Phase="Pending", Reason="", readiness=false. Elapsed: 15.984464ms
Jun 22 16:50:25.559: INFO: Pod "pod-projected-configmaps-0b08d8c9-4cd5-4352-ad32-2fb91fbcda41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027810392s
STEP: Saw pod success
Jun 22 16:50:25.559: INFO: Pod "pod-projected-configmaps-0b08d8c9-4cd5-4352-ad32-2fb91fbcda41" satisfied condition "success or failure"
Jun 22 16:50:25.571: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-configmaps-0b08d8c9-4cd5-4352-ad32-2fb91fbcda41 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 16:50:25.631: INFO: Waiting for pod pod-projected-configmaps-0b08d8c9-4cd5-4352-ad32-2fb91fbcda41 to disappear
Jun 22 16:50:25.642: INFO: Pod pod-projected-configmaps-0b08d8c9-4cd5-4352-ad32-2fb91fbcda41 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:50:25.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4374" for this suite.
Jun 22 16:50:31.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:50:32.280: INFO: namespace projected-4374 deletion completed in 6.611735318s

• [SLOW TEST:9.086 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:50:32.280: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Jun 22 16:50:32.561: INFO: Waiting up to 5m0s for pod "client-containers-76fe7fe0-c841-4590-9227-8711bb926037" in namespace "containers-1432" to be "success or failure"
Jun 22 16:50:32.573: INFO: Pod "client-containers-76fe7fe0-c841-4590-9227-8711bb926037": Phase="Pending", Reason="", readiness=false. Elapsed: 11.548525ms
Jun 22 16:50:34.585: INFO: Pod "client-containers-76fe7fe0-c841-4590-9227-8711bb926037": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023395523s
STEP: Saw pod success
Jun 22 16:50:34.585: INFO: Pod "client-containers-76fe7fe0-c841-4590-9227-8711bb926037" satisfied condition "success or failure"
Jun 22 16:50:34.599: INFO: Trying to get logs from node 10.45.191.150 pod client-containers-76fe7fe0-c841-4590-9227-8711bb926037 container test-container: <nil>
STEP: delete the pod
Jun 22 16:50:34.680: INFO: Waiting for pod client-containers-76fe7fe0-c841-4590-9227-8711bb926037 to disappear
Jun 22 16:50:34.691: INFO: Pod client-containers-76fe7fe0-c841-4590-9227-8711bb926037 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:50:34.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1432" for this suite.
Jun 22 16:50:40.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:50:41.332: INFO: namespace containers-1432 deletion completed in 6.615285166s

• [SLOW TEST:9.052 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:50:41.333: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:50:41.594: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:50:43.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-820" for this suite.
Jun 22 16:51:29.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:51:30.412: INFO: namespace pods-820 deletion completed in 46.62435108s

• [SLOW TEST:49.080 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:51:30.413: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1372.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1372.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1372.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1372.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1372.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1372.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 16:51:34.976: INFO: DNS probes using dns-1372/dns-test-0c32b4b6-2cde-48bb-b962-5d70e7c580fd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:51:35.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1372" for this suite.
Jun 22 16:51:43.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:51:43.674: INFO: namespace dns-1372 deletion completed in 8.636357528s

• [SLOW TEST:13.262 seconds]
[sig-network] DNS
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:51:43.678: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 16:51:52.152: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 16:51:52.166: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 16:51:54.167: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 16:51:54.181: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 16:51:56.167: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 16:51:56.179: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 16:51:58.167: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 16:51:58.180: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 16:52:00.167: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 16:52:00.191: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 16:52:02.167: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 16:52:02.180: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:52:02.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2706" for this suite.
Jun 22 16:52:32.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:52:32.811: INFO: namespace container-lifecycle-hook-2706 deletion completed in 30.602848547s

• [SLOW TEST:49.133 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:52:32.811: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:52:40.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8539" for this suite.
Jun 22 16:52:46.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:52:47.117: INFO: namespace resourcequota-8539 deletion completed in 6.621376265s

• [SLOW TEST:14.306 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:52:47.118: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 22 16:53:17.609: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 16:53:17.609775      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:53:17.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7234" for this suite.
Jun 22 16:53:25.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:53:26.243: INFO: namespace gc-7234 deletion completed in 8.614612055s

• [SLOW TEST:39.125 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:53:26.244: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5048.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5048.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5048.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5048.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5048.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 139.33.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.33.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.33.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.33.139_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5048.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5048.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5048.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5048.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5048.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5048.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 139.33.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.33.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.33.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.33.139_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 16:53:28.717: INFO: Unable to read wheezy_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:28.736: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:28.756: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:28.782: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:28.941: INFO: Unable to read jessie_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:28.963: INFO: Unable to read jessie_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:28.985: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:29.018: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:29.143: INFO: Lookups using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 failed for: [wheezy_udp@dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_udp@dns-test-service.dns-5048.svc.cluster.local jessie_tcp@dns-test-service.dns-5048.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local]

Jun 22 16:53:34.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.185: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.208: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.227: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.387: INFO: Unable to read jessie_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.409: INFO: Unable to read jessie_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.429: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.450: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:34.576: INFO: Lookups using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 failed for: [wheezy_udp@dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_udp@dns-test-service.dns-5048.svc.cluster.local jessie_tcp@dns-test-service.dns-5048.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local]

Jun 22 16:53:39.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.183: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.206: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.244: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.407: INFO: Unable to read jessie_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.427: INFO: Unable to read jessie_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.447: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.467: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:39.594: INFO: Lookups using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 failed for: [wheezy_udp@dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_udp@dns-test-service.dns-5048.svc.cluster.local jessie_tcp@dns-test-service.dns-5048.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local]

Jun 22 16:53:44.168: INFO: Unable to read wheezy_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.191: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.211: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.234: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.384: INFO: Unable to read jessie_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.406: INFO: Unable to read jessie_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.425: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.444: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:44.575: INFO: Lookups using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 failed for: [wheezy_udp@dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_udp@dns-test-service.dns-5048.svc.cluster.local jessie_tcp@dns-test-service.dns-5048.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local]

Jun 22 16:53:49.165: INFO: Unable to read wheezy_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.185: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.206: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.242: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.383: INFO: Unable to read jessie_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.402: INFO: Unable to read jessie_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.422: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.440: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:49.565: INFO: Lookups using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 failed for: [wheezy_udp@dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_udp@dns-test-service.dns-5048.svc.cluster.local jessie_tcp@dns-test-service.dns-5048.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local]

Jun 22 16:53:54.166: INFO: Unable to read wheezy_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.207: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.227: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.370: INFO: Unable to read jessie_udp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.396: INFO: Unable to read jessie_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.420: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.440: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:54.575: INFO: Lookups using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 failed for: [wheezy_udp@dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_udp@dns-test-service.dns-5048.svc.cluster.local jessie_tcp@dns-test-service.dns-5048.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local]

Jun 22 16:53:59.190: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:59.211: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local from pod dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946: the server could not find the requested resource (get pods dns-test-e3a04e40-153d-40df-b412-e677dafab946)
Jun 22 16:53:59.582: INFO: Lookups using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 failed for: [wheezy_tcp@dns-test-service.dns-5048.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5048.svc.cluster.local]

Jun 22 16:54:04.606: INFO: DNS probes using dns-5048/dns-test-e3a04e40-153d-40df-b412-e677dafab946 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:54:04.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5048" for this suite.
Jun 22 16:54:12.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:54:13.518: INFO: namespace dns-5048 deletion completed in 8.669223635s

• [SLOW TEST:47.274 seconds]
[sig-network] DNS
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:54:13.518: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 22 16:54:14.960: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0622 16:54:14.959944      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 16:54:14.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-135" for this suite.
Jun 22 16:54:23.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:54:23.589: INFO: namespace gc-135 deletion completed in 8.612298123s

• [SLOW TEST:10.071 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:54:23.591: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:54:23.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1086" for this suite.
Jun 22 16:54:29.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:54:30.539: INFO: namespace services-1086 deletion completed in 6.612880576s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.949 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:54:30.540: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 16:54:30.824: INFO: Waiting up to 5m0s for pod "pod-95131f97-1fa7-41ad-b53f-8acd0b03b81a" in namespace "emptydir-9019" to be "success or failure"
Jun 22 16:54:30.837: INFO: Pod "pod-95131f97-1fa7-41ad-b53f-8acd0b03b81a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.173676ms
Jun 22 16:54:32.851: INFO: Pod "pod-95131f97-1fa7-41ad-b53f-8acd0b03b81a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027715574s
STEP: Saw pod success
Jun 22 16:54:32.852: INFO: Pod "pod-95131f97-1fa7-41ad-b53f-8acd0b03b81a" satisfied condition "success or failure"
Jun 22 16:54:32.863: INFO: Trying to get logs from node 10.45.191.150 pod pod-95131f97-1fa7-41ad-b53f-8acd0b03b81a container test-container: <nil>
STEP: delete the pod
Jun 22 16:54:32.977: INFO: Waiting for pod pod-95131f97-1fa7-41ad-b53f-8acd0b03b81a to disappear
Jun 22 16:54:32.988: INFO: Pod pod-95131f97-1fa7-41ad-b53f-8acd0b03b81a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:54:32.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9019" for this suite.
Jun 22 16:54:39.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:54:39.648: INFO: namespace emptydir-9019 deletion completed in 6.628957793s

• [SLOW TEST:9.109 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:54:39.650: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 16:54:44.070: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 16:54:44.083: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 16:54:46.084: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 16:54:46.096: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 16:54:48.084: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 16:54:48.096: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 16:54:50.084: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 16:54:50.104: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 16:54:52.084: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 16:54:52.095: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:54:52.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4459" for this suite.
Jun 22 16:55:22.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:55:22.848: INFO: namespace container-lifecycle-hook-4459 deletion completed in 30.67892895s

• [SLOW TEST:43.198 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:55:22.849: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:55:39.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2502" for this suite.
Jun 22 16:55:47.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:55:48.435: INFO: namespace resourcequota-2502 deletion completed in 8.767640798s

• [SLOW TEST:25.587 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:55:48.435: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 16:55:48.729: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3" in namespace "projected-4471" to be "success or failure"
Jun 22 16:55:48.742: INFO: Pod "downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.210918ms
Jun 22 16:55:50.755: INFO: Pod "downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026495086s
Jun 22 16:55:52.768: INFO: Pod "downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038809432s
STEP: Saw pod success
Jun 22 16:55:52.768: INFO: Pod "downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3" satisfied condition "success or failure"
Jun 22 16:55:52.780: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3 container client-container: <nil>
STEP: delete the pod
Jun 22 16:55:52.845: INFO: Waiting for pod downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3 to disappear
Jun 22 16:55:52.856: INFO: Pod downwardapi-volume-9539a885-2704-486d-8cfc-95e6c77509c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:55:52.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4471" for this suite.
Jun 22 16:55:58.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:55:59.561: INFO: namespace projected-4471 deletion completed in 6.675535298s

• [SLOW TEST:11.125 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:55:59.561: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 22 16:56:02.422: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2620 pod-service-account-bb4a4096-4209-4163-bcbf-240f5c88d45b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 22 16:56:02.929: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2620 pod-service-account-bb4a4096-4209-4163-bcbf-240f5c88d45b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 22 16:56:03.323: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2620 pod-service-account-bb4a4096-4209-4163-bcbf-240f5c88d45b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:56:03.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2620" for this suite.
Jun 22 16:56:09.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:56:10.328: INFO: namespace svcaccounts-2620 deletion completed in 6.642296402s

• [SLOW TEST:10.767 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:56:10.328: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:57:10.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6415" for this suite.
Jun 22 16:57:22.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:57:23.281: INFO: namespace container-probe-6415 deletion completed in 12.602673945s

• [SLOW TEST:72.953 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:57:23.283: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:57:23.680: INFO: Create a RollingUpdate DaemonSet
Jun 22 16:57:23.702: INFO: Check that daemon pods launch on every node of the cluster
Jun 22 16:57:23.733: INFO: Number of nodes with available pods: 0
Jun 22 16:57:23.733: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:57:24.775: INFO: Number of nodes with available pods: 0
Jun 22 16:57:24.775: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:57:25.775: INFO: Number of nodes with available pods: 2
Jun 22 16:57:25.775: INFO: Node 10.45.191.150 is running more than one daemon pod
Jun 22 16:57:26.771: INFO: Number of nodes with available pods: 3
Jun 22 16:57:26.771: INFO: Number of running nodes: 3, number of available pods: 3
Jun 22 16:57:26.771: INFO: Update the DaemonSet to trigger a rollout
Jun 22 16:57:26.812: INFO: Updating DaemonSet daemon-set
Jun 22 16:57:34.877: INFO: Roll back the DaemonSet before rollout is complete
Jun 22 16:57:34.918: INFO: Updating DaemonSet daemon-set
Jun 22 16:57:34.918: INFO: Make sure DaemonSet rollback is complete
Jun 22 16:57:34.929: INFO: Wrong image for pod: daemon-set-qqcnb. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 16:57:34.929: INFO: Pod daemon-set-qqcnb is not available
Jun 22 16:57:35.962: INFO: Wrong image for pod: daemon-set-qqcnb. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 22 16:57:35.962: INFO: Pod daemon-set-qqcnb is not available
Jun 22 16:57:36.962: INFO: Pod daemon-set-9nkqw is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9023, will wait for the garbage collector to delete the pods
Jun 22 16:57:37.124: INFO: Deleting DaemonSet.extensions daemon-set took: 41.530922ms
Jun 22 16:57:37.225: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.248395ms
Jun 22 16:57:39.438: INFO: Number of nodes with available pods: 0
Jun 22 16:57:39.438: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 16:57:39.458: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9023/daemonsets","resourceVersion":"41047"},"items":null}

Jun 22 16:57:39.469: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9023/pods","resourceVersion":"41047"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:57:39.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9023" for this suite.
Jun 22 16:57:47.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:57:48.184: INFO: namespace daemonsets-9023 deletion completed in 8.612532615s

• [SLOW TEST:24.901 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:57:48.185: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5124
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 16:57:48.447: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 16:58:12.923: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.131.101 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5124 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 16:58:12.923: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 16:58:14.159: INFO: Found all expected endpoints: [netserver-0]
Jun 22 16:58:14.171: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.44.22 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5124 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 16:58:14.171: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 16:58:15.406: INFO: Found all expected endpoints: [netserver-1]
Jun 22 16:58:15.417: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.226.86 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5124 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 16:58:15.417: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 16:58:16.622: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:58:16.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5124" for this suite.
Jun 22 16:58:30.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:58:31.395: INFO: namespace pod-network-test-5124 deletion completed in 14.747944989s

• [SLOW TEST:43.210 seconds]
[sig-network] Networking
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:58:31.395: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 22 16:58:41.776: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 16:58:41.776912      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:58:41.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6368" for this suite.
Jun 22 16:58:49.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:58:50.441: INFO: namespace gc-6368 deletion completed in 8.644203956s

• [SLOW TEST:19.046 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:58:50.442: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 16:58:50.823: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 16:58:50.877: INFO: Number of nodes with available pods: 0
Jun 22 16:58:50.877: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:58:51.914: INFO: Number of nodes with available pods: 0
Jun 22 16:58:51.914: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:58:52.916: INFO: Number of nodes with available pods: 2
Jun 22 16:58:52.916: INFO: Node 10.45.191.150 is running more than one daemon pod
Jun 22 16:58:53.925: INFO: Number of nodes with available pods: 3
Jun 22 16:58:53.925: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 22 16:58:54.045: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:54.046: INFO: Wrong image for pod: daemon-set-86k4p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:54.046: INFO: Wrong image for pod: daemon-set-r9ld5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:55.117: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:55.117: INFO: Wrong image for pod: daemon-set-86k4p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:55.117: INFO: Wrong image for pod: daemon-set-r9ld5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:56.087: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:56.087: INFO: Wrong image for pod: daemon-set-86k4p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:56.087: INFO: Wrong image for pod: daemon-set-r9ld5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:57.081: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:57.081: INFO: Wrong image for pod: daemon-set-86k4p. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:57.081: INFO: Pod daemon-set-86k4p is not available
Jun 22 16:58:57.081: INFO: Wrong image for pod: daemon-set-r9ld5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:58.087: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:58.087: INFO: Wrong image for pod: daemon-set-r9ld5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:58.087: INFO: Pod daemon-set-xv47m is not available
Jun 22 16:58:59.081: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:59.082: INFO: Wrong image for pod: daemon-set-r9ld5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:58:59.082: INFO: Pod daemon-set-xv47m is not available
Jun 22 16:59:00.084: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:00.084: INFO: Wrong image for pod: daemon-set-r9ld5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:00.084: INFO: Pod daemon-set-r9ld5 is not available
Jun 22 16:59:01.080: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:01.080: INFO: Pod daemon-set-828d6 is not available
Jun 22 16:59:02.081: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:02.081: INFO: Pod daemon-set-828d6 is not available
Jun 22 16:59:03.081: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:04.094: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:04.094: INFO: Pod daemon-set-5mmnz is not available
Jun 22 16:59:05.080: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:05.080: INFO: Pod daemon-set-5mmnz is not available
Jun 22 16:59:06.082: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:06.082: INFO: Pod daemon-set-5mmnz is not available
Jun 22 16:59:07.081: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:07.081: INFO: Pod daemon-set-5mmnz is not available
Jun 22 16:59:08.082: INFO: Wrong image for pod: daemon-set-5mmnz. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 22 16:59:08.082: INFO: Pod daemon-set-5mmnz is not available
Jun 22 16:59:09.082: INFO: Pod daemon-set-qg2sg is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 22 16:59:09.142: INFO: Number of nodes with available pods: 2
Jun 22 16:59:09.142: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:59:10.180: INFO: Number of nodes with available pods: 2
Jun 22 16:59:10.180: INFO: Node 10.45.191.131 is running more than one daemon pod
Jun 22 16:59:11.183: INFO: Number of nodes with available pods: 3
Jun 22 16:59:11.183: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8899, will wait for the garbage collector to delete the pods
Jun 22 16:59:11.376: INFO: Deleting DaemonSet.extensions daemon-set took: 36.992815ms
Jun 22 16:59:11.777: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.316588ms
Jun 22 16:59:24.591: INFO: Number of nodes with available pods: 0
Jun 22 16:59:24.591: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 16:59:24.609: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8899/daemonsets","resourceVersion":"41562"},"items":null}

Jun 22 16:59:24.629: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8899/pods","resourceVersion":"41562"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 16:59:24.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8899" for this suite.
Jun 22 16:59:32.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 16:59:33.356: INFO: namespace daemonsets-8899 deletion completed in 8.623222255s

• [SLOW TEST:42.915 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 16:59:33.357: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7388
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7388
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7388
Jun 22 16:59:33.671: INFO: Found 0 stateful pods, waiting for 1
Jun 22 16:59:43.685: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 22 16:59:43.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 16:59:44.023: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 16:59:44.023: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 16:59:44.023: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 16:59:44.035: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 16:59:54.048: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 16:59:54.048: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 16:59:54.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997791s
Jun 22 16:59:55.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988507518s
Jun 22 16:59:56.141: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.974672728s
Jun 22 16:59:57.211: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.958467469s
Jun 22 16:59:58.225: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.888968684s
Jun 22 16:59:59.237: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.87499489s
Jun 22 17:00:00.251: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.86231841s
Jun 22 17:00:01.267: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.848537648s
Jun 22 17:00:02.283: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.832874477s
Jun 22 17:00:03.295: INFO: Verifying statefulset ss doesn't scale past 1 for another 816.915604ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7388
Jun 22 17:00:04.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 17:00:04.730: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 17:00:04.730: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 17:00:04.730: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 17:00:04.746: INFO: Found 1 stateful pods, waiting for 3
Jun 22 17:00:14.760: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 17:00:14.760: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 17:00:14.760: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 22 17:00:14.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 17:00:15.115: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 17:00:15.115: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 17:00:15.115: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 17:00:15.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 17:00:15.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 17:00:15.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 17:00:15.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 17:00:15.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 17:00:15.872: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 17:00:15.872: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 17:00:15.872: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 17:00:15.872: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 17:00:15.892: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 22 17:00:25.927: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 17:00:25.927: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 17:00:25.927: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 17:00:25.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998023s
Jun 22 17:00:26.989: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987768313s
Jun 22 17:00:28.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974448694s
Jun 22 17:00:29.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96160326s
Jun 22 17:00:30.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.949094565s
Jun 22 17:00:31.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.935319739s
Jun 22 17:00:32.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.923380989s
Jun 22 17:00:33.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.910447978s
Jun 22 17:00:34.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.896152751s
Jun 22 17:00:35.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 883.642749ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7388
Jun 22 17:00:36.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 17:00:36.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 17:00:36.507: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 17:00:36.507: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 17:00:36.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 17:00:36.816: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 17:00:36.816: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 17:00:36.816: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 17:00:36.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=statefulset-7388 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 17:00:37.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 17:00:37.135: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 17:00:37.135: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 17:00:37.135: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 22 17:01:07.208: INFO: Deleting all statefulset in ns statefulset-7388
Jun 22 17:01:07.224: INFO: Scaling statefulset ss to 0
Jun 22 17:01:07.272: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 17:01:07.288: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:01:07.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7388" for this suite.
Jun 22 17:01:15.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:01:16.040: INFO: namespace statefulset-7388 deletion completed in 8.651882181s

• [SLOW TEST:102.683 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:01:16.040: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Jun 22 17:01:16.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 cluster-info'
Jun 22 17:01:16.433: INFO: stderr: ""
Jun 22 17:01:16.434: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mNodeLocalDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/node-local-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:01:16.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9034" for this suite.
Jun 22 17:01:22.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:01:23.073: INFO: namespace kubectl-9034 deletion completed in 6.611296874s

• [SLOW TEST:7.033 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:01:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 17:01:23.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 17:01:26.841: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:01:26.858: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:01:27.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5006" for this suite.
Jun 22 17:01:35.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:01:36.817: INFO: namespace webhook-5006 deletion completed in 8.933494898s
STEP: Destroying namespace "webhook-5006-markers" for this suite.
Jun 22 17:01:42.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:01:43.482: INFO: namespace webhook-5006-markers deletion completed in 6.664256247s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.491 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:01:43.565: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 22 17:01:43.822: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 17:01:43.882: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 17:01:43.903: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.131 before test
Jun 22 17:01:43.994: INFO: ibm-master-proxy-static-10.45.191.131 from kube-system started at 2020-06-22 13:23:38 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:43.994: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 17:01:43.994: INFO: 	Container pause ready: true, restart count 0
Jun 22 17:01:43.994: INFO: ibm-keepalived-watcher-jmnzb from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:43.994: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 17:01:43.994: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-p5tk9 from ibm-system started at 2020-06-22 16:24:23 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:43.994: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 17:01:43.994: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-jfbn9 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:43.994: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 17:01:43.994: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 17:01:43.995: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-h9wdw from kube-system started at 2020-06-22 16:24:23 +0000 UTC (4 container statuses recorded)
Jun 22 17:01:43.995: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 17:01:43.995: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 17:01:43.995: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 17:01:43.995: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 17:01:43.995: INFO: calico-node-s8lh4 from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:43.995: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 17:01:43.995: INFO: sonobuoy from sonobuoy started at 2020-06-22 15:11:50 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:43.995: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 17:01:43.995: INFO: sonobuoy-e2e-job-d71e65d5f08d4631 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:43.995: INFO: 	Container e2e ready: true, restart count 0
Jun 22 17:01:43.995: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 17:01:43.995: INFO: coredns-697bf86f8c-gpt5n from kube-system started at 2020-06-22 16:24:23 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:43.995: INFO: 	Container coredns ready: true, restart count 0
Jun 22 17:01:43.996: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.149 before test
Jun 22 17:01:44.109: INFO: ibm-file-plugin-7c85454984-k87nb from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.109: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 22 17:01:44.109: INFO: ibm-storage-watcher-7b49c697c8-fmvjk from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.109: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 22 17:01:44.110: INFO: olm-operator-557b484679-lvl7g from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container olm-operator ready: true, restart count 0
Jun 22 17:01:44.110: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-vrs4m from kube-system started at 2020-06-22 15:47:30 +0000 UTC (4 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 17:01:44.110: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 17:01:44.110: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 17:01:44.110: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 17:01:44.110: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-7gntq from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 17:01:44.110: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 17:01:44.110: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-7m74w from ibm-system started at 2020-06-22 15:47:30 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 17:01:44.110: INFO: metrics-server-759d6d9f96-q5lgr from kube-system started at 2020-06-22 13:23:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 17:01:44.110: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun 22 17:01:44.110: INFO: catalog-operator-57d89fd5c4-bdgnq from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 22 17:01:44.110: INFO: vpn-58b48cdc7c-z2bv5 from kube-system started at 2020-06-22 13:47:07 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container vpn ready: true, restart count 0
Jun 22 17:01:44.110: INFO: coredns-697bf86f8c-2z94m from kube-system started at 2020-06-22 13:47:52 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container coredns ready: true, restart count 0
Jun 22 17:01:44.110: INFO: ibm-keepalived-watcher-n8x49 from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.110: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 17:01:44.111: INFO: calico-node-k8xbh from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.111: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 17:01:44.111: INFO: dashboard-metrics-scraper-576c46d9bd-8tnxf from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.111: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun 22 17:01:44.111: INFO: calico-kube-controllers-7c7d954b58-zwmf2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.111: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 17:01:44.111: INFO: kubernetes-dashboard-c6b4b9d77-wmvpw from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.111: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 22 17:01:44.111: INFO: coredns-autoscaler-6b97b7f9b-bldh2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.111: INFO: 	Container autoscaler ready: true, restart count 0
Jun 22 17:01:44.111: INFO: ibm-master-proxy-static-10.45.191.149 from kube-system started at 2020-06-22 13:22:54 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:44.111: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 17:01:44.111: INFO: 	Container pause ready: true, restart count 0
Jun 22 17:01:44.111: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.150 before test
Jun 22 17:01:44.214: INFO: ibm-keepalived-watcher-wkknt from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.214: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 17:01:44.214: INFO: addon-catalog-source-fh2f8 from ibm-system started at 2020-06-22 13:27:43 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.214: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jun 22 17:01:44.214: INFO: ibm-master-proxy-static-10.45.191.150 from kube-system started at 2020-06-22 13:23:20 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:44.214: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 17:01:44.214: INFO: 	Container pause ready: true, restart count 0
Jun 22 17:01:44.214: INFO: calico-node-gvtlw from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.214: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 17:01:44.214: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-rg86f from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:01:44.214: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 17:01:44.214: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 17:01:44.214: INFO: coredns-697bf86f8c-ql8vb from kube-system started at 2020-06-22 16:24:23 +0000 UTC (1 container statuses recorded)
Jun 22 17:01:44.214: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.161aec5d70c2e8a4], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:01:45.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1939" for this suite.
Jun 22 17:01:51.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:01:52.249: INFO: namespace sched-pred-1939 deletion completed in 6.904567604s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:8.684 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:01:52.251: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 17:01:52.541: INFO: Waiting up to 5m0s for pod "pod-a391e922-e56a-458c-a81b-814f86896e3d" in namespace "emptydir-3907" to be "success or failure"
Jun 22 17:01:52.554: INFO: Pod "pod-a391e922-e56a-458c-a81b-814f86896e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.610191ms
Jun 22 17:01:54.568: INFO: Pod "pod-a391e922-e56a-458c-a81b-814f86896e3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02737952s
STEP: Saw pod success
Jun 22 17:01:54.569: INFO: Pod "pod-a391e922-e56a-458c-a81b-814f86896e3d" satisfied condition "success or failure"
Jun 22 17:01:54.583: INFO: Trying to get logs from node 10.45.191.150 pod pod-a391e922-e56a-458c-a81b-814f86896e3d container test-container: <nil>
STEP: delete the pod
Jun 22 17:01:54.659: INFO: Waiting for pod pod-a391e922-e56a-458c-a81b-814f86896e3d to disappear
Jun 22 17:01:54.672: INFO: Pod pod-a391e922-e56a-458c-a81b-814f86896e3d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:01:54.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3907" for this suite.
Jun 22 17:02:00.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:02:01.551: INFO: namespace emptydir-3907 deletion completed in 6.850019648s

• [SLOW TEST:9.300 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:02:01.552: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-6d49eed7-a124-432b-ad4b-677b67ada577 in namespace container-probe-4189
Jun 22 17:02:05.878: INFO: Started pod test-webserver-6d49eed7-a124-432b-ad4b-677b67ada577 in namespace container-probe-4189
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 17:02:05.890: INFO: Initial restart count of pod test-webserver-6d49eed7-a124-432b-ad4b-677b67ada577 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:06:06.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4189" for this suite.
Jun 22 17:06:12.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:06:12.719: INFO: namespace container-probe-4189 deletion completed in 6.638556984s

• [SLOW TEST:251.167 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:06:12.722: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1549
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1549.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1549.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1549.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1549.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1549.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1549.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1549.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 17:06:15.120: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.143: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.163: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.183: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.250: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.270: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.293: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.318: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:15.361: INFO: Lookups using dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1549.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1549.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_udp@dns-test-service-2.dns-1549.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1549.svc.cluster.local]

Jun 22 17:06:20.385: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:20.409: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:20.517: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:20.545: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:20.644: INFO: Lookups using dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local]

Jun 22 17:06:25.382: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:25.403: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:25.504: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:25.524: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:25.619: INFO: Lookups using dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local]

Jun 22 17:06:30.381: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:30.691: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:30.798: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:30.818: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:30.894: INFO: Lookups using dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local]

Jun 22 17:06:35.381: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:35.401: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:35.520: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:35.540: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:35.633: INFO: Lookups using dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local]

Jun 22 17:06:40.381: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:40.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:40.509: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:40.529: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local from pod dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6: the server could not find the requested resource (get pods dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6)
Jun 22 17:06:40.622: INFO: Lookups using dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1549.svc.cluster.local]

Jun 22 17:06:45.624: INFO: DNS probes using dns-1549/dns-test-f2775ec0-eb47-4211-8509-be9ac48351f6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:06:45.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1549" for this suite.
Jun 22 17:06:53.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:06:54.704: INFO: namespace dns-1549 deletion completed in 8.92615142s

• [SLOW TEST:41.982 seconds]
[sig-network] DNS
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:06:54.704: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-01b740ab-a8e9-47f6-aee7-6fb18692ebf7
STEP: Creating a pod to test consume configMaps
Jun 22 17:06:55.029: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a" in namespace "configmap-6496" to be "success or failure"
Jun 22 17:06:55.044: INFO: Pod "pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.30182ms
Jun 22 17:06:57.056: INFO: Pod "pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027447093s
Jun 22 17:06:59.069: INFO: Pod "pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04038341s
STEP: Saw pod success
Jun 22 17:06:59.069: INFO: Pod "pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a" satisfied condition "success or failure"
Jun 22 17:06:59.082: INFO: Trying to get logs from node 10.45.191.150 pod pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 17:06:59.195: INFO: Waiting for pod pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a to disappear
Jun 22 17:06:59.214: INFO: Pod pod-configmaps-8e830ecd-cc83-47aa-9b42-7199296d725a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:06:59.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6496" for this suite.
Jun 22 17:07:07.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:07:07.897: INFO: namespace configmap-6496 deletion completed in 8.65729167s

• [SLOW TEST:13.193 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:07:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:07:08.253: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1abe4c61-70fb-4972-98b2-35b143c4d3d3" in namespace "security-context-test-3263" to be "success or failure"
Jun 22 17:07:08.274: INFO: Pod "busybox-readonly-false-1abe4c61-70fb-4972-98b2-35b143c4d3d3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.804483ms
Jun 22 17:07:10.286: INFO: Pod "busybox-readonly-false-1abe4c61-70fb-4972-98b2-35b143c4d3d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032833455s
Jun 22 17:07:10.286: INFO: Pod "busybox-readonly-false-1abe4c61-70fb-4972-98b2-35b143c4d3d3" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:07:10.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3263" for this suite.
Jun 22 17:07:16.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:07:16.980: INFO: namespace security-context-test-3263 deletion completed in 6.670695904s

• [SLOW TEST:9.083 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:07:16.981: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5951
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 17:07:17.255: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 17:07:41.596: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.131.116:8080/dial?request=hostName&protocol=udp&host=172.30.131.115&port=8081&tries=1'] Namespace:pod-network-test-5951 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:07:41.596: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 17:07:41.787: INFO: Waiting for endpoints: map[]
Jun 22 17:07:41.801: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.131.116:8080/dial?request=hostName&protocol=udp&host=172.30.226.89&port=8081&tries=1'] Namespace:pod-network-test-5951 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:07:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 17:07:41.994: INFO: Waiting for endpoints: map[]
Jun 22 17:07:42.007: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.131.116:8080/dial?request=hostName&protocol=udp&host=172.30.44.24&port=8081&tries=1'] Namespace:pod-network-test-5951 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:07:42.007: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 17:07:42.209: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:07:42.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5951" for this suite.
Jun 22 17:07:54.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:07:54.868: INFO: namespace pod-network-test-5951 deletion completed in 12.634574109s

• [SLOW TEST:37.887 seconds]
[sig-network] Networking
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:07:54.868: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 17:07:56.128: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 17:07:58.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442476, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442476, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442476, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442476, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 17:08:01.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:08:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4312-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:08:02.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-312" for this suite.
Jun 22 17:08:10.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:08:11.536: INFO: namespace webhook-312 deletion completed in 8.63534881s
STEP: Destroying namespace "webhook-312-markers" for this suite.
Jun 22 17:08:17.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:08:18.457: INFO: namespace webhook-312-markers deletion completed in 6.921443809s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:08:18.541: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Jun 22 17:08:18.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 api-versions'
Jun 22 17:08:18.940: INFO: stderr: ""
Jun 22 17:08:18.940: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:08:18.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8821" for this suite.
Jun 22 17:08:25.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:08:25.592: INFO: namespace kubectl-8821 deletion completed in 6.628450502s

• [SLOW TEST:7.051 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:08:25.593: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 22 17:08:25.854: INFO: PodSpec: initContainers in spec.initContainers
Jun 22 17:09:13.407: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e91d62e9-e373-4ef0-bb07-de2d8f6215e3", GenerateName:"", Namespace:"init-container-8930", SelfLink:"/api/v1/namespaces/init-container-8930/pods/pod-init-e91d62e9-e373-4ef0-bb07-de2d8f6215e3", UID:"c578a570-4e7e-46d7-965b-2f0eefa54e80", ResourceVersion:"43210", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63728442505, loc:(*time.Location)(0x78a6940)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"854327805"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9zgmm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0058c0400), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9zgmm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9zgmm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9zgmm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c381a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.45.191.150", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002af3ec0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c38230)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c38250)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c38258), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c3825c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442505, loc:(*time.Location)(0x78a6940)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442505, loc:(*time.Location)(0x78a6940)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442505, loc:(*time.Location)(0x78a6940)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442505, loc:(*time.Location)(0x78a6940)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.191.150", PodIP:"172.30.131.118", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.131.118"}}, StartTime:(*v1.Time)(0xc002238520), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032f4070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032f40e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://f9ece72700f29d780810197144e39c9b2e7504211fc1b715aa3da64808d5a887", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0022385a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002238560), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002c382d4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:09:13.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8930" for this suite.
Jun 22 17:09:43.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:09:44.352: INFO: namespace init-container-8930 deletion completed in 30.626397777s

• [SLOW TEST:78.759 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:09:44.352: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 22 17:09:46.824: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-114803800 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 22 17:10:02.004: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:10:02.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3566" for this suite.
Jun 22 17:10:10.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:10:10.801: INFO: namespace pods-3566 deletion completed in 8.75614448s

• [SLOW TEST:26.449 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:10:10.801: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-8b30a334-04ae-4cd1-bb1f-e946966649ac
STEP: Creating a pod to test consume configMaps
Jun 22 17:10:11.113: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24" in namespace "configmap-958" to be "success or failure"
Jun 22 17:10:11.136: INFO: Pod "pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24": Phase="Pending", Reason="", readiness=false. Elapsed: 22.958768ms
Jun 22 17:10:13.150: INFO: Pod "pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24": Phase="Running", Reason="", readiness=true. Elapsed: 2.036254769s
Jun 22 17:10:15.161: INFO: Pod "pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048030899s
STEP: Saw pod success
Jun 22 17:10:15.161: INFO: Pod "pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24" satisfied condition "success or failure"
Jun 22 17:10:15.195: INFO: Trying to get logs from node 10.45.191.150 pod pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 17:10:15.261: INFO: Waiting for pod pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24 to disappear
Jun 22 17:10:15.278: INFO: Pod pod-configmaps-7a82a192-e670-4da6-b8e7-4006a2629e24 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:10:15.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-958" for this suite.
Jun 22 17:10:21.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:10:21.920: INFO: namespace configmap-958 deletion completed in 6.616625082s

• [SLOW TEST:11.119 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:10:21.920: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 17:10:22.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3745'
Jun 22 17:10:22.451: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 17:10:22.451: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jun 22 17:10:22.474: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jun 22 17:10:22.507: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 22 17:10:22.522: INFO: scanned /root for discovery docs: <nil>
Jun 22 17:10:22.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3745'
Jun 22 17:10:38.770: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 17:10:38.770: INFO: stdout: "Created e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96\nScaling up e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jun 22 17:10:38.770: INFO: stdout: "Created e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96\nScaling up e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jun 22 17:10:38.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3745'
Jun 22 17:10:38.921: INFO: stderr: ""
Jun 22 17:10:38.921: INFO: stdout: "e2e-test-httpd-rc-6mhc4 e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96-7npbn "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Jun 22 17:10:43.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3745'
Jun 22 17:10:44.049: INFO: stderr: ""
Jun 22 17:10:44.049: INFO: stdout: "e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96-7npbn "
Jun 22 17:10:44.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96-7npbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3745'
Jun 22 17:10:44.262: INFO: stderr: ""
Jun 22 17:10:44.262: INFO: stdout: "true"
Jun 22 17:10:44.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pods e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96-7npbn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3745'
Jun 22 17:10:44.387: INFO: stderr: ""
Jun 22 17:10:44.387: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jun 22 17:10:44.387: INFO: e2e-test-httpd-rc-90fe80f2658847c4a6a2af794f35be96-7npbn is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Jun 22 17:10:44.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete rc e2e-test-httpd-rc --namespace=kubectl-3745'
Jun 22 17:10:44.559: INFO: stderr: ""
Jun 22 17:10:44.559: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:10:44.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3745" for this suite.
Jun 22 17:10:52.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:10:53.212: INFO: namespace kubectl-3745 deletion completed in 8.624674003s

• [SLOW TEST:31.292 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:10:53.213: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6605
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1744
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:11:25.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2407" for this suite.
Jun 22 17:11:31.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:11:31.824: INFO: namespace namespaces-2407 deletion completed in 6.688888993s
STEP: Destroying namespace "nsdeletetest-6605" for this suite.
Jun 22 17:11:31.841: INFO: Namespace nsdeletetest-6605 was already deleted
STEP: Destroying namespace "nsdeletetest-1744" for this suite.
Jun 22 17:11:37.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:11:38.760: INFO: namespace nsdeletetest-1744 deletion completed in 6.919165414s

• [SLOW TEST:45.548 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:11:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 17:11:39.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ab16432-7f3f-4248-85a4-7efe01cedfcd" in namespace "projected-3271" to be "success or failure"
Jun 22 17:11:39.203: INFO: Pod "downwardapi-volume-9ab16432-7f3f-4248-85a4-7efe01cedfcd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.401144ms
Jun 22 17:11:41.218: INFO: Pod "downwardapi-volume-9ab16432-7f3f-4248-85a4-7efe01cedfcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026681381s
STEP: Saw pod success
Jun 22 17:11:41.218: INFO: Pod "downwardapi-volume-9ab16432-7f3f-4248-85a4-7efe01cedfcd" satisfied condition "success or failure"
Jun 22 17:11:41.231: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-9ab16432-7f3f-4248-85a4-7efe01cedfcd container client-container: <nil>
STEP: delete the pod
Jun 22 17:11:41.496: INFO: Waiting for pod downwardapi-volume-9ab16432-7f3f-4248-85a4-7efe01cedfcd to disappear
Jun 22 17:11:41.511: INFO: Pod downwardapi-volume-9ab16432-7f3f-4248-85a4-7efe01cedfcd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:11:41.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3271" for this suite.
Jun 22 17:11:47.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:11:48.153: INFO: namespace projected-3271 deletion completed in 6.613296426s

• [SLOW TEST:9.393 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:11:48.154: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-e25da4b9-ff06-423c-b764-98aef4e99e12
STEP: Creating a pod to test consume secrets
Jun 22 17:11:48.443: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6" in namespace "projected-6310" to be "success or failure"
Jun 22 17:11:48.461: INFO: Pod "pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.199066ms
Jun 22 17:11:50.474: INFO: Pod "pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031516061s
Jun 22 17:11:52.488: INFO: Pod "pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045486433s
STEP: Saw pod success
Jun 22 17:11:52.488: INFO: Pod "pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6" satisfied condition "success or failure"
Jun 22 17:11:52.501: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 17:11:52.566: INFO: Waiting for pod pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6 to disappear
Jun 22 17:11:52.576: INFO: Pod pod-projected-secrets-bc02bb77-9b64-4c04-9586-5af1856218f6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:11:52.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6310" for this suite.
Jun 22 17:11:58.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:11:59.294: INFO: namespace projected-6310 deletion completed in 6.691449093s

• [SLOW TEST:11.140 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:11:59.294: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-346
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-346
STEP: Creating statefulset with conflicting port in namespace statefulset-346
STEP: Waiting until pod test-pod will start running in namespace statefulset-346
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-346
Jun 22 17:12:01.690: INFO: Observed stateful pod in namespace: statefulset-346, name: ss-0, uid: f413e0fc-3c3c-44f5-8ce1-80c533fb68dd, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 17:12:01.695: INFO: Observed stateful pod in namespace: statefulset-346, name: ss-0, uid: f413e0fc-3c3c-44f5-8ce1-80c533fb68dd, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 17:12:01.712: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-346
STEP: Removing pod with conflicting port in namespace statefulset-346
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-346 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 22 17:12:05.790: INFO: Deleting all statefulset in ns statefulset-346
Jun 22 17:12:05.811: INFO: Scaling statefulset ss to 0
Jun 22 17:12:15.884: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 17:12:16.039: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:12:16.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-346" for this suite.
Jun 22 17:12:24.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:12:24.999: INFO: namespace statefulset-346 deletion completed in 8.70518768s

• [SLOW TEST:25.705 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:12:25.000: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 17:12:25.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8269'
Jun 22 17:12:25.416: INFO: stderr: ""
Jun 22 17:12:25.416: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 22 17:12:30.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 get pod e2e-test-httpd-pod --namespace=kubectl-8269 -o json'
Jun 22 17:12:30.610: INFO: stderr: ""
Jun 22 17:12:30.610: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-06-22T17:12:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8269\",\n        \"resourceVersion\": \"44071\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8269/pods/e2e-test-httpd-pod\",\n        \"uid\": \"bf335c7f-e7f6-49cb-a63f-f93f6c1fa4db\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-lq4rf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.45.191.150\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-lq4rf\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-lq4rf\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T17:12:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T17:12:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T17:12:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-22T17:12:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d20e85f6a7344ba1577f75f7c806939d06ae077a7fd5cb58beafbd95fee05b08\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-06-22T17:12:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.191.150\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.131.125\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.131.125\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-06-22T17:12:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 22 17:12:30.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 replace -f - --namespace=kubectl-8269'
Jun 22 17:12:30.964: INFO: stderr: ""
Jun 22 17:12:30.964: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Jun 22 17:12:30.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete pods e2e-test-httpd-pod --namespace=kubectl-8269'
Jun 22 17:12:32.830: INFO: stderr: ""
Jun 22 17:12:32.830: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:12:32.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8269" for this suite.
Jun 22 17:12:38.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:12:39.473: INFO: namespace kubectl-8269 deletion completed in 6.617527819s

• [SLOW TEST:14.474 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:12:39.473: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-5529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5529
I0622 17:12:39.757429      24 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5529, replica count: 1
I0622 17:12:40.808102      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 17:12:41.808358      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 17:12:42.808697      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 17:12:42.943: INFO: Created: latency-svc-xjgg6
Jun 22 17:12:42.953: INFO: Got endpoints: latency-svc-xjgg6 [44.025966ms]
Jun 22 17:12:42.985: INFO: Created: latency-svc-xjfhq
Jun 22 17:12:42.996: INFO: Created: latency-svc-qttc5
Jun 22 17:12:42.998: INFO: Got endpoints: latency-svc-xjfhq [45.201656ms]
Jun 22 17:12:43.012: INFO: Got endpoints: latency-svc-qttc5 [59.467734ms]
Jun 22 17:12:43.012: INFO: Created: latency-svc-fvgb9
Jun 22 17:12:43.021: INFO: Got endpoints: latency-svc-fvgb9 [68.115596ms]
Jun 22 17:12:43.022: INFO: Created: latency-svc-sd8hx
Jun 22 17:12:43.032: INFO: Created: latency-svc-5vqb2
Jun 22 17:12:43.049: INFO: Created: latency-svc-rdm7f
Jun 22 17:12:43.049: INFO: Got endpoints: latency-svc-sd8hx [95.986566ms]
Jun 22 17:12:43.050: INFO: Got endpoints: latency-svc-5vqb2 [95.806225ms]
Jun 22 17:12:43.053: INFO: Created: latency-svc-5rcr6
Jun 22 17:12:43.059: INFO: Got endpoints: latency-svc-rdm7f [105.711749ms]
Jun 22 17:12:43.062: INFO: Created: latency-svc-krrhr
Jun 22 17:12:43.067: INFO: Got endpoints: latency-svc-5rcr6 [113.874479ms]
Jun 22 17:12:43.088: INFO: Created: latency-svc-9zvn2
Jun 22 17:12:43.088: INFO: Got endpoints: latency-svc-krrhr [134.215768ms]
Jun 22 17:12:43.090: INFO: Created: latency-svc-6b8hr
Jun 22 17:12:43.091: INFO: Got endpoints: latency-svc-9zvn2 [137.11379ms]
Jun 22 17:12:43.101: INFO: Got endpoints: latency-svc-6b8hr [147.970326ms]
Jun 22 17:12:43.102: INFO: Created: latency-svc-fnkrt
Jun 22 17:12:43.113: INFO: Created: latency-svc-2zwcq
Jun 22 17:12:43.114: INFO: Got endpoints: latency-svc-fnkrt [160.138377ms]
Jun 22 17:12:43.127: INFO: Got endpoints: latency-svc-2zwcq [173.486456ms]
Jun 22 17:12:43.130: INFO: Created: latency-svc-rmbnr
Jun 22 17:12:43.140: INFO: Created: latency-svc-7h47q
Jun 22 17:12:43.140: INFO: Got endpoints: latency-svc-rmbnr [186.548381ms]
Jun 22 17:12:43.154: INFO: Got endpoints: latency-svc-7h47q [199.953819ms]
Jun 22 17:12:43.154: INFO: Created: latency-svc-nfbmg
Jun 22 17:12:43.169: INFO: Got endpoints: latency-svc-nfbmg [215.173758ms]
Jun 22 17:12:43.179: INFO: Created: latency-svc-b74r6
Jun 22 17:12:43.179: INFO: Created: latency-svc-4ds4k
Jun 22 17:12:43.191: INFO: Got endpoints: latency-svc-4ds4k [178.853441ms]
Jun 22 17:12:43.191: INFO: Got endpoints: latency-svc-b74r6 [192.942929ms]
Jun 22 17:12:43.192: INFO: Created: latency-svc-lqq8s
Jun 22 17:12:43.204: INFO: Got endpoints: latency-svc-lqq8s [182.771658ms]
Jun 22 17:12:43.206: INFO: Created: latency-svc-rnb8t
Jun 22 17:12:43.215: INFO: Created: latency-svc-nbx59
Jun 22 17:12:43.218: INFO: Got endpoints: latency-svc-rnb8t [169.208382ms]
Jun 22 17:12:43.224: INFO: Created: latency-svc-jkpkd
Jun 22 17:12:43.234: INFO: Got endpoints: latency-svc-nbx59 [184.112519ms]
Jun 22 17:12:43.236: INFO: Got endpoints: latency-svc-jkpkd [176.828592ms]
Jun 22 17:12:43.241: INFO: Created: latency-svc-z69mv
Jun 22 17:12:43.252: INFO: Created: latency-svc-5wjgz
Jun 22 17:12:43.256: INFO: Got endpoints: latency-svc-z69mv [189.111727ms]
Jun 22 17:12:43.264: INFO: Created: latency-svc-65jdb
Jun 22 17:12:43.268: INFO: Got endpoints: latency-svc-5wjgz [179.669059ms]
Jun 22 17:12:43.274: INFO: Got endpoints: latency-svc-65jdb [183.736092ms]
Jun 22 17:12:43.279: INFO: Created: latency-svc-qzkwp
Jun 22 17:12:43.289: INFO: Got endpoints: latency-svc-qzkwp [187.63747ms]
Jun 22 17:12:43.290: INFO: Created: latency-svc-d8h5g
Jun 22 17:12:43.300: INFO: Created: latency-svc-plcgz
Jun 22 17:12:43.300: INFO: Got endpoints: latency-svc-d8h5g [186.114287ms]
Jun 22 17:12:43.313: INFO: Created: latency-svc-8hg6x
Jun 22 17:12:43.313: INFO: Got endpoints: latency-svc-plcgz [185.64445ms]
Jun 22 17:12:43.329: INFO: Created: latency-svc-dxtjt
Jun 22 17:12:43.329: INFO: Got endpoints: latency-svc-8hg6x [188.909681ms]
Jun 22 17:12:43.335: INFO: Got endpoints: latency-svc-dxtjt [181.570447ms]
Jun 22 17:12:43.336: INFO: Created: latency-svc-sg669
Jun 22 17:12:43.352: INFO: Created: latency-svc-nkfr6
Jun 22 17:12:43.355: INFO: Got endpoints: latency-svc-sg669 [186.471504ms]
Jun 22 17:12:43.365: INFO: Got endpoints: latency-svc-nkfr6 [173.662048ms]
Jun 22 17:12:43.367: INFO: Created: latency-svc-t9bst
Jun 22 17:12:43.379: INFO: Created: latency-svc-cmxnz
Jun 22 17:12:43.384: INFO: Got endpoints: latency-svc-t9bst [192.797743ms]
Jun 22 17:12:43.390: INFO: Got endpoints: latency-svc-cmxnz [185.844037ms]
Jun 22 17:12:43.395: INFO: Created: latency-svc-nlw7s
Jun 22 17:12:43.403: INFO: Created: latency-svc-4jp5h
Jun 22 17:12:43.410: INFO: Got endpoints: latency-svc-nlw7s [191.074037ms]
Jun 22 17:12:43.420: INFO: Created: latency-svc-h57gn
Jun 22 17:12:43.420: INFO: Got endpoints: latency-svc-4jp5h [185.748108ms]
Jun 22 17:12:43.431: INFO: Created: latency-svc-bs9jj
Jun 22 17:12:43.432: INFO: Got endpoints: latency-svc-h57gn [47.496761ms]
Jun 22 17:12:43.445: INFO: Got endpoints: latency-svc-bs9jj [208.547486ms]
Jun 22 17:12:43.448: INFO: Created: latency-svc-msjx8
Jun 22 17:12:43.460: INFO: Created: latency-svc-xtg7g
Jun 22 17:12:43.467: INFO: Got endpoints: latency-svc-msjx8 [210.366113ms]
Jun 22 17:12:43.470: INFO: Created: latency-svc-6qf5s
Jun 22 17:12:43.477: INFO: Got endpoints: latency-svc-xtg7g [209.8693ms]
Jun 22 17:12:43.481: INFO: Created: latency-svc-rx5fl
Jun 22 17:12:43.488: INFO: Got endpoints: latency-svc-6qf5s [213.23334ms]
Jun 22 17:12:43.492: INFO: Created: latency-svc-spg4j
Jun 22 17:12:43.495: INFO: Got endpoints: latency-svc-rx5fl [205.552554ms]
Jun 22 17:12:43.503: INFO: Created: latency-svc-8hscc
Jun 22 17:12:43.506: INFO: Got endpoints: latency-svc-spg4j [205.650185ms]
Jun 22 17:12:43.513: INFO: Created: latency-svc-4gnnl
Jun 22 17:12:43.515: INFO: Got endpoints: latency-svc-8hscc [201.898433ms]
Jun 22 17:12:43.528: INFO: Created: latency-svc-g8lfh
Jun 22 17:12:43.529: INFO: Got endpoints: latency-svc-4gnnl [199.224733ms]
Jun 22 17:12:43.551: INFO: Created: latency-svc-pxpc9
Jun 22 17:12:43.551: INFO: Got endpoints: latency-svc-g8lfh [215.810056ms]
Jun 22 17:12:43.556: INFO: Got endpoints: latency-svc-pxpc9 [200.390637ms]
Jun 22 17:12:43.558: INFO: Created: latency-svc-bqn6j
Jun 22 17:12:43.574: INFO: Created: latency-svc-qbsk4
Jun 22 17:12:43.575: INFO: Got endpoints: latency-svc-bqn6j [209.696091ms]
Jun 22 17:12:43.582: INFO: Created: latency-svc-8zh8c
Jun 22 17:12:43.583: INFO: Got endpoints: latency-svc-qbsk4 [192.606754ms]
Jun 22 17:12:43.590: INFO: Got endpoints: latency-svc-8zh8c [180.700849ms]
Jun 22 17:12:43.594: INFO: Created: latency-svc-kfvqj
Jun 22 17:12:43.604: INFO: Created: latency-svc-fth5x
Jun 22 17:12:43.608: INFO: Got endpoints: latency-svc-kfvqj [188.211476ms]
Jun 22 17:12:43.616: INFO: Got endpoints: latency-svc-fth5x [184.140093ms]
Jun 22 17:12:43.621: INFO: Created: latency-svc-pj2rb
Jun 22 17:12:43.633: INFO: Got endpoints: latency-svc-pj2rb [188.587542ms]
Jun 22 17:12:43.636: INFO: Created: latency-svc-tslzx
Jun 22 17:12:43.648: INFO: Got endpoints: latency-svc-tslzx [181.155204ms]
Jun 22 17:12:43.649: INFO: Created: latency-svc-2w9cd
Jun 22 17:12:43.657: INFO: Got endpoints: latency-svc-2w9cd [179.259639ms]
Jun 22 17:12:43.661: INFO: Created: latency-svc-t29xz
Jun 22 17:12:43.673: INFO: Created: latency-svc-2t8f9
Jun 22 17:12:43.673: INFO: Got endpoints: latency-svc-t29xz [184.88863ms]
Jun 22 17:12:43.685: INFO: Created: latency-svc-rqmkv
Jun 22 17:12:43.685: INFO: Got endpoints: latency-svc-2t8f9 [189.80077ms]
Jun 22 17:12:43.695: INFO: Created: latency-svc-ls2r5
Jun 22 17:12:43.697: INFO: Got endpoints: latency-svc-rqmkv [191.02943ms]
Jun 22 17:12:43.710: INFO: Got endpoints: latency-svc-ls2r5 [194.703571ms]
Jun 22 17:12:43.717: INFO: Created: latency-svc-8blks
Jun 22 17:12:43.727: INFO: Created: latency-svc-lt92w
Jun 22 17:12:43.729: INFO: Got endpoints: latency-svc-8blks [200.386856ms]
Jun 22 17:12:43.743: INFO: Created: latency-svc-pwjqq
Jun 22 17:12:43.743: INFO: Got endpoints: latency-svc-lt92w [191.520835ms]
Jun 22 17:12:43.753: INFO: Got endpoints: latency-svc-pwjqq [196.74595ms]
Jun 22 17:12:43.758: INFO: Created: latency-svc-tbflb
Jun 22 17:12:43.769: INFO: Created: latency-svc-glvdj
Jun 22 17:12:43.770: INFO: Got endpoints: latency-svc-tbflb [195.315855ms]
Jun 22 17:12:43.779: INFO: Got endpoints: latency-svc-glvdj [196.217184ms]
Jun 22 17:12:43.780: INFO: Created: latency-svc-w4hg8
Jun 22 17:12:43.792: INFO: Got endpoints: latency-svc-w4hg8 [201.726618ms]
Jun 22 17:12:43.793: INFO: Created: latency-svc-llrxb
Jun 22 17:12:43.803: INFO: Got endpoints: latency-svc-llrxb [194.54944ms]
Jun 22 17:12:43.804: INFO: Created: latency-svc-zp5kv
Jun 22 17:12:43.815: INFO: Created: latency-svc-jfm26
Jun 22 17:12:43.817: INFO: Got endpoints: latency-svc-zp5kv [200.858809ms]
Jun 22 17:12:43.829: INFO: Got endpoints: latency-svc-jfm26 [195.33137ms]
Jun 22 17:12:43.830: INFO: Created: latency-svc-9m25v
Jun 22 17:12:43.841: INFO: Created: latency-svc-ftxff
Jun 22 17:12:43.843: INFO: Got endpoints: latency-svc-9m25v [195.332952ms]
Jun 22 17:12:43.848: INFO: Got endpoints: latency-svc-ftxff [191.41449ms]
Jun 22 17:12:43.850: INFO: Created: latency-svc-lxnmm
Jun 22 17:12:43.862: INFO: Created: latency-svc-qg8jg
Jun 22 17:12:43.863: INFO: Got endpoints: latency-svc-lxnmm [189.680104ms]
Jun 22 17:12:43.872: INFO: Created: latency-svc-68l8f
Jun 22 17:12:43.875: INFO: Got endpoints: latency-svc-qg8jg [189.908459ms]
Jun 22 17:12:43.882: INFO: Got endpoints: latency-svc-68l8f [185.460365ms]
Jun 22 17:12:43.885: INFO: Created: latency-svc-cfdng
Jun 22 17:12:43.908: INFO: Created: latency-svc-t6n44
Jun 22 17:12:43.908: INFO: Got endpoints: latency-svc-cfdng [198.110344ms]
Jun 22 17:12:43.910: INFO: Created: latency-svc-g82dp
Jun 22 17:12:43.911: INFO: Got endpoints: latency-svc-g82dp [182.120454ms]
Jun 22 17:12:43.920: INFO: Got endpoints: latency-svc-t6n44 [177.575641ms]
Jun 22 17:12:43.922: INFO: Created: latency-svc-qbzn6
Jun 22 17:12:43.932: INFO: Got endpoints: latency-svc-qbzn6 [178.573742ms]
Jun 22 17:12:43.933: INFO: Created: latency-svc-zkrp9
Jun 22 17:12:43.947: INFO: Got endpoints: latency-svc-zkrp9 [176.733407ms]
Jun 22 17:12:43.948: INFO: Created: latency-svc-945s5
Jun 22 17:12:43.954: INFO: Created: latency-svc-684hg
Jun 22 17:12:43.959: INFO: Got endpoints: latency-svc-945s5 [179.193213ms]
Jun 22 17:12:43.968: INFO: Got endpoints: latency-svc-684hg [175.301037ms]
Jun 22 17:12:43.971: INFO: Created: latency-svc-8l5md
Jun 22 17:12:43.983: INFO: Got endpoints: latency-svc-8l5md [180.58598ms]
Jun 22 17:12:43.987: INFO: Created: latency-svc-vqf2s
Jun 22 17:12:44.000: INFO: Got endpoints: latency-svc-vqf2s [182.890084ms]
Jun 22 17:12:44.001: INFO: Created: latency-svc-fnlqp
Jun 22 17:12:44.008: INFO: Got endpoints: latency-svc-fnlqp [179.467778ms]
Jun 22 17:12:44.015: INFO: Created: latency-svc-z27vp
Jun 22 17:12:44.023: INFO: Created: latency-svc-zs57c
Jun 22 17:12:44.024: INFO: Got endpoints: latency-svc-z27vp [180.361969ms]
Jun 22 17:12:44.034: INFO: Created: latency-svc-7jw9t
Jun 22 17:12:44.034: INFO: Got endpoints: latency-svc-zs57c [185.897262ms]
Jun 22 17:12:44.046: INFO: Created: latency-svc-72v28
Jun 22 17:12:44.046: INFO: Got endpoints: latency-svc-7jw9t [183.173938ms]
Jun 22 17:12:44.057: INFO: Created: latency-svc-q5htw
Jun 22 17:12:44.058: INFO: Got endpoints: latency-svc-72v28 [183.431917ms]
Jun 22 17:12:44.068: INFO: Created: latency-svc-h7kcc
Jun 22 17:12:44.069: INFO: Got endpoints: latency-svc-q5htw [186.544378ms]
Jun 22 17:12:44.081: INFO: Got endpoints: latency-svc-h7kcc [172.352311ms]
Jun 22 17:12:44.081: INFO: Created: latency-svc-m9ff5
Jun 22 17:12:44.092: INFO: Created: latency-svc-8hqt9
Jun 22 17:12:44.093: INFO: Got endpoints: latency-svc-m9ff5 [181.794985ms]
Jun 22 17:12:44.108: INFO: Got endpoints: latency-svc-8hqt9 [187.58694ms]
Jun 22 17:12:44.108: INFO: Created: latency-svc-h9gxq
Jun 22 17:12:44.112: INFO: Got endpoints: latency-svc-h9gxq [180.52873ms]
Jun 22 17:12:44.116: INFO: Created: latency-svc-wzlz2
Jun 22 17:12:44.129: INFO: Created: latency-svc-9qb5w
Jun 22 17:12:44.129: INFO: Got endpoints: latency-svc-wzlz2 [182.519048ms]
Jun 22 17:12:44.139: INFO: Created: latency-svc-qmnp9
Jun 22 17:12:44.141: INFO: Got endpoints: latency-svc-9qb5w [182.360437ms]
Jun 22 17:12:44.153: INFO: Created: latency-svc-7fw9h
Jun 22 17:12:44.154: INFO: Got endpoints: latency-svc-qmnp9 [186.217378ms]
Jun 22 17:12:44.163: INFO: Created: latency-svc-4dbr5
Jun 22 17:12:44.166: INFO: Got endpoints: latency-svc-7fw9h [182.863364ms]
Jun 22 17:12:44.184: INFO: Got endpoints: latency-svc-4dbr5 [184.308606ms]
Jun 22 17:12:44.185: INFO: Created: latency-svc-8qfjs
Jun 22 17:12:44.187: INFO: Got endpoints: latency-svc-8qfjs [178.24337ms]
Jun 22 17:12:44.196: INFO: Created: latency-svc-mjgvp
Jun 22 17:12:44.198: INFO: Created: latency-svc-f9n72
Jun 22 17:12:44.201: INFO: Got endpoints: latency-svc-mjgvp [176.833969ms]
Jun 22 17:12:44.210: INFO: Created: latency-svc-b4kqg
Jun 22 17:12:44.212: INFO: Got endpoints: latency-svc-f9n72 [177.441507ms]
Jun 22 17:12:44.222: INFO: Created: latency-svc-9fwhm
Jun 22 17:12:44.223: INFO: Got endpoints: latency-svc-b4kqg [177.345981ms]
Jun 22 17:12:44.234: INFO: Created: latency-svc-7qnrg
Jun 22 17:12:44.235: INFO: Got endpoints: latency-svc-9fwhm [177.064136ms]
Jun 22 17:12:44.245: INFO: Created: latency-svc-g4n44
Jun 22 17:12:44.246: INFO: Got endpoints: latency-svc-7qnrg [177.373322ms]
Jun 22 17:12:44.256: INFO: Created: latency-svc-2c92g
Jun 22 17:12:44.259: INFO: Got endpoints: latency-svc-g4n44 [178.659777ms]
Jun 22 17:12:44.268: INFO: Got endpoints: latency-svc-2c92g [174.6606ms]
Jun 22 17:12:44.269: INFO: Created: latency-svc-7mkdl
Jun 22 17:12:44.280: INFO: Got endpoints: latency-svc-7mkdl [171.86328ms]
Jun 22 17:12:44.281: INFO: Created: latency-svc-cs7vf
Jun 22 17:12:44.292: INFO: Created: latency-svc-8qrnc
Jun 22 17:12:44.292: INFO: Got endpoints: latency-svc-cs7vf [179.373549ms]
Jun 22 17:12:44.304: INFO: Created: latency-svc-pbql2
Jun 22 17:12:44.305: INFO: Got endpoints: latency-svc-8qrnc [175.342876ms]
Jun 22 17:12:44.316: INFO: Got endpoints: latency-svc-pbql2 [174.930372ms]
Jun 22 17:12:44.318: INFO: Created: latency-svc-pfj74
Jun 22 17:12:44.329: INFO: Got endpoints: latency-svc-pfj74 [174.552182ms]
Jun 22 17:12:44.331: INFO: Created: latency-svc-95xfg
Jun 22 17:12:44.344: INFO: Created: latency-svc-pklb5
Jun 22 17:12:44.357: INFO: Got endpoints: latency-svc-pklb5 [172.38938ms]
Jun 22 17:12:44.357: INFO: Got endpoints: latency-svc-95xfg [190.633834ms]
Jun 22 17:12:44.368: INFO: Created: latency-svc-kcd9f
Jun 22 17:12:44.383: INFO: Created: latency-svc-7xnpc
Jun 22 17:12:44.385: INFO: Got endpoints: latency-svc-kcd9f [198.017878ms]
Jun 22 17:12:44.393: INFO: Got endpoints: latency-svc-7xnpc [192.08704ms]
Jun 22 17:12:44.396: INFO: Created: latency-svc-w46cx
Jun 22 17:12:44.411: INFO: Created: latency-svc-fhcm5
Jun 22 17:12:44.412: INFO: Got endpoints: latency-svc-w46cx [200.027064ms]
Jun 22 17:12:44.419: INFO: Created: latency-svc-9jffm
Jun 22 17:12:44.419: INFO: Got endpoints: latency-svc-fhcm5 [196.22515ms]
Jun 22 17:12:44.429: INFO: Created: latency-svc-wsd58
Jun 22 17:12:44.430: INFO: Got endpoints: latency-svc-9jffm [194.536786ms]
Jun 22 17:12:44.442: INFO: Created: latency-svc-xfggl
Jun 22 17:12:44.448: INFO: Got endpoints: latency-svc-wsd58 [201.138973ms]
Jun 22 17:12:44.454: INFO: Created: latency-svc-vhdbx
Jun 22 17:12:44.454: INFO: Got endpoints: latency-svc-xfggl [194.628088ms]
Jun 22 17:12:44.465: INFO: Created: latency-svc-fzpl7
Jun 22 17:12:44.469: INFO: Got endpoints: latency-svc-vhdbx [201.324702ms]
Jun 22 17:12:44.475: INFO: Created: latency-svc-slswj
Jun 22 17:12:44.477: INFO: Got endpoints: latency-svc-fzpl7 [196.524708ms]
Jun 22 17:12:44.488: INFO: Got endpoints: latency-svc-slswj [196.278096ms]
Jun 22 17:12:44.490: INFO: Created: latency-svc-k7hpf
Jun 22 17:12:44.500: INFO: Created: latency-svc-pvw2c
Jun 22 17:12:44.509: INFO: Got endpoints: latency-svc-k7hpf [203.63681ms]
Jun 22 17:12:44.512: INFO: Got endpoints: latency-svc-pvw2c [195.988733ms]
Jun 22 17:12:44.513: INFO: Created: latency-svc-jczrr
Jun 22 17:12:44.526: INFO: Got endpoints: latency-svc-jczrr [197.769541ms]
Jun 22 17:12:44.527: INFO: Created: latency-svc-52jtk
Jun 22 17:12:44.537: INFO: Got endpoints: latency-svc-52jtk [179.531379ms]
Jun 22 17:12:44.542: INFO: Created: latency-svc-6w5r2
Jun 22 17:12:44.553: INFO: Got endpoints: latency-svc-6w5r2 [195.901729ms]
Jun 22 17:12:44.558: INFO: Created: latency-svc-n2nd2
Jun 22 17:12:44.569: INFO: Created: latency-svc-tr5c6
Jun 22 17:12:44.571: INFO: Got endpoints: latency-svc-n2nd2 [186.316514ms]
Jun 22 17:12:44.579: INFO: Created: latency-svc-btffd
Jun 22 17:12:44.588: INFO: Got endpoints: latency-svc-tr5c6 [194.922345ms]
Jun 22 17:12:44.593: INFO: Got endpoints: latency-svc-btffd [180.984552ms]
Jun 22 17:12:44.598: INFO: Created: latency-svc-ctdfw
Jun 22 17:12:44.608: INFO: Created: latency-svc-rlc6r
Jun 22 17:12:44.611: INFO: Got endpoints: latency-svc-ctdfw [191.275301ms]
Jun 22 17:12:44.619: INFO: Got endpoints: latency-svc-rlc6r [188.890753ms]
Jun 22 17:12:44.625: INFO: Created: latency-svc-tj75f
Jun 22 17:12:44.635: INFO: Got endpoints: latency-svc-tj75f [187.127893ms]
Jun 22 17:12:44.635: INFO: Created: latency-svc-pjdkk
Jun 22 17:12:44.647: INFO: Created: latency-svc-pdsbc
Jun 22 17:12:44.647: INFO: Got endpoints: latency-svc-pjdkk [192.437641ms]
Jun 22 17:12:44.657: INFO: Created: latency-svc-gczbd
Jun 22 17:12:44.658: INFO: Got endpoints: latency-svc-pdsbc [188.457015ms]
Jun 22 17:12:44.669: INFO: Got endpoints: latency-svc-gczbd [192.114535ms]
Jun 22 17:12:44.673: INFO: Created: latency-svc-f4szz
Jun 22 17:12:44.681: INFO: Created: latency-svc-p8fvx
Jun 22 17:12:44.683: INFO: Got endpoints: latency-svc-f4szz [194.414099ms]
Jun 22 17:12:44.697: INFO: Got endpoints: latency-svc-p8fvx [188.576145ms]
Jun 22 17:12:44.697: INFO: Created: latency-svc-ql6b6
Jun 22 17:12:44.705: INFO: Got endpoints: latency-svc-ql6b6 [193.068969ms]
Jun 22 17:12:44.707: INFO: Created: latency-svc-q4d8q
Jun 22 17:12:44.717: INFO: Created: latency-svc-fx5mk
Jun 22 17:12:44.719: INFO: Got endpoints: latency-svc-q4d8q [192.800772ms]
Jun 22 17:12:44.727: INFO: Created: latency-svc-qbslt
Jun 22 17:12:44.733: INFO: Got endpoints: latency-svc-fx5mk [195.894095ms]
Jun 22 17:12:44.738: INFO: Got endpoints: latency-svc-qbslt [185.20974ms]
Jun 22 17:12:44.744: INFO: Created: latency-svc-hx9r4
Jun 22 17:12:44.751: INFO: Got endpoints: latency-svc-hx9r4 [179.37085ms]
Jun 22 17:12:44.756: INFO: Created: latency-svc-hrns7
Jun 22 17:12:44.768: INFO: Created: latency-svc-s66tv
Jun 22 17:12:44.769: INFO: Got endpoints: latency-svc-hrns7 [180.920739ms]
Jun 22 17:12:44.780: INFO: Created: latency-svc-6hv7h
Jun 22 17:12:44.800: INFO: Created: latency-svc-54zrr
Jun 22 17:12:44.800: INFO: Got endpoints: latency-svc-s66tv [207.331748ms]
Jun 22 17:12:44.800: INFO: Got endpoints: latency-svc-6hv7h [189.462761ms]
Jun 22 17:12:44.808: INFO: Created: latency-svc-5kzb8
Jun 22 17:12:44.808: INFO: Got endpoints: latency-svc-54zrr [189.230108ms]
Jun 22 17:12:44.816: INFO: Created: latency-svc-mg4zv
Jun 22 17:12:44.818: INFO: Got endpoints: latency-svc-5kzb8 [182.959974ms]
Jun 22 17:12:44.829: INFO: Got endpoints: latency-svc-mg4zv [182.3359ms]
Jun 22 17:12:44.829: INFO: Created: latency-svc-g9mbg
Jun 22 17:12:44.839: INFO: Created: latency-svc-rjk9m
Jun 22 17:12:44.852: INFO: Created: latency-svc-jdk87
Jun 22 17:12:44.857: INFO: Got endpoints: latency-svc-g9mbg [199.453673ms]
Jun 22 17:12:44.875: INFO: Created: latency-svc-2f9hb
Jun 22 17:12:44.881: INFO: Got endpoints: latency-svc-jdk87 [198.648091ms]
Jun 22 17:12:44.881: INFO: Got endpoints: latency-svc-rjk9m [212.656987ms]
Jun 22 17:12:44.886: INFO: Got endpoints: latency-svc-2f9hb [188.895112ms]
Jun 22 17:12:44.887: INFO: Created: latency-svc-rvdwg
Jun 22 17:12:44.898: INFO: Created: latency-svc-rkgbj
Jun 22 17:12:44.899: INFO: Got endpoints: latency-svc-rvdwg [193.461251ms]
Jun 22 17:12:44.911: INFO: Created: latency-svc-z4wlq
Jun 22 17:12:44.916: INFO: Got endpoints: latency-svc-rkgbj [195.673008ms]
Jun 22 17:12:44.924: INFO: Got endpoints: latency-svc-z4wlq [191.375627ms]
Jun 22 17:12:44.925: INFO: Created: latency-svc-42lj8
Jun 22 17:12:44.944: INFO: Created: latency-svc-zbqpn
Jun 22 17:12:44.946: INFO: Got endpoints: latency-svc-42lj8 [208.125122ms]
Jun 22 17:12:44.950: INFO: Got endpoints: latency-svc-zbqpn [199.160069ms]
Jun 22 17:12:44.958: INFO: Created: latency-svc-mnng9
Jun 22 17:12:44.969: INFO: Got endpoints: latency-svc-mnng9 [200.117581ms]
Jun 22 17:12:44.970: INFO: Created: latency-svc-2zkwx
Jun 22 17:12:44.986: INFO: Created: latency-svc-4nh5f
Jun 22 17:12:44.986: INFO: Got endpoints: latency-svc-2zkwx [185.355607ms]
Jun 22 17:12:44.997: INFO: Got endpoints: latency-svc-4nh5f [197.052299ms]
Jun 22 17:12:44.999: INFO: Created: latency-svc-q87xq
Jun 22 17:12:45.010: INFO: Created: latency-svc-5j64w
Jun 22 17:12:45.010: INFO: Got endpoints: latency-svc-q87xq [201.175183ms]
Jun 22 17:12:45.020: INFO: Created: latency-svc-xjg5z
Jun 22 17:12:45.029: INFO: Got endpoints: latency-svc-5j64w [211.454458ms]
Jun 22 17:12:45.031: INFO: Created: latency-svc-6m89g
Jun 22 17:12:45.037: INFO: Got endpoints: latency-svc-xjg5z [207.94004ms]
Jun 22 17:12:45.048: INFO: Got endpoints: latency-svc-6m89g [191.133217ms]
Jun 22 17:12:45.049: INFO: Created: latency-svc-l8brg
Jun 22 17:12:45.055: INFO: Created: latency-svc-v2znh
Jun 22 17:12:45.060: INFO: Got endpoints: latency-svc-l8brg [177.998893ms]
Jun 22 17:12:45.070: INFO: Got endpoints: latency-svc-v2znh [188.23363ms]
Jun 22 17:12:45.072: INFO: Created: latency-svc-q9njz
Jun 22 17:12:45.080: INFO: Created: latency-svc-5jc5r
Jun 22 17:12:45.086: INFO: Got endpoints: latency-svc-q9njz [199.269582ms]
Jun 22 17:12:45.089: INFO: Created: latency-svc-5hsbr
Jun 22 17:12:45.092: INFO: Got endpoints: latency-svc-5jc5r [192.621295ms]
Jun 22 17:12:45.106: INFO: Got endpoints: latency-svc-5hsbr [190.047971ms]
Jun 22 17:12:45.106: INFO: Created: latency-svc-f694m
Jun 22 17:12:45.118: INFO: Got endpoints: latency-svc-f694m [193.035347ms]
Jun 22 17:12:45.120: INFO: Created: latency-svc-lwxfl
Jun 22 17:12:45.126: INFO: Got endpoints: latency-svc-lwxfl [179.871712ms]
Jun 22 17:12:45.127: INFO: Created: latency-svc-wtr29
Jun 22 17:12:45.139: INFO: Created: latency-svc-fm67m
Jun 22 17:12:45.140: INFO: Got endpoints: latency-svc-wtr29 [190.363595ms]
Jun 22 17:12:45.150: INFO: Created: latency-svc-k29ww
Jun 22 17:12:45.153: INFO: Got endpoints: latency-svc-fm67m [183.164722ms]
Jun 22 17:12:45.169: INFO: Got endpoints: latency-svc-k29ww [183.025139ms]
Jun 22 17:12:45.169: INFO: Created: latency-svc-f6glk
Jun 22 17:12:45.176: INFO: Created: latency-svc-4tqwz
Jun 22 17:12:45.177: INFO: Got endpoints: latency-svc-f6glk [179.691528ms]
Jun 22 17:12:45.185: INFO: Got endpoints: latency-svc-4tqwz [175.064164ms]
Jun 22 17:12:45.187: INFO: Created: latency-svc-9z29j
Jun 22 17:12:45.195: INFO: Created: latency-svc-gzxdn
Jun 22 17:12:45.199: INFO: Got endpoints: latency-svc-9z29j [169.713014ms]
Jun 22 17:12:45.205: INFO: Got endpoints: latency-svc-gzxdn [167.703544ms]
Jun 22 17:12:45.211: INFO: Created: latency-svc-bvpsc
Jun 22 17:12:45.222: INFO: Created: latency-svc-nlcc7
Jun 22 17:12:45.225: INFO: Got endpoints: latency-svc-bvpsc [176.447172ms]
Jun 22 17:12:45.234: INFO: Created: latency-svc-jrctk
Jun 22 17:12:45.236: INFO: Got endpoints: latency-svc-nlcc7 [176.838254ms]
Jun 22 17:12:45.245: INFO: Created: latency-svc-k92r6
Jun 22 17:12:45.246: INFO: Got endpoints: latency-svc-jrctk [176.486446ms]
Jun 22 17:12:45.255: INFO: Created: latency-svc-c9hw6
Jun 22 17:12:45.271: INFO: Got endpoints: latency-svc-k92r6 [184.831297ms]
Jun 22 17:12:45.273: INFO: Got endpoints: latency-svc-c9hw6 [181.388235ms]
Jun 22 17:12:45.273: INFO: Created: latency-svc-kpds5
Jun 22 17:12:45.284: INFO: Created: latency-svc-vfzxp
Jun 22 17:12:45.288: INFO: Got endpoints: latency-svc-kpds5 [182.228621ms]
Jun 22 17:12:45.295: INFO: Created: latency-svc-chmxt
Jun 22 17:12:45.312: INFO: Created: latency-svc-mvwrp
Jun 22 17:12:45.312: INFO: Got endpoints: latency-svc-chmxt [186.011161ms]
Jun 22 17:12:45.313: INFO: Got endpoints: latency-svc-vfzxp [194.929297ms]
Jun 22 17:12:45.317: INFO: Got endpoints: latency-svc-mvwrp [176.692973ms]
Jun 22 17:12:45.317: INFO: Created: latency-svc-hg2pj
Jun 22 17:12:45.327: INFO: Got endpoints: latency-svc-hg2pj [174.680568ms]
Jun 22 17:12:45.331: INFO: Created: latency-svc-j8lgh
Jun 22 17:12:45.341: INFO: Got endpoints: latency-svc-j8lgh [172.439039ms]
Jun 22 17:12:45.343: INFO: Created: latency-svc-26gzj
Jun 22 17:12:45.358: INFO: Created: latency-svc-r4ndp
Jun 22 17:12:45.358: INFO: Got endpoints: latency-svc-26gzj [181.153868ms]
Jun 22 17:12:45.370: INFO: Got endpoints: latency-svc-r4ndp [184.729298ms]
Jun 22 17:12:45.370: INFO: Created: latency-svc-whlqz
Jun 22 17:12:45.382: INFO: Created: latency-svc-t55nm
Jun 22 17:12:45.383: INFO: Got endpoints: latency-svc-whlqz [184.192509ms]
Jun 22 17:12:45.399: INFO: Created: latency-svc-sngj8
Jun 22 17:12:45.399: INFO: Got endpoints: latency-svc-t55nm [194.094244ms]
Jun 22 17:12:45.409: INFO: Created: latency-svc-7xfv7
Jun 22 17:12:45.409: INFO: Got endpoints: latency-svc-sngj8 [184.439537ms]
Jun 22 17:12:45.449: INFO: Created: latency-svc-p5g7q
Jun 22 17:12:45.449: INFO: Created: latency-svc-j5zdk
Jun 22 17:12:45.449: INFO: Got endpoints: latency-svc-7xfv7 [212.805245ms]
Jun 22 17:12:45.449: INFO: Got endpoints: latency-svc-j5zdk [178.377333ms]
Jun 22 17:12:45.449: INFO: Got endpoints: latency-svc-p5g7q [202.856027ms]
Jun 22 17:12:45.449: INFO: Created: latency-svc-9gpdb
Jun 22 17:12:45.458: INFO: Created: latency-svc-hf7x9
Jun 22 17:12:45.461: INFO: Got endpoints: latency-svc-9gpdb [187.64213ms]
Jun 22 17:12:45.467: INFO: Created: latency-svc-lq72f
Jun 22 17:12:45.467: INFO: Got endpoints: latency-svc-hf7x9 [179.499607ms]
Jun 22 17:12:45.476: INFO: Got endpoints: latency-svc-lq72f [163.551017ms]
Jun 22 17:12:45.476: INFO: Latencies: [45.201656ms 47.496761ms 59.467734ms 68.115596ms 95.806225ms 95.986566ms 105.711749ms 113.874479ms 134.215768ms 137.11379ms 147.970326ms 160.138377ms 163.551017ms 167.703544ms 169.208382ms 169.713014ms 171.86328ms 172.352311ms 172.38938ms 172.439039ms 173.486456ms 173.662048ms 174.552182ms 174.6606ms 174.680568ms 174.930372ms 175.064164ms 175.301037ms 175.342876ms 176.447172ms 176.486446ms 176.692973ms 176.733407ms 176.828592ms 176.833969ms 176.838254ms 177.064136ms 177.345981ms 177.373322ms 177.441507ms 177.575641ms 177.998893ms 178.24337ms 178.377333ms 178.573742ms 178.659777ms 178.853441ms 179.193213ms 179.259639ms 179.37085ms 179.373549ms 179.467778ms 179.499607ms 179.531379ms 179.669059ms 179.691528ms 179.871712ms 180.361969ms 180.52873ms 180.58598ms 180.700849ms 180.920739ms 180.984552ms 181.153868ms 181.155204ms 181.388235ms 181.570447ms 181.794985ms 182.120454ms 182.228621ms 182.3359ms 182.360437ms 182.519048ms 182.771658ms 182.863364ms 182.890084ms 182.959974ms 183.025139ms 183.164722ms 183.173938ms 183.431917ms 183.736092ms 184.112519ms 184.140093ms 184.192509ms 184.308606ms 184.439537ms 184.729298ms 184.831297ms 184.88863ms 185.20974ms 185.355607ms 185.460365ms 185.64445ms 185.748108ms 185.844037ms 185.897262ms 186.011161ms 186.114287ms 186.217378ms 186.316514ms 186.471504ms 186.544378ms 186.548381ms 187.127893ms 187.58694ms 187.63747ms 187.64213ms 188.211476ms 188.23363ms 188.457015ms 188.576145ms 188.587542ms 188.890753ms 188.895112ms 188.909681ms 189.111727ms 189.230108ms 189.462761ms 189.680104ms 189.80077ms 189.908459ms 190.047971ms 190.363595ms 190.633834ms 191.02943ms 191.074037ms 191.133217ms 191.275301ms 191.375627ms 191.41449ms 191.520835ms 192.08704ms 192.114535ms 192.437641ms 192.606754ms 192.621295ms 192.797743ms 192.800772ms 192.942929ms 193.035347ms 193.068969ms 193.461251ms 194.094244ms 194.414099ms 194.536786ms 194.54944ms 194.628088ms 194.703571ms 194.922345ms 194.929297ms 195.315855ms 195.33137ms 195.332952ms 195.673008ms 195.894095ms 195.901729ms 195.988733ms 196.217184ms 196.22515ms 196.278096ms 196.524708ms 196.74595ms 197.052299ms 197.769541ms 198.017878ms 198.110344ms 198.648091ms 199.160069ms 199.224733ms 199.269582ms 199.453673ms 199.953819ms 200.027064ms 200.117581ms 200.386856ms 200.390637ms 200.858809ms 201.138973ms 201.175183ms 201.324702ms 201.726618ms 201.898433ms 202.856027ms 203.63681ms 205.552554ms 205.650185ms 207.331748ms 207.94004ms 208.125122ms 208.547486ms 209.696091ms 209.8693ms 210.366113ms 211.454458ms 212.656987ms 212.805245ms 213.23334ms 215.173758ms 215.810056ms]
Jun 22 17:12:45.476: INFO: 50 %ile: 186.316514ms
Jun 22 17:12:45.476: INFO: 90 %ile: 201.324702ms
Jun 22 17:12:45.477: INFO: 99 %ile: 215.173758ms
Jun 22 17:12:45.477: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:12:45.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5529" for this suite.
Jun 22 17:13:23.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:13:24.178: INFO: namespace svc-latency-5529 deletion completed in 38.672918702s

• [SLOW TEST:44.705 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:13:24.180: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 22 17:13:25.108: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 22 17:13:27.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442805, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442805, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442805, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442805, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 17:13:30.201: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:13:30.219: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:13:31.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1614" for this suite.
Jun 22 17:13:39.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:13:40.282: INFO: namespace crd-webhook-1614 deletion completed in 8.671129666s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.189 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:13:40.369: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 22 17:13:41.093: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-961 /api/v1/namespaces/watch-961/configmaps/e2e-watch-test-label-changed 43db3db9-61f9-4fc9-b680-3d79e1f41eb8 45541 0 2020-06-22 17:13:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 17:13:41.093: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-961 /api/v1/namespaces/watch-961/configmaps/e2e-watch-test-label-changed 43db3db9-61f9-4fc9-b680-3d79e1f41eb8 45542 0 2020-06-22 17:13:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 17:13:41.093: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-961 /api/v1/namespaces/watch-961/configmaps/e2e-watch-test-label-changed 43db3db9-61f9-4fc9-b680-3d79e1f41eb8 45543 0 2020-06-22 17:13:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 22 17:13:51.242: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-961 /api/v1/namespaces/watch-961/configmaps/e2e-watch-test-label-changed 43db3db9-61f9-4fc9-b680-3d79e1f41eb8 45559 0 2020-06-22 17:13:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 17:13:51.243: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-961 /api/v1/namespaces/watch-961/configmaps/e2e-watch-test-label-changed 43db3db9-61f9-4fc9-b680-3d79e1f41eb8 45560 0 2020-06-22 17:13:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 22 17:13:51.243: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-961 /api/v1/namespaces/watch-961/configmaps/e2e-watch-test-label-changed 43db3db9-61f9-4fc9-b680-3d79e1f41eb8 45561 0 2020-06-22 17:13:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:13:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-961" for this suite.
Jun 22 17:13:57.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:13:57.946: INFO: namespace watch-961 deletion completed in 6.67826758s

• [SLOW TEST:17.577 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:13:57.947: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3936
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jun 22 17:13:58.212: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:14:19.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3936" for this suite.
Jun 22 17:14:25.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:14:26.291: INFO: namespace crd-publish-openapi-3936 deletion completed in 6.881197169s

• [SLOW TEST:28.344 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:14:26.292: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 22 17:14:29.230: INFO: Successfully updated pod "labelsupdate5d6b0e22-8130-4138-8ec4-3f4f916e8650"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:14:33.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7040" for this suite.
Jun 22 17:15:03.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:15:03.986: INFO: namespace downward-api-7040 deletion completed in 30.626018714s

• [SLOW TEST:37.694 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:15:03.986: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-3285cb0d-e623-4a16-8c39-c17830249176
STEP: Creating a pod to test consume secrets
Jun 22 17:15:04.293: INFO: Waiting up to 5m0s for pod "pod-secrets-5b0fee0e-95f7-453e-9715-8b4514bcf351" in namespace "secrets-2009" to be "success or failure"
Jun 22 17:15:04.306: INFO: Pod "pod-secrets-5b0fee0e-95f7-453e-9715-8b4514bcf351": Phase="Pending", Reason="", readiness=false. Elapsed: 13.021153ms
Jun 22 17:15:06.321: INFO: Pod "pod-secrets-5b0fee0e-95f7-453e-9715-8b4514bcf351": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028195018s
STEP: Saw pod success
Jun 22 17:15:06.322: INFO: Pod "pod-secrets-5b0fee0e-95f7-453e-9715-8b4514bcf351" satisfied condition "success or failure"
Jun 22 17:15:06.333: INFO: Trying to get logs from node 10.45.191.150 pod pod-secrets-5b0fee0e-95f7-453e-9715-8b4514bcf351 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 17:15:06.400: INFO: Waiting for pod pod-secrets-5b0fee0e-95f7-453e-9715-8b4514bcf351 to disappear
Jun 22 17:15:06.412: INFO: Pod pod-secrets-5b0fee0e-95f7-453e-9715-8b4514bcf351 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:15:06.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2009" for this suite.
Jun 22 17:15:12.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:15:13.084: INFO: namespace secrets-2009 deletion completed in 6.636394589s

• [SLOW TEST:9.097 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:15:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun 22 17:15:53.524: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:15:53.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0622 17:15:53.524456      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6967" for this suite.
Jun 22 17:16:01.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:16:02.231: INFO: namespace gc-6967 deletion completed in 8.689024728s

• [SLOW TEST:49.147 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:16:02.234: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 17:16:03.241: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 17:16:05.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442963, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442963, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442963, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728442963, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 17:16:08.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:16:08.351: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1681-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:16:09.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4119" for this suite.
Jun 22 17:16:17.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:16:17.823: INFO: namespace webhook-4119 deletion completed in 8.620057613s
STEP: Destroying namespace "webhook-4119-markers" for this suite.
Jun 22 17:16:23.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:16:24.743: INFO: namespace webhook-4119-markers deletion completed in 6.919790525s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:22.588 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:16:24.822: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-bfbf8cca-deba-4c64-bf13-10bb395260a6
STEP: Creating a pod to test consume secrets
Jun 22 17:16:25.120: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a" in namespace "projected-8260" to be "success or failure"
Jun 22 17:16:25.132: INFO: Pod "pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.879452ms
Jun 22 17:16:27.145: INFO: Pod "pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024579345s
Jun 22 17:16:29.158: INFO: Pod "pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037651737s
STEP: Saw pod success
Jun 22 17:16:29.158: INFO: Pod "pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a" satisfied condition "success or failure"
Jun 22 17:16:29.169: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 17:16:29.232: INFO: Waiting for pod pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a to disappear
Jun 22 17:16:29.246: INFO: Pod pod-projected-secrets-5e056365-dc21-4c00-8f37-7b613c3ee55a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:16:29.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8260" for this suite.
Jun 22 17:16:35.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:16:35.912: INFO: namespace projected-8260 deletion completed in 6.642008419s

• [SLOW TEST:11.091 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:16:35.913: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2423
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:16:36.182: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Creating first CR 
Jun 22 17:16:36.914: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T17:16:36Z generation:1 name:name1 resourceVersion:46325 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:dccbb1d1-0a23-4314-8ec1-3bd418e64a90] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 22 17:16:46.939: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T17:16:46Z generation:1 name:name2 resourceVersion:46340 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6a15fa9b-21d6-42f5-b273-1acae69f5600] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 22 17:16:56.961: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T17:16:36Z generation:2 name:name1 resourceVersion:46353 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:dccbb1d1-0a23-4314-8ec1-3bd418e64a90] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 22 17:17:06.985: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T17:16:46Z generation:2 name:name2 resourceVersion:46367 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6a15fa9b-21d6-42f5-b273-1acae69f5600] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 22 17:17:17.031: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T17:16:36Z generation:2 name:name1 resourceVersion:46381 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:dccbb1d1-0a23-4314-8ec1-3bd418e64a90] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 22 17:17:27.076: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-22T17:16:46Z generation:2 name:name2 resourceVersion:46394 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6a15fa9b-21d6-42f5-b273-1acae69f5600] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:17:37.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2423" for this suite.
Jun 22 17:17:45.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:17:46.274: INFO: namespace crd-watch-2423 deletion completed in 8.624787264s

• [SLOW TEST:70.362 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:17:46.276: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 22 17:17:47.589: INFO: Pod name wrapped-volume-race-d85bdb01-28ab-4a00-a567-0e03fb179ea3: Found 0 pods out of 5
Jun 22 17:17:52.609: INFO: Pod name wrapped-volume-race-d85bdb01-28ab-4a00-a567-0e03fb179ea3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d85bdb01-28ab-4a00-a567-0e03fb179ea3 in namespace emptydir-wrapper-5578, will wait for the garbage collector to delete the pods
Jun 22 17:17:52.785: INFO: Deleting ReplicationController wrapped-volume-race-d85bdb01-28ab-4a00-a567-0e03fb179ea3 took: 44.270924ms
Jun 22 17:17:52.986: INFO: Terminating ReplicationController wrapped-volume-race-d85bdb01-28ab-4a00-a567-0e03fb179ea3 pods took: 200.289621ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 17:18:29.062: INFO: Pod name wrapped-volume-race-1194df61-ffff-4bf3-953d-b6ec73f47aa9: Found 0 pods out of 5
Jun 22 17:18:34.083: INFO: Pod name wrapped-volume-race-1194df61-ffff-4bf3-953d-b6ec73f47aa9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1194df61-ffff-4bf3-953d-b6ec73f47aa9 in namespace emptydir-wrapper-5578, will wait for the garbage collector to delete the pods
Jun 22 17:18:34.260: INFO: Deleting ReplicationController wrapped-volume-race-1194df61-ffff-4bf3-953d-b6ec73f47aa9 took: 39.74601ms
Jun 22 17:18:34.461: INFO: Terminating ReplicationController wrapped-volume-race-1194df61-ffff-4bf3-953d-b6ec73f47aa9 pods took: 200.316291ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 17:19:18.955: INFO: Pod name wrapped-volume-race-1fe9af6b-6e0b-45d6-afba-2b40d3864606: Found 0 pods out of 5
Jun 22 17:19:23.977: INFO: Pod name wrapped-volume-race-1fe9af6b-6e0b-45d6-afba-2b40d3864606: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1fe9af6b-6e0b-45d6-afba-2b40d3864606 in namespace emptydir-wrapper-5578, will wait for the garbage collector to delete the pods
Jun 22 17:19:24.159: INFO: Deleting ReplicationController wrapped-volume-race-1fe9af6b-6e0b-45d6-afba-2b40d3864606 took: 54.943097ms
Jun 22 17:19:24.360: INFO: Terminating ReplicationController wrapped-volume-race-1fe9af6b-6e0b-45d6-afba-2b40d3864606 pods took: 200.367062ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:20:01.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5578" for this suite.
Jun 22 17:20:09.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:20:09.940: INFO: namespace emptydir-wrapper-5578 deletion completed in 8.654045416s

• [SLOW TEST:143.664 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:20:09.940: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 22 17:20:16.583: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0622 17:20:16.583837      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 17:20:16.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3869" for this suite.
Jun 22 17:20:24.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:20:25.316: INFO: namespace gc-3869 deletion completed in 8.710788397s

• [SLOW TEST:15.376 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:20:25.317: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 17:20:25.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f" in namespace "downward-api-2316" to be "success or failure"
Jun 22 17:20:25.629: INFO: Pod "downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.55266ms
Jun 22 17:20:27.641: INFO: Pod "downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024375499s
Jun 22 17:20:29.653: INFO: Pod "downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036823177s
STEP: Saw pod success
Jun 22 17:20:29.653: INFO: Pod "downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f" satisfied condition "success or failure"
Jun 22 17:20:29.666: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f container client-container: <nil>
STEP: delete the pod
Jun 22 17:20:29.771: INFO: Waiting for pod downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f to disappear
Jun 22 17:20:29.783: INFO: Pod downwardapi-volume-36758443-b614-46eb-8ad6-038d2d3c7f2f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:20:29.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2316" for this suite.
Jun 22 17:20:35.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:20:36.445: INFO: namespace downward-api-2316 deletion completed in 6.629051536s

• [SLOW TEST:11.128 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:20:36.445: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff
Jun 22 17:20:36.732: INFO: Pod name my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff: Found 0 pods out of 1
Jun 22 17:20:41.747: INFO: Pod name my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff: Found 1 pods out of 1
Jun 22 17:20:41.747: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff" are running
Jun 22 17:20:41.759: INFO: Pod "my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff-nx2cs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 17:20:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 17:20:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 17:20:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-22 17:20:36 +0000 UTC Reason: Message:}])
Jun 22 17:20:41.759: INFO: Trying to dial the pod
Jun 22 17:20:46.811: INFO: Controller my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff: Got expected result from replica 1 [my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff-nx2cs]: "my-hostname-basic-f0291367-a1b2-415c-b808-670f3aaa90ff-nx2cs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:20:46.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1347" for this suite.
Jun 22 17:20:54.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:20:55.507: INFO: namespace replication-controller-1347 deletion completed in 8.669318933s

• [SLOW TEST:19.062 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:20:55.508: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:21:06.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8652" for this suite.
Jun 22 17:21:13.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:21:13.594: INFO: namespace resourcequota-8652 deletion completed in 6.60858883s

• [SLOW TEST:18.086 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:21:13.594: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Jun 22 17:21:13.855: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-114803800 proxy --unix-socket=/tmp/kubectl-proxy-unix668312131/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:21:13.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4402" for this suite.
Jun 22 17:21:20.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:21:20.863: INFO: namespace kubectl-4402 deletion completed in 6.915817826s

• [SLOW TEST:7.269 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:21:20.864: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:21:21.173: INFO: Waiting up to 5m0s for pod "busybox-user-65534-12417856-2ef2-4c65-8092-a4b1ce04aeed" in namespace "security-context-test-7084" to be "success or failure"
Jun 22 17:21:21.186: INFO: Pod "busybox-user-65534-12417856-2ef2-4c65-8092-a4b1ce04aeed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.245258ms
Jun 22 17:21:23.198: INFO: Pod "busybox-user-65534-12417856-2ef2-4c65-8092-a4b1ce04aeed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024293025s
Jun 22 17:21:23.198: INFO: Pod "busybox-user-65534-12417856-2ef2-4c65-8092-a4b1ce04aeed" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:21:23.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7084" for this suite.
Jun 22 17:21:29.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:21:29.858: INFO: namespace security-context-test-7084 deletion completed in 6.628324426s

• [SLOW TEST:8.994 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:21:29.859: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 22 17:21:30.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3588'
Jun 22 17:21:30.384: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 17:21:30.384: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jun 22 17:21:30.412: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-llwfh]
Jun 22 17:21:30.412: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-llwfh" in namespace "kubectl-3588" to be "running and ready"
Jun 22 17:21:30.434: INFO: Pod "e2e-test-httpd-rc-llwfh": Phase="Pending", Reason="", readiness=false. Elapsed: 21.850702ms
Jun 22 17:21:32.448: INFO: Pod "e2e-test-httpd-rc-llwfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.036682161s
Jun 22 17:21:32.448: INFO: Pod "e2e-test-httpd-rc-llwfh" satisfied condition "running and ready"
Jun 22 17:21:32.448: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-llwfh]
Jun 22 17:21:32.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 logs rc/e2e-test-httpd-rc --namespace=kubectl-3588'
Jun 22 17:21:32.741: INFO: stderr: ""
Jun 22 17:21:32.741: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.131.72. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.131.72. Set the 'ServerName' directive globally to suppress this message\n[Mon Jun 22 17:21:31.721389 2020] [mpm_event:notice] [pid 1:tid 140048924334952] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Jun 22 17:21:31.721447 2020] [core:notice] [pid 1:tid 140048924334952] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Jun 22 17:21:32.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 delete rc e2e-test-httpd-rc --namespace=kubectl-3588'
Jun 22 17:21:32.918: INFO: stderr: ""
Jun 22 17:21:32.918: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:21:32.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3588" for this suite.
Jun 22 17:21:45.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:21:45.597: INFO: namespace kubectl-3588 deletion completed in 12.652551534s

• [SLOW TEST:15.738 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:21:45.598: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 17:21:46.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 17:21:48.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728443306, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728443306, loc:(*time.Location)(0x78a6940)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63728443306, loc:(*time.Location)(0x78a6940)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63728443306, loc:(*time.Location)(0x78a6940)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 17:21:51.942: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:21:52.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4614" for this suite.
Jun 22 17:22:22.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:22:22.974: INFO: namespace webhook-4614 deletion completed in 30.604128202s
STEP: Destroying namespace "webhook-4614-markers" for this suite.
Jun 22 17:22:29.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:22:29.758: INFO: namespace webhook-4614-markers deletion completed in 6.784401692s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:44.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:22:29.844: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-439
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-6bd8ec30-908d-40bb-94cf-ede2da8ebca4
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:22:32.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-439" for this suite.
Jun 22 17:22:52.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:22:53.313: INFO: namespace configmap-439 deletion completed in 20.96487415s

• [SLOW TEST:23.469 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:22:53.313: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1079
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 17:22:53.622: INFO: Waiting up to 5m0s for pod "pod-7553b193-500b-4002-8fc1-78ab922983e1" in namespace "emptydir-1079" to be "success or failure"
Jun 22 17:22:53.636: INFO: Pod "pod-7553b193-500b-4002-8fc1-78ab922983e1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.156931ms
Jun 22 17:22:55.651: INFO: Pod "pod-7553b193-500b-4002-8fc1-78ab922983e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028204338s
STEP: Saw pod success
Jun 22 17:22:55.651: INFO: Pod "pod-7553b193-500b-4002-8fc1-78ab922983e1" satisfied condition "success or failure"
Jun 22 17:22:55.662: INFO: Trying to get logs from node 10.45.191.150 pod pod-7553b193-500b-4002-8fc1-78ab922983e1 container test-container: <nil>
STEP: delete the pod
Jun 22 17:22:55.722: INFO: Waiting for pod pod-7553b193-500b-4002-8fc1-78ab922983e1 to disappear
Jun 22 17:22:55.734: INFO: Pod pod-7553b193-500b-4002-8fc1-78ab922983e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:22:55.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1079" for this suite.
Jun 22 17:23:01.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:23:02.457: INFO: namespace emptydir-1079 deletion completed in 6.69664648s

• [SLOW TEST:9.144 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:23:02.458: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3526
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3526
I0622 17:23:02.865187      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3526, replica count: 2
Jun 22 17:23:05.918: INFO: Creating new exec pod
I0622 17:23:05.918028      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 17:23:10.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-3526 execpod9m79k -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 22 17:23:11.338: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 17:23:11.338: INFO: stdout: ""
Jun 22 17:23:11.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-3526 execpod9m79k -- /bin/sh -x -c nc -zv -t -w 2 172.21.30.140 80'
Jun 22 17:23:11.642: INFO: stderr: "+ nc -zv -t -w 2 172.21.30.140 80\nConnection to 172.21.30.140 80 port [tcp/http] succeeded!\n"
Jun 22 17:23:11.643: INFO: stdout: ""
Jun 22 17:23:11.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-3526 execpod9m79k -- /bin/sh -x -c nc -zv -t -w 2 10.45.191.131 32416'
Jun 22 17:23:11.952: INFO: stderr: "+ nc -zv -t -w 2 10.45.191.131 32416\nConnection to 10.45.191.131 32416 port [tcp/32416] succeeded!\n"
Jun 22 17:23:11.952: INFO: stdout: ""
Jun 22 17:23:11.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-3526 execpod9m79k -- /bin/sh -x -c nc -zv -t -w 2 10.45.191.149 32416'
Jun 22 17:23:12.330: INFO: stderr: "+ nc -zv -t -w 2 10.45.191.149 32416\nConnection to 10.45.191.149 32416 port [tcp/32416] succeeded!\n"
Jun 22 17:23:12.330: INFO: stdout: ""
Jun 22 17:23:12.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-3526 execpod9m79k -- /bin/sh -x -c nc -zv -t -w 2 158.175.96.162 32416'
Jun 22 17:23:12.677: INFO: stderr: "+ nc -zv -t -w 2 158.175.96.162 32416\nConnection to 158.175.96.162 32416 port [tcp/32416] succeeded!\n"
Jun 22 17:23:12.677: INFO: stdout: ""
Jun 22 17:23:12.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-3526 execpod9m79k -- /bin/sh -x -c nc -zv -t -w 2 158.175.96.168 32416'
Jun 22 17:23:12.991: INFO: stderr: "+ nc -zv -t -w 2 158.175.96.168 32416\nConnection to 158.175.96.168 32416 port [tcp/32416] succeeded!\n"
Jun 22 17:23:12.991: INFO: stdout: ""
Jun 22 17:23:12.991: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:23:13.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3526" for this suite.
Jun 22 17:23:21.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:23:21.753: INFO: namespace services-3526 deletion completed in 8.651459888s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:19.295 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:23:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6755
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6755
I0622 17:23:22.117278      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6755, replica count: 2
Jun 22 17:23:25.170: INFO: Creating new exec pod
I0622 17:23:25.170594      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 17:23:28.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-6755 execpodpfg5s -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 22 17:23:28.547: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 17:23:28.547: INFO: stdout: ""
Jun 22 17:23:28.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 exec --namespace=services-6755 execpodpfg5s -- /bin/sh -x -c nc -zv -t -w 2 172.21.76.129 80'
Jun 22 17:23:28.887: INFO: stderr: "+ nc -zv -t -w 2 172.21.76.129 80\nConnection to 172.21.76.129 80 port [tcp/http] succeeded!\n"
Jun 22 17:23:28.887: INFO: stdout: ""
Jun 22 17:23:28.887: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:23:28.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6755" for this suite.
Jun 22 17:23:37.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:23:37.660: INFO: namespace services-6755 deletion completed in 8.635348679s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.908 seconds]
[sig-network] Services
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:23:37.661: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-bcadd5a5-3702-4653-b9c5-b59c132a4039
STEP: Creating a pod to test consume secrets
Jun 22 17:23:37.972: INFO: Waiting up to 5m0s for pod "pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be" in namespace "secrets-9404" to be "success or failure"
Jun 22 17:23:37.983: INFO: Pod "pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be": Phase="Pending", Reason="", readiness=false. Elapsed: 11.538858ms
Jun 22 17:23:39.995: INFO: Pod "pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023026518s
Jun 22 17:23:42.010: INFO: Pod "pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038659281s
STEP: Saw pod success
Jun 22 17:23:42.010: INFO: Pod "pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be" satisfied condition "success or failure"
Jun 22 17:23:42.023: INFO: Trying to get logs from node 10.45.191.150 pod pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 17:23:42.092: INFO: Waiting for pod pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be to disappear
Jun 22 17:23:42.104: INFO: Pod pod-secrets-615ff67c-f055-4e90-b411-4857ab6570be no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:23:42.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9404" for this suite.
Jun 22 17:23:48.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:23:48.760: INFO: namespace secrets-9404 deletion completed in 6.628012179s

• [SLOW TEST:11.099 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:23:48.761: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8300.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8300.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 17:23:53.271: INFO: DNS probes using dns-8300/dns-test-2f726a3a-c755-486e-90ea-6a26da7398e9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:23:53.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8300" for this suite.
Jun 22 17:24:01.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:24:02.024: INFO: namespace dns-8300 deletion completed in 8.697064202s

• [SLOW TEST:13.264 seconds]
[sig-network] DNS
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:24:02.025: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 17:24:02.331: INFO: Waiting up to 5m0s for pod "pod-79a673d7-d3d0-42f2-b020-da392f5ee8f8" in namespace "emptydir-9119" to be "success or failure"
Jun 22 17:24:02.351: INFO: Pod "pod-79a673d7-d3d0-42f2-b020-da392f5ee8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.42933ms
Jun 22 17:24:04.363: INFO: Pod "pod-79a673d7-d3d0-42f2-b020-da392f5ee8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031684064s
STEP: Saw pod success
Jun 22 17:24:04.363: INFO: Pod "pod-79a673d7-d3d0-42f2-b020-da392f5ee8f8" satisfied condition "success or failure"
Jun 22 17:24:04.375: INFO: Trying to get logs from node 10.45.191.150 pod pod-79a673d7-d3d0-42f2-b020-da392f5ee8f8 container test-container: <nil>
STEP: delete the pod
Jun 22 17:24:04.444: INFO: Waiting for pod pod-79a673d7-d3d0-42f2-b020-da392f5ee8f8 to disappear
Jun 22 17:24:04.460: INFO: Pod pod-79a673d7-d3d0-42f2-b020-da392f5ee8f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:24:04.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9119" for this suite.
Jun 22 17:24:10.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:24:11.156: INFO: namespace emptydir-9119 deletion completed in 6.660482161s

• [SLOW TEST:9.131 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:24:11.156: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-423/secret-test-2d3f3cd6-fb25-4970-a255-1209c42098de
STEP: Creating a pod to test consume secrets
Jun 22 17:24:11.456: INFO: Waiting up to 5m0s for pod "pod-configmaps-6456e66b-762b-411e-b3d0-4d2badede166" in namespace "secrets-423" to be "success or failure"
Jun 22 17:24:11.471: INFO: Pod "pod-configmaps-6456e66b-762b-411e-b3d0-4d2badede166": Phase="Pending", Reason="", readiness=false. Elapsed: 15.120555ms
Jun 22 17:24:13.484: INFO: Pod "pod-configmaps-6456e66b-762b-411e-b3d0-4d2badede166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028654374s
STEP: Saw pod success
Jun 22 17:24:13.485: INFO: Pod "pod-configmaps-6456e66b-762b-411e-b3d0-4d2badede166" satisfied condition "success or failure"
Jun 22 17:24:13.495: INFO: Trying to get logs from node 10.45.191.150 pod pod-configmaps-6456e66b-762b-411e-b3d0-4d2badede166 container env-test: <nil>
STEP: delete the pod
Jun 22 17:24:13.560: INFO: Waiting for pod pod-configmaps-6456e66b-762b-411e-b3d0-4d2badede166 to disappear
Jun 22 17:24:13.571: INFO: Pod pod-configmaps-6456e66b-762b-411e-b3d0-4d2badede166 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:24:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-423" for this suite.
Jun 22 17:24:19.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:24:20.273: INFO: namespace secrets-423 deletion completed in 6.674893436s

• [SLOW TEST:9.117 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:24:20.278: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 22 17:24:23.659: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:24:23.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-236" for this suite.
Jun 22 17:24:53.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:24:54.377: INFO: namespace replicaset-236 deletion completed in 30.64274754s

• [SLOW TEST:34.099 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:24:54.379: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 17:24:54.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28503316-7543-4a41-8d32-2d85fc69e536" in namespace "downward-api-399" to be "success or failure"
Jun 22 17:24:54.728: INFO: Pod "downwardapi-volume-28503316-7543-4a41-8d32-2d85fc69e536": Phase="Pending", Reason="", readiness=false. Elapsed: 17.894234ms
Jun 22 17:24:56.741: INFO: Pod "downwardapi-volume-28503316-7543-4a41-8d32-2d85fc69e536": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030599503s
STEP: Saw pod success
Jun 22 17:24:56.741: INFO: Pod "downwardapi-volume-28503316-7543-4a41-8d32-2d85fc69e536" satisfied condition "success or failure"
Jun 22 17:24:56.753: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-28503316-7543-4a41-8d32-2d85fc69e536 container client-container: <nil>
STEP: delete the pod
Jun 22 17:24:56.811: INFO: Waiting for pod downwardapi-volume-28503316-7543-4a41-8d32-2d85fc69e536 to disappear
Jun 22 17:24:56.822: INFO: Pod downwardapi-volume-28503316-7543-4a41-8d32-2d85fc69e536 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:24:56.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-399" for this suite.
Jun 22 17:25:04.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:25:05.486: INFO: namespace downward-api-399 deletion completed in 8.639640575s

• [SLOW TEST:11.107 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:25:05.486: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-85v9
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 17:25:05.842: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-85v9" in namespace "subpath-9557" to be "success or failure"
Jun 22 17:25:05.855: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.795798ms
Jun 22 17:25:07.870: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 2.028560177s
Jun 22 17:25:09.883: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 4.041192305s
Jun 22 17:25:11.895: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 6.053588006s
Jun 22 17:25:13.910: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 8.068638639s
Jun 22 17:25:15.925: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.083075947s
Jun 22 17:25:17.941: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 12.098792659s
Jun 22 17:25:19.953: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 14.11143617s
Jun 22 17:25:21.965: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 16.123287562s
Jun 22 17:25:23.978: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 18.135890447s
Jun 22 17:25:25.989: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Running", Reason="", readiness=true. Elapsed: 20.147624727s
Jun 22 17:25:28.003: INFO: Pod "pod-subpath-test-secret-85v9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.160887703s
STEP: Saw pod success
Jun 22 17:25:28.003: INFO: Pod "pod-subpath-test-secret-85v9" satisfied condition "success or failure"
Jun 22 17:25:28.014: INFO: Trying to get logs from node 10.45.191.150 pod pod-subpath-test-secret-85v9 container test-container-subpath-secret-85v9: <nil>
STEP: delete the pod
Jun 22 17:25:28.073: INFO: Waiting for pod pod-subpath-test-secret-85v9 to disappear
Jun 22 17:25:28.085: INFO: Pod pod-subpath-test-secret-85v9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-85v9
Jun 22 17:25:28.085: INFO: Deleting pod "pod-subpath-test-secret-85v9" in namespace "subpath-9557"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:25:28.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9557" for this suite.
Jun 22 17:25:36.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:25:36.752: INFO: namespace subpath-9557 deletion completed in 8.63171803s

• [SLOW TEST:31.266 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:25:36.752: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9438
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 17:25:37.020: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 17:26:01.513: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.226.104:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9438 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:26:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 17:26:01.760: INFO: Found all expected endpoints: [netserver-0]
Jun 22 17:26:01.775: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.131.100:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9438 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:26:01.776: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 17:26:02.078: INFO: Found all expected endpoints: [netserver-1]
Jun 22 17:26:02.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.44.34:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9438 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:26:02.091: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 17:26:02.297: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:26:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9438" for this suite.
Jun 22 17:26:16.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:26:16.964: INFO: namespace pod-network-test-9438 deletion completed in 14.641702886s

• [SLOW TEST:40.212 seconds]
[sig-network] Networking
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:26:16.965: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5999
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 22 17:26:17.228: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
Jun 22 17:26:21.132: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:26:36.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5999" for this suite.
Jun 22 17:26:42.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:26:43.275: INFO: namespace crd-publish-openapi-5999 deletion completed in 6.616412785s

• [SLOW TEST:26.310 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:26:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 22 17:26:43.544: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:26:46.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-901" for this suite.
Jun 22 17:26:52.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:26:53.242: INFO: namespace init-container-901 deletion completed in 6.618858893s

• [SLOW TEST:9.966 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:26:53.242: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-d611cb41-5950-4d41-8c4e-7bad8b8cc54a in namespace container-probe-2463
Jun 22 17:26:57.568: INFO: Started pod liveness-d611cb41-5950-4d41-8c4e-7bad8b8cc54a in namespace container-probe-2463
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 17:26:57.579: INFO: Initial restart count of pod liveness-d611cb41-5950-4d41-8c4e-7bad8b8cc54a is 0
Jun 22 17:27:19.766: INFO: Restart count of pod container-probe-2463/liveness-d611cb41-5950-4d41-8c4e-7bad8b8cc54a is now 1 (22.187737784s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:27:19.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2463" for this suite.
Jun 22 17:27:25.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:27:26.458: INFO: namespace container-probe-2463 deletion completed in 6.635258384s

• [SLOW TEST:33.215 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:27:26.458: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 17:27:30.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:27:30.876: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:27:32.876: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:27:32.889: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:27:34.877: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:27:34.889: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:27:34.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2337" for this suite.
Jun 22 17:28:05.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:28:05.745: INFO: namespace container-lifecycle-hook-2337 deletion completed in 30.731005337s

• [SLOW TEST:39.287 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:28:05.746: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-4139f817-d40d-43f1-ab28-0e90fd7df103
STEP: Creating a pod to test consume secrets
Jun 22 17:28:06.067: INFO: Waiting up to 5m0s for pod "pod-secrets-fe548ef4-168c-40cc-8a56-a3484ccf9125" in namespace "secrets-776" to be "success or failure"
Jun 22 17:28:06.081: INFO: Pod "pod-secrets-fe548ef4-168c-40cc-8a56-a3484ccf9125": Phase="Pending", Reason="", readiness=false. Elapsed: 13.536985ms
Jun 22 17:28:08.094: INFO: Pod "pod-secrets-fe548ef4-168c-40cc-8a56-a3484ccf9125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026428656s
STEP: Saw pod success
Jun 22 17:28:08.094: INFO: Pod "pod-secrets-fe548ef4-168c-40cc-8a56-a3484ccf9125" satisfied condition "success or failure"
Jun 22 17:28:08.106: INFO: Trying to get logs from node 10.45.191.150 pod pod-secrets-fe548ef4-168c-40cc-8a56-a3484ccf9125 container secret-env-test: <nil>
STEP: delete the pod
Jun 22 17:28:08.172: INFO: Waiting for pod pod-secrets-fe548ef4-168c-40cc-8a56-a3484ccf9125 to disappear
Jun 22 17:28:08.184: INFO: Pod pod-secrets-fe548ef4-168c-40cc-8a56-a3484ccf9125 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:28:08.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-776" for this suite.
Jun 22 17:28:14.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:28:14.895: INFO: namespace secrets-776 deletion completed in 6.685427598s

• [SLOW TEST:9.150 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:28:14.895: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 22 17:28:15.311: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2857 /api/v1/namespaces/watch-2857/configmaps/e2e-watch-test-resource-version 00c87cae-55d2-4046-b9ca-c93bf406f4c6 49309 0 2020-06-22 17:28:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 17:28:15.311: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2857 /api/v1/namespaces/watch-2857/configmaps/e2e-watch-test-resource-version 00c87cae-55d2-4046-b9ca-c93bf406f4c6 49310 0 2020-06-22 17:28:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:28:15.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2857" for this suite.
Jun 22 17:28:21.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:28:21.999: INFO: namespace watch-2857 deletion completed in 6.661705168s

• [SLOW TEST:7.104 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:28:22.000: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 22 17:28:22.288: INFO: Waiting up to 5m0s for pod "downward-api-a26605ed-c68c-44e5-b633-9507a40d5063" in namespace "downward-api-3141" to be "success or failure"
Jun 22 17:28:22.302: INFO: Pod "downward-api-a26605ed-c68c-44e5-b633-9507a40d5063": Phase="Pending", Reason="", readiness=false. Elapsed: 13.889953ms
Jun 22 17:28:24.315: INFO: Pod "downward-api-a26605ed-c68c-44e5-b633-9507a40d5063": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02762522s
STEP: Saw pod success
Jun 22 17:28:24.315: INFO: Pod "downward-api-a26605ed-c68c-44e5-b633-9507a40d5063" satisfied condition "success or failure"
Jun 22 17:28:24.327: INFO: Trying to get logs from node 10.45.191.150 pod downward-api-a26605ed-c68c-44e5-b633-9507a40d5063 container dapi-container: <nil>
STEP: delete the pod
Jun 22 17:28:24.391: INFO: Waiting for pod downward-api-a26605ed-c68c-44e5-b633-9507a40d5063 to disappear
Jun 22 17:28:24.407: INFO: Pod downward-api-a26605ed-c68c-44e5-b633-9507a40d5063 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:28:24.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3141" for this suite.
Jun 22 17:28:30.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:28:31.381: INFO: namespace downward-api-3141 deletion completed in 6.94127379s

• [SLOW TEST:9.381 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:28:31.382: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9443
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 22 17:28:31.709: INFO: Waiting up to 5m0s for pod "downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0" in namespace "downward-api-9443" to be "success or failure"
Jun 22 17:28:31.722: INFO: Pod "downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.72355ms
Jun 22 17:28:33.735: INFO: Pod "downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025647277s
Jun 22 17:28:35.748: INFO: Pod "downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038132993s
STEP: Saw pod success
Jun 22 17:28:35.748: INFO: Pod "downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0" satisfied condition "success or failure"
Jun 22 17:28:35.763: INFO: Trying to get logs from node 10.45.191.150 pod downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0 container dapi-container: <nil>
STEP: delete the pod
Jun 22 17:28:35.825: INFO: Waiting for pod downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0 to disappear
Jun 22 17:28:35.838: INFO: Pod downward-api-160402f0-6f2c-4277-85c7-210f47fcc9e0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:28:35.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9443" for this suite.
Jun 22 17:28:43.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:28:44.502: INFO: namespace downward-api-9443 deletion completed in 8.636702928s

• [SLOW TEST:13.121 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:28:44.503: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2837
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:28:44.767: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 22 17:28:48.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 create -f -'
Jun 22 17:28:48.721: INFO: stderr: ""
Jun 22 17:28:48.721: INFO: stdout: "e2e-test-crd-publish-openapi-3488-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 17:28:48.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 delete e2e-test-crd-publish-openapi-3488-crds test-foo'
Jun 22 17:28:48.985: INFO: stderr: ""
Jun 22 17:28:48.985: INFO: stdout: "e2e-test-crd-publish-openapi-3488-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 22 17:28:48.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 apply -f -'
Jun 22 17:28:49.252: INFO: stderr: ""
Jun 22 17:28:49.252: INFO: stdout: "e2e-test-crd-publish-openapi-3488-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 17:28:49.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 delete e2e-test-crd-publish-openapi-3488-crds test-foo'
Jun 22 17:28:49.385: INFO: stderr: ""
Jun 22 17:28:49.385: INFO: stdout: "e2e-test-crd-publish-openapi-3488-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 22 17:28:49.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 create -f -'
Jun 22 17:28:49.690: INFO: rc: 1
Jun 22 17:28:49.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 apply -f -'
Jun 22 17:28:50.059: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 22 17:28:50.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 create -f -'
Jun 22 17:28:50.387: INFO: rc: 1
Jun 22 17:28:50.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 --namespace=crd-publish-openapi-2837 apply -f -'
Jun 22 17:28:50.746: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 22 17:28:50.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-3488-crds'
Jun 22 17:28:50.990: INFO: stderr: ""
Jun 22 17:28:50.990: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3488-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 22 17:28:50.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-3488-crds.metadata'
Jun 22 17:28:51.324: INFO: stderr: ""
Jun 22 17:28:51.325: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3488-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 22 17:28:51.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-3488-crds.spec'
Jun 22 17:28:51.549: INFO: stderr: ""
Jun 22 17:28:51.549: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3488-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 22 17:28:51.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-3488-crds.spec.bars'
Jun 22 17:28:51.918: INFO: stderr: ""
Jun 22 17:28:51.918: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3488-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 22 17:28:51.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-114803800 explain e2e-test-crd-publish-openapi-3488-crds.spec.bars2'
Jun 22 17:28:52.155: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:28:56.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2837" for this suite.
Jun 22 17:29:02.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:29:03.016: INFO: namespace crd-publish-openapi-2837 deletion completed in 6.929008953s

• [SLOW TEST:18.513 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:29:03.016: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 22 17:29:03.360: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4800 /api/v1/namespaces/watch-4800/configmaps/e2e-watch-test-watch-closed 96aa90df-2cff-4214-b2e1-b1b29b9d72e9 49489 0 2020-06-22 17:29:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 17:29:03.360: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4800 /api/v1/namespaces/watch-4800/configmaps/e2e-watch-test-watch-closed 96aa90df-2cff-4214-b2e1-b1b29b9d72e9 49490 0 2020-06-22 17:29:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 22 17:29:03.457: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4800 /api/v1/namespaces/watch-4800/configmaps/e2e-watch-test-watch-closed 96aa90df-2cff-4214-b2e1-b1b29b9d72e9 49491 0 2020-06-22 17:29:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 17:29:03.457: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4800 /api/v1/namespaces/watch-4800/configmaps/e2e-watch-test-watch-closed 96aa90df-2cff-4214-b2e1-b1b29b9d72e9 49492 0 2020-06-22 17:29:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:29:03.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4800" for this suite.
Jun 22 17:29:11.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:29:12.109: INFO: namespace watch-4800 deletion completed in 8.624547571s

• [SLOW TEST:9.093 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:29:12.109: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-35009c7c-3d9c-4747-bc89-9bbcb03888fc
STEP: Creating a pod to test consume configMaps
Jun 22 17:29:12.439: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48a03d71-a118-4772-abdb-4e315cac313c" in namespace "projected-9331" to be "success or failure"
Jun 22 17:29:12.455: INFO: Pod "pod-projected-configmaps-48a03d71-a118-4772-abdb-4e315cac313c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.337187ms
Jun 22 17:29:14.468: INFO: Pod "pod-projected-configmaps-48a03d71-a118-4772-abdb-4e315cac313c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028826358s
STEP: Saw pod success
Jun 22 17:29:14.468: INFO: Pod "pod-projected-configmaps-48a03d71-a118-4772-abdb-4e315cac313c" satisfied condition "success or failure"
Jun 22 17:29:14.479: INFO: Trying to get logs from node 10.45.191.150 pod pod-projected-configmaps-48a03d71-a118-4772-abdb-4e315cac313c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 17:29:14.554: INFO: Waiting for pod pod-projected-configmaps-48a03d71-a118-4772-abdb-4e315cac313c to disappear
Jun 22 17:29:14.566: INFO: Pod pod-projected-configmaps-48a03d71-a118-4772-abdb-4e315cac313c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:29:14.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9331" for this suite.
Jun 22 17:29:20.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:29:21.353: INFO: namespace projected-9331 deletion completed in 6.761893557s

• [SLOW TEST:9.244 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:29:21.353: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 22 17:29:21.620: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 17:29:21.683: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 17:29:21.702: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.131 before test
Jun 22 17:29:21.810: INFO: sonobuoy-e2e-job-d71e65d5f08d4631 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.810: INFO: 	Container e2e ready: true, restart count 0
Jun 22 17:29:21.810: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 17:29:21.810: INFO: coredns-697bf86f8c-gpt5n from kube-system started at 2020-06-22 16:24:23 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.810: INFO: 	Container coredns ready: true, restart count 0
Jun 22 17:29:21.810: INFO: ibm-master-proxy-static-10.45.191.131 from kube-system started at 2020-06-22 13:23:38 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.810: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 17:29:21.810: INFO: 	Container pause ready: true, restart count 0
Jun 22 17:29:21.811: INFO: ibm-keepalived-watcher-jmnzb from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.811: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 17:29:21.811: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-p5tk9 from ibm-system started at 2020-06-22 16:24:23 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.811: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 17:29:21.811: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-jfbn9 from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.811: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jun 22 17:29:21.811: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 17:29:21.811: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-h9wdw from kube-system started at 2020-06-22 16:24:23 +0000 UTC (4 container statuses recorded)
Jun 22 17:29:21.811: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 17:29:21.811: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 17:29:21.811: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 17:29:21.811: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 17:29:21.811: INFO: calico-node-s8lh4 from kube-system started at 2020-06-22 13:23:40 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.811: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 17:29:21.811: INFO: sonobuoy from sonobuoy started at 2020-06-22 15:11:50 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.811: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 17:29:21.811: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.149 before test
Jun 22 17:29:21.907: INFO: metrics-server-759d6d9f96-q5lgr from kube-system started at 2020-06-22 13:23:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 17:29:21.907: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun 22 17:29:21.907: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-7gntq from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jun 22 17:29:21.907: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 17:29:21.907: INFO: ibm-cloud-provider-ip-158-175-112-94-55bb6c8447-7m74w from ibm-system started at 2020-06-22 15:47:30 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container ibm-cloud-provider-ip-158-175-112-94 ready: true, restart count 0
Jun 22 17:29:21.907: INFO: coredns-697bf86f8c-2z94m from kube-system started at 2020-06-22 13:47:52 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container coredns ready: true, restart count 0
Jun 22 17:29:21.907: INFO: catalog-operator-57d89fd5c4-bdgnq from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 22 17:29:21.907: INFO: vpn-58b48cdc7c-z2bv5 from kube-system started at 2020-06-22 13:47:07 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container vpn ready: true, restart count 0
Jun 22 17:29:21.907: INFO: ibm-master-proxy-static-10.45.191.149 from kube-system started at 2020-06-22 13:22:54 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 17:29:21.907: INFO: 	Container pause ready: true, restart count 0
Jun 22 17:29:21.907: INFO: ibm-keepalived-watcher-n8x49 from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 17:29:21.907: INFO: calico-node-k8xbh from kube-system started at 2020-06-22 13:22:56 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 17:29:21.907: INFO: dashboard-metrics-scraper-576c46d9bd-8tnxf from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun 22 17:29:21.907: INFO: calico-kube-controllers-7c7d954b58-zwmf2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 17:29:21.907: INFO: kubernetes-dashboard-c6b4b9d77-wmvpw from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 22 17:29:21.907: INFO: coredns-autoscaler-6b97b7f9b-bldh2 from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container autoscaler ready: true, restart count 0
Jun 22 17:29:21.907: INFO: public-crbroaq0bl0dps8fkp34o0-alb1-9b85cb9cd-vrs4m from kube-system started at 2020-06-22 15:47:30 +0000 UTC (4 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Jun 22 17:29:21.907: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Jun 22 17:29:21.907: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Jun 22 17:29:21.907: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun 22 17:29:21.907: INFO: ibm-file-plugin-7c85454984-k87nb from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 22 17:29:21.907: INFO: ibm-storage-watcher-7b49c697c8-fmvjk from kube-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 22 17:29:21.907: INFO: olm-operator-557b484679-lvl7g from ibm-system started at 2020-06-22 13:23:14 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.907: INFO: 	Container olm-operator ready: true, restart count 0
Jun 22 17:29:21.907: INFO: 
Logging pods the kubelet thinks is on node 10.45.191.150 before test
Jun 22 17:29:21.947: INFO: ibm-keepalived-watcher-wkknt from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.947: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 17:29:21.947: INFO: addon-catalog-source-fh2f8 from ibm-system started at 2020-06-22 13:27:43 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.947: INFO: 	Container configmap-registry-server ready: true, restart count 0
Jun 22 17:29:21.947: INFO: ibm-master-proxy-static-10.45.191.150 from kube-system started at 2020-06-22 13:23:20 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.947: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 17:29:21.947: INFO: 	Container pause ready: true, restart count 0
Jun 22 17:29:21.947: INFO: calico-node-gvtlw from kube-system started at 2020-06-22 13:23:22 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.947: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 17:29:21.948: INFO: sonobuoy-systemd-logs-daemon-set-2eedfb6d476b47a6-rg86f from sonobuoy started at 2020-06-22 15:12:44 +0000 UTC (2 container statuses recorded)
Jun 22 17:29:21.948: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jun 22 17:29:21.948: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 17:29:21.948: INFO: coredns-697bf86f8c-ql8vb from kube-system started at 2020-06-22 16:24:23 +0000 UTC (1 container statuses recorded)
Jun 22 17:29:21.948: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-126334e2-2c24-43a0-95d9-3d452f9360f8 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-126334e2-2c24-43a0-95d9-3d452f9360f8 off the node 10.45.191.150
STEP: verifying the node doesn't have the label kubernetes.io/e2e-126334e2-2c24-43a0-95d9-3d452f9360f8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:29:26.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2986" for this suite.
Jun 22 17:29:42.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:29:42.966: INFO: namespace sched-pred-2986 deletion completed in 16.626752105s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:21.613 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:29:42.967: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 22 17:29:43.256: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6061187f-dffb-46c4-913f-3c7a3a174b3c" in namespace "downward-api-4512" to be "success or failure"
Jun 22 17:29:43.269: INFO: Pod "downwardapi-volume-6061187f-dffb-46c4-913f-3c7a3a174b3c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.941593ms
Jun 22 17:29:45.283: INFO: Pod "downwardapi-volume-6061187f-dffb-46c4-913f-3c7a3a174b3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027581304s
STEP: Saw pod success
Jun 22 17:29:45.283: INFO: Pod "downwardapi-volume-6061187f-dffb-46c4-913f-3c7a3a174b3c" satisfied condition "success or failure"
Jun 22 17:29:45.295: INFO: Trying to get logs from node 10.45.191.150 pod downwardapi-volume-6061187f-dffb-46c4-913f-3c7a3a174b3c container client-container: <nil>
STEP: delete the pod
Jun 22 17:29:45.360: INFO: Waiting for pod downwardapi-volume-6061187f-dffb-46c4-913f-3c7a3a174b3c to disappear
Jun 22 17:29:45.374: INFO: Pod downwardapi-volume-6061187f-dffb-46c4-913f-3c7a3a174b3c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:29:45.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4512" for this suite.
Jun 22 17:29:51.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:29:52.017: INFO: namespace downward-api-4512 deletion completed in 6.619538752s

• [SLOW TEST:9.050 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:29:52.018: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6557.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6557.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 17:29:56.424: INFO: DNS probes using dns-test-0ec80850-b6a6-4ebe-995f-1368e1128743 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6557.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6557.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 17:30:00.599: INFO: File wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:00.629: INFO: File jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:00.629: INFO: Lookups using dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf failed for: [wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local]

Jun 22 17:30:05.649: INFO: File wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:05.671: INFO: File jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:05.671: INFO: Lookups using dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf failed for: [wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local]

Jun 22 17:30:10.911: INFO: File wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:10.931: INFO: File jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:10.931: INFO: Lookups using dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf failed for: [wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local]

Jun 22 17:30:15.649: INFO: File wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:15.668: INFO: File jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:15.668: INFO: Lookups using dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf failed for: [wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local]

Jun 22 17:30:20.650: INFO: File wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:20.670: INFO: File jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local from pod  dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 17:30:20.670: INFO: Lookups using dns-6557/dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf failed for: [wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local]

Jun 22 17:30:25.677: INFO: DNS probes using dns-test-80fac15a-77f6-4ae0-a3c5-90286f264adf succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6557.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6557.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6557.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6557.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 17:30:30.167: INFO: DNS probes using dns-test-bfd249c0-9945-4d13-af4b-efc488858e28 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:30:30.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6557" for this suite.
Jun 22 17:30:38.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:30:39.060: INFO: namespace dns-6557 deletion completed in 8.778432416s

• [SLOW TEST:47.043 seconds]
[sig-network] DNS
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:30:39.062: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:30:39.339: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 22 17:30:39.370: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 17:30:44.383: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 17:30:44.383: INFO: Creating deployment "test-rolling-update-deployment"
Jun 22 17:30:44.399: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 22 17:30:44.428: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 22 17:30:46.460: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 22 17:30:46.472: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 17:30:46.514: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5327 /apis/apps/v1/namespaces/deployment-5327/deployments/test-rolling-update-deployment 8723b47c-30ae-4f60-856f-390f8744afd4 49977 1 2020-06-22 17:30:44 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0043e2748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-22 17:30:44 +0000 UTC,LastTransitionTime:2020-06-22 17:30:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-06-22 17:30:46 +0000 UTC,LastTransitionTime:2020-06-22 17:30:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 17:30:46.532: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-5327 /apis/apps/v1/namespaces/deployment-5327/replicasets/test-rolling-update-deployment-55d946486 476215aa-301c-4189-ad15-781c7bd53d10 49967 1 2020-06-22 17:30:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 8723b47c-30ae-4f60-856f-390f8744afd4 0xc0043e2d50 0xc0043e2d51}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0043e2df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 17:30:46.532: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 22 17:30:46.532: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5327 /apis/apps/v1/namespaces/deployment-5327/replicasets/test-rolling-update-controller 0d2ad7b4-cd9c-4389-be14-052def01a13d 49976 2 2020-06-22 17:30:39 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 8723b47c-30ae-4f60-856f-390f8744afd4 0xc0043e2c07 0xc0043e2c08}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043e2c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 17:30:46.544: INFO: Pod "test-rolling-update-deployment-55d946486-sj64d" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-sj64d test-rolling-update-deployment-55d946486- deployment-5327 /api/v1/namespaces/deployment-5327/pods/test-rolling-update-deployment-55d946486-sj64d 2b5744a5-db79-4c57-9929-d303b8ebb75c 49966 0 2020-06-22 17:30:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 476215aa-301c-4189-ad15-781c7bd53d10 0xc0043e33b0 0xc0043e33b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-54752,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-54752,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-54752,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.191.150,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 17:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 17:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 17:30:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-22 17:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.191.150,PodIP:172.30.131.119,StartTime:2020-06-22 17:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-22 17:30:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:containerd://01a74b83471a1b0ded73d140d725231d8425d1c9cc3ad34080a9c5d26e8a9412,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.131.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:30:46.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5327" for this suite.
Jun 22 17:30:54.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:30:55.182: INFO: namespace deployment-5327 deletion completed in 8.612450931s

• [SLOW TEST:16.120 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:30:55.184: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-921
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 22 17:30:55.448: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:30:58.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-921" for this suite.
Jun 22 17:31:04.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:31:05.582: INFO: namespace custom-resource-definition-921 deletion completed in 7.051674978s

• [SLOW TEST:10.398 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 22 17:31:05.582: INFO: >>> kubeConfig: /tmp/kubeconfig-114803800
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 17:31:05.867: INFO: Waiting up to 5m0s for pod "pod-29ab895e-7842-4fa8-9bc0-18d603a6d811" in namespace "emptydir-8843" to be "success or failure"
Jun 22 17:31:05.882: INFO: Pod "pod-29ab895e-7842-4fa8-9bc0-18d603a6d811": Phase="Pending", Reason="", readiness=false. Elapsed: 14.875659ms
Jun 22 17:31:07.894: INFO: Pod "pod-29ab895e-7842-4fa8-9bc0-18d603a6d811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026873599s
STEP: Saw pod success
Jun 22 17:31:07.894: INFO: Pod "pod-29ab895e-7842-4fa8-9bc0-18d603a6d811" satisfied condition "success or failure"
Jun 22 17:31:07.905: INFO: Trying to get logs from node 10.45.191.150 pod pod-29ab895e-7842-4fa8-9bc0-18d603a6d811 container test-container: <nil>
STEP: delete the pod
Jun 22 17:31:07.973: INFO: Waiting for pod pod-29ab895e-7842-4fa8-9bc0-18d603a6d811 to disappear
Jun 22 17:31:07.987: INFO: Pod pod-29ab895e-7842-4fa8-9bc0-18d603a6d811 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 22 17:31:07.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8843" for this suite.
Jun 22 17:31:14.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:31:14.656: INFO: namespace emptydir-8843 deletion completed in 6.641664147s

• [SLOW TEST:9.073 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.11-rc.0.46+1a6b7d0d26472f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSJun 22 17:31:14.656: INFO: Running AfterSuite actions on all nodes
Jun 22 17:31:14.656: INFO: Running AfterSuite actions on node 1
Jun 22 17:31:14.656: INFO: Skipping dumping logs from cluster

Ran 276 of 4847 Specs in 8208.138 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4571 Skipped
PASS

Ginkgo ran 1 suite in 2h16m49.870226376s
Test Suite Passed
