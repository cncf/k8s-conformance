I0506 19:48:01.937377      22 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-161989487
I0506 19:48:01.937499      22 e2e.go:92] Starting e2e run "f8a5fda3-0c5b-41e5-a3c1-b57cd0c2bcdc" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1588794480 - Will randomize all specs
Will run 276 of 4732 specs

May  6 19:48:01.948: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:48:01.950: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May  6 19:48:01.962: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  6 19:48:01.990: INFO: 32 / 32 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  6 19:48:01.990: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
May  6 19:48:01.990: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  6 19:48:01.996: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May  6 19:48:01.996: INFO: e2e test version: v1.16.9
May  6 19:48:01.997: INFO: kube-apiserver version: v1.16.9+d2iq.1
May  6 19:48:01.997: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:48:02.001: INFO: Cluster IP family: ipv4
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:48:02.001: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
May  6 19:48:02.030: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-7d418758-3406-4f98-b628-4c4cd1f8d498
STEP: Creating a pod to test consume secrets
May  6 19:48:02.042: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c3f0ffb-5872-4672-bf3c-c027d858d5f6" in namespace "projected-4791" to be "success or failure"
May  6 19:48:02.044: INFO: Pod "pod-projected-secrets-6c3f0ffb-5872-4672-bf3c-c027d858d5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.310578ms
May  6 19:48:04.047: INFO: Pod "pod-projected-secrets-6c3f0ffb-5872-4672-bf3c-c027d858d5f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005756185s
STEP: Saw pod success
May  6 19:48:04.047: INFO: Pod "pod-projected-secrets-6c3f0ffb-5872-4672-bf3c-c027d858d5f6" satisfied condition "success or failure"
May  6 19:48:04.051: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-secrets-6c3f0ffb-5872-4672-bf3c-c027d858d5f6 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 19:48:04.076: INFO: Waiting for pod pod-projected-secrets-6c3f0ffb-5872-4672-bf3c-c027d858d5f6 to disappear
May  6 19:48:04.079: INFO: Pod pod-projected-secrets-6c3f0ffb-5872-4672-bf3c-c027d858d5f6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:48:04.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4791" for this suite.
May  6 19:48:10.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:48:10.197: INFO: namespace projected-4791 deletion completed in 6.114639196s

• [SLOW TEST:8.196 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:48:10.197: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 19:48:10.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-5529'
May  6 19:48:10.503: INFO: stderr: ""
May  6 19:48:10.503: INFO: stdout: "replicationcontroller/redis-master created\n"
May  6 19:48:10.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-5529'
May  6 19:48:10.698: INFO: stderr: ""
May  6 19:48:10.698: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May  6 19:48:11.702: INFO: Selector matched 1 pods for map[app:redis]
May  6 19:48:11.702: INFO: Found 0 / 1
May  6 19:48:12.701: INFO: Selector matched 1 pods for map[app:redis]
May  6 19:48:12.701: INFO: Found 1 / 1
May  6 19:48:12.701: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  6 19:48:12.704: INFO: Selector matched 1 pods for map[app:redis]
May  6 19:48:12.704: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 19:48:12.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 describe pod redis-master-lkfg5 --namespace=kubectl-5529'
May  6 19:48:12.775: INFO: stderr: ""
May  6 19:48:12.775: INFO: stdout: "Name:         redis-master-lkfg5\nNamespace:    kubectl-5529\nPriority:     0\nNode:         kube-node-2-kubelet.kubernetes-cluster.mesos/9.0.1.5\nStart Time:   Wed, 06 May 2020 19:48:10 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.32.229/32\n              cni.projectcalico.org/podIPs: 192.168.32.229/32\nStatus:       Running\nIP:           192.168.32.229\nIPs:\n  IP:           192.168.32.229\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://3d3b8a1fadc8c1984d68ad5c28f163e37b644eb65544b32d322c5ace4e2956f3\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 06 May 2020 19:48:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-smm4l (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-smm4l:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-smm4l\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                                   Message\n  ----    ------     ----       ----                                                   -------\n  Normal  Scheduled  <unknown>  default-scheduler                                      Successfully assigned kubectl-5529/redis-master-lkfg5 to kube-node-2-kubelet.kubernetes-cluster.mesos\n  Normal  Pulled     1s         kubelet, kube-node-2-kubelet.kubernetes-cluster.mesos  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, kube-node-2-kubelet.kubernetes-cluster.mesos  Created container redis-master\n  Normal  Started    1s         kubelet, kube-node-2-kubelet.kubernetes-cluster.mesos  Started container redis-master\n"
May  6 19:48:12.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 describe rc redis-master --namespace=kubectl-5529'
May  6 19:48:12.852: INFO: stderr: ""
May  6 19:48:12.852: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5529\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-lkfg5\n"
May  6 19:48:12.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 describe service redis-master --namespace=kubectl-5529'
May  6 19:48:12.921: INFO: stderr: ""
May  6 19:48:12.921: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5529\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.100.28.154\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.32.229:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  6 19:48:12.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 describe node kube-control-plane-0-instance.kubernetes-cluster.mesos'
May  6 19:48:13.001: INFO: stderr: ""
May  6 19:48:13.001: INFO: stdout: "Name:               kube-control-plane-0-instance.kubernetes-cluster.mesos\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube-control-plane-0-instance.kubernetes-cluster.mesos\n                    kubernetes.io/os=linux\n                    name=kube-control-plane-0-instance.kubernetes-cluster.mesos\n                    node-role.kubernetes.io/master=\n                    tier=kube-control-plane\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 9.0.2.3/25\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.96.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 May 2020 16:31:42 +0000\nTaints:             node-role.kubernetes.io/master:NoExecute\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 06 May 2020 16:32:32 +0000   Wed, 06 May 2020 16:32:32 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 06 May 2020 19:47:24 +0000   Wed, 06 May 2020 16:31:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 06 May 2020 19:47:24 +0000   Wed, 06 May 2020 16:31:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 06 May 2020 19:47:24 +0000   Wed, 06 May 2020 16:31:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 06 May 2020 19:47:24 +0000   Wed, 06 May 2020 16:32:32 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  9.0.2.3\n  Hostname:    kube-control-plane-0-instance.kubernetes-cluster.mesos\nCapacity:\n cpu:                8\n ephemeral-storage:  125817836Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             31961360Ki\n pods:               10\nAllocatable:\n cpu:                1500m\n ephemeral-storage:  125715436Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3000Mi\n pods:               10\nSystem Info:\n Machine ID:                 75aae01548875f7e4079e74658c03f64\n System UUID:                EC2C1F51-309A-E11E-78A5-CE8CDDA71E95\n Boot ID:                    ff7c40dc-ddc7-4a82-96a4-1577fb87be1c\n Kernel Version:             3.10.0-862.3.2.el7.x86_64\n OS Image:                   Debian GNU/Linux 10 (buster)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://19.3.3\n Kubelet Version:            v1.16.9\n Kube-Proxy Version:         v1.16.9\nPodCIDR:                     192.168.0.0/24\nPodCIDRs:                    192.168.0.0/24\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                                              ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-d7hvw                                                                 250m (16%)    0 (0%)      0 (0%)           0 (0%)         3h16m\n  kube-system                kube-apiserver-kube-control-plane-0-instance.kubernetes-cluster.mesos             0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h15m\n  kube-system                kube-controller-manager-kube-control-plane-0-instance.kubernetes-cluster.mesos    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h16m\n  kube-system                kube-proxy-kube-control-plane-0-instance.kubernetes-cluster.mesos                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h16m\n  kube-system                kube-scheduler-kube-control-plane-0-instance.kubernetes-cluster.mesos             0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h16m\n  kube-system                local-dns-dispatcher-kube-control-plane-0-instance.kubernetes-cluster.mesos       100m (6%)     100m (6%)   32Mi (1%)        32Mi (1%)      3h15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                350m (23%)  100m (6%)\n  memory             32Mi (1%)   32Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
May  6 19:48:13.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 describe namespace kubectl-5529'
May  6 19:48:13.067: INFO: stderr: ""
May  6 19:48:13.067: INFO: stdout: "Name:         kubectl-5529\nLabels:       e2e-framework=kubectl\n              e2e-run=f8a5fda3-0c5b-41e5-a3c1-b57cd0c2bcdc\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:48:13.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5529" for this suite.
May  6 19:48:25.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:48:25.186: INFO: namespace kubectl-5529 deletion completed in 12.115359505s

• [SLOW TEST:14.989 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:48:25.186: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May  6 19:48:28.242: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:48:28.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8788" for this suite.
May  6 19:48:40.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:48:40.377: INFO: namespace replicaset-8788 deletion completed in 12.119945402s

• [SLOW TEST:15.191 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:48:40.377: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 19:48:40.425: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"162149b5-9ef3-4feb-b3ce-51d689e0ac4b", Controller:(*bool)(0xc002ca3586), BlockOwnerDeletion:(*bool)(0xc002ca3587)}}
May  6 19:48:40.430: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"33cd67b2-5744-44f7-9593-2bd0544e5b27", Controller:(*bool)(0xc003038c06), BlockOwnerDeletion:(*bool)(0xc003038c07)}}
May  6 19:48:40.435: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8253d57e-5654-4fe6-8d85-974d17fe3bfd", Controller:(*bool)(0xc002ef55e6), BlockOwnerDeletion:(*bool)(0xc002ef55e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:48:45.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3271" for this suite.
May  6 19:48:51.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:48:51.565: INFO: namespace gc-3271 deletion completed in 6.116930112s

• [SLOW TEST:11.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:48:51.565: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 19:48:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:48:51.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9489" for this suite.
May  6 19:48:57.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:48:57.752: INFO: namespace custom-resource-definition-9489 deletion completed in 6.116325221s

• [SLOW TEST:6.187 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:48:57.753: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
May  6 19:48:57.786: INFO: Waiting up to 5m0s for pod "client-containers-5b743208-0b57-4543-8966-fbd2b3261206" in namespace "containers-864" to be "success or failure"
May  6 19:48:57.789: INFO: Pod "client-containers-5b743208-0b57-4543-8966-fbd2b3261206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733455ms
May  6 19:48:59.793: INFO: Pod "client-containers-5b743208-0b57-4543-8966-fbd2b3261206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00666827s
STEP: Saw pod success
May  6 19:48:59.793: INFO: Pod "client-containers-5b743208-0b57-4543-8966-fbd2b3261206" satisfied condition "success or failure"
May  6 19:48:59.795: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod client-containers-5b743208-0b57-4543-8966-fbd2b3261206 container test-container: <nil>
STEP: delete the pod
May  6 19:48:59.816: INFO: Waiting for pod client-containers-5b743208-0b57-4543-8966-fbd2b3261206 to disappear
May  6 19:48:59.818: INFO: Pod client-containers-5b743208-0b57-4543-8966-fbd2b3261206 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:48:59.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-864" for this suite.
May  6 19:49:05.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:49:05.942: INFO: namespace containers-864 deletion completed in 6.120092184s

• [SLOW TEST:8.189 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:49:05.942: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 19:49:05.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8372'
May  6 19:49:06.057: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 19:49:06.057: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
May  6 19:49:06.066: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-dnjr9]
May  6 19:49:06.066: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-dnjr9" in namespace "kubectl-8372" to be "running and ready"
May  6 19:49:06.071: INFO: Pod "e2e-test-httpd-rc-dnjr9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.786558ms
May  6 19:49:08.075: INFO: Pod "e2e-test-httpd-rc-dnjr9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008648776s
May  6 19:49:08.075: INFO: Pod "e2e-test-httpd-rc-dnjr9" satisfied condition "running and ready"
May  6 19:49:08.075: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-dnjr9]
May  6 19:49:08.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs rc/e2e-test-httpd-rc --namespace=kubectl-8372'
May  6 19:49:08.149: INFO: stderr: ""
May  6 19:49:08.149: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.32.231. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.32.231. Set the 'ServerName' directive globally to suppress this message\n[Wed May 06 19:49:06.849739 2020] [mpm_event:notice] [pid 1:tid 140035113720680] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Wed May 06 19:49:06.849770 2020] [core:notice] [pid 1:tid 140035113720680] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
May  6 19:49:08.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete rc e2e-test-httpd-rc --namespace=kubectl-8372'
May  6 19:49:08.215: INFO: stderr: ""
May  6 19:49:08.215: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:49:08.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8372" for this suite.
May  6 19:49:20.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:49:20.352: INFO: namespace kubectl-8372 deletion completed in 12.131079434s

• [SLOW TEST:14.410 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:49:20.352: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May  6 19:49:20.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-7069'
May  6 19:49:20.564: INFO: stderr: ""
May  6 19:49:20.564: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  6 19:49:21.570: INFO: Selector matched 1 pods for map[app:redis]
May  6 19:49:21.570: INFO: Found 1 / 1
May  6 19:49:21.570: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May  6 19:49:21.572: INFO: Selector matched 1 pods for map[app:redis]
May  6 19:49:21.572: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 19:49:21.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 patch pod redis-master-kwbg5 --namespace=kubectl-7069 -p {"metadata":{"annotations":{"x":"y"}}}'
May  6 19:49:21.637: INFO: stderr: ""
May  6 19:49:21.637: INFO: stdout: "pod/redis-master-kwbg5 patched\n"
STEP: checking annotations
May  6 19:49:21.641: INFO: Selector matched 1 pods for map[app:redis]
May  6 19:49:21.641: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:49:21.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7069" for this suite.
May  6 19:49:33.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:49:33.843: INFO: namespace kubectl-7069 deletion completed in 12.196595361s

• [SLOW TEST:13.491 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:49:33.843: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-9994557a-beef-44a2-8571-b448d14599eb
STEP: Creating a pod to test consume configMaps
May  6 19:49:33.883: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac83182a-a560-4152-93f0-49da91f1017a" in namespace "configmap-8340" to be "success or failure"
May  6 19:49:33.886: INFO: Pod "pod-configmaps-ac83182a-a560-4152-93f0-49da91f1017a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.772327ms
May  6 19:49:35.890: INFO: Pod "pod-configmaps-ac83182a-a560-4152-93f0-49da91f1017a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006726753s
STEP: Saw pod success
May  6 19:49:35.890: INFO: Pod "pod-configmaps-ac83182a-a560-4152-93f0-49da91f1017a" satisfied condition "success or failure"
May  6 19:49:35.892: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-ac83182a-a560-4152-93f0-49da91f1017a container configmap-volume-test: <nil>
STEP: delete the pod
May  6 19:49:35.911: INFO: Waiting for pod pod-configmaps-ac83182a-a560-4152-93f0-49da91f1017a to disappear
May  6 19:49:35.913: INFO: Pod pod-configmaps-ac83182a-a560-4152-93f0-49da91f1017a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:49:35.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8340" for this suite.
May  6 19:49:41.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:49:42.029: INFO: namespace configmap-8340 deletion completed in 6.111477095s

• [SLOW TEST:8.187 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:49:42.030: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 19:49:42.062: INFO: Waiting up to 5m0s for pod "downward-api-187d25ee-ff97-4463-b9ff-2e11c173de84" in namespace "downward-api-6710" to be "success or failure"
May  6 19:49:42.064: INFO: Pod "downward-api-187d25ee-ff97-4463-b9ff-2e11c173de84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.538569ms
May  6 19:49:44.068: INFO: Pod "downward-api-187d25ee-ff97-4463-b9ff-2e11c173de84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005891604s
STEP: Saw pod success
May  6 19:49:44.068: INFO: Pod "downward-api-187d25ee-ff97-4463-b9ff-2e11c173de84" satisfied condition "success or failure"
May  6 19:49:44.071: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downward-api-187d25ee-ff97-4463-b9ff-2e11c173de84 container dapi-container: <nil>
STEP: delete the pod
May  6 19:49:44.090: INFO: Waiting for pod downward-api-187d25ee-ff97-4463-b9ff-2e11c173de84 to disappear
May  6 19:49:44.093: INFO: Pod downward-api-187d25ee-ff97-4463-b9ff-2e11c173de84 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:49:44.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6710" for this suite.
May  6 19:49:50.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:49:50.211: INFO: namespace downward-api-6710 deletion completed in 6.114788657s

• [SLOW TEST:8.182 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:49:50.212: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 19:49:50.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91c96128-65a4-4b3b-ab9d-11b1d1bebbf6" in namespace "projected-4116" to be "success or failure"
May  6 19:49:50.256: INFO: Pod "downwardapi-volume-91c96128-65a4-4b3b-ab9d-11b1d1bebbf6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.414137ms
May  6 19:49:52.260: INFO: Pod "downwardapi-volume-91c96128-65a4-4b3b-ab9d-11b1d1bebbf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010757782s
STEP: Saw pod success
May  6 19:49:52.260: INFO: Pod "downwardapi-volume-91c96128-65a4-4b3b-ab9d-11b1d1bebbf6" satisfied condition "success or failure"
May  6 19:49:52.263: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-91c96128-65a4-4b3b-ab9d-11b1d1bebbf6 container client-container: <nil>
STEP: delete the pod
May  6 19:49:52.281: INFO: Waiting for pod downwardapi-volume-91c96128-65a4-4b3b-ab9d-11b1d1bebbf6 to disappear
May  6 19:49:52.284: INFO: Pod downwardapi-volume-91c96128-65a4-4b3b-ab9d-11b1d1bebbf6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:49:52.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4116" for this suite.
May  6 19:49:58.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:49:58.408: INFO: namespace projected-4116 deletion completed in 6.118497056s

• [SLOW TEST:8.197 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:49:58.409: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-922622ab-b067-42c5-8d67-c16823985352
STEP: Creating secret with name secret-projected-all-test-volume-f44796a4-c875-4387-be1b-fc9be3dd1a0b
STEP: Creating a pod to test Check all projections for projected volume plugin
May  6 19:49:58.462: INFO: Waiting up to 5m0s for pod "projected-volume-b4cfa9df-ce4e-4b60-bde2-c695c40d637d" in namespace "projected-388" to be "success or failure"
May  6 19:49:58.466: INFO: Pod "projected-volume-b4cfa9df-ce4e-4b60-bde2-c695c40d637d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879337ms
May  6 19:50:00.469: INFO: Pod "projected-volume-b4cfa9df-ce4e-4b60-bde2-c695c40d637d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006741571s
STEP: Saw pod success
May  6 19:50:00.469: INFO: Pod "projected-volume-b4cfa9df-ce4e-4b60-bde2-c695c40d637d" satisfied condition "success or failure"
May  6 19:50:00.472: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod projected-volume-b4cfa9df-ce4e-4b60-bde2-c695c40d637d container projected-all-volume-test: <nil>
STEP: delete the pod
May  6 19:50:00.491: INFO: Waiting for pod projected-volume-b4cfa9df-ce4e-4b60-bde2-c695c40d637d to disappear
May  6 19:50:00.494: INFO: Pod projected-volume-b4cfa9df-ce4e-4b60-bde2-c695c40d637d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:50:00.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-388" for this suite.
May  6 19:50:06.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:50:06.622: INFO: namespace projected-388 deletion completed in 6.124940549s

• [SLOW TEST:8.214 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:50:06.623: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May  6 19:50:06.654: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
May  6 19:50:07.100: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  6 19:50:09.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 19:50:11.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 19:50:13.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391407, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 19:50:15.770: INFO: Waited 620.088364ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:50:16.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8430" for this suite.
May  6 19:50:22.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:50:22.579: INFO: namespace aggregator-8430 deletion completed in 6.192070598s

• [SLOW TEST:15.956 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:50:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 19:50:22.905: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 19:50:24.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391422, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391422, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391422, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391422, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 19:50:27.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 19:50:27.932: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7613-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:50:29.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-662" for this suite.
May  6 19:50:35.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:50:35.146: INFO: namespace webhook-662 deletion completed in 6.117774511s
STEP: Destroying namespace "webhook-662-markers" for this suite.
May  6 19:50:41.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:50:41.276: INFO: namespace webhook-662-markers deletion completed in 6.130010023s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.711 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:50:41.290: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 19:50:41.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 19:50:45.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:50:45.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5265" for this suite.
May  6 19:50:51.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:50:51.187: INFO: namespace webhook-5265 deletion completed in 6.118704154s
STEP: Destroying namespace "webhook-5265-markers" for this suite.
May  6 19:50:57.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:50:57.298: INFO: namespace webhook-5265-markers deletion completed in 6.1108058s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.022 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:50:57.312: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 19:50:59.361: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:50:59.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5109" for this suite.
May  6 19:51:05.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:51:05.498: INFO: namespace container-runtime-5109 deletion completed in 6.117832455s

• [SLOW TEST:8.186 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:51:05.498: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-2c4d26f9-5d5c-4251-a31f-6f288ca022cb
STEP: Creating a pod to test consume secrets
May  6 19:51:05.541: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-169fc86e-d996-4fea-a085-376aaa672e51" in namespace "projected-4227" to be "success or failure"
May  6 19:51:05.544: INFO: Pod "pod-projected-secrets-169fc86e-d996-4fea-a085-376aaa672e51": Phase="Pending", Reason="", readiness=false. Elapsed: 3.193033ms
May  6 19:51:07.547: INFO: Pod "pod-projected-secrets-169fc86e-d996-4fea-a085-376aaa672e51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006413844s
STEP: Saw pod success
May  6 19:51:07.547: INFO: Pod "pod-projected-secrets-169fc86e-d996-4fea-a085-376aaa672e51" satisfied condition "success or failure"
May  6 19:51:07.550: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-secrets-169fc86e-d996-4fea-a085-376aaa672e51 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 19:51:07.572: INFO: Waiting for pod pod-projected-secrets-169fc86e-d996-4fea-a085-376aaa672e51 to disappear
May  6 19:51:07.575: INFO: Pod pod-projected-secrets-169fc86e-d996-4fea-a085-376aaa672e51 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:51:07.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4227" for this suite.
May  6 19:51:13.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:51:13.699: INFO: namespace projected-4227 deletion completed in 6.119008779s

• [SLOW TEST:8.201 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:51:13.699: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:51:29.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7486" for this suite.
May  6 19:51:35.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:51:35.928: INFO: namespace resourcequota-7486 deletion completed in 6.116653029s

• [SLOW TEST:22.229 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:51:35.928: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:51:49.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2173" for this suite.
May  6 19:51:55.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:51:55.156: INFO: namespace namespaces-2173 deletion completed in 6.118959194s
STEP: Destroying namespace "nsdeletetest-1878" for this suite.
May  6 19:51:55.158: INFO: Namespace nsdeletetest-1878 was already deleted
STEP: Destroying namespace "nsdeletetest-4382" for this suite.
May  6 19:52:01.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:52:01.273: INFO: namespace nsdeletetest-4382 deletion completed in 6.114283699s

• [SLOW TEST:25.345 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:52:01.273: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 19:52:01.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2172'
May  6 19:52:01.367: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 19:52:01.367: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
May  6 19:52:01.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete jobs e2e-test-httpd-job --namespace=kubectl-2172'
May  6 19:52:01.446: INFO: stderr: ""
May  6 19:52:01.446: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:52:01.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2172" for this suite.
May  6 19:52:07.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:52:07.566: INFO: namespace kubectl-2172 deletion completed in 6.116425472s

• [SLOW TEST:6.293 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:52:07.567: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-c45fcff8-2676-460f-bb97-f638c881df41
STEP: Creating a pod to test consume secrets
May  6 19:52:07.602: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e946a5d-24fe-4add-a2cc-f62211d988c1" in namespace "projected-5443" to be "success or failure"
May  6 19:52:07.605: INFO: Pod "pod-projected-secrets-7e946a5d-24fe-4add-a2cc-f62211d988c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987288ms
May  6 19:52:09.609: INFO: Pod "pod-projected-secrets-7e946a5d-24fe-4add-a2cc-f62211d988c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006721191s
STEP: Saw pod success
May  6 19:52:09.609: INFO: Pod "pod-projected-secrets-7e946a5d-24fe-4add-a2cc-f62211d988c1" satisfied condition "success or failure"
May  6 19:52:09.612: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-secrets-7e946a5d-24fe-4add-a2cc-f62211d988c1 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 19:52:09.631: INFO: Waiting for pod pod-projected-secrets-7e946a5d-24fe-4add-a2cc-f62211d988c1 to disappear
May  6 19:52:09.634: INFO: Pod pod-projected-secrets-7e946a5d-24fe-4add-a2cc-f62211d988c1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:52:09.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5443" for this suite.
May  6 19:52:15.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:52:15.750: INFO: namespace projected-5443 deletion completed in 6.113082246s

• [SLOW TEST:8.184 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:52:15.751: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May  6 19:52:15.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-9628'
May  6 19:52:15.964: INFO: stderr: ""
May  6 19:52:15.964: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 19:52:15.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9628'
May  6 19:52:16.027: INFO: stderr: ""
May  6 19:52:16.027: INFO: stdout: "update-demo-nautilus-5m4ss update-demo-nautilus-z8q5q "
May  6 19:52:16.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-5m4ss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:16.087: INFO: stderr: ""
May  6 19:52:16.087: INFO: stdout: ""
May  6 19:52:16.087: INFO: update-demo-nautilus-5m4ss is created but not running
May  6 19:52:21.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9628'
May  6 19:52:21.148: INFO: stderr: ""
May  6 19:52:21.148: INFO: stdout: "update-demo-nautilus-5m4ss update-demo-nautilus-z8q5q "
May  6 19:52:21.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-5m4ss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:21.206: INFO: stderr: ""
May  6 19:52:21.206: INFO: stdout: "true"
May  6 19:52:21.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-5m4ss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:21.262: INFO: stderr: ""
May  6 19:52:21.262: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 19:52:21.262: INFO: validating pod update-demo-nautilus-5m4ss
May  6 19:52:21.267: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 19:52:21.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 19:52:21.267: INFO: update-demo-nautilus-5m4ss is verified up and running
May  6 19:52:21.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-z8q5q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:21.325: INFO: stderr: ""
May  6 19:52:21.325: INFO: stdout: "true"
May  6 19:52:21.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-z8q5q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:21.382: INFO: stderr: ""
May  6 19:52:21.382: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 19:52:21.382: INFO: validating pod update-demo-nautilus-z8q5q
May  6 19:52:21.386: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 19:52:21.386: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 19:52:21.386: INFO: update-demo-nautilus-z8q5q is verified up and running
STEP: scaling down the replication controller
May  6 19:52:21.387: INFO: scanned /root for discovery docs: <nil>
May  6 19:52:21.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9628'
May  6 19:52:22.469: INFO: stderr: ""
May  6 19:52:22.469: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 19:52:22.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9628'
May  6 19:52:22.531: INFO: stderr: ""
May  6 19:52:22.531: INFO: stdout: "update-demo-nautilus-5m4ss update-demo-nautilus-z8q5q "
STEP: Replicas for name=update-demo: expected=1 actual=2
May  6 19:52:27.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9628'
May  6 19:52:27.594: INFO: stderr: ""
May  6 19:52:27.594: INFO: stdout: "update-demo-nautilus-5m4ss update-demo-nautilus-z8q5q "
STEP: Replicas for name=update-demo: expected=1 actual=2
May  6 19:52:32.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9628'
May  6 19:52:32.653: INFO: stderr: ""
May  6 19:52:32.653: INFO: stdout: "update-demo-nautilus-z8q5q "
May  6 19:52:32.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-z8q5q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:32.713: INFO: stderr: ""
May  6 19:52:32.713: INFO: stdout: "true"
May  6 19:52:32.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-z8q5q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:32.772: INFO: stderr: ""
May  6 19:52:32.772: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 19:52:32.772: INFO: validating pod update-demo-nautilus-z8q5q
May  6 19:52:32.776: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 19:52:32.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 19:52:32.776: INFO: update-demo-nautilus-z8q5q is verified up and running
STEP: scaling up the replication controller
May  6 19:52:32.777: INFO: scanned /root for discovery docs: <nil>
May  6 19:52:32.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9628'
May  6 19:52:33.887: INFO: stderr: ""
May  6 19:52:33.887: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 19:52:33.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9628'
May  6 19:52:33.948: INFO: stderr: ""
May  6 19:52:33.948: INFO: stdout: "update-demo-nautilus-gx5ww update-demo-nautilus-z8q5q "
May  6 19:52:33.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-gx5ww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:34.004: INFO: stderr: ""
May  6 19:52:34.004: INFO: stdout: ""
May  6 19:52:34.004: INFO: update-demo-nautilus-gx5ww is created but not running
May  6 19:52:39.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9628'
May  6 19:52:39.065: INFO: stderr: ""
May  6 19:52:39.065: INFO: stdout: "update-demo-nautilus-gx5ww update-demo-nautilus-z8q5q "
May  6 19:52:39.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-gx5ww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:39.124: INFO: stderr: ""
May  6 19:52:39.124: INFO: stdout: "true"
May  6 19:52:39.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-gx5ww -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:39.180: INFO: stderr: ""
May  6 19:52:39.180: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 19:52:39.180: INFO: validating pod update-demo-nautilus-gx5ww
May  6 19:52:39.185: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 19:52:39.186: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 19:52:39.186: INFO: update-demo-nautilus-gx5ww is verified up and running
May  6 19:52:39.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-z8q5q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:39.243: INFO: stderr: ""
May  6 19:52:39.243: INFO: stdout: "true"
May  6 19:52:39.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-z8q5q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9628'
May  6 19:52:39.299: INFO: stderr: ""
May  6 19:52:39.299: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 19:52:39.299: INFO: validating pod update-demo-nautilus-z8q5q
May  6 19:52:39.303: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 19:52:39.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 19:52:39.303: INFO: update-demo-nautilus-z8q5q is verified up and running
STEP: using delete to clean up resources
May  6 19:52:39.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-9628'
May  6 19:52:39.367: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 19:52:39.367: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  6 19:52:39.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9628'
May  6 19:52:39.434: INFO: stderr: "No resources found in kubectl-9628 namespace.\n"
May  6 19:52:39.434: INFO: stdout: ""
May  6 19:52:39.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -l name=update-demo --namespace=kubectl-9628 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 19:52:39.494: INFO: stderr: ""
May  6 19:52:39.494: INFO: stdout: "update-demo-nautilus-gx5ww\nupdate-demo-nautilus-z8q5q\n"
May  6 19:52:39.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9628'
May  6 19:52:40.074: INFO: stderr: "No resources found in kubectl-9628 namespace.\n"
May  6 19:52:40.074: INFO: stdout: ""
May  6 19:52:40.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -l name=update-demo --namespace=kubectl-9628 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 19:52:40.133: INFO: stderr: ""
May  6 19:52:40.133: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:52:40.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9628" for this suite.
May  6 19:52:52.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:52:52.262: INFO: namespace kubectl-9628 deletion completed in 12.124956499s

• [SLOW TEST:36.512 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:52:52.262: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:52:52.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-114" for this suite.
May  6 19:53:20.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:53:20.418: INFO: namespace pods-114 deletion completed in 28.11333697s

• [SLOW TEST:28.155 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:53:20.418: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 19:53:20.901: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 19:53:22.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391600, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391600, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391600, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391600, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 19:53:25.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May  6 19:53:27.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 attach --namespace=webhook-338 to-be-attached-pod -i -c=container1'
May  6 19:53:28.024: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:53:28.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-338" for this suite.
May  6 19:53:40.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:53:40.164: INFO: namespace webhook-338 deletion completed in 12.127093497s
STEP: Destroying namespace "webhook-338-markers" for this suite.
May  6 19:53:46.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:53:46.281: INFO: namespace webhook-338-markers deletion completed in 6.11708539s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.877 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:53:46.295: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May  6 19:53:46.322: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:53:49.684: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:54:00.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6120" for this suite.
May  6 19:54:07.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:54:07.106: INFO: namespace crd-publish-openapi-6120 deletion completed in 6.113596344s

• [SLOW TEST:20.811 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:54:07.106: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
May  6 19:54:07.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-1132'
May  6 19:54:07.319: INFO: stderr: ""
May  6 19:54:07.319: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 19:54:07.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1132'
May  6 19:54:07.382: INFO: stderr: ""
May  6 19:54:07.382: INFO: stdout: "update-demo-nautilus-cch6x update-demo-nautilus-qrlwr "
May  6 19:54:07.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-cch6x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:07.442: INFO: stderr: ""
May  6 19:54:07.442: INFO: stdout: ""
May  6 19:54:07.442: INFO: update-demo-nautilus-cch6x is created but not running
May  6 19:54:12.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1132'
May  6 19:54:12.501: INFO: stderr: ""
May  6 19:54:12.501: INFO: stdout: "update-demo-nautilus-cch6x update-demo-nautilus-qrlwr "
May  6 19:54:12.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-cch6x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:12.560: INFO: stderr: ""
May  6 19:54:12.560: INFO: stdout: "true"
May  6 19:54:12.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-cch6x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:12.618: INFO: stderr: ""
May  6 19:54:12.618: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 19:54:12.618: INFO: validating pod update-demo-nautilus-cch6x
May  6 19:54:12.623: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 19:54:12.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 19:54:12.623: INFO: update-demo-nautilus-cch6x is verified up and running
May  6 19:54:12.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-qrlwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:12.680: INFO: stderr: ""
May  6 19:54:12.680: INFO: stdout: "true"
May  6 19:54:12.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-qrlwr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:12.736: INFO: stderr: ""
May  6 19:54:12.736: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 19:54:12.736: INFO: validating pod update-demo-nautilus-qrlwr
May  6 19:54:12.739: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 19:54:12.739: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 19:54:12.739: INFO: update-demo-nautilus-qrlwr is verified up and running
STEP: rolling-update to new replication controller
May  6 19:54:12.740: INFO: scanned /root for discovery docs: <nil>
May  6 19:54:12.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-1132'
May  6 19:54:35.065: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  6 19:54:35.065: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 19:54:35.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1132'
May  6 19:54:35.125: INFO: stderr: ""
May  6 19:54:35.125: INFO: stdout: "update-demo-kitten-s64zn update-demo-kitten-vrp69 "
May  6 19:54:35.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-kitten-s64zn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:35.183: INFO: stderr: ""
May  6 19:54:35.183: INFO: stdout: "true"
May  6 19:54:35.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-kitten-s64zn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:35.241: INFO: stderr: ""
May  6 19:54:35.241: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  6 19:54:35.241: INFO: validating pod update-demo-kitten-s64zn
May  6 19:54:35.247: INFO: got data: {
  "image": "kitten.jpg"
}

May  6 19:54:35.247: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  6 19:54:35.247: INFO: update-demo-kitten-s64zn is verified up and running
May  6 19:54:35.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-kitten-vrp69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:35.308: INFO: stderr: ""
May  6 19:54:35.308: INFO: stdout: "true"
May  6 19:54:35.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-kitten-vrp69 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1132'
May  6 19:54:35.378: INFO: stderr: ""
May  6 19:54:35.378: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  6 19:54:35.378: INFO: validating pod update-demo-kitten-vrp69
May  6 19:54:35.384: INFO: got data: {
  "image": "kitten.jpg"
}

May  6 19:54:35.384: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  6 19:54:35.384: INFO: update-demo-kitten-vrp69 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:54:35.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1132" for this suite.
May  6 19:54:47.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:54:47.514: INFO: namespace kubectl-1132 deletion completed in 12.125417241s

• [SLOW TEST:40.408 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:54:47.514: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-770a10d9-29a3-4e68-b00e-174c92b51a15
STEP: Creating secret with name s-test-opt-upd-4596d500-e38c-44e7-9997-431f6acff685
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-770a10d9-29a3-4e68-b00e-174c92b51a15
STEP: Updating secret s-test-opt-upd-4596d500-e38c-44e7-9997-431f6acff685
STEP: Creating secret with name s-test-opt-create-49aa4d55-1a2e-4d67-8ee5-74aec9ce35f0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:54:53.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-422" for this suite.
May  6 19:55:05.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:55:05.778: INFO: namespace secrets-422 deletion completed in 12.123508863s

• [SLOW TEST:18.265 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:55:05.778: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May  6 19:55:05.811: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46903 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 19:55:05.811: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46903 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May  6 19:55:15.818: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46923 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  6 19:55:15.818: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46923 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May  6 19:55:25.827: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46944 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 19:55:25.827: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46944 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May  6 19:55:35.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46967 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 19:55:35.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-a 2d754b47-d52c-4ccb-9bf0-79eb27170a11 46967 0 2020-05-06 19:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May  6 19:55:45.845: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-b 59e30ae9-ee39-4be7-9152-25697966bb76 46987 0 2020-05-06 19:55:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 19:55:45.845: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-b 59e30ae9-ee39-4be7-9152-25697966bb76 46987 0 2020-05-06 19:55:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May  6 19:55:55.853: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-b 59e30ae9-ee39-4be7-9152-25697966bb76 47007 0 2020-05-06 19:55:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 19:55:55.853: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1647 /api/v1/namespaces/watch-1647/configmaps/e2e-watch-test-configmap-b 59e30ae9-ee39-4be7-9152-25697966bb76 47007 0 2020-05-06 19:55:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:56:05.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1647" for this suite.
May  6 19:56:11.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:56:11.975: INFO: namespace watch-1647 deletion completed in 6.11678136s

• [SLOW TEST:66.196 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:56:11.975: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-680/configmap-test-f9ec1cbc-f166-407e-aefb-3acb4116a8ac
STEP: Creating a pod to test consume configMaps
May  6 19:56:12.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-410e8560-9338-436e-bccf-75bff5b5d767" in namespace "configmap-680" to be "success or failure"
May  6 19:56:12.017: INFO: Pod "pod-configmaps-410e8560-9338-436e-bccf-75bff5b5d767": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231478ms
May  6 19:56:14.020: INFO: Pod "pod-configmaps-410e8560-9338-436e-bccf-75bff5b5d767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00712296s
STEP: Saw pod success
May  6 19:56:14.020: INFO: Pod "pod-configmaps-410e8560-9338-436e-bccf-75bff5b5d767" satisfied condition "success or failure"
May  6 19:56:14.022: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-410e8560-9338-436e-bccf-75bff5b5d767 container env-test: <nil>
STEP: delete the pod
May  6 19:56:14.042: INFO: Waiting for pod pod-configmaps-410e8560-9338-436e-bccf-75bff5b5d767 to disappear
May  6 19:56:14.045: INFO: Pod pod-configmaps-410e8560-9338-436e-bccf-75bff5b5d767 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:56:14.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-680" for this suite.
May  6 19:56:20.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:56:20.161: INFO: namespace configmap-680 deletion completed in 6.112517206s

• [SLOW TEST:8.186 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:56:20.161: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-06ff6f43-1951-4af6-a876-b716005b1182
STEP: Creating a pod to test consume secrets
May  6 19:56:20.198: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1dca3dd5-02a2-4015-83cf-c6157467d9a6" in namespace "projected-6122" to be "success or failure"
May  6 19:56:20.201: INFO: Pod "pod-projected-secrets-1dca3dd5-02a2-4015-83cf-c6157467d9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05491ms
May  6 19:56:22.205: INFO: Pod "pod-projected-secrets-1dca3dd5-02a2-4015-83cf-c6157467d9a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006585572s
STEP: Saw pod success
May  6 19:56:22.205: INFO: Pod "pod-projected-secrets-1dca3dd5-02a2-4015-83cf-c6157467d9a6" satisfied condition "success or failure"
May  6 19:56:22.207: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-secrets-1dca3dd5-02a2-4015-83cf-c6157467d9a6 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 19:56:22.226: INFO: Waiting for pod pod-projected-secrets-1dca3dd5-02a2-4015-83cf-c6157467d9a6 to disappear
May  6 19:56:22.228: INFO: Pod pod-projected-secrets-1dca3dd5-02a2-4015-83cf-c6157467d9a6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:56:22.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6122" for this suite.
May  6 19:56:28.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:56:28.345: INFO: namespace projected-6122 deletion completed in 6.112899834s

• [SLOW TEST:8.184 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:56:28.345: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-4563
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4563
STEP: Deleting pre-stop pod
May  6 19:56:37.461: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:56:37.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4563" for this suite.
May  6 19:57:11.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:57:11.588: INFO: namespace prestop-4563 deletion completed in 34.115052125s

• [SLOW TEST:43.243 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:57:11.589: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May  6 19:57:15.642: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:15.642: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:15.723: INFO: Exec stderr: ""
May  6 19:57:15.723: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:15.723: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:15.816: INFO: Exec stderr: ""
May  6 19:57:15.817: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:15.817: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:15.904: INFO: Exec stderr: ""
May  6 19:57:15.904: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:15.904: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:16.026: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May  6 19:57:16.026: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:16.026: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:16.120: INFO: Exec stderr: ""
May  6 19:57:16.120: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:16.120: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:16.204: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May  6 19:57:16.204: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:16.204: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:16.284: INFO: Exec stderr: ""
May  6 19:57:16.284: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:16.284: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:16.371: INFO: Exec stderr: ""
May  6 19:57:16.371: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:16.371: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:16.451: INFO: Exec stderr: ""
May  6 19:57:16.451: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5652 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 19:57:16.451: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 19:57:16.532: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:57:16.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5652" for this suite.
May  6 19:58:00.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:58:00.651: INFO: namespace e2e-kubelet-etc-hosts-5652 deletion completed in 44.114780981s

• [SLOW TEST:49.062 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:58:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 19:58:01.102: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 19:58:03.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391881, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391881, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391881, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724391881, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 19:58:06.122: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:58:06.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4346" for this suite.
May  6 19:58:12.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:58:12.265: INFO: namespace webhook-4346 deletion completed in 6.106754459s
STEP: Destroying namespace "webhook-4346-markers" for this suite.
May  6 19:58:18.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:58:18.378: INFO: namespace webhook-4346-markers deletion completed in 6.112927229s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:58:18.393: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 19:58:18.426: INFO: Waiting up to 5m0s for pod "downward-api-39c417ed-a7f7-425c-bff6-7d50268f6545" in namespace "downward-api-341" to be "success or failure"
May  6 19:58:18.429: INFO: Pod "downward-api-39c417ed-a7f7-425c-bff6-7d50268f6545": Phase="Pending", Reason="", readiness=false. Elapsed: 3.050847ms
May  6 19:58:20.434: INFO: Pod "downward-api-39c417ed-a7f7-425c-bff6-7d50268f6545": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007327254s
STEP: Saw pod success
May  6 19:58:20.434: INFO: Pod "downward-api-39c417ed-a7f7-425c-bff6-7d50268f6545" satisfied condition "success or failure"
May  6 19:58:20.436: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downward-api-39c417ed-a7f7-425c-bff6-7d50268f6545 container dapi-container: <nil>
STEP: delete the pod
May  6 19:58:20.505: INFO: Waiting for pod downward-api-39c417ed-a7f7-425c-bff6-7d50268f6545 to disappear
May  6 19:58:20.507: INFO: Pod downward-api-39c417ed-a7f7-425c-bff6-7d50268f6545 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:58:20.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-341" for this suite.
May  6 19:58:26.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:58:26.631: INFO: namespace downward-api-341 deletion completed in 6.118132727s

• [SLOW TEST:8.238 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:58:26.631: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:58:29.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2048" for this suite.
May  6 19:58:57.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:58:57.822: INFO: namespace replication-controller-2048 deletion completed in 28.116644666s

• [SLOW TEST:31.191 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:58:57.822: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 19:58:57.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-126'
May  6 19:58:58.015: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 19:58:58.015: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
May  6 19:59:00.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete deployment e2e-test-httpd-deployment --namespace=kubectl-126'
May  6 19:59:00.102: INFO: stderr: ""
May  6 19:59:00.102: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:59:00.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-126" for this suite.
May  6 19:59:12.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 19:59:12.221: INFO: namespace kubectl-126 deletion completed in 12.111579611s

• [SLOW TEST:14.399 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 19:59:12.221: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 19:59:32.262: INFO: Container started at 2020-05-06 19:59:13 +0000 UTC, pod became ready at 2020-05-06 19:59:31 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 19:59:32.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9155" for this suite.
May  6 20:00:00.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:00:00.389: INFO: namespace container-probe-9155 deletion completed in 28.12245697s

• [SLOW TEST:48.168 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:00:00.389: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-b4c69a49-75b2-4d0c-9bd9-7e0e2b5c9e0d in namespace container-probe-7572
May  6 20:00:02.432: INFO: Started pod liveness-b4c69a49-75b2-4d0c-9bd9-7e0e2b5c9e0d in namespace container-probe-7572
STEP: checking the pod's current state and verifying that restartCount is present
May  6 20:00:02.435: INFO: Initial restart count of pod liveness-b4c69a49-75b2-4d0c-9bd9-7e0e2b5c9e0d is 0
May  6 20:00:22.529: INFO: Restart count of pod container-probe-7572/liveness-b4c69a49-75b2-4d0c-9bd9-7e0e2b5c9e0d is now 1 (20.093613289s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:00:22.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7572" for this suite.
May  6 20:00:28.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:00:28.661: INFO: namespace container-probe-7572 deletion completed in 6.116462163s

• [SLOW TEST:28.272 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:00:28.661: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
May  6 20:00:28.694: INFO: Waiting up to 5m0s for pod "pod-56b379ac-bf4b-4970-a2a6-5dd0eeef906e" in namespace "emptydir-656" to be "success or failure"
May  6 20:00:28.696: INFO: Pod "pod-56b379ac-bf4b-4970-a2a6-5dd0eeef906e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462464ms
May  6 20:00:30.700: INFO: Pod "pod-56b379ac-bf4b-4970-a2a6-5dd0eeef906e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006452019s
STEP: Saw pod success
May  6 20:00:30.700: INFO: Pod "pod-56b379ac-bf4b-4970-a2a6-5dd0eeef906e" satisfied condition "success or failure"
May  6 20:00:30.703: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-56b379ac-bf4b-4970-a2a6-5dd0eeef906e container test-container: <nil>
STEP: delete the pod
May  6 20:00:30.732: INFO: Waiting for pod pod-56b379ac-bf4b-4970-a2a6-5dd0eeef906e to disappear
May  6 20:00:30.734: INFO: Pod pod-56b379ac-bf4b-4970-a2a6-5dd0eeef906e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:00:30.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-656" for this suite.
May  6 20:00:36.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:00:36.857: INFO: namespace emptydir-656 deletion completed in 6.11915631s

• [SLOW TEST:8.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:00:36.857: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
May  6 20:00:36.884: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-161989487 proxy --unix-socket=/tmp/kubectl-proxy-unix124363814/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:00:36.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8922" for this suite.
May  6 20:00:42.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:00:43.047: INFO: namespace kubectl-8922 deletion completed in 6.114445906s

• [SLOW TEST:6.190 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:00:43.048: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  6 20:00:43.101: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:43.101: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:43.101: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:43.103: INFO: Number of nodes with available pods: 0
May  6 20:00:43.103: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 20:00:44.108: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:44.108: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:44.108: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:44.111: INFO: Number of nodes with available pods: 2
May  6 20:00:44.111: INFO: Node kube-node-2-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 20:00:45.109: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:45.109: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:45.109: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:45.112: INFO: Number of nodes with available pods: 3
May  6 20:00:45.112: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May  6 20:00:45.127: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:45.127: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:45.127: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:00:45.132: INFO: Number of nodes with available pods: 3
May  6 20:00:45.132: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6719, will wait for the garbage collector to delete the pods
May  6 20:00:46.202: INFO: Deleting DaemonSet.extensions daemon-set took: 8.359496ms
May  6 20:00:48.502: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.300191355s
May  6 20:00:58.705: INFO: Number of nodes with available pods: 0
May  6 20:00:58.705: INFO: Number of running nodes: 0, number of available pods: 0
May  6 20:00:58.709: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6719/daemonsets","resourceVersion":"48198"},"items":null}

May  6 20:00:58.712: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6719/pods","resourceVersion":"48198"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:00:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6719" for this suite.
May  6 20:01:04.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:01:04.839: INFO: namespace daemonsets-6719 deletion completed in 6.113213427s

• [SLOW TEST:21.792 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:01:04.840: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:01:04.868: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  6 20:01:07.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-1227 create -f -'
May  6 20:01:08.057: INFO: stderr: ""
May  6 20:01:08.057: INFO: stdout: "e2e-test-crd-publish-openapi-9543-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  6 20:01:08.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-1227 delete e2e-test-crd-publish-openapi-9543-crds test-cr'
May  6 20:01:08.123: INFO: stderr: ""
May  6 20:01:08.123: INFO: stdout: "e2e-test-crd-publish-openapi-9543-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  6 20:01:08.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-1227 apply -f -'
May  6 20:01:08.343: INFO: stderr: ""
May  6 20:01:08.343: INFO: stdout: "e2e-test-crd-publish-openapi-9543-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  6 20:01:08.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-1227 delete e2e-test-crd-publish-openapi-9543-crds test-cr'
May  6 20:01:08.439: INFO: stderr: ""
May  6 20:01:08.439: INFO: stdout: "e2e-test-crd-publish-openapi-9543-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May  6 20:01:08.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-9543-crds'
May  6 20:01:08.669: INFO: stderr: ""
May  6 20:01:08.669: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9543-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:01:12.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1227" for this suite.
May  6 20:01:18.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:01:18.150: INFO: namespace crd-publish-openapi-1227 deletion completed in 6.107484462s

• [SLOW TEST:13.311 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:01:18.151: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:01:18.178: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:01:23.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8338" for this suite.
May  6 20:01:29.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:01:29.645: INFO: namespace custom-resource-definition-8338 deletion completed in 6.110622161s

• [SLOW TEST:11.494 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:01:29.645: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:01:29.670: INFO: Creating ReplicaSet my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41
May  6 20:01:29.680: INFO: Pod name my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41: Found 0 pods out of 1
May  6 20:01:34.684: INFO: Pod name my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41: Found 1 pods out of 1
May  6 20:01:34.684: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41" is running
May  6 20:01:34.687: INFO: Pod "my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41-vnhcl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:01:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:01:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:01:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:01:29 +0000 UTC Reason: Message:}])
May  6 20:01:34.687: INFO: Trying to dial the pod
May  6 20:01:39.699: INFO: Controller my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41: Got expected result from replica 1 [my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41-vnhcl]: "my-hostname-basic-18ccb47a-2a79-4f58-a8d2-2c008e7e7c41-vnhcl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:01:39.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8112" for this suite.
May  6 20:01:45.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:01:45.817: INFO: namespace replicaset-8112 deletion completed in 6.110057784s

• [SLOW TEST:16.173 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:01:45.818: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-91147ef5-aa76-4481-8331-13e9a5144b28
STEP: Creating a pod to test consume secrets
May  6 20:01:45.855: INFO: Waiting up to 5m0s for pod "pod-secrets-2625d07b-4566-47f1-9cad-25c0bdbdc7f5" in namespace "secrets-7727" to be "success or failure"
May  6 20:01:45.857: INFO: Pod "pod-secrets-2625d07b-4566-47f1-9cad-25c0bdbdc7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361369ms
May  6 20:01:47.860: INFO: Pod "pod-secrets-2625d07b-4566-47f1-9cad-25c0bdbdc7f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005342517s
STEP: Saw pod success
May  6 20:01:47.860: INFO: Pod "pod-secrets-2625d07b-4566-47f1-9cad-25c0bdbdc7f5" satisfied condition "success or failure"
May  6 20:01:47.863: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-2625d07b-4566-47f1-9cad-25c0bdbdc7f5 container secret-volume-test: <nil>
STEP: delete the pod
May  6 20:01:47.886: INFO: Waiting for pod pod-secrets-2625d07b-4566-47f1-9cad-25c0bdbdc7f5 to disappear
May  6 20:01:47.889: INFO: Pod pod-secrets-2625d07b-4566-47f1-9cad-25c0bdbdc7f5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:01:47.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7727" for this suite.
May  6 20:01:53.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:01:54.018: INFO: namespace secrets-7727 deletion completed in 6.125446252s

• [SLOW TEST:8.201 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:01:54.018: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-17f729b7-4b88-49ad-9097-ac2ed75e3f38
STEP: Creating a pod to test consume configMaps
May  6 20:01:54.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1286d1f9-6b6b-4010-8541-f9e4d8a3affa" in namespace "projected-3497" to be "success or failure"
May  6 20:01:54.063: INFO: Pod "pod-projected-configmaps-1286d1f9-6b6b-4010-8541-f9e4d8a3affa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.279514ms
May  6 20:01:56.066: INFO: Pod "pod-projected-configmaps-1286d1f9-6b6b-4010-8541-f9e4d8a3affa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008419983s
STEP: Saw pod success
May  6 20:01:56.066: INFO: Pod "pod-projected-configmaps-1286d1f9-6b6b-4010-8541-f9e4d8a3affa" satisfied condition "success or failure"
May  6 20:01:56.068: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-configmaps-1286d1f9-6b6b-4010-8541-f9e4d8a3affa container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:01:56.087: INFO: Waiting for pod pod-projected-configmaps-1286d1f9-6b6b-4010-8541-f9e4d8a3affa to disappear
May  6 20:01:56.090: INFO: Pod pod-projected-configmaps-1286d1f9-6b6b-4010-8541-f9e4d8a3affa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:01:56.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3497" for this suite.
May  6 20:02:02.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:02:02.202: INFO: namespace projected-3497 deletion completed in 6.109338079s

• [SLOW TEST:8.184 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:02:02.203: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:02:02.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9fd453d-c7ef-4dd8-84a2-b0d2c98362c2" in namespace "projected-2073" to be "success or failure"
May  6 20:02:02.241: INFO: Pod "downwardapi-volume-f9fd453d-c7ef-4dd8-84a2-b0d2c98362c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.031316ms
May  6 20:02:04.244: INFO: Pod "downwardapi-volume-f9fd453d-c7ef-4dd8-84a2-b0d2c98362c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006301373s
STEP: Saw pod success
May  6 20:02:04.244: INFO: Pod "downwardapi-volume-f9fd453d-c7ef-4dd8-84a2-b0d2c98362c2" satisfied condition "success or failure"
May  6 20:02:04.247: INFO: Trying to get logs from node kube-node-1-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-f9fd453d-c7ef-4dd8-84a2-b0d2c98362c2 container client-container: <nil>
STEP: delete the pod
May  6 20:02:04.275: INFO: Waiting for pod downwardapi-volume-f9fd453d-c7ef-4dd8-84a2-b0d2c98362c2 to disappear
May  6 20:02:04.278: INFO: Pod downwardapi-volume-f9fd453d-c7ef-4dd8-84a2-b0d2c98362c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:02:04.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2073" for this suite.
May  6 20:02:10.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:02:10.392: INFO: namespace projected-2073 deletion completed in 6.110362829s

• [SLOW TEST:8.189 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:02:10.392: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:02:12.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3600" for this suite.
May  6 20:02:58.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:02:58.599: INFO: namespace kubelet-test-3600 deletion completed in 46.149507572s

• [SLOW TEST:48.207 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:02:58.599: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:03:00.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6550" for this suite.
May  6 20:03:06.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:03:06.809: INFO: namespace emptydir-wrapper-6550 deletion completed in 6.123209159s

• [SLOW TEST:8.209 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:03:06.809: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 20:03:06.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6076'
May  6 20:03:06.906: INFO: stderr: ""
May  6 20:03:06.906: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May  6 20:03:11.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pod e2e-test-httpd-pod --namespace=kubectl-6076 -o json'
May  6 20:03:12.012: INFO: stderr: ""
May  6 20:03:12.012: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.32.209/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.32.209/32\"\n        },\n        \"creationTimestamp\": \"2020-05-06T20:03:06Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6076\",\n        \"resourceVersion\": \"48789\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6076/pods/e2e-test-httpd-pod\",\n        \"uid\": \"66ab8a84-2a8a-41be-98a2-484ca9390ed4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-c25dz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-node-2-kubelet.kubernetes-cluster.mesos\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-c25dz\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-c25dz\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T20:03:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T20:03:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T20:03:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-06T20:03:06Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a2601ca7fd4a9232b0b2712f2d0094b4c1ba82bfc9d9669f493d492fb86eb4cf\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-05-06T20:03:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"9.0.1.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.32.209\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.32.209\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-05-06T20:03:06Z\"\n    }\n}\n"
STEP: replace the image in the pod
May  6 20:03:12.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 replace -f - --namespace=kubectl-6076'
May  6 20:03:12.196: INFO: stderr: ""
May  6 20:03:12.196: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
May  6 20:03:12.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete pods e2e-test-httpd-pod --namespace=kubectl-6076'
May  6 20:03:17.858: INFO: stderr: ""
May  6 20:03:17.858: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:03:17.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6076" for this suite.
May  6 20:03:23.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:03:23.985: INFO: namespace kubectl-6076 deletion completed in 6.121785581s

• [SLOW TEST:17.176 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:03:23.986: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:03:40.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1873" for this suite.
May  6 20:03:46.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:03:46.356: INFO: namespace resourcequota-1873 deletion completed in 6.187963541s

• [SLOW TEST:22.371 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:03:46.357: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:04:03.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3951" for this suite.
May  6 20:04:09.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:04:09.544: INFO: namespace resourcequota-3951 deletion completed in 6.118253959s

• [SLOW TEST:23.188 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:04:09.545: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
May  6 20:04:09.578: INFO: Waiting up to 5m0s for pod "client-containers-ce862261-16ea-4a29-9640-20b6e224bc4d" in namespace "containers-3536" to be "success or failure"
May  6 20:04:09.581: INFO: Pod "client-containers-ce862261-16ea-4a29-9640-20b6e224bc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.442115ms
May  6 20:04:11.584: INFO: Pod "client-containers-ce862261-16ea-4a29-9640-20b6e224bc4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006091952s
STEP: Saw pod success
May  6 20:04:11.584: INFO: Pod "client-containers-ce862261-16ea-4a29-9640-20b6e224bc4d" satisfied condition "success or failure"
May  6 20:04:11.588: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod client-containers-ce862261-16ea-4a29-9640-20b6e224bc4d container test-container: <nil>
STEP: delete the pod
May  6 20:04:11.615: INFO: Waiting for pod client-containers-ce862261-16ea-4a29-9640-20b6e224bc4d to disappear
May  6 20:04:11.618: INFO: Pod client-containers-ce862261-16ea-4a29-9640-20b6e224bc4d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:04:11.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3536" for this suite.
May  6 20:04:17.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:04:17.730: INFO: namespace containers-3536 deletion completed in 6.108067094s

• [SLOW TEST:8.185 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:04:17.730: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:04:17.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-739f5406-53d5-49bd-a358-ab8411aaadcb" in namespace "downward-api-2214" to be "success or failure"
May  6 20:04:17.768: INFO: Pod "downwardapi-volume-739f5406-53d5-49bd-a358-ab8411aaadcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.583593ms
May  6 20:04:19.772: INFO: Pod "downwardapi-volume-739f5406-53d5-49bd-a358-ab8411aaadcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008331235s
STEP: Saw pod success
May  6 20:04:19.772: INFO: Pod "downwardapi-volume-739f5406-53d5-49bd-a358-ab8411aaadcb" satisfied condition "success or failure"
May  6 20:04:19.775: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-739f5406-53d5-49bd-a358-ab8411aaadcb container client-container: <nil>
STEP: delete the pod
May  6 20:04:19.795: INFO: Waiting for pod downwardapi-volume-739f5406-53d5-49bd-a358-ab8411aaadcb to disappear
May  6 20:04:19.798: INFO: Pod downwardapi-volume-739f5406-53d5-49bd-a358-ab8411aaadcb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:04:19.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2214" for this suite.
May  6 20:04:25.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:04:25.915: INFO: namespace downward-api-2214 deletion completed in 6.112270449s

• [SLOW TEST:8.185 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:04:25.915: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7917.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7917.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 20:04:27.987: INFO: DNS probes using dns-7917/dns-test-fa7bc8bf-a585-4c7e-bf11-10e8300e4ba9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:04:28.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7917" for this suite.
May  6 20:04:34.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:04:34.119: INFO: namespace dns-7917 deletion completed in 6.11372457s

• [SLOW TEST:8.204 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:04:34.119: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  6 20:04:34.156: INFO: Waiting up to 5m0s for pod "pod-20774535-5e83-4045-83c2-19fd50ce5765" in namespace "emptydir-9565" to be "success or failure"
May  6 20:04:34.158: INFO: Pod "pod-20774535-5e83-4045-83c2-19fd50ce5765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.350261ms
May  6 20:04:36.161: INFO: Pod "pod-20774535-5e83-4045-83c2-19fd50ce5765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005485962s
STEP: Saw pod success
May  6 20:04:36.161: INFO: Pod "pod-20774535-5e83-4045-83c2-19fd50ce5765" satisfied condition "success or failure"
May  6 20:04:36.165: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-20774535-5e83-4045-83c2-19fd50ce5765 container test-container: <nil>
STEP: delete the pod
May  6 20:04:36.183: INFO: Waiting for pod pod-20774535-5e83-4045-83c2-19fd50ce5765 to disappear
May  6 20:04:36.187: INFO: Pod pod-20774535-5e83-4045-83c2-19fd50ce5765 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:04:36.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9565" for this suite.
May  6 20:04:42.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:04:42.304: INFO: namespace emptydir-9565 deletion completed in 6.112628346s

• [SLOW TEST:8.185 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:04:42.304: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0506 20:04:52.389427      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  6 20:04:52.389: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:04:52.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3230" for this suite.
May  6 20:04:58.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:04:58.512: INFO: namespace gc-3230 deletion completed in 6.119384868s

• [SLOW TEST:16.208 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:04:58.512: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:04:58.541: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:04:59.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-903" for this suite.
May  6 20:05:05.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:05:05.711: INFO: namespace custom-resource-definition-903 deletion completed in 6.141909252s

• [SLOW TEST:7.199 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:05:05.712: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:05:05.762: INFO: Create a RollingUpdate DaemonSet
May  6 20:05:05.767: INFO: Check that daemon pods launch on every node of the cluster
May  6 20:05:05.771: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:05.771: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:05.771: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:05.773: INFO: Number of nodes with available pods: 0
May  6 20:05:05.773: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 20:05:06.778: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:06.778: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:06.778: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:06.781: INFO: Number of nodes with available pods: 0
May  6 20:05:06.781: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 20:05:07.777: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:07.777: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:07.777: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:07.780: INFO: Number of nodes with available pods: 3
May  6 20:05:07.780: INFO: Number of running nodes: 3, number of available pods: 3
May  6 20:05:07.780: INFO: Update the DaemonSet to trigger a rollout
May  6 20:05:07.788: INFO: Updating DaemonSet daemon-set
May  6 20:05:18.803: INFO: Roll back the DaemonSet before rollout is complete
May  6 20:05:18.810: INFO: Updating DaemonSet daemon-set
May  6 20:05:18.810: INFO: Make sure DaemonSet rollback is complete
May  6 20:05:18.813: INFO: Wrong image for pod: daemon-set-6p4pm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May  6 20:05:18.813: INFO: Pod daemon-set-6p4pm is not available
May  6 20:05:18.816: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:18.816: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:18.816: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:19.820: INFO: Wrong image for pod: daemon-set-6p4pm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May  6 20:05:19.820: INFO: Pod daemon-set-6p4pm is not available
May  6 20:05:19.825: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:19.825: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:19.825: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:20.819: INFO: Pod daemon-set-gmvtc is not available
May  6 20:05:20.823: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:20.823: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 20:05:20.823: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7477, will wait for the garbage collector to delete the pods
May  6 20:05:20.891: INFO: Deleting DaemonSet.extensions daemon-set took: 8.210479ms
May  6 20:05:21.291: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.176511ms
May  6 20:06:46.194: INFO: Number of nodes with available pods: 0
May  6 20:06:46.195: INFO: Number of running nodes: 0, number of available pods: 0
May  6 20:06:46.197: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7477/daemonsets","resourceVersion":"49910"},"items":null}

May  6 20:06:46.201: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7477/pods","resourceVersion":"49910"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:06:46.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7477" for this suite.
May  6 20:06:52.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:06:52.335: INFO: namespace daemonsets-7477 deletion completed in 6.115776766s

• [SLOW TEST:106.623 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:06:52.335: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5452
I0506 20:06:52.366432      22 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5452, replica count: 1
I0506 20:06:53.416720      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0506 20:06:54.416876      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 20:06:54.527: INFO: Created: latency-svc-wrfrw
May  6 20:06:54.533: INFO: Got endpoints: latency-svc-wrfrw [16.131506ms]
May  6 20:06:54.542: INFO: Created: latency-svc-zqgz2
May  6 20:06:54.567: INFO: Created: latency-svc-kcrqx
May  6 20:06:54.567: INFO: Got endpoints: latency-svc-zqgz2 [34.475231ms]
May  6 20:06:54.574: INFO: Got endpoints: latency-svc-kcrqx [40.784459ms]
May  6 20:06:54.578: INFO: Created: latency-svc-j6nzg
May  6 20:06:54.583: INFO: Got endpoints: latency-svc-j6nzg [49.787763ms]
May  6 20:06:54.584: INFO: Created: latency-svc-r9tdc
May  6 20:06:54.595: INFO: Got endpoints: latency-svc-r9tdc [61.298508ms]
May  6 20:06:54.600: INFO: Created: latency-svc-s8685
May  6 20:06:54.603: INFO: Created: latency-svc-v6kbv
May  6 20:06:54.605: INFO: Got endpoints: latency-svc-s8685 [71.52975ms]
May  6 20:06:54.610: INFO: Created: latency-svc-mlckd
May  6 20:06:54.610: INFO: Got endpoints: latency-svc-v6kbv [76.963897ms]
May  6 20:06:54.618: INFO: Got endpoints: latency-svc-mlckd [84.903689ms]
May  6 20:06:54.621: INFO: Created: latency-svc-dkdwt
May  6 20:06:54.637: INFO: Got endpoints: latency-svc-dkdwt [103.627101ms]
May  6 20:06:54.641: INFO: Created: latency-svc-gbxf6
May  6 20:06:54.646: INFO: Created: latency-svc-2b5r4
May  6 20:06:54.652: INFO: Got endpoints: latency-svc-gbxf6 [118.207591ms]
May  6 20:06:54.658: INFO: Got endpoints: latency-svc-2b5r4 [124.091875ms]
May  6 20:06:54.682: INFO: Created: latency-svc-5t274
May  6 20:06:54.688: INFO: Created: latency-svc-nfnj6
May  6 20:06:54.688: INFO: Got endpoints: latency-svc-5t274 [154.500458ms]
May  6 20:06:54.692: INFO: Got endpoints: latency-svc-nfnj6 [158.278834ms]
May  6 20:06:54.695: INFO: Created: latency-svc-dskt8
May  6 20:06:54.700: INFO: Got endpoints: latency-svc-dskt8 [166.675948ms]
May  6 20:06:54.709: INFO: Created: latency-svc-rbrlh
May  6 20:06:54.715: INFO: Got endpoints: latency-svc-rbrlh [181.530859ms]
May  6 20:06:54.719: INFO: Created: latency-svc-xltmg
May  6 20:06:54.725: INFO: Got endpoints: latency-svc-xltmg [191.252458ms]
May  6 20:06:54.725: INFO: Created: latency-svc-prt9x
May  6 20:06:54.783: INFO: Got endpoints: latency-svc-prt9x [216.143887ms]
May  6 20:06:54.790: INFO: Created: latency-svc-dxlqz
May  6 20:06:54.794: INFO: Got endpoints: latency-svc-dxlqz [220.331352ms]
May  6 20:06:54.794: INFO: Created: latency-svc-qncvs
May  6 20:06:54.799: INFO: Got endpoints: latency-svc-qncvs [216.030646ms]
May  6 20:06:54.809: INFO: Created: latency-svc-dxk5z
May  6 20:06:54.896: INFO: Created: latency-svc-4n7dl
May  6 20:06:54.898: INFO: Got endpoints: latency-svc-dxk5z [302.747859ms]
May  6 20:06:54.900: INFO: Created: latency-svc-xzmll
May  6 20:06:54.902: INFO: Got endpoints: latency-svc-4n7dl [297.267209ms]
May  6 20:06:54.906: INFO: Created: latency-svc-nc8p7
May  6 20:06:54.909: INFO: Got endpoints: latency-svc-xzmll [298.066605ms]
May  6 20:06:54.910: INFO: Got endpoints: latency-svc-nc8p7 [291.515091ms]
May  6 20:06:54.983: INFO: Created: latency-svc-nkg5b
May  6 20:06:54.989: INFO: Got endpoints: latency-svc-nkg5b [351.399322ms]
May  6 20:06:54.989: INFO: Created: latency-svc-wd8sr
May  6 20:06:54.993: INFO: Got endpoints: latency-svc-wd8sr [341.356676ms]
May  6 20:06:54.996: INFO: Created: latency-svc-z24vk
May  6 20:06:55.001: INFO: Got endpoints: latency-svc-z24vk [343.74561ms]
May  6 20:06:55.006: INFO: Created: latency-svc-7rw7x
May  6 20:06:55.014: INFO: Got endpoints: latency-svc-7rw7x [325.539579ms]
May  6 20:06:55.017: INFO: Created: latency-svc-m96bj
May  6 20:06:55.024: INFO: Created: latency-svc-92r7q
May  6 20:06:55.024: INFO: Got endpoints: latency-svc-m96bj [332.273767ms]
May  6 20:06:55.030: INFO: Got endpoints: latency-svc-92r7q [329.958698ms]
May  6 20:06:55.034: INFO: Created: latency-svc-m7kd5
May  6 20:06:55.039: INFO: Got endpoints: latency-svc-m7kd5 [323.321566ms]
May  6 20:06:55.042: INFO: Created: latency-svc-lrqz5
May  6 20:06:55.046: INFO: Got endpoints: latency-svc-lrqz5 [320.901405ms]
May  6 20:06:55.050: INFO: Created: latency-svc-zxtm5
May  6 20:06:55.056: INFO: Got endpoints: latency-svc-zxtm5 [272.019619ms]
May  6 20:06:55.059: INFO: Created: latency-svc-k7dsz
May  6 20:06:55.082: INFO: Got endpoints: latency-svc-k7dsz [287.923395ms]
May  6 20:06:55.085: INFO: Created: latency-svc-8ht84
May  6 20:06:55.091: INFO: Got endpoints: latency-svc-8ht84 [291.416169ms]
May  6 20:06:55.093: INFO: Created: latency-svc-pr4kr
May  6 20:06:55.100: INFO: Got endpoints: latency-svc-pr4kr [202.411308ms]
May  6 20:06:55.104: INFO: Created: latency-svc-xlljt
May  6 20:06:55.109: INFO: Got endpoints: latency-svc-xlljt [206.815422ms]
May  6 20:06:55.112: INFO: Created: latency-svc-bl64z
May  6 20:06:55.117: INFO: Got endpoints: latency-svc-bl64z [208.104081ms]
May  6 20:06:55.119: INFO: Created: latency-svc-8dn84
May  6 20:06:55.125: INFO: Got endpoints: latency-svc-8dn84 [214.95907ms]
May  6 20:06:55.128: INFO: Created: latency-svc-p44v4
May  6 20:06:55.133: INFO: Got endpoints: latency-svc-p44v4 [144.121645ms]
May  6 20:06:55.137: INFO: Created: latency-svc-xwnhj
May  6 20:06:55.142: INFO: Got endpoints: latency-svc-xwnhj [148.325408ms]
May  6 20:06:55.143: INFO: Created: latency-svc-zhwjp
May  6 20:06:55.147: INFO: Got endpoints: latency-svc-zhwjp [145.871445ms]
May  6 20:06:55.155: INFO: Created: latency-svc-22fgm
May  6 20:06:55.182: INFO: Got endpoints: latency-svc-22fgm [168.197421ms]
May  6 20:06:55.186: INFO: Created: latency-svc-fjqnv
May  6 20:06:55.191: INFO: Got endpoints: latency-svc-fjqnv [166.396966ms]
May  6 20:06:55.193: INFO: Created: latency-svc-c22cv
May  6 20:06:55.204: INFO: Created: latency-svc-t7swk
May  6 20:06:55.211: INFO: Created: latency-svc-55bn9
May  6 20:06:55.218: INFO: Created: latency-svc-2vk77
May  6 20:06:55.225: INFO: Created: latency-svc-855xk
May  6 20:06:55.231: INFO: Got endpoints: latency-svc-c22cv [201.180386ms]
May  6 20:06:55.233: INFO: Created: latency-svc-hz4qc
May  6 20:06:55.240: INFO: Created: latency-svc-59wtw
May  6 20:06:55.247: INFO: Created: latency-svc-76wc8
May  6 20:06:55.254: INFO: Created: latency-svc-86cm6
May  6 20:06:55.260: INFO: Created: latency-svc-7bqll
May  6 20:06:55.270: INFO: Created: latency-svc-j2blh
May  6 20:06:55.277: INFO: Created: latency-svc-l6cjj
May  6 20:06:55.286: INFO: Created: latency-svc-9sjlq
May  6 20:06:55.286: INFO: Got endpoints: latency-svc-t7swk [247.873723ms]
May  6 20:06:55.311: INFO: Created: latency-svc-pbz22
May  6 20:06:55.321: INFO: Created: latency-svc-wppp5
May  6 20:06:55.326: INFO: Created: latency-svc-grc9f
May  6 20:06:55.332: INFO: Got endpoints: latency-svc-55bn9 [285.850178ms]
May  6 20:06:55.335: INFO: Created: latency-svc-8gft8
May  6 20:06:55.344: INFO: Created: latency-svc-c4k2x
May  6 20:06:55.382: INFO: Got endpoints: latency-svc-2vk77 [326.115783ms]
May  6 20:06:55.392: INFO: Created: latency-svc-pr7p6
May  6 20:06:55.431: INFO: Got endpoints: latency-svc-855xk [349.171987ms]
May  6 20:06:55.443: INFO: Created: latency-svc-ls7xb
May  6 20:06:55.481: INFO: Got endpoints: latency-svc-hz4qc [390.354914ms]
May  6 20:06:55.493: INFO: Created: latency-svc-lwgl5
May  6 20:06:55.532: INFO: Got endpoints: latency-svc-59wtw [431.576001ms]
May  6 20:06:55.542: INFO: Created: latency-svc-4lhll
May  6 20:06:55.582: INFO: Got endpoints: latency-svc-76wc8 [472.534741ms]
May  6 20:06:55.593: INFO: Created: latency-svc-5gvr5
May  6 20:06:55.631: INFO: Got endpoints: latency-svc-86cm6 [514.688584ms]
May  6 20:06:55.643: INFO: Created: latency-svc-8259w
May  6 20:06:55.681: INFO: Got endpoints: latency-svc-7bqll [556.237631ms]
May  6 20:06:55.693: INFO: Created: latency-svc-dt8bb
May  6 20:06:55.732: INFO: Got endpoints: latency-svc-j2blh [599.601256ms]
May  6 20:06:55.744: INFO: Created: latency-svc-l5djg
May  6 20:06:55.781: INFO: Got endpoints: latency-svc-l6cjj [639.632276ms]
May  6 20:06:55.792: INFO: Created: latency-svc-m7xnx
May  6 20:06:55.831: INFO: Got endpoints: latency-svc-9sjlq [683.851409ms]
May  6 20:06:55.843: INFO: Created: latency-svc-cwc89
May  6 20:06:55.882: INFO: Got endpoints: latency-svc-pbz22 [700.154409ms]
May  6 20:06:55.894: INFO: Created: latency-svc-cdn2l
May  6 20:06:55.931: INFO: Got endpoints: latency-svc-wppp5 [740.521379ms]
May  6 20:06:55.944: INFO: Created: latency-svc-9njg8
May  6 20:06:55.981: INFO: Got endpoints: latency-svc-grc9f [749.81806ms]
May  6 20:06:55.991: INFO: Created: latency-svc-thqw5
May  6 20:06:56.032: INFO: Got endpoints: latency-svc-8gft8 [745.284731ms]
May  6 20:06:56.043: INFO: Created: latency-svc-s925h
May  6 20:06:56.081: INFO: Got endpoints: latency-svc-c4k2x [749.11821ms]
May  6 20:06:56.092: INFO: Created: latency-svc-tjbfb
May  6 20:06:56.132: INFO: Got endpoints: latency-svc-pr7p6 [749.938428ms]
May  6 20:06:56.143: INFO: Created: latency-svc-j7nfn
May  6 20:06:56.182: INFO: Got endpoints: latency-svc-ls7xb [750.323917ms]
May  6 20:06:56.193: INFO: Created: latency-svc-nhxv9
May  6 20:06:56.231: INFO: Got endpoints: latency-svc-lwgl5 [750.443672ms]
May  6 20:06:56.244: INFO: Created: latency-svc-r7g94
May  6 20:06:56.281: INFO: Got endpoints: latency-svc-4lhll [749.822834ms]
May  6 20:06:56.292: INFO: Created: latency-svc-qg2g2
May  6 20:06:56.332: INFO: Got endpoints: latency-svc-5gvr5 [750.096989ms]
May  6 20:06:56.343: INFO: Created: latency-svc-xsn9d
May  6 20:06:56.381: INFO: Got endpoints: latency-svc-8259w [749.678378ms]
May  6 20:06:56.393: INFO: Created: latency-svc-nb6sz
May  6 20:06:56.431: INFO: Got endpoints: latency-svc-dt8bb [750.236951ms]
May  6 20:06:56.445: INFO: Created: latency-svc-2jxm5
May  6 20:06:56.481: INFO: Got endpoints: latency-svc-l5djg [748.980668ms]
May  6 20:06:56.493: INFO: Created: latency-svc-n6bn7
May  6 20:06:56.532: INFO: Got endpoints: latency-svc-m7xnx [750.535539ms]
May  6 20:06:56.547: INFO: Created: latency-svc-8f8fs
May  6 20:06:56.582: INFO: Got endpoints: latency-svc-cwc89 [750.897711ms]
May  6 20:06:56.593: INFO: Created: latency-svc-kx7lk
May  6 20:06:56.633: INFO: Got endpoints: latency-svc-cdn2l [750.781085ms]
May  6 20:06:56.644: INFO: Created: latency-svc-l65pp
May  6 20:06:56.681: INFO: Got endpoints: latency-svc-9njg8 [749.981086ms]
May  6 20:06:56.692: INFO: Created: latency-svc-7ctng
May  6 20:06:56.732: INFO: Got endpoints: latency-svc-thqw5 [750.201862ms]
May  6 20:06:56.742: INFO: Created: latency-svc-zfhxn
May  6 20:06:56.781: INFO: Got endpoints: latency-svc-s925h [749.23706ms]
May  6 20:06:56.794: INFO: Created: latency-svc-2qbrl
May  6 20:06:56.832: INFO: Got endpoints: latency-svc-tjbfb [751.145226ms]
May  6 20:06:56.843: INFO: Created: latency-svc-q7l8v
May  6 20:06:56.882: INFO: Got endpoints: latency-svc-j7nfn [750.113522ms]
May  6 20:06:56.892: INFO: Created: latency-svc-86n59
May  6 20:06:56.932: INFO: Got endpoints: latency-svc-nhxv9 [749.746338ms]
May  6 20:06:56.944: INFO: Created: latency-svc-6zq7b
May  6 20:06:56.982: INFO: Got endpoints: latency-svc-r7g94 [750.168207ms]
May  6 20:06:56.993: INFO: Created: latency-svc-2sm7m
May  6 20:06:57.032: INFO: Got endpoints: latency-svc-qg2g2 [750.276923ms]
May  6 20:06:57.042: INFO: Created: latency-svc-88gfq
May  6 20:06:57.082: INFO: Got endpoints: latency-svc-xsn9d [749.767665ms]
May  6 20:06:57.095: INFO: Created: latency-svc-92dhq
May  6 20:06:57.131: INFO: Got endpoints: latency-svc-nb6sz [749.503799ms]
May  6 20:06:57.143: INFO: Created: latency-svc-7z97f
May  6 20:06:57.182: INFO: Got endpoints: latency-svc-2jxm5 [750.327551ms]
May  6 20:06:57.193: INFO: Created: latency-svc-h8cz4
May  6 20:06:57.232: INFO: Got endpoints: latency-svc-n6bn7 [750.759571ms]
May  6 20:06:57.245: INFO: Created: latency-svc-zpf7h
May  6 20:06:57.281: INFO: Got endpoints: latency-svc-8f8fs [749.401683ms]
May  6 20:06:57.294: INFO: Created: latency-svc-mvc29
May  6 20:06:57.332: INFO: Got endpoints: latency-svc-kx7lk [749.397068ms]
May  6 20:06:57.343: INFO: Created: latency-svc-ck6tr
May  6 20:06:57.382: INFO: Got endpoints: latency-svc-l65pp [749.188074ms]
May  6 20:06:57.394: INFO: Created: latency-svc-5rh9d
May  6 20:06:57.434: INFO: Got endpoints: latency-svc-7ctng [753.01901ms]
May  6 20:06:57.458: INFO: Created: latency-svc-zmmrr
May  6 20:06:57.483: INFO: Got endpoints: latency-svc-zfhxn [750.960342ms]
May  6 20:06:57.506: INFO: Created: latency-svc-5lbbq
May  6 20:06:57.552: INFO: Got endpoints: latency-svc-2qbrl [770.576847ms]
May  6 20:06:57.566: INFO: Created: latency-svc-xxvpg
May  6 20:06:57.582: INFO: Got endpoints: latency-svc-q7l8v [749.54508ms]
May  6 20:06:57.652: INFO: Got endpoints: latency-svc-86n59 [769.891213ms]
May  6 20:06:57.655: INFO: Created: latency-svc-rh4tk
May  6 20:06:57.666: INFO: Created: latency-svc-k2fk7
May  6 20:06:57.683: INFO: Got endpoints: latency-svc-6zq7b [751.890638ms]
May  6 20:06:57.695: INFO: Created: latency-svc-rv76c
May  6 20:06:57.731: INFO: Got endpoints: latency-svc-2sm7m [749.299831ms]
May  6 20:06:57.742: INFO: Created: latency-svc-kw4rl
May  6 20:06:57.782: INFO: Got endpoints: latency-svc-88gfq [750.41263ms]
May  6 20:06:57.792: INFO: Created: latency-svc-9g9rv
May  6 20:06:57.832: INFO: Got endpoints: latency-svc-92dhq [750.139158ms]
May  6 20:06:57.847: INFO: Created: latency-svc-79sjt
May  6 20:06:57.881: INFO: Got endpoints: latency-svc-7z97f [750.50196ms]
May  6 20:06:57.917: INFO: Created: latency-svc-wkvzg
May  6 20:06:57.932: INFO: Got endpoints: latency-svc-h8cz4 [749.759675ms]
May  6 20:06:57.942: INFO: Created: latency-svc-nmdfr
May  6 20:06:57.982: INFO: Got endpoints: latency-svc-zpf7h [749.798597ms]
May  6 20:06:57.995: INFO: Created: latency-svc-vfl7h
May  6 20:06:58.031: INFO: Got endpoints: latency-svc-mvc29 [749.970315ms]
May  6 20:06:58.044: INFO: Created: latency-svc-sjkcn
May  6 20:06:58.081: INFO: Got endpoints: latency-svc-ck6tr [749.855623ms]
May  6 20:06:58.095: INFO: Created: latency-svc-q2fcx
May  6 20:06:58.132: INFO: Got endpoints: latency-svc-5rh9d [749.442217ms]
May  6 20:06:58.143: INFO: Created: latency-svc-snfcj
May  6 20:06:58.181: INFO: Got endpoints: latency-svc-zmmrr [747.212209ms]
May  6 20:06:58.193: INFO: Created: latency-svc-rh58w
May  6 20:06:58.231: INFO: Got endpoints: latency-svc-5lbbq [748.70586ms]
May  6 20:06:58.242: INFO: Created: latency-svc-rb4bz
May  6 20:06:58.283: INFO: Got endpoints: latency-svc-xxvpg [731.738149ms]
May  6 20:06:58.296: INFO: Created: latency-svc-4r6sm
May  6 20:06:58.332: INFO: Got endpoints: latency-svc-rh4tk [749.877265ms]
May  6 20:06:58.344: INFO: Created: latency-svc-v9glt
May  6 20:06:58.382: INFO: Got endpoints: latency-svc-k2fk7 [729.912202ms]
May  6 20:06:58.392: INFO: Created: latency-svc-27klm
May  6 20:06:58.432: INFO: Got endpoints: latency-svc-rv76c [748.231744ms]
May  6 20:06:58.443: INFO: Created: latency-svc-jg8t9
May  6 20:06:58.481: INFO: Got endpoints: latency-svc-kw4rl [749.979531ms]
May  6 20:06:58.493: INFO: Created: latency-svc-cr2wm
May  6 20:06:58.532: INFO: Got endpoints: latency-svc-9g9rv [749.550309ms]
May  6 20:06:58.544: INFO: Created: latency-svc-f92lq
May  6 20:06:58.582: INFO: Got endpoints: latency-svc-79sjt [749.865233ms]
May  6 20:06:58.594: INFO: Created: latency-svc-pmkwk
May  6 20:06:58.631: INFO: Got endpoints: latency-svc-wkvzg [749.921954ms]
May  6 20:06:58.645: INFO: Created: latency-svc-2b7fl
May  6 20:06:58.682: INFO: Got endpoints: latency-svc-nmdfr [749.960098ms]
May  6 20:06:58.693: INFO: Created: latency-svc-kb7qh
May  6 20:06:58.732: INFO: Got endpoints: latency-svc-vfl7h [749.437269ms]
May  6 20:06:58.744: INFO: Created: latency-svc-jwdx4
May  6 20:06:58.784: INFO: Got endpoints: latency-svc-sjkcn [753.194523ms]
May  6 20:06:58.797: INFO: Created: latency-svc-s2rj9
May  6 20:06:58.832: INFO: Got endpoints: latency-svc-q2fcx [750.192411ms]
May  6 20:06:58.842: INFO: Created: latency-svc-dx74m
May  6 20:06:58.881: INFO: Got endpoints: latency-svc-snfcj [749.791761ms]
May  6 20:06:58.893: INFO: Created: latency-svc-rdclb
May  6 20:06:58.932: INFO: Got endpoints: latency-svc-rh58w [750.297711ms]
May  6 20:06:58.945: INFO: Created: latency-svc-5qzrv
May  6 20:06:58.982: INFO: Got endpoints: latency-svc-rb4bz [750.610601ms]
May  6 20:06:58.992: INFO: Created: latency-svc-zn9rg
May  6 20:06:59.032: INFO: Got endpoints: latency-svc-4r6sm [748.465954ms]
May  6 20:06:59.044: INFO: Created: latency-svc-lmwdx
May  6 20:06:59.081: INFO: Got endpoints: latency-svc-v9glt [749.828823ms]
May  6 20:06:59.093: INFO: Created: latency-svc-2gtpb
May  6 20:06:59.131: INFO: Got endpoints: latency-svc-27klm [749.836154ms]
May  6 20:06:59.141: INFO: Created: latency-svc-797kn
May  6 20:06:59.182: INFO: Got endpoints: latency-svc-jg8t9 [750.417041ms]
May  6 20:06:59.199: INFO: Created: latency-svc-zzgt5
May  6 20:06:59.231: INFO: Got endpoints: latency-svc-cr2wm [750.417023ms]
May  6 20:06:59.243: INFO: Created: latency-svc-hnxd8
May  6 20:06:59.281: INFO: Got endpoints: latency-svc-f92lq [749.627287ms]
May  6 20:06:59.292: INFO: Created: latency-svc-2sg6q
May  6 20:06:59.331: INFO: Got endpoints: latency-svc-pmkwk [749.657172ms]
May  6 20:06:59.343: INFO: Created: latency-svc-kgzgf
May  6 20:06:59.382: INFO: Got endpoints: latency-svc-2b7fl [750.442062ms]
May  6 20:06:59.393: INFO: Created: latency-svc-mksz7
May  6 20:06:59.431: INFO: Got endpoints: latency-svc-kb7qh [749.534794ms]
May  6 20:06:59.442: INFO: Created: latency-svc-tb46h
May  6 20:06:59.481: INFO: Got endpoints: latency-svc-jwdx4 [749.899618ms]
May  6 20:06:59.493: INFO: Created: latency-svc-f9lth
May  6 20:06:59.533: INFO: Got endpoints: latency-svc-s2rj9 [748.129446ms]
May  6 20:06:59.546: INFO: Created: latency-svc-bmzqz
May  6 20:06:59.581: INFO: Got endpoints: latency-svc-dx74m [749.471534ms]
May  6 20:06:59.592: INFO: Created: latency-svc-hjt28
May  6 20:06:59.633: INFO: Got endpoints: latency-svc-rdclb [751.970153ms]
May  6 20:06:59.648: INFO: Created: latency-svc-7pvjk
May  6 20:06:59.682: INFO: Got endpoints: latency-svc-5qzrv [750.225102ms]
May  6 20:06:59.694: INFO: Created: latency-svc-2wf86
May  6 20:06:59.735: INFO: Got endpoints: latency-svc-zn9rg [753.317455ms]
May  6 20:06:59.747: INFO: Created: latency-svc-nhrc4
May  6 20:06:59.782: INFO: Got endpoints: latency-svc-lmwdx [749.768706ms]
May  6 20:06:59.795: INFO: Created: latency-svc-gznvn
May  6 20:06:59.831: INFO: Got endpoints: latency-svc-2gtpb [749.996769ms]
May  6 20:06:59.843: INFO: Created: latency-svc-44bp7
May  6 20:06:59.887: INFO: Got endpoints: latency-svc-797kn [755.124874ms]
May  6 20:06:59.897: INFO: Created: latency-svc-qzmk6
May  6 20:06:59.932: INFO: Got endpoints: latency-svc-zzgt5 [749.632946ms]
May  6 20:06:59.943: INFO: Created: latency-svc-92727
May  6 20:06:59.982: INFO: Got endpoints: latency-svc-hnxd8 [750.051467ms]
May  6 20:06:59.995: INFO: Created: latency-svc-gl847
May  6 20:07:00.043: INFO: Got endpoints: latency-svc-2sg6q [761.28525ms]
May  6 20:07:00.057: INFO: Created: latency-svc-6qwjl
May  6 20:07:00.082: INFO: Got endpoints: latency-svc-kgzgf [750.347424ms]
May  6 20:07:00.095: INFO: Created: latency-svc-hls4w
May  6 20:07:00.131: INFO: Got endpoints: latency-svc-mksz7 [749.766194ms]
May  6 20:07:00.144: INFO: Created: latency-svc-gxbjg
May  6 20:07:00.186: INFO: Got endpoints: latency-svc-tb46h [754.4417ms]
May  6 20:07:00.198: INFO: Created: latency-svc-sgjt9
May  6 20:07:00.232: INFO: Got endpoints: latency-svc-f9lth [750.642214ms]
May  6 20:07:00.282: INFO: Created: latency-svc-w5zd9
May  6 20:07:00.293: INFO: Got endpoints: latency-svc-bmzqz [760.516212ms]
May  6 20:07:00.307: INFO: Created: latency-svc-fgfd7
May  6 20:07:00.332: INFO: Got endpoints: latency-svc-hjt28 [750.341171ms]
May  6 20:07:00.343: INFO: Created: latency-svc-8xlhp
May  6 20:07:00.402: INFO: Got endpoints: latency-svc-7pvjk [768.253985ms]
May  6 20:07:00.488: INFO: Created: latency-svc-dn6vl
May  6 20:07:00.493: INFO: Got endpoints: latency-svc-2wf86 [810.517328ms]
May  6 20:07:00.493: INFO: Got endpoints: latency-svc-nhrc4 [757.507573ms]
May  6 20:07:00.507: INFO: Created: latency-svc-dnpmv
May  6 20:07:00.589: INFO: Got endpoints: latency-svc-44bp7 [757.668332ms]
May  6 20:07:00.589: INFO: Got endpoints: latency-svc-gznvn [807.398821ms]
May  6 20:07:00.591: INFO: Created: latency-svc-kg56g
May  6 20:07:00.627: INFO: Created: latency-svc-mmfpq
May  6 20:07:00.685: INFO: Got endpoints: latency-svc-qzmk6 [798.484478ms]
May  6 20:07:00.687: INFO: Got endpoints: latency-svc-92727 [755.509073ms]
May  6 20:07:00.688: INFO: Created: latency-svc-n6v7g
May  6 20:07:00.697: INFO: Created: latency-svc-wjz4l
May  6 20:07:00.706: INFO: Created: latency-svc-mf7v9
May  6 20:07:00.732: INFO: Got endpoints: latency-svc-gl847 [750.085053ms]
May  6 20:07:00.746: INFO: Created: latency-svc-m7vp5
May  6 20:07:00.782: INFO: Got endpoints: latency-svc-6qwjl [738.894146ms]
May  6 20:07:00.792: INFO: Created: latency-svc-kgrfm
May  6 20:07:00.832: INFO: Got endpoints: latency-svc-hls4w [750.004587ms]
May  6 20:07:00.843: INFO: Created: latency-svc-ffkdh
May  6 20:07:00.881: INFO: Got endpoints: latency-svc-gxbjg [749.616456ms]
May  6 20:07:00.893: INFO: Created: latency-svc-kh9mx
May  6 20:07:00.931: INFO: Got endpoints: latency-svc-sgjt9 [745.577151ms]
May  6 20:07:00.942: INFO: Created: latency-svc-b695z
May  6 20:07:00.982: INFO: Got endpoints: latency-svc-w5zd9 [749.517929ms]
May  6 20:07:00.994: INFO: Created: latency-svc-79h6z
May  6 20:07:01.031: INFO: Got endpoints: latency-svc-fgfd7 [738.047457ms]
May  6 20:07:01.043: INFO: Created: latency-svc-gbj2w
May  6 20:07:01.082: INFO: Got endpoints: latency-svc-8xlhp [750.034894ms]
May  6 20:07:01.092: INFO: Created: latency-svc-5l2f4
May  6 20:07:01.132: INFO: Got endpoints: latency-svc-dn6vl [729.852673ms]
May  6 20:07:01.144: INFO: Created: latency-svc-njtgv
May  6 20:07:01.184: INFO: Got endpoints: latency-svc-dnpmv [690.68179ms]
May  6 20:07:01.200: INFO: Created: latency-svc-65pcv
May  6 20:07:01.231: INFO: Got endpoints: latency-svc-kg56g [738.768881ms]
May  6 20:07:01.255: INFO: Created: latency-svc-cpzsj
May  6 20:07:01.282: INFO: Got endpoints: latency-svc-mmfpq [692.462278ms]
May  6 20:07:01.299: INFO: Created: latency-svc-2pbps
May  6 20:07:01.350: INFO: Got endpoints: latency-svc-n6v7g [760.829526ms]
May  6 20:07:01.363: INFO: Created: latency-svc-682zh
May  6 20:07:01.382: INFO: Got endpoints: latency-svc-wjz4l [696.906569ms]
May  6 20:07:01.394: INFO: Created: latency-svc-mxz7q
May  6 20:07:01.449: INFO: Got endpoints: latency-svc-mf7v9 [762.006053ms]
May  6 20:07:01.463: INFO: Created: latency-svc-kpxqh
May  6 20:07:01.482: INFO: Got endpoints: latency-svc-m7vp5 [750.708726ms]
May  6 20:07:01.494: INFO: Created: latency-svc-ksjcq
May  6 20:07:01.531: INFO: Got endpoints: latency-svc-kgrfm [749.802475ms]
May  6 20:07:01.542: INFO: Created: latency-svc-lnmw5
May  6 20:07:01.582: INFO: Got endpoints: latency-svc-ffkdh [749.789057ms]
May  6 20:07:01.595: INFO: Created: latency-svc-dtq4c
May  6 20:07:01.631: INFO: Got endpoints: latency-svc-kh9mx [750.376257ms]
May  6 20:07:01.644: INFO: Created: latency-svc-vgfzv
May  6 20:07:01.682: INFO: Got endpoints: latency-svc-b695z [750.31925ms]
May  6 20:07:01.692: INFO: Created: latency-svc-mqbsp
May  6 20:07:01.732: INFO: Got endpoints: latency-svc-79h6z [749.933467ms]
May  6 20:07:01.743: INFO: Created: latency-svc-qx8nn
May  6 20:07:01.781: INFO: Got endpoints: latency-svc-gbj2w [750.008151ms]
May  6 20:07:01.795: INFO: Created: latency-svc-nhnvb
May  6 20:07:01.832: INFO: Got endpoints: latency-svc-5l2f4 [750.34126ms]
May  6 20:07:01.842: INFO: Created: latency-svc-txbk2
May  6 20:07:01.882: INFO: Got endpoints: latency-svc-njtgv [750.148962ms]
May  6 20:07:01.899: INFO: Created: latency-svc-jm8sm
May  6 20:07:01.931: INFO: Got endpoints: latency-svc-65pcv [747.555581ms]
May  6 20:07:01.943: INFO: Created: latency-svc-vqwrh
May  6 20:07:01.981: INFO: Got endpoints: latency-svc-cpzsj [750.013558ms]
May  6 20:07:01.991: INFO: Created: latency-svc-mjz9p
May  6 20:07:02.032: INFO: Got endpoints: latency-svc-2pbps [749.997373ms]
May  6 20:07:02.043: INFO: Created: latency-svc-f769x
May  6 20:07:02.081: INFO: Got endpoints: latency-svc-682zh [731.181047ms]
May  6 20:07:02.093: INFO: Created: latency-svc-bbggw
May  6 20:07:02.132: INFO: Got endpoints: latency-svc-mxz7q [749.701218ms]
May  6 20:07:02.142: INFO: Created: latency-svc-xnlm7
May  6 20:07:02.182: INFO: Got endpoints: latency-svc-kpxqh [732.785446ms]
May  6 20:07:02.194: INFO: Created: latency-svc-k5w2c
May  6 20:07:02.236: INFO: Got endpoints: latency-svc-ksjcq [753.431179ms]
May  6 20:07:02.251: INFO: Created: latency-svc-rndgp
May  6 20:07:02.292: INFO: Got endpoints: latency-svc-lnmw5 [760.134876ms]
May  6 20:07:02.304: INFO: Created: latency-svc-9k6zf
May  6 20:07:02.346: INFO: Got endpoints: latency-svc-dtq4c [764.863078ms]
May  6 20:07:02.362: INFO: Created: latency-svc-dldhd
May  6 20:07:02.385: INFO: Got endpoints: latency-svc-vgfzv [753.481464ms]
May  6 20:07:02.432: INFO: Got endpoints: latency-svc-mqbsp [750.609567ms]
May  6 20:07:02.482: INFO: Got endpoints: latency-svc-qx8nn [749.869732ms]
May  6 20:07:02.531: INFO: Got endpoints: latency-svc-nhnvb [749.694518ms]
May  6 20:07:02.583: INFO: Got endpoints: latency-svc-txbk2 [750.601853ms]
May  6 20:07:02.633: INFO: Got endpoints: latency-svc-jm8sm [750.981102ms]
May  6 20:07:02.682: INFO: Got endpoints: latency-svc-vqwrh [750.378797ms]
May  6 20:07:02.731: INFO: Got endpoints: latency-svc-mjz9p [749.909153ms]
May  6 20:07:02.782: INFO: Got endpoints: latency-svc-f769x [750.189844ms]
May  6 20:07:02.831: INFO: Got endpoints: latency-svc-bbggw [749.716239ms]
May  6 20:07:02.882: INFO: Got endpoints: latency-svc-xnlm7 [750.086481ms]
May  6 20:07:02.932: INFO: Got endpoints: latency-svc-k5w2c [749.229066ms]
May  6 20:07:02.981: INFO: Got endpoints: latency-svc-rndgp [745.463009ms]
May  6 20:07:03.032: INFO: Got endpoints: latency-svc-9k6zf [740.253458ms]
May  6 20:07:03.082: INFO: Got endpoints: latency-svc-dldhd [735.224063ms]
May  6 20:07:03.082: INFO: Latencies: [34.475231ms 40.784459ms 49.787763ms 61.298508ms 71.52975ms 76.963897ms 84.903689ms 103.627101ms 118.207591ms 124.091875ms 144.121645ms 145.871445ms 148.325408ms 154.500458ms 158.278834ms 166.396966ms 166.675948ms 168.197421ms 181.530859ms 191.252458ms 201.180386ms 202.411308ms 206.815422ms 208.104081ms 214.95907ms 216.030646ms 216.143887ms 220.331352ms 247.873723ms 272.019619ms 285.850178ms 287.923395ms 291.416169ms 291.515091ms 297.267209ms 298.066605ms 302.747859ms 320.901405ms 323.321566ms 325.539579ms 326.115783ms 329.958698ms 332.273767ms 341.356676ms 343.74561ms 349.171987ms 351.399322ms 390.354914ms 431.576001ms 472.534741ms 514.688584ms 556.237631ms 599.601256ms 639.632276ms 683.851409ms 690.68179ms 692.462278ms 696.906569ms 700.154409ms 729.852673ms 729.912202ms 731.181047ms 731.738149ms 732.785446ms 735.224063ms 738.047457ms 738.768881ms 738.894146ms 740.253458ms 740.521379ms 745.284731ms 745.463009ms 745.577151ms 747.212209ms 747.555581ms 748.129446ms 748.231744ms 748.465954ms 748.70586ms 748.980668ms 749.11821ms 749.188074ms 749.229066ms 749.23706ms 749.299831ms 749.397068ms 749.401683ms 749.437269ms 749.442217ms 749.471534ms 749.503799ms 749.517929ms 749.534794ms 749.54508ms 749.550309ms 749.616456ms 749.627287ms 749.632946ms 749.657172ms 749.678378ms 749.694518ms 749.701218ms 749.716239ms 749.746338ms 749.759675ms 749.766194ms 749.767665ms 749.768706ms 749.789057ms 749.791761ms 749.798597ms 749.802475ms 749.81806ms 749.822834ms 749.828823ms 749.836154ms 749.855623ms 749.865233ms 749.869732ms 749.877265ms 749.899618ms 749.909153ms 749.921954ms 749.933467ms 749.938428ms 749.960098ms 749.970315ms 749.979531ms 749.981086ms 749.996769ms 749.997373ms 750.004587ms 750.008151ms 750.013558ms 750.034894ms 750.051467ms 750.085053ms 750.086481ms 750.096989ms 750.113522ms 750.139158ms 750.148962ms 750.168207ms 750.189844ms 750.192411ms 750.201862ms 750.225102ms 750.236951ms 750.276923ms 750.297711ms 750.31925ms 750.323917ms 750.327551ms 750.341171ms 750.34126ms 750.347424ms 750.376257ms 750.378797ms 750.41263ms 750.417023ms 750.417041ms 750.442062ms 750.443672ms 750.50196ms 750.535539ms 750.601853ms 750.609567ms 750.610601ms 750.642214ms 750.708726ms 750.759571ms 750.781085ms 750.897711ms 750.960342ms 750.981102ms 751.145226ms 751.890638ms 751.970153ms 753.01901ms 753.194523ms 753.317455ms 753.431179ms 753.481464ms 754.4417ms 755.124874ms 755.509073ms 757.507573ms 757.668332ms 760.134876ms 760.516212ms 760.829526ms 761.28525ms 762.006053ms 764.863078ms 768.253985ms 769.891213ms 770.576847ms 798.484478ms 807.398821ms 810.517328ms]
May  6 20:07:03.082: INFO: 50 %ile: 749.694518ms
May  6 20:07:03.082: INFO: 90 %ile: 753.317455ms
May  6 20:07:03.082: INFO: 99 %ile: 807.398821ms
May  6 20:07:03.082: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:07:03.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5452" for this suite.
May  6 20:07:19.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:07:19.204: INFO: namespace svc-latency-5452 deletion completed in 16.117105731s

• [SLOW TEST:26.869 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:07:19.204: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-n4bn
STEP: Creating a pod to test atomic-volume-subpath
May  6 20:07:19.249: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-n4bn" in namespace "subpath-1562" to be "success or failure"
May  6 20:07:19.252: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.561008ms
May  6 20:07:21.255: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006038372s
May  6 20:07:23.259: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 4.009937562s
May  6 20:07:25.262: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 6.01280279s
May  6 20:07:27.265: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 8.01592248s
May  6 20:07:29.269: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 10.019664829s
May  6 20:07:31.272: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 12.022337199s
May  6 20:07:33.274: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 14.02523762s
May  6 20:07:35.279: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 16.029599684s
May  6 20:07:37.282: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 18.032674415s
May  6 20:07:39.285: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 20.036181144s
May  6 20:07:41.289: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.039601433s
STEP: Saw pod success
May  6 20:07:41.289: INFO: Pod "pod-subpath-test-configmap-n4bn" satisfied condition "success or failure"
May  6 20:07:41.292: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-subpath-test-configmap-n4bn container test-container-subpath-configmap-n4bn: <nil>
STEP: delete the pod
May  6 20:07:41.320: INFO: Waiting for pod pod-subpath-test-configmap-n4bn to disappear
May  6 20:07:41.323: INFO: Pod pod-subpath-test-configmap-n4bn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-n4bn
May  6 20:07:41.323: INFO: Deleting pod "pod-subpath-test-configmap-n4bn" in namespace "subpath-1562"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:07:41.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1562" for this suite.
May  6 20:07:47.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:07:47.444: INFO: namespace subpath-1562 deletion completed in 6.11304287s

• [SLOW TEST:28.240 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:07:47.444: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:07:47.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74e9e8bc-f9c7-4a9e-a699-7b4b545d3937" in namespace "downward-api-1786" to be "success or failure"
May  6 20:07:47.487: INFO: Pod "downwardapi-volume-74e9e8bc-f9c7-4a9e-a699-7b4b545d3937": Phase="Pending", Reason="", readiness=false. Elapsed: 3.102527ms
May  6 20:07:49.491: INFO: Pod "downwardapi-volume-74e9e8bc-f9c7-4a9e-a699-7b4b545d3937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007098866s
STEP: Saw pod success
May  6 20:07:49.491: INFO: Pod "downwardapi-volume-74e9e8bc-f9c7-4a9e-a699-7b4b545d3937" satisfied condition "success or failure"
May  6 20:07:49.493: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-74e9e8bc-f9c7-4a9e-a699-7b4b545d3937 container client-container: <nil>
STEP: delete the pod
May  6 20:07:49.513: INFO: Waiting for pod downwardapi-volume-74e9e8bc-f9c7-4a9e-a699-7b4b545d3937 to disappear
May  6 20:07:49.516: INFO: Pod downwardapi-volume-74e9e8bc-f9c7-4a9e-a699-7b4b545d3937 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:07:49.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1786" for this suite.
May  6 20:07:55.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:07:55.632: INFO: namespace downward-api-1786 deletion completed in 6.111438035s

• [SLOW TEST:8.188 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:07:55.632: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 20:07:55.659: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 20:07:55.671: INFO: Waiting for terminating namespaces to be deleted...
May  6 20:07:55.674: INFO: 
Logging pods the kubelet thinks is on node kube-node-0-kubelet.kubernetes-cluster.mesos before test
May  6 20:07:55.685: INFO: kube-proxy-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.685: INFO: 	Container kp ready: true, restart count 0
May  6 20:07:55.685: INFO: calico-node-xjmdx from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.685: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:07:55.685: INFO: sonobuoy-e2e-job-76e28147b51c42fc from sonobuoy started at 2020-05-06 19:47:59 +0000 UTC (2 container statuses recorded)
May  6 20:07:55.685: INFO: 	Container e2e ready: true, restart count 0
May  6 20:07:55.685: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 20:07:55.685: INFO: local-dns-dispatcher-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.685: INFO: 	Container coredns ready: true, restart count 0
May  6 20:07:55.685: INFO: sonobuoy from sonobuoy started at 2020-05-06 19:47:58 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.685: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 20:07:55.685: INFO: 
Logging pods the kubelet thinks is on node kube-node-1-kubelet.kubernetes-cluster.mesos before test
May  6 20:07:55.700: INFO: calico-node-tsjfr from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.700: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:07:55.700: INFO: coredns-7fb4bb54b-p8tfs from kube-system started at 2020-05-06 17:52:40 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.700: INFO: 	Container coredns ready: true, restart count 0
May  6 20:07:55.700: INFO: calico-kube-controllers-788d6b9876-q8w7w from kube-system started at 2020-05-06 18:24:18 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.700: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 20:07:55.700: INFO: kube-proxy-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.700: INFO: 	Container kp ready: true, restart count 0
May  6 20:07:55.700: INFO: local-dns-dispatcher-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.700: INFO: 	Container coredns ready: true, restart count 0
May  6 20:07:55.700: INFO: metrics-server-859cc6f97b-mpsmb from kube-system started at 2020-05-06 16:32:55 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.700: INFO: 	Container metrics-server ready: true, restart count 0
May  6 20:07:55.700: INFO: 
Logging pods the kubelet thinks is on node kube-node-2-kubelet.kubernetes-cluster.mesos before test
May  6 20:07:55.706: INFO: local-dns-dispatcher-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.706: INFO: 	Container coredns ready: true, restart count 0
May  6 20:07:55.706: INFO: calico-node-tb2vk from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.706: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:07:55.706: INFO: coredns-7fb4bb54b-xtdwq from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.706: INFO: 	Container coredns ready: true, restart count 0
May  6 20:07:55.706: INFO: kube-proxy-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.706: INFO: 	Container kp ready: true, restart count 0
May  6 20:07:55.706: INFO: kubernetes-dashboard-68c7899b54-cq697 from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:07:55.706: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.160c893fd0dcc900], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:07:56.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5262" for this suite.
May  6 20:08:02.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:08:02.853: INFO: namespace sched-pred-5262 deletion completed in 6.115246329s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.222 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:08:02.854: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  6 20:08:06.923: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 20:08:06.926: INFO: Pod pod-with-prestop-exec-hook still exists
May  6 20:08:08.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 20:08:08.929: INFO: Pod pod-with-prestop-exec-hook still exists
May  6 20:08:10.926: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 20:08:10.930: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:08:10.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1001" for this suite.
May  6 20:08:38.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:08:39.059: INFO: namespace container-lifecycle-hook-1001 deletion completed in 28.116583849s

• [SLOW TEST:36.205 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:08:39.059: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:08:45.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8776" for this suite.
May  6 20:08:51.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:08:51.218: INFO: namespace job-8776 deletion completed in 6.119833642s

• [SLOW TEST:12.158 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:08:51.218: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9631
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9631
I0506 20:08:51.268762      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-9631, replica count: 2
May  6 20:08:54.319: INFO: Creating new exec pod
I0506 20:08:54.319032      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 20:08:57.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-9631 execpodgwwp8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May  6 20:08:59.495: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  6 20:08:59.495: INFO: stdout: ""
May  6 20:08:59.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-9631 execpodgwwp8 -- /bin/sh -x -c nc -zv -t -w 2 10.100.111.177 80'
May  6 20:08:59.647: INFO: stderr: "+ nc -zv -t -w 2 10.100.111.177 80\nConnection to 10.100.111.177 80 port [tcp/http] succeeded!\n"
May  6 20:08:59.647: INFO: stdout: ""
May  6 20:08:59.647: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:08:59.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9631" for this suite.
May  6 20:09:05.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:09:05.795: INFO: namespace services-9631 deletion completed in 6.120926397s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:14.577 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:09:05.795: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:09:21.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3531" for this suite.
May  6 20:09:27.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:09:28.033: INFO: namespace resourcequota-3531 deletion completed in 6.162952358s

• [SLOW TEST:22.237 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:09:28.033: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May  6 20:09:28.085: INFO: Waiting up to 5m0s for pod "pod-6b4752d2-534c-47c2-9533-eacff3815e86" in namespace "emptydir-2752" to be "success or failure"
May  6 20:09:28.088: INFO: Pod "pod-6b4752d2-534c-47c2-9533-eacff3815e86": Phase="Pending", Reason="", readiness=false. Elapsed: 3.215607ms
May  6 20:09:30.091: INFO: Pod "pod-6b4752d2-534c-47c2-9533-eacff3815e86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006562465s
STEP: Saw pod success
May  6 20:09:30.091: INFO: Pod "pod-6b4752d2-534c-47c2-9533-eacff3815e86" satisfied condition "success or failure"
May  6 20:09:30.095: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-6b4752d2-534c-47c2-9533-eacff3815e86 container test-container: <nil>
STEP: delete the pod
May  6 20:09:30.114: INFO: Waiting for pod pod-6b4752d2-534c-47c2-9533-eacff3815e86 to disappear
May  6 20:09:30.118: INFO: Pod pod-6b4752d2-534c-47c2-9533-eacff3815e86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:09:30.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2752" for this suite.
May  6 20:09:36.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:09:36.245: INFO: namespace emptydir-2752 deletion completed in 6.123673885s

• [SLOW TEST:8.212 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:09:36.245: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-961
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May  6 20:09:36.286: INFO: Found 0 stateful pods, waiting for 3
May  6 20:09:46.290: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:09:46.290: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:09:46.290: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May  6 20:09:46.317: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May  6 20:09:56.346: INFO: Updating stateful set ss2
May  6 20:09:56.352: INFO: Waiting for Pod statefulset-961/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 20:10:06.359: INFO: Waiting for Pod statefulset-961/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May  6 20:10:16.391: INFO: Found 2 stateful pods, waiting for 3
May  6 20:10:26.395: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:10:26.395: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:10:26.395: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May  6 20:10:26.419: INFO: Updating stateful set ss2
May  6 20:10:26.426: INFO: Waiting for Pod statefulset-961/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 20:10:36.452: INFO: Updating stateful set ss2
May  6 20:10:36.459: INFO: Waiting for StatefulSet statefulset-961/ss2 to complete update
May  6 20:10:36.459: INFO: Waiting for Pod statefulset-961/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 20:10:46.466: INFO: Deleting all statefulset in ns statefulset-961
May  6 20:10:46.469: INFO: Scaling statefulset ss2 to 0
May  6 20:11:06.482: INFO: Waiting for statefulset status.replicas updated to 0
May  6 20:11:06.486: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:11:06.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-961" for this suite.
May  6 20:11:12.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:11:12.721: INFO: namespace statefulset-961 deletion completed in 6.216828017s

• [SLOW TEST:96.476 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:11:12.721: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4991
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4991
STEP: creating replication controller externalsvc in namespace services-4991
I0506 20:11:12.782736      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4991, replica count: 2
I0506 20:11:15.833006      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May  6 20:11:15.855: INFO: Creating new exec pod
May  6 20:11:17.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-4991 execpodnz5cd -- /bin/sh -x -c nslookup nodeport-service'
May  6 20:11:18.116: INFO: stderr: "+ nslookup nodeport-service\n"
May  6 20:11:18.116: INFO: stdout: "Server:\t\t9.0.4.4\nAddress:\t9.0.4.4#53\n\nnodeport-service.services-4991.svc.cluster.local\tcanonical name = externalsvc.services-4991.svc.cluster.local.\nName:\texternalsvc.services-4991.svc.cluster.local\nAddress: 10.100.26.252\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4991, will wait for the garbage collector to delete the pods
May  6 20:11:18.177: INFO: Deleting ReplicationController externalsvc took: 8.042489ms
May  6 20:11:18.577: INFO: Terminating ReplicationController externalsvc pods took: 400.201256ms
May  6 20:11:22.699: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:11:22.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4991" for this suite.
May  6 20:11:28.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:11:28.839: INFO: namespace services-4991 deletion completed in 6.119287755s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.118 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:11:28.839: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
May  6 20:11:28.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-5890 -- logs-generator --log-lines-total 100 --run-duration 20s'
May  6 20:11:28.938: INFO: stderr: ""
May  6 20:11:28.938: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
May  6 20:11:28.938: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  6 20:11:28.938: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5890" to be "running and ready, or succeeded"
May  6 20:11:28.942: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38252ms
May  6 20:11:30.945: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006510367s
May  6 20:11:30.945: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  6 20:11:30.945: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May  6 20:11:30.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs logs-generator logs-generator --namespace=kubectl-5890'
May  6 20:11:31.024: INFO: stderr: ""
May  6 20:11:31.024: INFO: stdout: "I0506 20:11:29.707718       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/blgj 374\nI0506 20:11:29.907787       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/p8cm 511\nI0506 20:11:30.107817       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/824t 455\nI0506 20:11:30.307861       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/wwk 570\nI0506 20:11:30.507818       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/zhz 299\nI0506 20:11:30.707828       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/p27 237\nI0506 20:11:30.907828       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bjt 472\n"
STEP: limiting log lines
May  6 20:11:31.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs logs-generator logs-generator --namespace=kubectl-5890 --tail=1'
May  6 20:11:31.103: INFO: stderr: ""
May  6 20:11:31.103: INFO: stdout: "I0506 20:11:30.907828       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bjt 472\n"
STEP: limiting log bytes
May  6 20:11:31.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs logs-generator logs-generator --namespace=kubectl-5890 --limit-bytes=1'
May  6 20:11:31.176: INFO: stderr: ""
May  6 20:11:31.176: INFO: stdout: "I"
STEP: exposing timestamps
May  6 20:11:31.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs logs-generator logs-generator --namespace=kubectl-5890 --tail=1 --timestamps'
May  6 20:11:31.259: INFO: stderr: ""
May  6 20:11:31.259: INFO: stdout: "2020-05-06T20:11:31.107911844Z I0506 20:11:31.107822       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9gzx 404\n"
STEP: restricting to a time range
May  6 20:11:33.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs logs-generator logs-generator --namespace=kubectl-5890 --since=1s'
May  6 20:11:33.831: INFO: stderr: ""
May  6 20:11:33.831: INFO: stdout: "I0506 20:11:32.907823       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/f68g 341\nI0506 20:11:33.107779       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/jvhh 493\nI0506 20:11:33.307818       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/f52 581\nI0506 20:11:33.507815       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/49dc 300\nI0506 20:11:33.707762       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/2tw 206\n"
May  6 20:11:33.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs logs-generator logs-generator --namespace=kubectl-5890 --since=24h'
May  6 20:11:33.906: INFO: stderr: ""
May  6 20:11:33.906: INFO: stdout: "I0506 20:11:29.707718       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/blgj 374\nI0506 20:11:29.907787       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/p8cm 511\nI0506 20:11:30.107817       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/824t 455\nI0506 20:11:30.307861       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/wwk 570\nI0506 20:11:30.507818       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/zhz 299\nI0506 20:11:30.707828       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/p27 237\nI0506 20:11:30.907828       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bjt 472\nI0506 20:11:31.107822       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9gzx 404\nI0506 20:11:31.307824       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/25gj 212\nI0506 20:11:31.507755       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/g4n 441\nI0506 20:11:31.707830       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/bkxk 517\nI0506 20:11:31.907792       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/rsw4 598\nI0506 20:11:32.107772       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/8v7 211\nI0506 20:11:32.307771       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/88vw 486\nI0506 20:11:32.507775       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/c25x 252\nI0506 20:11:32.707831       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/rtbx 531\nI0506 20:11:32.907823       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/f68g 341\nI0506 20:11:33.107779       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/jvhh 493\nI0506 20:11:33.307818       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/f52 581\nI0506 20:11:33.507815       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/49dc 300\nI0506 20:11:33.707762       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/2tw 206\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
May  6 20:11:33.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete pod logs-generator --namespace=kubectl-5890'
May  6 20:11:37.859: INFO: stderr: ""
May  6 20:11:37.859: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:11:37.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5890" for this suite.
May  6 20:11:43.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:11:44.085: INFO: namespace kubectl-5890 deletion completed in 6.221711796s

• [SLOW TEST:15.246 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:11:44.085: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-11cd3bb2-ce49-4f80-a747-9d6516bb6f85
STEP: Creating a pod to test consume secrets
May  6 20:11:44.209: INFO: Waiting up to 5m0s for pod "pod-secrets-4ac60bee-6ab2-4415-a320-dcb729fa6328" in namespace "secrets-6765" to be "success or failure"
May  6 20:11:44.213: INFO: Pod "pod-secrets-4ac60bee-6ab2-4415-a320-dcb729fa6328": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765222ms
May  6 20:11:46.216: INFO: Pod "pod-secrets-4ac60bee-6ab2-4415-a320-dcb729fa6328": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006443424s
STEP: Saw pod success
May  6 20:11:46.216: INFO: Pod "pod-secrets-4ac60bee-6ab2-4415-a320-dcb729fa6328" satisfied condition "success or failure"
May  6 20:11:46.218: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-4ac60bee-6ab2-4415-a320-dcb729fa6328 container secret-volume-test: <nil>
STEP: delete the pod
May  6 20:11:46.236: INFO: Waiting for pod pod-secrets-4ac60bee-6ab2-4415-a320-dcb729fa6328 to disappear
May  6 20:11:46.239: INFO: Pod pod-secrets-4ac60bee-6ab2-4415-a320-dcb729fa6328 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:11:46.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6765" for this suite.
May  6 20:11:52.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:11:52.371: INFO: namespace secrets-6765 deletion completed in 6.128881538s
STEP: Destroying namespace "secret-namespace-863" for this suite.
May  6 20:11:58.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:11:58.491: INFO: namespace secret-namespace-863 deletion completed in 6.119868515s

• [SLOW TEST:14.406 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:11:58.491: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8677
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 20:11:58.519: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 20:12:16.597: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.32.240:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8677 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:12:16.597: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:12:16.689: INFO: Found all expected endpoints: [netserver-0]
May  6 20:12:16.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.80.99:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8677 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:12:16.693: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:12:16.791: INFO: Found all expected endpoints: [netserver-1]
May  6 20:12:16.793: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.43.226:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8677 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:12:16.793: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:12:16.879: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:12:16.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8677" for this suite.
May  6 20:12:28.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:12:28.998: INFO: namespace pod-network-test-8677 deletion completed in 12.11391075s

• [SLOW TEST:30.506 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:12:28.998: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f
May  6 20:12:29.034: INFO: Pod name my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f: Found 0 pods out of 1
May  6 20:12:34.037: INFO: Pod name my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f: Found 1 pods out of 1
May  6 20:12:34.037: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f" are running
May  6 20:12:34.040: INFO: Pod "my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f-f76qg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:12:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:12:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:12:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 20:12:29 +0000 UTC Reason: Message:}])
May  6 20:12:34.040: INFO: Trying to dial the pod
May  6 20:12:39.050: INFO: Controller my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f: Got expected result from replica 1 [my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f-f76qg]: "my-hostname-basic-adfbf522-00be-4b16-aa88-44afa66a0b8f-f76qg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:12:39.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7012" for this suite.
May  6 20:12:45.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:12:45.171: INFO: namespace replication-controller-7012 deletion completed in 6.116324925s

• [SLOW TEST:16.173 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:12:45.171: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5508
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5508
I0506 20:12:45.225495      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-5508, replica count: 2
May  6 20:12:48.275: INFO: Creating new exec pod
I0506 20:12:48.275788      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 20:12:51.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-5508 execpodq6c6t -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May  6 20:12:51.437: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  6 20:12:51.437: INFO: stdout: ""
May  6 20:12:51.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-5508 execpodq6c6t -- /bin/sh -x -c nc -zv -t -w 2 10.100.67.227 80'
May  6 20:12:51.584: INFO: stderr: "+ nc -zv -t -w 2 10.100.67.227 80\nConnection to 10.100.67.227 80 port [tcp/http] succeeded!\n"
May  6 20:12:51.584: INFO: stdout: ""
May  6 20:12:51.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-5508 execpodq6c6t -- /bin/sh -x -c nc -zv -t -w 2 9.0.4.4 31264'
May  6 20:12:51.732: INFO: stderr: "+ nc -zv -t -w 2 9.0.4.4 31264\nConnection to 9.0.4.4 31264 port [tcp/31264] succeeded!\n"
May  6 20:12:51.732: INFO: stdout: ""
May  6 20:12:51.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-5508 execpodq6c6t -- /bin/sh -x -c nc -zv -t -w 2 9.0.2.4 31264'
May  6 20:12:51.883: INFO: stderr: "+ nc -zv -t -w 2 9.0.2.4 31264\nConnection to 9.0.2.4 31264 port [tcp/31264] succeeded!\n"
May  6 20:12:51.883: INFO: stdout: ""
May  6 20:12:51.883: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:12:51.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5508" for this suite.
May  6 20:12:57.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:12:58.030: INFO: namespace services-5508 deletion completed in 6.115989224s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.858 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:12:58.030: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May  6 20:12:58.074: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1852 /api/v1/namespaces/watch-1852/configmaps/e2e-watch-test-label-changed d656f71a-8a94-4922-b498-a212a25ef831 53068 0 2020-05-06 20:12:58 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 20:12:58.074: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1852 /api/v1/namespaces/watch-1852/configmaps/e2e-watch-test-label-changed d656f71a-8a94-4922-b498-a212a25ef831 53069 0 2020-05-06 20:12:58 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  6 20:12:58.074: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1852 /api/v1/namespaces/watch-1852/configmaps/e2e-watch-test-label-changed d656f71a-8a94-4922-b498-a212a25ef831 53070 0 2020-05-06 20:12:58 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May  6 20:13:08.101: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1852 /api/v1/namespaces/watch-1852/configmaps/e2e-watch-test-label-changed d656f71a-8a94-4922-b498-a212a25ef831 53098 0 2020-05-06 20:12:58 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 20:13:08.101: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1852 /api/v1/namespaces/watch-1852/configmaps/e2e-watch-test-label-changed d656f71a-8a94-4922-b498-a212a25ef831 53099 0 2020-05-06 20:12:58 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May  6 20:13:08.101: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1852 /api/v1/namespaces/watch-1852/configmaps/e2e-watch-test-label-changed d656f71a-8a94-4922-b498-a212a25ef831 53100 0 2020-05-06 20:12:58 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:13:08.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1852" for this suite.
May  6 20:13:14.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:13:14.216: INFO: namespace watch-1852 deletion completed in 6.110161662s

• [SLOW TEST:16.186 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:13:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:13:16.271: INFO: Waiting up to 5m0s for pod "client-envvars-02c78ccb-eb0b-4c16-943e-880efd0b54e6" in namespace "pods-2704" to be "success or failure"
May  6 20:13:16.276: INFO: Pod "client-envvars-02c78ccb-eb0b-4c16-943e-880efd0b54e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.324658ms
May  6 20:13:18.280: INFO: Pod "client-envvars-02c78ccb-eb0b-4c16-943e-880efd0b54e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008594028s
STEP: Saw pod success
May  6 20:13:18.280: INFO: Pod "client-envvars-02c78ccb-eb0b-4c16-943e-880efd0b54e6" satisfied condition "success or failure"
May  6 20:13:18.283: INFO: Trying to get logs from node kube-node-0-kubelet.kubernetes-cluster.mesos pod client-envvars-02c78ccb-eb0b-4c16-943e-880efd0b54e6 container env3cont: <nil>
STEP: delete the pod
May  6 20:13:18.351: INFO: Waiting for pod client-envvars-02c78ccb-eb0b-4c16-943e-880efd0b54e6 to disappear
May  6 20:13:18.355: INFO: Pod client-envvars-02c78ccb-eb0b-4c16-943e-880efd0b54e6 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:13:18.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2704" for this suite.
May  6 20:13:30.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:13:30.480: INFO: namespace pods-2704 deletion completed in 12.121902353s

• [SLOW TEST:16.265 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:13:30.481: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:13:30.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79390f31-3035-4d07-a80e-64790808f2bd" in namespace "downward-api-2717" to be "success or failure"
May  6 20:13:30.519: INFO: Pod "downwardapi-volume-79390f31-3035-4d07-a80e-64790808f2bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738404ms
May  6 20:13:32.523: INFO: Pod "downwardapi-volume-79390f31-3035-4d07-a80e-64790808f2bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006643395s
STEP: Saw pod success
May  6 20:13:32.523: INFO: Pod "downwardapi-volume-79390f31-3035-4d07-a80e-64790808f2bd" satisfied condition "success or failure"
May  6 20:13:32.525: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-79390f31-3035-4d07-a80e-64790808f2bd container client-container: <nil>
STEP: delete the pod
May  6 20:13:32.553: INFO: Waiting for pod downwardapi-volume-79390f31-3035-4d07-a80e-64790808f2bd to disappear
May  6 20:13:32.557: INFO: Pod downwardapi-volume-79390f31-3035-4d07-a80e-64790808f2bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:13:32.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2717" for this suite.
May  6 20:13:38.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:13:38.712: INFO: namespace downward-api-2717 deletion completed in 6.151500503s

• [SLOW TEST:8.231 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:13:38.712: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
May  6 20:13:39.782: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0506 20:13:39.782424      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:13:39.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3349" for this suite.
May  6 20:13:45.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:13:45.904: INFO: namespace gc-3349 deletion completed in 6.118351983s

• [SLOW TEST:7.192 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:13:45.904: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May  6 20:13:45.937: INFO: Waiting up to 5m0s for pod "pod-97e8a465-55f9-4f38-8bb5-120b4c7cb8c3" in namespace "emptydir-9654" to be "success or failure"
May  6 20:13:45.941: INFO: Pod "pod-97e8a465-55f9-4f38-8bb5-120b4c7cb8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197121ms
May  6 20:13:47.944: INFO: Pod "pod-97e8a465-55f9-4f38-8bb5-120b4c7cb8c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006165204s
STEP: Saw pod success
May  6 20:13:47.944: INFO: Pod "pod-97e8a465-55f9-4f38-8bb5-120b4c7cb8c3" satisfied condition "success or failure"
May  6 20:13:47.946: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-97e8a465-55f9-4f38-8bb5-120b4c7cb8c3 container test-container: <nil>
STEP: delete the pod
May  6 20:13:47.965: INFO: Waiting for pod pod-97e8a465-55f9-4f38-8bb5-120b4c7cb8c3 to disappear
May  6 20:13:47.968: INFO: Pod pod-97e8a465-55f9-4f38-8bb5-120b4c7cb8c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:13:47.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9654" for this suite.
May  6 20:13:53.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:13:54.087: INFO: namespace emptydir-9654 deletion completed in 6.115560986s

• [SLOW TEST:8.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:13:54.087: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:13:54.176: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-64436467-dcef-46d4-851f-c46aac24407b" in namespace "security-context-test-6078" to be "success or failure"
May  6 20:13:54.179: INFO: Pod "busybox-privileged-false-64436467-dcef-46d4-851f-c46aac24407b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.931175ms
May  6 20:13:56.183: INFO: Pod "busybox-privileged-false-64436467-dcef-46d4-851f-c46aac24407b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006779869s
May  6 20:13:56.183: INFO: Pod "busybox-privileged-false-64436467-dcef-46d4-851f-c46aac24407b" satisfied condition "success or failure"
May  6 20:13:56.190: INFO: Got logs for pod "busybox-privileged-false-64436467-dcef-46d4-851f-c46aac24407b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:13:56.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6078" for this suite.
May  6 20:14:02.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:14:02.310: INFO: namespace security-context-test-6078 deletion completed in 6.11480317s

• [SLOW TEST:8.223 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:14:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
May  6 20:14:02.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=kubectl-276 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May  6 20:14:04.322: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May  6 20:14:04.322: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:14:06.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-276" for this suite.
May  6 20:14:12.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:14:12.450: INFO: namespace kubectl-276 deletion completed in 6.117419499s

• [SLOW TEST:10.139 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:14:12.450: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:14:12.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ddace5f-078c-4cb4-8a13-cb9a509c6121" in namespace "downward-api-9597" to be "success or failure"
May  6 20:14:12.486: INFO: Pod "downwardapi-volume-2ddace5f-078c-4cb4-8a13-cb9a509c6121": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7817ms
May  6 20:14:14.489: INFO: Pod "downwardapi-volume-2ddace5f-078c-4cb4-8a13-cb9a509c6121": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005842861s
STEP: Saw pod success
May  6 20:14:14.490: INFO: Pod "downwardapi-volume-2ddace5f-078c-4cb4-8a13-cb9a509c6121" satisfied condition "success or failure"
May  6 20:14:14.492: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-2ddace5f-078c-4cb4-8a13-cb9a509c6121 container client-container: <nil>
STEP: delete the pod
May  6 20:14:14.512: INFO: Waiting for pod downwardapi-volume-2ddace5f-078c-4cb4-8a13-cb9a509c6121 to disappear
May  6 20:14:14.515: INFO: Pod downwardapi-volume-2ddace5f-078c-4cb4-8a13-cb9a509c6121 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:14:14.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9597" for this suite.
May  6 20:14:20.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:14:20.630: INFO: namespace downward-api-9597 deletion completed in 6.109679474s

• [SLOW TEST:8.180 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:14:20.630: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
May  6 20:14:20.664: INFO: Waiting up to 5m0s for pod "pod-14dfff17-151e-4894-8ea7-4ee27496747e" in namespace "emptydir-7422" to be "success or failure"
May  6 20:14:20.667: INFO: Pod "pod-14dfff17-151e-4894-8ea7-4ee27496747e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765343ms
May  6 20:14:22.671: INFO: Pod "pod-14dfff17-151e-4894-8ea7-4ee27496747e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006030676s
STEP: Saw pod success
May  6 20:14:22.671: INFO: Pod "pod-14dfff17-151e-4894-8ea7-4ee27496747e" satisfied condition "success or failure"
May  6 20:14:22.673: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-14dfff17-151e-4894-8ea7-4ee27496747e container test-container: <nil>
STEP: delete the pod
May  6 20:14:22.693: INFO: Waiting for pod pod-14dfff17-151e-4894-8ea7-4ee27496747e to disappear
May  6 20:14:22.695: INFO: Pod pod-14dfff17-151e-4894-8ea7-4ee27496747e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:14:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7422" for this suite.
May  6 20:14:28.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:14:28.815: INFO: namespace emptydir-7422 deletion completed in 6.113138666s

• [SLOW TEST:8.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:14:28.815: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:14:28.849: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  6 20:14:33.854: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  6 20:14:33.854: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  6 20:14:35.857: INFO: Creating deployment "test-rollover-deployment"
May  6 20:14:35.864: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  6 20:14:37.871: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  6 20:14:37.878: INFO: Ensure that both replica sets have 1 created replica
May  6 20:14:37.883: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  6 20:14:37.889: INFO: Updating deployment test-rollover-deployment
May  6 20:14:37.889: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  6 20:14:39.901: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  6 20:14:39.908: INFO: Make sure deployment "test-rollover-deployment" is complete
May  6 20:14:39.914: INFO: all replica sets need to contain the pod-template-hash label
May  6 20:14:39.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392879, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 20:14:41.921: INFO: all replica sets need to contain the pod-template-hash label
May  6 20:14:41.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392879, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 20:14:43.921: INFO: all replica sets need to contain the pod-template-hash label
May  6 20:14:43.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392879, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 20:14:45.920: INFO: all replica sets need to contain the pod-template-hash label
May  6 20:14:45.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392879, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 20:14:47.920: INFO: all replica sets need to contain the pod-template-hash label
May  6 20:14:47.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392879, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724392875, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 20:14:49.921: INFO: 
May  6 20:14:49.921: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 20:14:49.929: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5712 /apis/apps/v1/namespaces/deployment-5712/deployments/test-rollover-deployment 8320c283-4f03-4348-8be0-ea0cda93b632 53708 2 2020-05-06 20:14:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038f4778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-06 20:14:35 +0000 UTC,LastTransitionTime:2020-05-06 20:14:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-05-06 20:14:49 +0000 UTC,LastTransitionTime:2020-05-06 20:14:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 20:14:49.932: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-5712 /apis/apps/v1/namespaces/deployment-5712/replicasets/test-rollover-deployment-7d7dc6548c d529ab47-35d4-4816-b628-52af14ce4d54 53697 2 2020-05-06 20:14:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8320c283-4f03-4348-8be0-ea0cda93b632 0xc0038f4c47 0xc0038f4c48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038f4ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 20:14:49.932: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  6 20:14:49.932: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5712 /apis/apps/v1/namespaces/deployment-5712/replicasets/test-rollover-controller 21d09237-4f66-49e6-ac44-23aef3dc3858 53706 2 2020-05-06 20:14:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8320c283-4f03-4348-8be0-ea0cda93b632 0xc0038f4b77 0xc0038f4b78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0038f4bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 20:14:49.932: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-5712 /apis/apps/v1/namespaces/deployment-5712/replicasets/test-rollover-deployment-f6c94f66c bbbec5f5-eed0-4fcd-bd7d-19f8046a2c76 53660 2 2020-05-06 20:14:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8320c283-4f03-4348-8be0-ea0cda93b632 0xc0038f4d10 0xc0038f4d11}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038f4d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 20:14:49.936: INFO: Pod "test-rollover-deployment-7d7dc6548c-lsmdn" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-lsmdn test-rollover-deployment-7d7dc6548c- deployment-5712 /api/v1/namespaces/deployment-5712/pods/test-rollover-deployment-7d7dc6548c-lsmdn a0a5aed6-a8c0-4310-9992-b03bda236bbb 53675 0 2020-05-06 20:14:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:192.168.32.249/32 cni.projectcalico.org/podIPs:192.168.32.249/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c d529ab47-35d4-4816-b628-52af14ce4d54 0xc0039a41f7 0xc0039a41f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pw88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pw88,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pw88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:14:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:14:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:14:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:14:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:192.168.32.249,StartTime:2020-05-06 20:14:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 20:14:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://7b0c4ee0fa7c91d5d3c6cb378eed0ff03e251ae539e61989dee849af8970eb3e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.32.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:14:49.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5712" for this suite.
May  6 20:14:55.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:14:56.056: INFO: namespace deployment-5712 deletion completed in 6.11538577s

• [SLOW TEST:27.241 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:14:56.056: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 20:14:58.103: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:14:58.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4296" for this suite.
May  6 20:15:04.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:15:04.228: INFO: namespace container-runtime-4296 deletion completed in 6.107627036s

• [SLOW TEST:8.172 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:15:04.228: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
May  6 20:15:04.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 api-versions'
May  6 20:15:04.318: INFO: stderr: ""
May  6 20:15:04.318: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:15:04.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9327" for this suite.
May  6 20:15:10.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:15:10.442: INFO: namespace kubectl-9327 deletion completed in 6.119022081s

• [SLOW TEST:6.214 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:15:10.442: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:15:23.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8502" for this suite.
May  6 20:15:29.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:15:29.641: INFO: namespace resourcequota-8502 deletion completed in 6.109166811s

• [SLOW TEST:19.200 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:15:29.641: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May  6 20:15:29.674: INFO: Waiting up to 5m0s for pod "pod-023929bf-1c26-41f8-a663-ca17f57e2dae" in namespace "emptydir-7836" to be "success or failure"
May  6 20:15:29.679: INFO: Pod "pod-023929bf-1c26-41f8-a663-ca17f57e2dae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968825ms
May  6 20:15:31.683: INFO: Pod "pod-023929bf-1c26-41f8-a663-ca17f57e2dae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008353003s
STEP: Saw pod success
May  6 20:15:31.683: INFO: Pod "pod-023929bf-1c26-41f8-a663-ca17f57e2dae" satisfied condition "success or failure"
May  6 20:15:31.686: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-023929bf-1c26-41f8-a663-ca17f57e2dae container test-container: <nil>
STEP: delete the pod
May  6 20:15:31.705: INFO: Waiting for pod pod-023929bf-1c26-41f8-a663-ca17f57e2dae to disappear
May  6 20:15:31.709: INFO: Pod pod-023929bf-1c26-41f8-a663-ca17f57e2dae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:15:31.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7836" for this suite.
May  6 20:15:37.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:15:37.821: INFO: namespace emptydir-7836 deletion completed in 6.108312577s

• [SLOW TEST:8.179 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:15:37.821: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May  6 20:15:37.852: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May  6 20:15:49.586: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:15:52.450: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:16:03.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4348" for this suite.
May  6 20:16:09.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:16:09.963: INFO: namespace crd-publish-openapi-4348 deletion completed in 6.117687525s

• [SLOW TEST:32.142 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:16:09.963: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May  6 20:16:12.014: INFO: &Pod{ObjectMeta:{send-events-e664d4aa-3be1-4498-ad9f-ced1231f089d  events-2108 /api/v1/namespaces/events-2108/pods/send-events-e664d4aa-3be1-4498-ad9f-ced1231f089d 6d1f3745-c39a-48f9-8a48-2975cddb38da 54053 0 2020-05-06 20:16:09 +0000 UTC <nil> <nil> map[name:foo time:994221881] map[cni.projectcalico.org/podIP:192.168.32.252/32 cni.projectcalico.org/podIPs:192.168.32.252/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c949p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c949p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c949p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:16:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:192.168.32.252,StartTime:2020-05-06 20:16:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 20:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://75ca474d05c699dba85c6865acdbf5220e57136dc9c14eaab0816856c5b345ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.32.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May  6 20:16:14.018: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May  6 20:16:16.021: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:16:16.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2108" for this suite.
May  6 20:17:00.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:17:00.156: INFO: namespace events-2108 deletion completed in 44.123203705s

• [SLOW TEST:50.193 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:17:00.156: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  6 20:17:04.223: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:04.226: INFO: Pod pod-with-prestop-http-hook still exists
May  6 20:17:06.226: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:06.229: INFO: Pod pod-with-prestop-http-hook still exists
May  6 20:17:08.226: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:08.230: INFO: Pod pod-with-prestop-http-hook still exists
May  6 20:17:10.226: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:10.229: INFO: Pod pod-with-prestop-http-hook still exists
May  6 20:17:12.226: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:12.229: INFO: Pod pod-with-prestop-http-hook still exists
May  6 20:17:14.226: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:14.230: INFO: Pod pod-with-prestop-http-hook still exists
May  6 20:17:16.226: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:16.229: INFO: Pod pod-with-prestop-http-hook still exists
May  6 20:17:18.226: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 20:17:18.229: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:17:18.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9649" for this suite.
May  6 20:17:30.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:17:30.367: INFO: namespace container-lifecycle-hook-9649 deletion completed in 12.119167653s

• [SLOW TEST:30.211 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:17:30.367: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 20:17:31.032: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 20:17:34.052: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:17:34.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9832" for this suite.
May  6 20:17:40.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:17:40.378: INFO: namespace webhook-9832 deletion completed in 6.156329838s
STEP: Destroying namespace "webhook-9832-markers" for this suite.
May  6 20:17:46.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:17:46.498: INFO: namespace webhook-9832-markers deletion completed in 6.119538891s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.146 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:17:46.513: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
May  6 20:17:46.540: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-161989487 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:17:46.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1829" for this suite.
May  6 20:17:52.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:17:52.722: INFO: namespace kubectl-1829 deletion completed in 6.123156812s

• [SLOW TEST:6.209 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:17:52.722: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May  6 20:17:52.752: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May  6 20:17:59.783: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:17:59.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2747" for this suite.
May  6 20:18:05.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:18:05.915: INFO: namespace pods-2747 deletion completed in 6.123433242s

• [SLOW TEST:13.193 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:18:05.915: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May  6 20:18:05.948: INFO: Waiting up to 5m0s for pod "pod-fb579a45-4b60-4daf-9ffa-1a4ce2032c5d" in namespace "emptydir-3833" to be "success or failure"
May  6 20:18:05.951: INFO: Pod "pod-fb579a45-4b60-4daf-9ffa-1a4ce2032c5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.729599ms
May  6 20:18:07.956: INFO: Pod "pod-fb579a45-4b60-4daf-9ffa-1a4ce2032c5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007649419s
STEP: Saw pod success
May  6 20:18:07.956: INFO: Pod "pod-fb579a45-4b60-4daf-9ffa-1a4ce2032c5d" satisfied condition "success or failure"
May  6 20:18:07.959: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-fb579a45-4b60-4daf-9ffa-1a4ce2032c5d container test-container: <nil>
STEP: delete the pod
May  6 20:18:07.978: INFO: Waiting for pod pod-fb579a45-4b60-4daf-9ffa-1a4ce2032c5d to disappear
May  6 20:18:07.980: INFO: Pod pod-fb579a45-4b60-4daf-9ffa-1a4ce2032c5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:18:07.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3833" for this suite.
May  6 20:18:13.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:18:14.100: INFO: namespace emptydir-3833 deletion completed in 6.115558064s

• [SLOW TEST:8.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:18:14.100: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:18:18.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1686" for this suite.
May  6 20:18:24.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:18:24.563: INFO: namespace watch-1686 deletion completed in 6.207497461s

• [SLOW TEST:10.463 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:18:24.563: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-10d6ad74-96c4-4674-8a34-9bea82847e5d
STEP: Creating a pod to test consume configMaps
May  6 20:18:24.602: INFO: Waiting up to 5m0s for pod "pod-configmaps-b21a1224-803e-4b70-9204-9849ded1360a" in namespace "configmap-7174" to be "success or failure"
May  6 20:18:24.604: INFO: Pod "pod-configmaps-b21a1224-803e-4b70-9204-9849ded1360a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.644353ms
May  6 20:18:26.608: INFO: Pod "pod-configmaps-b21a1224-803e-4b70-9204-9849ded1360a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006681087s
STEP: Saw pod success
May  6 20:18:26.608: INFO: Pod "pod-configmaps-b21a1224-803e-4b70-9204-9849ded1360a" satisfied condition "success or failure"
May  6 20:18:26.611: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-b21a1224-803e-4b70-9204-9849ded1360a container configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:18:26.630: INFO: Waiting for pod pod-configmaps-b21a1224-803e-4b70-9204-9849ded1360a to disappear
May  6 20:18:26.633: INFO: Pod pod-configmaps-b21a1224-803e-4b70-9204-9849ded1360a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:18:26.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7174" for this suite.
May  6 20:18:32.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:18:32.753: INFO: namespace configmap-7174 deletion completed in 6.115946635s

• [SLOW TEST:8.190 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:18:32.754: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  6 20:18:35.308: INFO: Successfully updated pod "pod-update-activedeadlineseconds-002f5dc2-75d5-47d0-8aef-605fd8e2dc1c"
May  6 20:18:35.308: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-002f5dc2-75d5-47d0-8aef-605fd8e2dc1c" in namespace "pods-1437" to be "terminated due to deadline exceeded"
May  6 20:18:35.310: INFO: Pod "pod-update-activedeadlineseconds-002f5dc2-75d5-47d0-8aef-605fd8e2dc1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.524033ms
May  6 20:18:37.314: INFO: Pod "pod-update-activedeadlineseconds-002f5dc2-75d5-47d0-8aef-605fd8e2dc1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005978893s
May  6 20:18:39.317: INFO: Pod "pod-update-activedeadlineseconds-002f5dc2-75d5-47d0-8aef-605fd8e2dc1c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.008854319s
May  6 20:18:39.317: INFO: Pod "pod-update-activedeadlineseconds-002f5dc2-75d5-47d0-8aef-605fd8e2dc1c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:18:39.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1437" for this suite.
May  6 20:18:45.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:18:45.493: INFO: namespace pods-1437 deletion completed in 6.172294498s

• [SLOW TEST:12.739 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:18:45.493: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:18:45.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c479f0de-0f22-492f-8a27-00a3128a8dd5" in namespace "projected-9881" to be "success or failure"
May  6 20:18:45.534: INFO: Pod "downwardapi-volume-c479f0de-0f22-492f-8a27-00a3128a8dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392293ms
May  6 20:18:47.538: INFO: Pod "downwardapi-volume-c479f0de-0f22-492f-8a27-00a3128a8dd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008191165s
STEP: Saw pod success
May  6 20:18:47.538: INFO: Pod "downwardapi-volume-c479f0de-0f22-492f-8a27-00a3128a8dd5" satisfied condition "success or failure"
May  6 20:18:47.540: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-c479f0de-0f22-492f-8a27-00a3128a8dd5 container client-container: <nil>
STEP: delete the pod
May  6 20:18:47.562: INFO: Waiting for pod downwardapi-volume-c479f0de-0f22-492f-8a27-00a3128a8dd5 to disappear
May  6 20:18:47.565: INFO: Pod downwardapi-volume-c479f0de-0f22-492f-8a27-00a3128a8dd5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:18:47.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9881" for this suite.
May  6 20:18:53.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:18:53.686: INFO: namespace projected-9881 deletion completed in 6.117445854s

• [SLOW TEST:8.193 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:18:53.686: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:18:53.727: INFO: (0) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 8.708994ms)
May  6 20:18:53.730: INFO: (1) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.408542ms)
May  6 20:18:53.734: INFO: (2) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.808069ms)
May  6 20:18:53.737: INFO: (3) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.742942ms)
May  6 20:18:53.740: INFO: (4) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.00612ms)
May  6 20:18:53.743: INFO: (5) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.499034ms)
May  6 20:18:53.746: INFO: (6) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.751836ms)
May  6 20:18:53.749: INFO: (7) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.055451ms)
May  6 20:18:53.753: INFO: (8) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.59699ms)
May  6 20:18:53.756: INFO: (9) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.950934ms)
May  6 20:18:53.759: INFO: (10) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.171215ms)
May  6 20:18:53.763: INFO: (11) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.596646ms)
May  6 20:18:53.766: INFO: (12) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.979112ms)
May  6 20:18:53.769: INFO: (13) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.332517ms)
May  6 20:18:53.773: INFO: (14) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.582219ms)
May  6 20:18:53.775: INFO: (15) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.837955ms)
May  6 20:18:53.779: INFO: (16) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.139486ms)
May  6 20:18:53.782: INFO: (17) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.598057ms)
May  6 20:18:53.785: INFO: (18) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.899005ms)
May  6 20:18:53.788: INFO: (19) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.163357ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:18:53.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9175" for this suite.
May  6 20:18:59.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:18:59.911: INFO: namespace proxy-9175 deletion completed in 6.118459922s

• [SLOW TEST:6.225 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:18:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May  6 20:19:02.459: INFO: Successfully updated pod "adopt-release-rhccb"
STEP: Checking that the Job readopts the Pod
May  6 20:19:02.459: INFO: Waiting up to 15m0s for pod "adopt-release-rhccb" in namespace "job-7282" to be "adopted"
May  6 20:19:02.461: INFO: Pod "adopt-release-rhccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.452969ms
May  6 20:19:04.464: INFO: Pod "adopt-release-rhccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005515245s
May  6 20:19:04.465: INFO: Pod "adopt-release-rhccb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May  6 20:19:04.973: INFO: Successfully updated pod "adopt-release-rhccb"
STEP: Checking that the Job releases the Pod
May  6 20:19:04.973: INFO: Waiting up to 15m0s for pod "adopt-release-rhccb" in namespace "job-7282" to be "released"
May  6 20:19:04.976: INFO: Pod "adopt-release-rhccb": Phase="Running", Reason="", readiness=true. Elapsed: 3.394824ms
May  6 20:19:06.979: INFO: Pod "adopt-release-rhccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006314469s
May  6 20:19:06.979: INFO: Pod "adopt-release-rhccb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:19:06.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7282" for this suite.
May  6 20:19:50.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:19:51.100: INFO: namespace job-7282 deletion completed in 44.117364899s

• [SLOW TEST:51.188 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:19:51.100: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-fd030428-66a8-49a8-a4b2-b5152f5d4e89 in namespace container-probe-5057
May  6 20:19:53.142: INFO: Started pod busybox-fd030428-66a8-49a8-a4b2-b5152f5d4e89 in namespace container-probe-5057
STEP: checking the pod's current state and verifying that restartCount is present
May  6 20:19:53.145: INFO: Initial restart count of pod busybox-fd030428-66a8-49a8-a4b2-b5152f5d4e89 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:23:53.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5057" for this suite.
May  6 20:23:59.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:23:59.706: INFO: namespace container-probe-5057 deletion completed in 6.123434502s

• [SLOW TEST:248.606 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:23:59.706: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 20:23:59.734: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 20:24:21.833: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.80.110:8080/dial?request=hostName&protocol=udp&host=192.168.43.229&port=8081&tries=1'] Namespace:pod-network-test-8 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:24:21.833: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:24:21.922: INFO: Waiting for endpoints: map[]
May  6 20:24:21.925: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.80.110:8080/dial?request=hostName&protocol=udp&host=192.168.32.204&port=8081&tries=1'] Namespace:pod-network-test-8 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:24:21.925: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:24:22.009: INFO: Waiting for endpoints: map[]
May  6 20:24:22.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.80.110:8080/dial?request=hostName&protocol=udp&host=192.168.80.102&port=8081&tries=1'] Namespace:pod-network-test-8 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:24:22.012: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:24:22.094: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:24:22.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8" for this suite.
May  6 20:24:34.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:24:34.213: INFO: namespace pod-network-test-8 deletion completed in 12.113273231s

• [SLOW TEST:34.507 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:24:34.213: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8550
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 20:24:34.241: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 20:24:56.321: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.43.233 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8550 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:24:56.321: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:24:57.411: INFO: Found all expected endpoints: [netserver-0]
May  6 20:24:57.414: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.80.103 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8550 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:24:57.414: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:24:58.507: INFO: Found all expected endpoints: [netserver-1]
May  6 20:24:58.510: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.32.205 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8550 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:24:58.510: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:24:59.607: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:24:59.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8550" for this suite.
May  6 20:25:11.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:25:11.726: INFO: namespace pod-network-test-8550 deletion completed in 12.113706555s

• [SLOW TEST:37.513 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:25:11.726: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 20:25:11.755: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 20:25:11.767: INFO: Waiting for terminating namespaces to be deleted...
May  6 20:25:11.770: INFO: 
Logging pods the kubelet thinks is on node kube-node-0-kubelet.kubernetes-cluster.mesos before test
May  6 20:25:11.781: INFO: sonobuoy from sonobuoy started at 2020-05-06 19:47:58 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.781: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 20:25:11.781: INFO: kube-proxy-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.781: INFO: 	Container kp ready: true, restart count 0
May  6 20:25:11.781: INFO: calico-node-xjmdx from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.781: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:25:11.781: INFO: sonobuoy-e2e-job-76e28147b51c42fc from sonobuoy started at 2020-05-06 19:47:59 +0000 UTC (2 container statuses recorded)
May  6 20:25:11.781: INFO: 	Container e2e ready: true, restart count 0
May  6 20:25:11.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 20:25:11.781: INFO: local-dns-dispatcher-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.781: INFO: 	Container coredns ready: true, restart count 0
May  6 20:25:11.781: INFO: 
Logging pods the kubelet thinks is on node kube-node-1-kubelet.kubernetes-cluster.mesos before test
May  6 20:25:11.796: INFO: local-dns-dispatcher-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.796: INFO: 	Container coredns ready: true, restart count 0
May  6 20:25:11.796: INFO: metrics-server-859cc6f97b-mpsmb from kube-system started at 2020-05-06 16:32:55 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.796: INFO: 	Container metrics-server ready: true, restart count 0
May  6 20:25:11.796: INFO: calico-node-tsjfr from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.796: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:25:11.796: INFO: coredns-7fb4bb54b-p8tfs from kube-system started at 2020-05-06 17:52:40 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.796: INFO: 	Container coredns ready: true, restart count 0
May  6 20:25:11.796: INFO: calico-kube-controllers-788d6b9876-q8w7w from kube-system started at 2020-05-06 18:24:18 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.796: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 20:25:11.796: INFO: kube-proxy-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.796: INFO: 	Container kp ready: true, restart count 0
May  6 20:25:11.796: INFO: 
Logging pods the kubelet thinks is on node kube-node-2-kubelet.kubernetes-cluster.mesos before test
May  6 20:25:11.814: INFO: coredns-7fb4bb54b-xtdwq from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.814: INFO: 	Container coredns ready: true, restart count 0
May  6 20:25:11.814: INFO: local-dns-dispatcher-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.814: INFO: 	Container coredns ready: true, restart count 0
May  6 20:25:11.814: INFO: calico-node-tb2vk from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.814: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:25:11.814: INFO: kube-proxy-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.814: INFO: 	Container kp ready: true, restart count 0
May  6 20:25:11.814: INFO: kubernetes-dashboard-68c7899b54-cq697 from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:25:11.814: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9d52a8dc-52ab-44c4-ad7a-8959f34ebe9b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9d52a8dc-52ab-44c4-ad7a-8959f34ebe9b off the node kube-node-2-kubelet.kubernetes-cluster.mesos
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9d52a8dc-52ab-44c4-ad7a-8959f34ebe9b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:25:15.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3707" for this suite.
May  6 20:25:23.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:25:23.998: INFO: namespace sched-pred-3707 deletion completed in 8.115162736s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:12.272 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:25:23.999: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 20:25:26.043: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:25:26.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4763" for this suite.
May  6 20:25:32.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:25:32.169: INFO: namespace container-runtime-4763 deletion completed in 6.10812893s

• [SLOW TEST:8.170 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:25:32.169: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May  6 20:25:32.195: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:25:47.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3229" for this suite.
May  6 20:25:53.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:25:53.743: INFO: namespace crd-publish-openapi-3229 deletion completed in 6.118703408s

• [SLOW TEST:21.575 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:25:53.744: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-b05fb8f9-84a2-4b7d-836b-d7ec9e94af62
STEP: Creating a pod to test consume configMaps
May  6 20:25:53.781: INFO: Waiting up to 5m0s for pod "pod-configmaps-b83004c0-132e-436c-a5e9-106b89b6b623" in namespace "configmap-655" to be "success or failure"
May  6 20:25:53.784: INFO: Pod "pod-configmaps-b83004c0-132e-436c-a5e9-106b89b6b623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603659ms
May  6 20:25:55.788: INFO: Pod "pod-configmaps-b83004c0-132e-436c-a5e9-106b89b6b623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006737157s
STEP: Saw pod success
May  6 20:25:55.788: INFO: Pod "pod-configmaps-b83004c0-132e-436c-a5e9-106b89b6b623" satisfied condition "success or failure"
May  6 20:25:55.790: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-b83004c0-132e-436c-a5e9-106b89b6b623 container configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:25:55.810: INFO: Waiting for pod pod-configmaps-b83004c0-132e-436c-a5e9-106b89b6b623 to disappear
May  6 20:25:55.813: INFO: Pod pod-configmaps-b83004c0-132e-436c-a5e9-106b89b6b623 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:25:55.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-655" for this suite.
May  6 20:26:01.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:26:01.931: INFO: namespace configmap-655 deletion completed in 6.114098779s

• [SLOW TEST:8.187 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:26:01.931: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:26:01.964: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dbb26d1-e86c-4e31-b607-7a72d983fe04" in namespace "projected-8492" to be "success or failure"
May  6 20:26:01.967: INFO: Pod "downwardapi-volume-7dbb26d1-e86c-4e31-b607-7a72d983fe04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.003738ms
May  6 20:26:03.970: INFO: Pod "downwardapi-volume-7dbb26d1-e86c-4e31-b607-7a72d983fe04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00608859s
STEP: Saw pod success
May  6 20:26:03.970: INFO: Pod "downwardapi-volume-7dbb26d1-e86c-4e31-b607-7a72d983fe04" satisfied condition "success or failure"
May  6 20:26:03.973: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-7dbb26d1-e86c-4e31-b607-7a72d983fe04 container client-container: <nil>
STEP: delete the pod
May  6 20:26:03.990: INFO: Waiting for pod downwardapi-volume-7dbb26d1-e86c-4e31-b607-7a72d983fe04 to disappear
May  6 20:26:03.993: INFO: Pod downwardapi-volume-7dbb26d1-e86c-4e31-b607-7a72d983fe04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:26:03.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8492" for this suite.
May  6 20:26:10.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:26:10.122: INFO: namespace projected-8492 deletion completed in 6.125197109s

• [SLOW TEST:8.191 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:26:10.122: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6905.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6905.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6905.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6905.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6905.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6905.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 20:26:12.220: INFO: DNS probes using dns-6905/dns-test-4748bc37-6409-4857-bda9-0db1b63e972b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:26:12.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6905" for this suite.
May  6 20:26:18.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:26:18.423: INFO: namespace dns-6905 deletion completed in 6.168477082s

• [SLOW TEST:8.301 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:26:18.423: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  6 20:26:18.505: INFO: Waiting up to 5m0s for pod "pod-9682f112-2dd1-4e8a-a90a-df24e50b3c68" in namespace "emptydir-2762" to be "success or failure"
May  6 20:26:18.509: INFO: Pod "pod-9682f112-2dd1-4e8a-a90a-df24e50b3c68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188045ms
May  6 20:26:20.512: INFO: Pod "pod-9682f112-2dd1-4e8a-a90a-df24e50b3c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00777674s
STEP: Saw pod success
May  6 20:26:20.512: INFO: Pod "pod-9682f112-2dd1-4e8a-a90a-df24e50b3c68" satisfied condition "success or failure"
May  6 20:26:20.516: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-9682f112-2dd1-4e8a-a90a-df24e50b3c68 container test-container: <nil>
STEP: delete the pod
May  6 20:26:20.534: INFO: Waiting for pod pod-9682f112-2dd1-4e8a-a90a-df24e50b3c68 to disappear
May  6 20:26:20.537: INFO: Pod pod-9682f112-2dd1-4e8a-a90a-df24e50b3c68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:26:20.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2762" for this suite.
May  6 20:26:26.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:26:26.660: INFO: namespace emptydir-2762 deletion completed in 6.117763227s

• [SLOW TEST:8.236 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:26:26.660: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:26:26.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3646" for this suite.
May  6 20:26:32.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:26:32.834: INFO: namespace kubelet-test-3646 deletion completed in 6.113940728s

• [SLOW TEST:6.174 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:26:32.835: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:26:32.869: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e9c69e44-7c90-4745-99ec-cae2d1989683" in namespace "security-context-test-3863" to be "success or failure"
May  6 20:26:32.872: INFO: Pod "busybox-user-65534-e9c69e44-7c90-4745-99ec-cae2d1989683": Phase="Pending", Reason="", readiness=false. Elapsed: 3.060008ms
May  6 20:26:34.876: INFO: Pod "busybox-user-65534-e9c69e44-7c90-4745-99ec-cae2d1989683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006569466s
May  6 20:26:34.876: INFO: Pod "busybox-user-65534-e9c69e44-7c90-4745-99ec-cae2d1989683" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:26:34.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3863" for this suite.
May  6 20:26:40.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:26:41.003: INFO: namespace security-context-test-3863 deletion completed in 6.121984085s

• [SLOW TEST:8.168 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:26:41.003: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 20:26:41.517: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 20:26:44.595: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:26:56.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5716" for this suite.
May  6 20:27:02.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:27:02.844: INFO: namespace webhook-5716 deletion completed in 6.127033141s
STEP: Destroying namespace "webhook-5716-markers" for this suite.
May  6 20:27:08.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:27:08.960: INFO: namespace webhook-5716-markers deletion completed in 6.116573412s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.971 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:27:08.974: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May  6 20:27:09.453: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 20:27:12.473: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:27:12.476: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:27:13.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1040" for this suite.
May  6 20:27:19.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:27:19.733: INFO: namespace crd-webhook-1040 deletion completed in 6.14459895s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.822 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:27:19.796: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:27:25.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4643" for this suite.
May  6 20:27:31.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:27:32.082: INFO: namespace namespaces-4643 deletion completed in 6.113427358s
STEP: Destroying namespace "nsdeletetest-117" for this suite.
May  6 20:27:32.084: INFO: Namespace nsdeletetest-117 was already deleted
STEP: Destroying namespace "nsdeletetest-2664" for this suite.
May  6 20:27:38.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:27:38.195: INFO: namespace nsdeletetest-2664 deletion completed in 6.11079393s

• [SLOW TEST:18.398 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:27:38.195: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-b8db1653-61d5-4480-8d31-f2045c20268e
STEP: Creating a pod to test consume configMaps
May  6 20:27:38.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6930298-261b-4b93-a483-f9cdbff064b8" in namespace "projected-6690" to be "success or failure"
May  6 20:27:38.237: INFO: Pod "pod-projected-configmaps-f6930298-261b-4b93-a483-f9cdbff064b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791712ms
May  6 20:27:40.240: INFO: Pod "pod-projected-configmaps-f6930298-261b-4b93-a483-f9cdbff064b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006004652s
STEP: Saw pod success
May  6 20:27:40.240: INFO: Pod "pod-projected-configmaps-f6930298-261b-4b93-a483-f9cdbff064b8" satisfied condition "success or failure"
May  6 20:27:40.244: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-configmaps-f6930298-261b-4b93-a483-f9cdbff064b8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:27:40.263: INFO: Waiting for pod pod-projected-configmaps-f6930298-261b-4b93-a483-f9cdbff064b8 to disappear
May  6 20:27:40.268: INFO: Pod pod-projected-configmaps-f6930298-261b-4b93-a483-f9cdbff064b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:27:40.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6690" for this suite.
May  6 20:27:46.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:27:46.412: INFO: namespace projected-6690 deletion completed in 6.14005177s

• [SLOW TEST:8.217 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:27:46.412: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:27:46.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e5ba9e6-2318-4788-9f83-dbcaf6dd5971" in namespace "downward-api-8638" to be "success or failure"
May  6 20:27:46.505: INFO: Pod "downwardapi-volume-2e5ba9e6-2318-4788-9f83-dbcaf6dd5971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.299134ms
May  6 20:27:48.513: INFO: Pod "downwardapi-volume-2e5ba9e6-2318-4788-9f83-dbcaf6dd5971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012108615s
STEP: Saw pod success
May  6 20:27:48.513: INFO: Pod "downwardapi-volume-2e5ba9e6-2318-4788-9f83-dbcaf6dd5971" satisfied condition "success or failure"
May  6 20:27:48.516: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-2e5ba9e6-2318-4788-9f83-dbcaf6dd5971 container client-container: <nil>
STEP: delete the pod
May  6 20:27:48.536: INFO: Waiting for pod downwardapi-volume-2e5ba9e6-2318-4788-9f83-dbcaf6dd5971 to disappear
May  6 20:27:48.538: INFO: Pod downwardapi-volume-2e5ba9e6-2318-4788-9f83-dbcaf6dd5971 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:27:48.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8638" for this suite.
May  6 20:27:54.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:27:54.697: INFO: namespace downward-api-8638 deletion completed in 6.154009208s

• [SLOW TEST:8.285 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:27:54.698: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 20:27:57.260: INFO: Successfully updated pod "annotationupdated73e859a-0bba-46ba-bc2b-de4609b68040"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:28:01.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4833" for this suite.
May  6 20:28:29.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:28:29.416: INFO: namespace projected-4833 deletion completed in 28.125906853s

• [SLOW TEST:34.718 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:28:29.416: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:28:29.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4489" for this suite.
May  6 20:28:35.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:28:35.563: INFO: namespace custom-resource-definition-4489 deletion completed in 6.112434191s

• [SLOW TEST:6.147 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:28:35.563: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:28:35.598: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1c660a86-4009-4766-9cde-a877b2f1bb05" in namespace "security-context-test-6366" to be "success or failure"
May  6 20:28:35.600: INFO: Pod "busybox-readonly-false-1c660a86-4009-4766-9cde-a877b2f1bb05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472636ms
May  6 20:28:37.603: INFO: Pod "busybox-readonly-false-1c660a86-4009-4766-9cde-a877b2f1bb05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005494682s
May  6 20:28:37.603: INFO: Pod "busybox-readonly-false-1c660a86-4009-4766-9cde-a877b2f1bb05" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:28:37.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6366" for this suite.
May  6 20:28:43.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:28:43.720: INFO: namespace security-context-test-6366 deletion completed in 6.113393058s

• [SLOW TEST:8.157 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:28:43.720: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:28:43.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c6b12df-5613-4387-852d-aa7c95da0e86" in namespace "downward-api-5077" to be "success or failure"
May  6 20:28:43.756: INFO: Pod "downwardapi-volume-8c6b12df-5613-4387-852d-aa7c95da0e86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915922ms
May  6 20:28:45.759: INFO: Pod "downwardapi-volume-8c6b12df-5613-4387-852d-aa7c95da0e86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005605893s
STEP: Saw pod success
May  6 20:28:45.759: INFO: Pod "downwardapi-volume-8c6b12df-5613-4387-852d-aa7c95da0e86" satisfied condition "success or failure"
May  6 20:28:45.762: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-8c6b12df-5613-4387-852d-aa7c95da0e86 container client-container: <nil>
STEP: delete the pod
May  6 20:28:45.781: INFO: Waiting for pod downwardapi-volume-8c6b12df-5613-4387-852d-aa7c95da0e86 to disappear
May  6 20:28:45.784: INFO: Pod downwardapi-volume-8c6b12df-5613-4387-852d-aa7c95da0e86 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:28:45.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5077" for this suite.
May  6 20:28:51.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:28:51.897: INFO: namespace downward-api-5077 deletion completed in 6.109037025s

• [SLOW TEST:8.176 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:28:51.897: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  6 20:28:51.930: INFO: Waiting up to 5m0s for pod "pod-fae6f6c0-6b22-4c46-8dbf-8f13f7bcf3d6" in namespace "emptydir-1995" to be "success or failure"
May  6 20:28:51.933: INFO: Pod "pod-fae6f6c0-6b22-4c46-8dbf-8f13f7bcf3d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018612ms
May  6 20:28:53.936: INFO: Pod "pod-fae6f6c0-6b22-4c46-8dbf-8f13f7bcf3d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006701782s
STEP: Saw pod success
May  6 20:28:53.937: INFO: Pod "pod-fae6f6c0-6b22-4c46-8dbf-8f13f7bcf3d6" satisfied condition "success or failure"
May  6 20:28:53.940: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-fae6f6c0-6b22-4c46-8dbf-8f13f7bcf3d6 container test-container: <nil>
STEP: delete the pod
May  6 20:28:53.959: INFO: Waiting for pod pod-fae6f6c0-6b22-4c46-8dbf-8f13f7bcf3d6 to disappear
May  6 20:28:53.963: INFO: Pod pod-fae6f6c0-6b22-4c46-8dbf-8f13f7bcf3d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:28:53.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1995" for this suite.
May  6 20:28:59.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:29:00.143: INFO: namespace emptydir-1995 deletion completed in 6.177016886s

• [SLOW TEST:8.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:29:00.143: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8527
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  6 20:29:00.172: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  6 20:29:20.246: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.32.225:8080/dial?request=hostName&protocol=http&host=192.168.32.221&port=8080&tries=1'] Namespace:pod-network-test-8527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:29:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:29:20.332: INFO: Waiting for endpoints: map[]
May  6 20:29:20.335: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.32.225:8080/dial?request=hostName&protocol=http&host=192.168.80.100&port=8080&tries=1'] Namespace:pod-network-test-8527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:29:20.335: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:29:20.422: INFO: Waiting for endpoints: map[]
May  6 20:29:20.424: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.32.225:8080/dial?request=hostName&protocol=http&host=192.168.43.230&port=8080&tries=1'] Namespace:pod-network-test-8527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  6 20:29:20.424: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 20:29:20.509: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:29:20.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8527" for this suite.
May  6 20:29:32.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:29:32.633: INFO: namespace pod-network-test-8527 deletion completed in 12.119931816s

• [SLOW TEST:32.490 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:29:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 20:29:35.191: INFO: Successfully updated pod "labelsupdatecf541e12-5654-4974-a1fd-5d5f5e1c5266"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:29:37.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6029" for this suite.
May  6 20:29:49.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:29:49.331: INFO: namespace projected-6029 deletion completed in 12.119044729s

• [SLOW TEST:16.698 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:29:49.331: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:29:49.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-932" for this suite.
May  6 20:29:55.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:29:55.482: INFO: namespace tables-932 deletion completed in 6.117624945s

• [SLOW TEST:6.151 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:29:55.482: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-3c0088b6-74b2-4685-afdf-60eadedab982
STEP: Creating a pod to test consume secrets
May  6 20:29:55.522: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ea1b76d-7bb2-4720-bbe4-10de3c333fff" in namespace "projected-4197" to be "success or failure"
May  6 20:29:55.525: INFO: Pod "pod-projected-secrets-8ea1b76d-7bb2-4720-bbe4-10de3c333fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389138ms
May  6 20:29:57.529: INFO: Pod "pod-projected-secrets-8ea1b76d-7bb2-4720-bbe4-10de3c333fff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006535039s
STEP: Saw pod success
May  6 20:29:57.529: INFO: Pod "pod-projected-secrets-8ea1b76d-7bb2-4720-bbe4-10de3c333fff" satisfied condition "success or failure"
May  6 20:29:57.532: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-secrets-8ea1b76d-7bb2-4720-bbe4-10de3c333fff container projected-secret-volume-test: <nil>
STEP: delete the pod
May  6 20:29:57.552: INFO: Waiting for pod pod-projected-secrets-8ea1b76d-7bb2-4720-bbe4-10de3c333fff to disappear
May  6 20:29:57.555: INFO: Pod pod-projected-secrets-8ea1b76d-7bb2-4720-bbe4-10de3c333fff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:29:57.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4197" for this suite.
May  6 20:30:03.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:30:03.677: INFO: namespace projected-4197 deletion completed in 6.118268263s

• [SLOW TEST:8.195 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:30:03.677: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 20:30:06.237: INFO: Successfully updated pod "annotationupdated8b02d0e-7d73-4e2f-a73f-a11b9a7ceac8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:30:10.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6245" for this suite.
May  6 20:30:38.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:30:38.387: INFO: namespace downward-api-6245 deletion completed in 28.117695284s

• [SLOW TEST:34.710 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:30:38.388: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:30:38.431: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d63655b-29fd-4c83-9909-77a1e0a0d473" in namespace "projected-7667" to be "success or failure"
May  6 20:30:38.435: INFO: Pod "downwardapi-volume-6d63655b-29fd-4c83-9909-77a1e0a0d473": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492635ms
May  6 20:30:40.438: INFO: Pod "downwardapi-volume-6d63655b-29fd-4c83-9909-77a1e0a0d473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006855669s
STEP: Saw pod success
May  6 20:30:40.438: INFO: Pod "downwardapi-volume-6d63655b-29fd-4c83-9909-77a1e0a0d473" satisfied condition "success or failure"
May  6 20:30:40.442: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-6d63655b-29fd-4c83-9909-77a1e0a0d473 container client-container: <nil>
STEP: delete the pod
May  6 20:30:40.472: INFO: Waiting for pod downwardapi-volume-6d63655b-29fd-4c83-9909-77a1e0a0d473 to disappear
May  6 20:30:40.477: INFO: Pod downwardapi-volume-6d63655b-29fd-4c83-9909-77a1e0a0d473 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:30:40.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7667" for this suite.
May  6 20:30:46.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:30:46.607: INFO: namespace projected-7667 deletion completed in 6.126103462s

• [SLOW TEST:8.219 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:30:46.607: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 20:30:46.634: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:30:48.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-58" for this suite.
May  6 20:30:54.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:30:54.934: INFO: namespace init-container-58 deletion completed in 6.175806332s

• [SLOW TEST:8.328 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:30:54.935: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
May  6 20:30:57.484: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7597 pod-service-account-4da53de2-ec33-4854-b15a-cbb92836aa9d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May  6 20:30:57.724: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7597 pod-service-account-4da53de2-ec33-4854-b15a-cbb92836aa9d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May  6 20:30:57.874: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7597 pod-service-account-4da53de2-ec33-4854-b15a-cbb92836aa9d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:30:58.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7597" for this suite.
May  6 20:31:04.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:31:04.143: INFO: namespace svcaccounts-7597 deletion completed in 6.117360978s

• [SLOW TEST:9.208 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:31:04.143: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:31:04.171: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  6 20:31:07.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-2973 create -f -'
May  6 20:31:07.334: INFO: stderr: ""
May  6 20:31:07.334: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  6 20:31:07.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-2973 delete e2e-test-crd-publish-openapi-3531-crds test-cr'
May  6 20:31:07.432: INFO: stderr: ""
May  6 20:31:07.432: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  6 20:31:07.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-2973 apply -f -'
May  6 20:31:07.576: INFO: stderr: ""
May  6 20:31:07.576: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  6 20:31:07.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-2973 delete e2e-test-crd-publish-openapi-3531-crds test-cr'
May  6 20:31:07.646: INFO: stderr: ""
May  6 20:31:07.646: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May  6 20:31:07.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-3531-crds'
May  6 20:31:07.783: INFO: stderr: ""
May  6 20:31:07.783: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:31:10.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2973" for this suite.
May  6 20:31:16.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:31:16.779: INFO: namespace crd-publish-openapi-2973 deletion completed in 6.117947305s

• [SLOW TEST:12.636 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:31:16.779: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-8214bf5c-6a94-4ad7-8cba-1f440eb972eb
STEP: Creating a pod to test consume configMaps
May  6 20:31:16.819: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b175f5c8-4d5d-4be5-be62-7d2c519eddc5" in namespace "projected-5383" to be "success or failure"
May  6 20:31:16.823: INFO: Pod "pod-projected-configmaps-b175f5c8-4d5d-4be5-be62-7d2c519eddc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250715ms
May  6 20:31:18.827: INFO: Pod "pod-projected-configmaps-b175f5c8-4d5d-4be5-be62-7d2c519eddc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007924516s
STEP: Saw pod success
May  6 20:31:18.827: INFO: Pod "pod-projected-configmaps-b175f5c8-4d5d-4be5-be62-7d2c519eddc5" satisfied condition "success or failure"
May  6 20:31:18.830: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-configmaps-b175f5c8-4d5d-4be5-be62-7d2c519eddc5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:31:18.849: INFO: Waiting for pod pod-projected-configmaps-b175f5c8-4d5d-4be5-be62-7d2c519eddc5 to disappear
May  6 20:31:18.852: INFO: Pod pod-projected-configmaps-b175f5c8-4d5d-4be5-be62-7d2c519eddc5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:31:18.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5383" for this suite.
May  6 20:31:24.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:31:24.975: INFO: namespace projected-5383 deletion completed in 6.117942955s

• [SLOW TEST:8.196 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:31:24.975: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May  6 20:31:25.010: INFO: Pod name pod-release: Found 0 pods out of 1
May  6 20:31:30.017: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:31:31.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4488" for this suite.
May  6 20:31:37.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:31:37.276: INFO: namespace replication-controller-4488 deletion completed in 6.23663671s

• [SLOW TEST:12.301 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:31:37.277: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  6 20:31:39.833: INFO: Successfully updated pod "pod-update-c4a1bc34-e337-427e-a38d-a11f765cf50b"
STEP: verifying the updated pod is in kubernetes
May  6 20:31:39.852: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:31:39.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5607" for this suite.
May  6 20:32:07.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:32:07.987: INFO: namespace pods-5607 deletion completed in 28.128033676s

• [SLOW TEST:30.711 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:32:07.987: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 20:32:08.018: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 20:32:08.030: INFO: Waiting for terminating namespaces to be deleted...
May  6 20:32:08.032: INFO: 
Logging pods the kubelet thinks is on node kube-node-0-kubelet.kubernetes-cluster.mesos before test
May  6 20:32:08.043: INFO: kube-proxy-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.043: INFO: 	Container kp ready: true, restart count 0
May  6 20:32:08.043: INFO: calico-node-xjmdx from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.043: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:32:08.043: INFO: sonobuoy-e2e-job-76e28147b51c42fc from sonobuoy started at 2020-05-06 19:47:59 +0000 UTC (2 container statuses recorded)
May  6 20:32:08.043: INFO: 	Container e2e ready: true, restart count 0
May  6 20:32:08.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 20:32:08.043: INFO: local-dns-dispatcher-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.043: INFO: 	Container coredns ready: true, restart count 0
May  6 20:32:08.043: INFO: sonobuoy from sonobuoy started at 2020-05-06 19:47:58 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.043: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 20:32:08.043: INFO: 
Logging pods the kubelet thinks is on node kube-node-1-kubelet.kubernetes-cluster.mesos before test
May  6 20:32:08.058: INFO: local-dns-dispatcher-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.058: INFO: 	Container coredns ready: true, restart count 0
May  6 20:32:08.058: INFO: metrics-server-859cc6f97b-mpsmb from kube-system started at 2020-05-06 16:32:55 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.058: INFO: 	Container metrics-server ready: true, restart count 0
May  6 20:32:08.058: INFO: calico-node-tsjfr from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.058: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:32:08.058: INFO: coredns-7fb4bb54b-p8tfs from kube-system started at 2020-05-06 17:52:40 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.058: INFO: 	Container coredns ready: true, restart count 0
May  6 20:32:08.058: INFO: calico-kube-controllers-788d6b9876-q8w7w from kube-system started at 2020-05-06 18:24:18 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.058: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 20:32:08.058: INFO: kube-proxy-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.058: INFO: 	Container kp ready: true, restart count 0
May  6 20:32:08.058: INFO: 
Logging pods the kubelet thinks is on node kube-node-2-kubelet.kubernetes-cluster.mesos before test
May  6 20:32:08.064: INFO: local-dns-dispatcher-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.064: INFO: 	Container coredns ready: true, restart count 0
May  6 20:32:08.064: INFO: calico-node-tb2vk from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.064: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:32:08.064: INFO: coredns-7fb4bb54b-xtdwq from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.064: INFO: 	Container coredns ready: true, restart count 0
May  6 20:32:08.064: INFO: kube-proxy-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.064: INFO: 	Container kp ready: true, restart count 0
May  6 20:32:08.064: INFO: kubernetes-dashboard-68c7899b54-cq697 from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:32:08.064: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a181a4cc-37dc-448b-98a4-4241d021067b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-a181a4cc-37dc-448b-98a4-4241d021067b off the node kube-node-2-kubelet.kubernetes-cluster.mesos
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a181a4cc-37dc-448b-98a4-4241d021067b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:32:16.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1915" for this suite.
May  6 20:32:30.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:32:30.300: INFO: namespace sched-pred-1915 deletion completed in 14.14500105s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:22.313 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:32:30.301: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3674
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3674
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3674
May  6 20:32:30.341: INFO: Found 0 stateful pods, waiting for 1
May  6 20:32:40.345: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May  6 20:32:40.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:32:40.501: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:32:40.501: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:32:40.501: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:32:40.504: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  6 20:32:50.509: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:32:50.509: INFO: Waiting for statefulset status.replicas updated to 0
May  6 20:32:50.523: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999544s
May  6 20:32:51.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99638941s
May  6 20:32:52.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992896797s
May  6 20:32:53.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988929966s
May  6 20:32:54.539: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983921455s
May  6 20:32:55.543: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980470775s
May  6 20:32:56.547: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976751166s
May  6 20:32:57.550: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972391976s
May  6 20:32:58.554: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968895098s
May  6 20:32:59.558: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.16387ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3674
May  6 20:33:00.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:33:00.772: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 20:33:00.772: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 20:33:00.772: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 20:33:00.775: INFO: Found 1 stateful pods, waiting for 3
May  6 20:33:10.781: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:33:10.781: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:33:10.781: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May  6 20:33:10.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:33:10.936: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:33:10.936: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:33:10.936: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:33:10.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:33:11.084: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:33:11.084: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:33:11.084: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:33:11.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:33:11.238: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:33:11.238: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:33:11.238: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:33:11.238: INFO: Waiting for statefulset status.replicas updated to 0
May  6 20:33:11.241: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  6 20:33:21.248: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:33:21.248: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:33:21.248: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:33:21.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999594s
May  6 20:33:22.263: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996675823s
May  6 20:33:23.267: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992378792s
May  6 20:33:24.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988582144s
May  6 20:33:25.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984766378s
May  6 20:33:26.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980489008s
May  6 20:33:27.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976970296s
May  6 20:33:28.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968071401s
May  6 20:33:29.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963613195s
May  6 20:33:30.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.087712ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3674
May  6 20:33:31.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:33:31.460: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 20:33:31.460: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 20:33:31.460: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 20:33:31.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:33:31.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 20:33:31.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 20:33:31.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 20:33:31.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-3674 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:33:31.777: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 20:33:31.777: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 20:33:31.777: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 20:33:31.777: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 20:33:51.792: INFO: Deleting all statefulset in ns statefulset-3674
May  6 20:33:51.795: INFO: Scaling statefulset ss to 0
May  6 20:33:51.804: INFO: Waiting for statefulset status.replicas updated to 0
May  6 20:33:51.807: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:33:51.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3674" for this suite.
May  6 20:33:57.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:33:58.015: INFO: namespace statefulset-3674 deletion completed in 6.191708857s

• [SLOW TEST:87.715 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:33:58.015: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:34:00.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7441" for this suite.
May  6 20:34:52.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:34:52.205: INFO: namespace kubelet-test-7441 deletion completed in 52.11628237s

• [SLOW TEST:54.190 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:34:52.206: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 20:34:52.545: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 20:34:55.563: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:34:55.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6418" for this suite.
May  6 20:35:07.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:35:07.714: INFO: namespace webhook-6418 deletion completed in 12.109992934s
STEP: Destroying namespace "webhook-6418-markers" for this suite.
May  6 20:35:13.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:35:13.824: INFO: namespace webhook-6418-markers deletion completed in 6.110039451s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:21.632 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:35:13.837: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May  6 20:35:16.401: INFO: Successfully updated pod "labelsupdatea4a4e784-93c6-4998-9927-aa5cfcaa824b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:35:18.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4580" for this suite.
May  6 20:35:30.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:35:30.558: INFO: namespace downward-api-4580 deletion completed in 12.13464201s

• [SLOW TEST:16.721 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:35:30.558: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 20:35:30.592: INFO: Waiting up to 5m0s for pod "downward-api-f1b29fb4-5b1a-4cb1-8fc4-880f2e8a083d" in namespace "downward-api-7603" to be "success or failure"
May  6 20:35:30.597: INFO: Pod "downward-api-f1b29fb4-5b1a-4cb1-8fc4-880f2e8a083d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.260637ms
May  6 20:35:32.600: INFO: Pod "downward-api-f1b29fb4-5b1a-4cb1-8fc4-880f2e8a083d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007859538s
STEP: Saw pod success
May  6 20:35:32.600: INFO: Pod "downward-api-f1b29fb4-5b1a-4cb1-8fc4-880f2e8a083d" satisfied condition "success or failure"
May  6 20:35:32.604: INFO: Trying to get logs from node kube-node-1-kubelet.kubernetes-cluster.mesos pod downward-api-f1b29fb4-5b1a-4cb1-8fc4-880f2e8a083d container dapi-container: <nil>
STEP: delete the pod
May  6 20:35:32.629: INFO: Waiting for pod downward-api-f1b29fb4-5b1a-4cb1-8fc4-880f2e8a083d to disappear
May  6 20:35:32.632: INFO: Pod downward-api-f1b29fb4-5b1a-4cb1-8fc4-880f2e8a083d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:35:32.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7603" for this suite.
May  6 20:35:38.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:35:38.748: INFO: namespace downward-api-7603 deletion completed in 6.112708745s

• [SLOW TEST:8.190 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:35:38.748: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 20:35:38.778: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 20:35:38.790: INFO: Waiting for terminating namespaces to be deleted...
May  6 20:35:38.793: INFO: 
Logging pods the kubelet thinks is on node kube-node-0-kubelet.kubernetes-cluster.mesos before test
May  6 20:35:38.802: INFO: sonobuoy from sonobuoy started at 2020-05-06 19:47:58 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.802: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 20:35:38.802: INFO: kube-proxy-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.802: INFO: 	Container kp ready: true, restart count 0
May  6 20:35:38.802: INFO: calico-node-xjmdx from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.802: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:35:38.802: INFO: sonobuoy-e2e-job-76e28147b51c42fc from sonobuoy started at 2020-05-06 19:47:59 +0000 UTC (2 container statuses recorded)
May  6 20:35:38.802: INFO: 	Container e2e ready: true, restart count 0
May  6 20:35:38.802: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 20:35:38.802: INFO: local-dns-dispatcher-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.802: INFO: 	Container coredns ready: true, restart count 0
May  6 20:35:38.802: INFO: 
Logging pods the kubelet thinks is on node kube-node-1-kubelet.kubernetes-cluster.mesos before test
May  6 20:35:38.809: INFO: calico-kube-controllers-788d6b9876-q8w7w from kube-system started at 2020-05-06 18:24:18 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.809: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 20:35:38.809: INFO: kube-proxy-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.809: INFO: 	Container kp ready: true, restart count 0
May  6 20:35:38.809: INFO: local-dns-dispatcher-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.809: INFO: 	Container coredns ready: true, restart count 0
May  6 20:35:38.809: INFO: metrics-server-859cc6f97b-mpsmb from kube-system started at 2020-05-06 16:32:55 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.809: INFO: 	Container metrics-server ready: true, restart count 0
May  6 20:35:38.809: INFO: calico-node-tsjfr from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.809: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:35:38.809: INFO: coredns-7fb4bb54b-p8tfs from kube-system started at 2020-05-06 17:52:40 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.809: INFO: 	Container coredns ready: true, restart count 0
May  6 20:35:38.809: INFO: 
Logging pods the kubelet thinks is on node kube-node-2-kubelet.kubernetes-cluster.mesos before test
May  6 20:35:38.815: INFO: coredns-7fb4bb54b-xtdwq from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.815: INFO: 	Container coredns ready: true, restart count 0
May  6 20:35:38.815: INFO: local-dns-dispatcher-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.815: INFO: 	Container coredns ready: true, restart count 0
May  6 20:35:38.815: INFO: calico-node-tb2vk from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.815: INFO: 	Container calico-node ready: true, restart count 0
May  6 20:35:38.815: INFO: kube-proxy-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.815: INFO: 	Container kp ready: true, restart count 0
May  6 20:35:38.815: INFO: kubernetes-dashboard-68c7899b54-cq697 from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 20:35:38.815: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e4ebcb9f-c1bb-4e38-b578-dc64acd80ba9 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e4ebcb9f-c1bb-4e38-b578-dc64acd80ba9 off the node kube-node-2-kubelet.kubernetes-cluster.mesos
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e4ebcb9f-c1bb-4e38-b578-dc64acd80ba9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:40:42.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6717" for this suite.
May  6 20:40:52.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:40:53.013: INFO: namespace sched-pred-6717 deletion completed in 10.118384989s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:314.265 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:40:53.013: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4341
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-4341
May  6 20:40:53.053: INFO: Found 0 stateful pods, waiting for 1
May  6 20:41:03.058: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 20:41:03.075: INFO: Deleting all statefulset in ns statefulset-4341
May  6 20:41:03.078: INFO: Scaling statefulset ss to 0
May  6 20:41:23.100: INFO: Waiting for statefulset status.replicas updated to 0
May  6 20:41:23.103: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:41:23.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4341" for this suite.
May  6 20:41:29.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:41:29.275: INFO: namespace statefulset-4341 deletion completed in 6.122623957s

• [SLOW TEST:36.263 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:41:29.276: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 20:41:29.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4940'
May  6 20:41:29.458: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 20:41:29.458: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
May  6 20:41:29.462: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May  6 20:41:29.464: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May  6 20:41:29.476: INFO: scanned /root for discovery docs: <nil>
May  6 20:41:29.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4940'
May  6 20:41:45.222: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  6 20:41:45.222: INFO: stdout: "Created e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c\nScaling up e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
May  6 20:41:45.223: INFO: stdout: "Created e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c\nScaling up e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
May  6 20:41:45.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4940'
May  6 20:41:45.285: INFO: stderr: ""
May  6 20:41:45.285: INFO: stdout: "e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c-gb6xn "
May  6 20:41:45.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c-gb6xn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4940'
May  6 20:41:45.349: INFO: stderr: ""
May  6 20:41:45.349: INFO: stdout: "true"
May  6 20:41:45.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c-gb6xn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4940'
May  6 20:41:45.413: INFO: stderr: ""
May  6 20:41:45.413: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
May  6 20:41:45.413: INFO: e2e-test-httpd-rc-f3b781f732ec115d9ef72a5e3905c04c-gb6xn is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
May  6 20:41:45.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete rc e2e-test-httpd-rc --namespace=kubectl-4940'
May  6 20:41:45.486: INFO: stderr: ""
May  6 20:41:45.486: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:41:45.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4940" for this suite.
May  6 20:42:11.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:42:11.605: INFO: namespace kubectl-4940 deletion completed in 26.113892667s

• [SLOW TEST:42.330 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:42:11.605: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-764
STEP: creating replication controller nodeport-test in namespace services-764
I0506 20:42:11.653588      22 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-764, replica count: 2
May  6 20:42:14.703: INFO: Creating new exec pod
I0506 20:42:14.703914      22 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 20:42:17.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-764 execpod4v5w7 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May  6 20:42:17.866: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  6 20:42:17.866: INFO: stdout: ""
May  6 20:42:17.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-764 execpod4v5w7 -- /bin/sh -x -c nc -zv -t -w 2 10.100.109.162 80'
May  6 20:42:18.013: INFO: stderr: "+ nc -zv -t -w 2 10.100.109.162 80\nConnection to 10.100.109.162 80 port [tcp/http] succeeded!\n"
May  6 20:42:18.013: INFO: stdout: ""
May  6 20:42:18.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-764 execpod4v5w7 -- /bin/sh -x -c nc -zv -t -w 2 9.0.4.4 32461'
May  6 20:42:18.160: INFO: stderr: "+ nc -zv -t -w 2 9.0.4.4 32461\nConnection to 9.0.4.4 32461 port [tcp/32461] succeeded!\n"
May  6 20:42:18.160: INFO: stdout: ""
May  6 20:42:18.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-764 execpod4v5w7 -- /bin/sh -x -c nc -zv -t -w 2 9.0.2.4 32461'
May  6 20:42:18.303: INFO: stderr: "+ nc -zv -t -w 2 9.0.2.4 32461\nConnection to 9.0.2.4 32461 port [tcp/32461] succeeded!\n"
May  6 20:42:18.303: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:42:18.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-764" for this suite.
May  6 20:42:24.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:42:24.436: INFO: namespace services-764 deletion completed in 6.128971497s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.831 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:42:24.436: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-63bf74d2-b18d-4e75-8d9c-9cad673c073d
STEP: Creating configMap with name cm-test-opt-upd-850e92e2-4bc2-41c9-8dc7-685d9d2f04cb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-63bf74d2-b18d-4e75-8d9c-9cad673c073d
STEP: Updating configmap cm-test-opt-upd-850e92e2-4bc2-41c9-8dc7-685d9d2f04cb
STEP: Creating configMap with name cm-test-opt-create-19eced60-2d24-46a3-9dc7-4f22ed927b2c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:42:28.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8450" for this suite.
May  6 20:42:40.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:42:40.694: INFO: namespace configmap-8450 deletion completed in 12.117326762s

• [SLOW TEST:16.258 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:42:40.694: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-2659
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2659 to expose endpoints map[]
May  6 20:42:40.736: INFO: Get endpoints failed (2.914337ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May  6 20:42:41.739: INFO: successfully validated that service endpoint-test2 in namespace services-2659 exposes endpoints map[] (1.006754359s elapsed)
STEP: Creating pod pod1 in namespace services-2659
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2659 to expose endpoints map[pod1:[80]]
May  6 20:42:43.765: INFO: successfully validated that service endpoint-test2 in namespace services-2659 exposes endpoints map[pod1:[80]] (2.01801358s elapsed)
STEP: Creating pod pod2 in namespace services-2659
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2659 to expose endpoints map[pod1:[80] pod2:[80]]
May  6 20:42:44.791: INFO: successfully validated that service endpoint-test2 in namespace services-2659 exposes endpoints map[pod1:[80] pod2:[80]] (1.022596564s elapsed)
STEP: Deleting pod pod1 in namespace services-2659
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2659 to expose endpoints map[pod2:[80]]
May  6 20:42:45.810: INFO: successfully validated that service endpoint-test2 in namespace services-2659 exposes endpoints map[pod2:[80]] (1.012960648s elapsed)
STEP: Deleting pod pod2 in namespace services-2659
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2659 to expose endpoints map[]
May  6 20:42:46.823: INFO: successfully validated that service endpoint-test2 in namespace services-2659 exposes endpoints map[] (1.006735014s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:42:46.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2659" for this suite.
May  6 20:43:14.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:43:14.978: INFO: namespace services-2659 deletion completed in 28.126896112s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.283 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:43:14.978: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:43:15.018: INFO: (0) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 9.4566ms)
May  6 20:43:15.021: INFO: (1) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.221997ms)
May  6 20:43:15.024: INFO: (2) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.366222ms)
May  6 20:43:15.030: INFO: (3) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 5.630669ms)
May  6 20:43:15.033: INFO: (4) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.13913ms)
May  6 20:43:15.037: INFO: (5) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.349232ms)
May  6 20:43:15.042: INFO: (6) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 5.027963ms)
May  6 20:43:15.045: INFO: (7) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.978779ms)
May  6 20:43:15.048: INFO: (8) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.266596ms)
May  6 20:43:15.052: INFO: (9) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.637774ms)
May  6 20:43:15.055: INFO: (10) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.913642ms)
May  6 20:43:15.058: INFO: (11) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.101441ms)
May  6 20:43:15.061: INFO: (12) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.606145ms)
May  6 20:43:15.064: INFO: (13) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 2.782459ms)
May  6 20:43:15.067: INFO: (14) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.017277ms)
May  6 20:43:15.071: INFO: (15) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.673513ms)
May  6 20:43:15.074: INFO: (16) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.07954ms)
May  6 20:43:15.077: INFO: (17) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.502942ms)
May  6 20:43:15.085: INFO: (18) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 7.316828ms)
May  6 20:43:15.088: INFO: (19) /api/v1/nodes/kube-node-0-kubelet.kubernetes-cluster.mesos/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="btmp">btmp... (200; 3.219319ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:43:15.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6832" for this suite.
May  6 20:43:21.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:43:21.209: INFO: namespace proxy-6832 deletion completed in 6.117765161s

• [SLOW TEST:6.232 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:43:21.210: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
May  6 20:43:21.246: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May  6 20:43:21.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-2801'
May  6 20:43:21.492: INFO: stderr: ""
May  6 20:43:21.492: INFO: stdout: "service/redis-slave created\n"
May  6 20:43:21.493: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May  6 20:43:21.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-2801'
May  6 20:43:21.697: INFO: stderr: ""
May  6 20:43:21.697: INFO: stdout: "service/redis-master created\n"
May  6 20:43:21.697: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  6 20:43:21.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-2801'
May  6 20:43:21.901: INFO: stderr: ""
May  6 20:43:21.901: INFO: stdout: "service/frontend created\n"
May  6 20:43:21.901: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May  6 20:43:21.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-2801'
May  6 20:43:22.047: INFO: stderr: ""
May  6 20:43:22.047: INFO: stdout: "deployment.apps/frontend created\n"
May  6 20:43:22.047: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  6 20:43:22.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-2801'
May  6 20:43:22.200: INFO: stderr: ""
May  6 20:43:22.200: INFO: stdout: "deployment.apps/redis-master created\n"
May  6 20:43:22.200: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May  6 20:43:22.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-2801'
May  6 20:43:22.345: INFO: stderr: ""
May  6 20:43:22.345: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May  6 20:43:22.345: INFO: Waiting for all frontend pods to be Running.
May  6 20:43:27.395: INFO: Waiting for frontend to serve content.
May  6 20:43:27.410: INFO: Trying to add a new entry to the guestbook.
May  6 20:43:27.424: INFO: Verifying that added entry can be retrieved.
May  6 20:43:27.435: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:43:32.448: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:43:37.463: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:43:42.476: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:43:47.491: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:43:52.505: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:43:57.521: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:44:02.534: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:44:07.550: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:44:12.563: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:44:17.577: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
May  6 20:44:22.591: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
May  6 20:44:27.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-2801'
May  6 20:44:27.688: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 20:44:27.688: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May  6 20:44:27.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-2801'
May  6 20:44:27.775: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 20:44:27.775: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  6 20:44:27.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-2801'
May  6 20:44:27.865: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 20:44:27.866: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  6 20:44:27.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-2801'
May  6 20:44:27.936: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 20:44:27.936: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  6 20:44:27.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-2801'
May  6 20:44:28.010: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 20:44:28.010: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  6 20:44:28.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-2801'
May  6 20:44:28.082: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 20:44:28.082: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:44:28.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2801" for this suite.
May  6 20:44:56.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:44:56.215: INFO: namespace kubectl-2801 deletion completed in 28.128445964s

• [SLOW TEST:95.006 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:44:56.215: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:45:56.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2784" for this suite.
May  6 20:46:24.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:46:24.374: INFO: namespace container-probe-2784 deletion completed in 28.116579931s

• [SLOW TEST:88.159 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:46:24.374: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:46:24.408: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May  6 20:46:29.412: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  6 20:46:29.412: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 20:46:29.431: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9697 /apis/apps/v1/namespaces/deployment-9697/deployments/test-cleanup-deployment cbe3065a-c02f-4346-a126-e271e4ae83a4 60892 1 2020-05-06 20:46:29 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005997c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May  6 20:46:29.435: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-9697 /apis/apps/v1/namespaces/deployment-9697/replicasets/test-cleanup-deployment-65db99849b dcef44b1-5d60-49f0-bb2d-87087a9efb27 60895 1 2020-05-06 20:46:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment cbe3065a-c02f-4346-a126-e271e4ae83a4 0xc00587c077 0xc00587c078}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00587c0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 20:46:29.435: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May  6 20:46:29.435: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9697 /apis/apps/v1/namespaces/deployment-9697/replicasets/test-cleanup-controller 91f6a3ad-5b54-4d15-945a-eac28c8999b2 60893 1 2020-05-06 20:46:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment cbe3065a-c02f-4346-a126-e271e4ae83a4 0xc005997fa7 0xc005997fa8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00587c008 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 20:46:29.441: INFO: Pod "test-cleanup-controller-cbg74" is available:
&Pod{ObjectMeta:{test-cleanup-controller-cbg74 test-cleanup-controller- deployment-9697 /api/v1/namespaces/deployment-9697/pods/test-cleanup-controller-cbg74 bdca2c3c-4890-46aa-bf16-cd9f4f292791 60882 0 2020-05-06 20:46:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.32.253/32 cni.projectcalico.org/podIPs:192.168.32.253/32] [{apps/v1 ReplicaSet test-cleanup-controller 91f6a3ad-5b54-4d15-945a-eac28c8999b2 0xc0054934a7 0xc0054934a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l2mjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l2mjv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l2mjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:46:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:46:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:46:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 20:46:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:192.168.32.253,StartTime:2020-05-06 20:46:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 20:46:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c887922de0215c6620937c148cffe1157cd4291ae5b68cd473401b7540ea3323,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.32.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 20:46:29.441: INFO: Pod "test-cleanup-deployment-65db99849b-bxr8j" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-bxr8j test-cleanup-deployment-65db99849b- deployment-9697 /api/v1/namespaces/deployment-9697/pods/test-cleanup-deployment-65db99849b-bxr8j 7f8c911c-3cc6-433f-a51d-51e99bdeb206 60897 0 2020-05-06 20:46:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b dcef44b1-5d60-49f0-bb2d-87087a9efb27 0xc005493627 0xc005493628}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l2mjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l2mjv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l2mjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:46:29.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9697" for this suite.
May  6 20:46:35.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:46:35.569: INFO: namespace deployment-9697 deletion completed in 6.11645999s

• [SLOW TEST:11.195 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:46:35.569: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-ea89ac14-f261-4061-93fe-8225e506da6c
STEP: Creating configMap with name cm-test-opt-upd-602f902f-5983-473a-b7fe-176eb7d44ceb
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ea89ac14-f261-4061-93fe-8225e506da6c
STEP: Updating configmap cm-test-opt-upd-602f902f-5983-473a-b7fe-176eb7d44ceb
STEP: Creating configMap with name cm-test-opt-create-9994f8b7-bbe6-4fc6-a630-5612e1705a56
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:47:43.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4143" for this suite.
May  6 20:47:59.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:48:00.136: INFO: namespace projected-4143 deletion completed in 16.149471029s

• [SLOW TEST:84.566 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:48:00.136: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:48:00.189: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0b6b390-efcc-463b-b32e-1e300025d16a" in namespace "projected-978" to be "success or failure"
May  6 20:48:00.196: INFO: Pod "downwardapi-volume-e0b6b390-efcc-463b-b32e-1e300025d16a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.532267ms
May  6 20:48:02.200: INFO: Pod "downwardapi-volume-e0b6b390-efcc-463b-b32e-1e300025d16a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01156138s
STEP: Saw pod success
May  6 20:48:02.200: INFO: Pod "downwardapi-volume-e0b6b390-efcc-463b-b32e-1e300025d16a" satisfied condition "success or failure"
May  6 20:48:02.203: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-e0b6b390-efcc-463b-b32e-1e300025d16a container client-container: <nil>
STEP: delete the pod
May  6 20:48:02.222: INFO: Waiting for pod downwardapi-volume-e0b6b390-efcc-463b-b32e-1e300025d16a to disappear
May  6 20:48:02.225: INFO: Pod downwardapi-volume-e0b6b390-efcc-463b-b32e-1e300025d16a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:48:02.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-978" for this suite.
May  6 20:48:08.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:48:08.346: INFO: namespace projected-978 deletion completed in 6.117159426s

• [SLOW TEST:8.211 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:48:08.347: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:48:08.381: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0cfe484d-ccd1-4bcc-9662-6b5080c0664e" in namespace "security-context-test-6776" to be "success or failure"
May  6 20:48:08.384: INFO: Pod "alpine-nnp-false-0cfe484d-ccd1-4bcc-9662-6b5080c0664e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407942ms
May  6 20:48:10.388: INFO: Pod "alpine-nnp-false-0cfe484d-ccd1-4bcc-9662-6b5080c0664e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006253472s
May  6 20:48:10.388: INFO: Pod "alpine-nnp-false-0cfe484d-ccd1-4bcc-9662-6b5080c0664e" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:48:10.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6776" for this suite.
May  6 20:48:16.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:48:16.520: INFO: namespace security-context-test-6776 deletion completed in 6.119399087s

• [SLOW TEST:8.173 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:48:16.520: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4381.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4381.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4381.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4381.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4381.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4381.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4381.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 20:48:18.575: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.579: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.583: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.586: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.597: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.601: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.604: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.607: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4381.svc.cluster.local from pod dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8: the server could not find the requested resource (get pods dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8)
May  6 20:48:18.614: INFO: Lookups using dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4381.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4381.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4381.svc.cluster.local jessie_udp@dns-test-service-2.dns-4381.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4381.svc.cluster.local]

May  6 20:48:23.656: INFO: DNS probes using dns-4381/dns-test-dc242627-39b0-4d59-a0b7-7ba543df99f8 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:48:23.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4381" for this suite.
May  6 20:48:29.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:48:29.825: INFO: namespace dns-4381 deletion completed in 6.126664s

• [SLOW TEST:13.304 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:48:29.825: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:48:40.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5959" for this suite.
May  6 20:48:46.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:48:47.018: INFO: namespace resourcequota-5959 deletion completed in 6.118078699s

• [SLOW TEST:17.193 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:48:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:48:51.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-336" for this suite.
May  6 20:48:57.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:48:57.176: INFO: namespace kubelet-test-336 deletion completed in 6.115461905s

• [SLOW TEST:10.158 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:48:57.176: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1172/secret-test-71b5f9d6-b272-457b-9d7a-03b5a40fd122
STEP: Creating a pod to test consume secrets
May  6 20:48:57.212: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1ba102d-6055-44fd-bc5e-88a9d2dc735e" in namespace "secrets-1172" to be "success or failure"
May  6 20:48:57.215: INFO: Pod "pod-configmaps-b1ba102d-6055-44fd-bc5e-88a9d2dc735e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180573ms
May  6 20:48:59.218: INFO: Pod "pod-configmaps-b1ba102d-6055-44fd-bc5e-88a9d2dc735e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006296208s
STEP: Saw pod success
May  6 20:48:59.218: INFO: Pod "pod-configmaps-b1ba102d-6055-44fd-bc5e-88a9d2dc735e" satisfied condition "success or failure"
May  6 20:48:59.222: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-b1ba102d-6055-44fd-bc5e-88a9d2dc735e container env-test: <nil>
STEP: delete the pod
May  6 20:48:59.240: INFO: Waiting for pod pod-configmaps-b1ba102d-6055-44fd-bc5e-88a9d2dc735e to disappear
May  6 20:48:59.243: INFO: Pod pod-configmaps-b1ba102d-6055-44fd-bc5e-88a9d2dc735e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:48:59.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1172" for this suite.
May  6 20:49:05.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:49:05.362: INFO: namespace secrets-1172 deletion completed in 6.114356712s

• [SLOW TEST:8.186 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:49:05.363: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:49:05.390: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:49:07.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1519" for this suite.
May  6 20:49:51.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:49:51.544: INFO: namespace pods-1519 deletion completed in 44.113336487s

• [SLOW TEST:46.181 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:49:51.544: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-d166f078-5d7c-492b-99f8-83b5fc8d4671
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:49:51.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7604" for this suite.
May  6 20:49:57.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:49:57.696: INFO: namespace secrets-7604 deletion completed in 6.118856569s

• [SLOW TEST:6.152 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:49:57.696: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:49:57.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67b1b585-474c-4225-af2f-2fe74711870a" in namespace "projected-9405" to be "success or failure"
May  6 20:49:57.734: INFO: Pod "downwardapi-volume-67b1b585-474c-4225-af2f-2fe74711870a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.597197ms
May  6 20:49:59.737: INFO: Pod "downwardapi-volume-67b1b585-474c-4225-af2f-2fe74711870a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007348431s
STEP: Saw pod success
May  6 20:49:59.737: INFO: Pod "downwardapi-volume-67b1b585-474c-4225-af2f-2fe74711870a" satisfied condition "success or failure"
May  6 20:49:59.741: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-67b1b585-474c-4225-af2f-2fe74711870a container client-container: <nil>
STEP: delete the pod
May  6 20:49:59.762: INFO: Waiting for pod downwardapi-volume-67b1b585-474c-4225-af2f-2fe74711870a to disappear
May  6 20:49:59.768: INFO: Pod downwardapi-volume-67b1b585-474c-4225-af2f-2fe74711870a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:49:59.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9405" for this suite.
May  6 20:50:05.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:50:05.891: INFO: namespace projected-9405 deletion completed in 6.117599929s

• [SLOW TEST:8.196 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:50:05.891: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May  6 20:50:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:50:22.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-822" for this suite.
May  6 20:50:28.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:50:28.925: INFO: namespace crd-publish-openapi-822 deletion completed in 6.112589304s

• [SLOW TEST:23.034 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:50:28.926: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-a667f50c-c400-48f1-9fc4-1cfc81f6762a
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:50:28.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9221" for this suite.
May  6 20:50:34.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:50:35.072: INFO: namespace configmap-9221 deletion completed in 6.114480994s

• [SLOW TEST:6.147 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:50:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-20d850c3-2d6d-42b8-8c21-1267dfbae2f2
STEP: Creating a pod to test consume configMaps
May  6 20:50:35.108: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bbb54952-47cf-44b7-ad59-a3082cd339bf" in namespace "projected-4384" to be "success or failure"
May  6 20:50:35.111: INFO: Pod "pod-projected-configmaps-bbb54952-47cf-44b7-ad59-a3082cd339bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632917ms
May  6 20:50:37.115: INFO: Pod "pod-projected-configmaps-bbb54952-47cf-44b7-ad59-a3082cd339bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006089588s
STEP: Saw pod success
May  6 20:50:37.115: INFO: Pod "pod-projected-configmaps-bbb54952-47cf-44b7-ad59-a3082cd339bf" satisfied condition "success or failure"
May  6 20:50:37.118: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-configmaps-bbb54952-47cf-44b7-ad59-a3082cd339bf container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:50:37.136: INFO: Waiting for pod pod-projected-configmaps-bbb54952-47cf-44b7-ad59-a3082cd339bf to disappear
May  6 20:50:37.139: INFO: Pod pod-projected-configmaps-bbb54952-47cf-44b7-ad59-a3082cd339bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:50:37.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4384" for this suite.
May  6 20:50:43.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:50:43.254: INFO: namespace projected-4384 deletion completed in 6.110128204s

• [SLOW TEST:8.182 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:50:43.254: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-04c08744-f616-41bf-bfce-c3193b13db53
STEP: Creating a pod to test consume configMaps
May  6 20:50:43.291: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-289dd805-71cf-416f-83c5-0f16dcabf7a6" in namespace "projected-8859" to be "success or failure"
May  6 20:50:43.295: INFO: Pod "pod-projected-configmaps-289dd805-71cf-416f-83c5-0f16dcabf7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.137805ms
May  6 20:50:45.298: INFO: Pod "pod-projected-configmaps-289dd805-71cf-416f-83c5-0f16dcabf7a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007029087s
STEP: Saw pod success
May  6 20:50:45.298: INFO: Pod "pod-projected-configmaps-289dd805-71cf-416f-83c5-0f16dcabf7a6" satisfied condition "success or failure"
May  6 20:50:45.302: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-configmaps-289dd805-71cf-416f-83c5-0f16dcabf7a6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:50:45.323: INFO: Waiting for pod pod-projected-configmaps-289dd805-71cf-416f-83c5-0f16dcabf7a6 to disappear
May  6 20:50:45.327: INFO: Pod pod-projected-configmaps-289dd805-71cf-416f-83c5-0f16dcabf7a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:50:45.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8859" for this suite.
May  6 20:50:51.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:50:51.451: INFO: namespace projected-8859 deletion completed in 6.118975072s

• [SLOW TEST:8.196 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:50:51.451: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-24987927-5d41-4340-9510-6f50c4d8e18c
STEP: Creating a pod to test consume configMaps
May  6 20:50:51.488: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21e0e177-3e4a-42c4-a328-bb7b5f4c97b2" in namespace "projected-7061" to be "success or failure"
May  6 20:50:51.490: INFO: Pod "pod-projected-configmaps-21e0e177-3e4a-42c4-a328-bb7b5f4c97b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499944ms
May  6 20:50:53.494: INFO: Pod "pod-projected-configmaps-21e0e177-3e4a-42c4-a328-bb7b5f4c97b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005969614s
STEP: Saw pod success
May  6 20:50:53.494: INFO: Pod "pod-projected-configmaps-21e0e177-3e4a-42c4-a328-bb7b5f4c97b2" satisfied condition "success or failure"
May  6 20:50:53.497: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-configmaps-21e0e177-3e4a-42c4-a328-bb7b5f4c97b2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:50:53.519: INFO: Waiting for pod pod-projected-configmaps-21e0e177-3e4a-42c4-a328-bb7b5f4c97b2 to disappear
May  6 20:50:53.522: INFO: Pod pod-projected-configmaps-21e0e177-3e4a-42c4-a328-bb7b5f4c97b2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:50:53.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7061" for this suite.
May  6 20:50:59.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:50:59.646: INFO: namespace projected-7061 deletion completed in 6.119102796s

• [SLOW TEST:8.195 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:50:59.646: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:50:59.672: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Creating first CR 
May  6 20:51:00.236: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T20:51:00Z generation:1 name:name1 resourceVersion:61949 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:75e15445-2cf9-428b-801e-a7b33c31234d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May  6 20:51:10.241: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T20:51:10Z generation:1 name:name2 resourceVersion:61970 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d9db6044-6633-4fd4-a17d-35b66a840a03] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May  6 20:51:20.247: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T20:51:00Z generation:2 name:name1 resourceVersion:61990 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:75e15445-2cf9-428b-801e-a7b33c31234d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May  6 20:51:30.254: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T20:51:10Z generation:2 name:name2 resourceVersion:62010 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d9db6044-6633-4fd4-a17d-35b66a840a03] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May  6 20:51:40.263: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T20:51:00Z generation:2 name:name1 resourceVersion:62036 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:75e15445-2cf9-428b-801e-a7b33c31234d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May  6 20:51:50.272: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-06T20:51:10Z generation:2 name:name2 resourceVersion:62056 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d9db6044-6633-4fd4-a17d-35b66a840a03] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:52:00.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1301" for this suite.
May  6 20:52:06.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:52:06.909: INFO: namespace crd-watch-1301 deletion completed in 6.121234816s

• [SLOW TEST:67.263 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:52:06.909: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
May  6 20:52:06.943: INFO: Waiting up to 5m0s for pod "var-expansion-5c818b6c-ca33-46dd-9d0e-dfecf94f2997" in namespace "var-expansion-3005" to be "success or failure"
May  6 20:52:06.945: INFO: Pod "var-expansion-5c818b6c-ca33-46dd-9d0e-dfecf94f2997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464592ms
May  6 20:52:08.948: INFO: Pod "var-expansion-5c818b6c-ca33-46dd-9d0e-dfecf94f2997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005471323s
STEP: Saw pod success
May  6 20:52:08.948: INFO: Pod "var-expansion-5c818b6c-ca33-46dd-9d0e-dfecf94f2997" satisfied condition "success or failure"
May  6 20:52:08.952: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod var-expansion-5c818b6c-ca33-46dd-9d0e-dfecf94f2997 container dapi-container: <nil>
STEP: delete the pod
May  6 20:52:08.970: INFO: Waiting for pod var-expansion-5c818b6c-ca33-46dd-9d0e-dfecf94f2997 to disappear
May  6 20:52:08.973: INFO: Pod var-expansion-5c818b6c-ca33-46dd-9d0e-dfecf94f2997 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:52:08.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3005" for this suite.
May  6 20:52:14.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:52:15.109: INFO: namespace var-expansion-3005 deletion completed in 6.131866372s

• [SLOW TEST:8.200 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:52:15.109: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 20:52:15.187: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May  6 20:52:18.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-7263 create -f -'
May  6 20:52:18.380: INFO: stderr: ""
May  6 20:52:18.380: INFO: stdout: "e2e-test-crd-publish-openapi-9947-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  6 20:52:18.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-7263 delete e2e-test-crd-publish-openapi-9947-crds test-cr'
May  6 20:52:18.480: INFO: stderr: ""
May  6 20:52:18.480: INFO: stdout: "e2e-test-crd-publish-openapi-9947-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  6 20:52:18.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-7263 apply -f -'
May  6 20:52:18.627: INFO: stderr: ""
May  6 20:52:18.627: INFO: stdout: "e2e-test-crd-publish-openapi-9947-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  6 20:52:18.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-7263 delete e2e-test-crd-publish-openapi-9947-crds test-cr'
May  6 20:52:18.727: INFO: stderr: ""
May  6 20:52:18.727: INFO: stdout: "e2e-test-crd-publish-openapi-9947-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May  6 20:52:18.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-9947-crds'
May  6 20:52:18.863: INFO: stderr: ""
May  6 20:52:18.863: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9947-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:52:21.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7263" for this suite.
May  6 20:52:27.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:52:27.848: INFO: namespace crd-publish-openapi-7263 deletion completed in 6.119856179s

• [SLOW TEST:12.738 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:52:27.848: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-ac824814-3ae8-47f7-b580-5247630c74b6
STEP: Creating a pod to test consume configMaps
May  6 20:52:27.886: INFO: Waiting up to 5m0s for pod "pod-configmaps-def68651-ccb8-4bf1-934d-e09ecb1a4889" in namespace "configmap-7546" to be "success or failure"
May  6 20:52:27.889: INFO: Pod "pod-configmaps-def68651-ccb8-4bf1-934d-e09ecb1a4889": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698707ms
May  6 20:52:29.892: INFO: Pod "pod-configmaps-def68651-ccb8-4bf1-934d-e09ecb1a4889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005757211s
STEP: Saw pod success
May  6 20:52:29.892: INFO: Pod "pod-configmaps-def68651-ccb8-4bf1-934d-e09ecb1a4889" satisfied condition "success or failure"
May  6 20:52:29.895: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-def68651-ccb8-4bf1-934d-e09ecb1a4889 container configmap-volume-test: <nil>
STEP: delete the pod
May  6 20:52:29.915: INFO: Waiting for pod pod-configmaps-def68651-ccb8-4bf1-934d-e09ecb1a4889 to disappear
May  6 20:52:29.917: INFO: Pod pod-configmaps-def68651-ccb8-4bf1-934d-e09ecb1a4889 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:52:29.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7546" for this suite.
May  6 20:52:35.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:52:36.041: INFO: namespace configmap-7546 deletion completed in 6.11888289s

• [SLOW TEST:8.194 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:52:36.042: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0506 20:53:16.098881      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  6 20:53:16.098: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:53:16.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2260" for this suite.
May  6 20:53:22.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:53:22.234: INFO: namespace gc-2260 deletion completed in 6.132309472s

• [SLOW TEST:46.193 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:53:22.234: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:53:22.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc93312c-a82a-43e0-80d8-b9f9e5709ed5" in namespace "projected-5410" to be "success or failure"
May  6 20:53:22.278: INFO: Pod "downwardapi-volume-bc93312c-a82a-43e0-80d8-b9f9e5709ed5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404647ms
May  6 20:53:24.281: INFO: Pod "downwardapi-volume-bc93312c-a82a-43e0-80d8-b9f9e5709ed5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005716116s
STEP: Saw pod success
May  6 20:53:24.281: INFO: Pod "downwardapi-volume-bc93312c-a82a-43e0-80d8-b9f9e5709ed5" satisfied condition "success or failure"
May  6 20:53:24.285: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-bc93312c-a82a-43e0-80d8-b9f9e5709ed5 container client-container: <nil>
STEP: delete the pod
May  6 20:53:24.303: INFO: Waiting for pod downwardapi-volume-bc93312c-a82a-43e0-80d8-b9f9e5709ed5 to disappear
May  6 20:53:24.306: INFO: Pod downwardapi-volume-bc93312c-a82a-43e0-80d8-b9f9e5709ed5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:53:24.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5410" for this suite.
May  6 20:53:30.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:53:30.423: INFO: namespace projected-5410 deletion completed in 6.111686504s

• [SLOW TEST:8.188 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:53:30.423: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-7794a32d-99b5-459a-bf49-9bf60988eb94
STEP: Creating a pod to test consume secrets
May  6 20:53:30.459: INFO: Waiting up to 5m0s for pod "pod-secrets-9e8da5b5-8949-4bcb-aea5-1687a268e361" in namespace "secrets-4576" to be "success or failure"
May  6 20:53:30.464: INFO: Pod "pod-secrets-9e8da5b5-8949-4bcb-aea5-1687a268e361": Phase="Pending", Reason="", readiness=false. Elapsed: 4.548669ms
May  6 20:53:32.467: INFO: Pod "pod-secrets-9e8da5b5-8949-4bcb-aea5-1687a268e361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007778568s
STEP: Saw pod success
May  6 20:53:32.467: INFO: Pod "pod-secrets-9e8da5b5-8949-4bcb-aea5-1687a268e361" satisfied condition "success or failure"
May  6 20:53:32.470: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-9e8da5b5-8949-4bcb-aea5-1687a268e361 container secret-volume-test: <nil>
STEP: delete the pod
May  6 20:53:32.488: INFO: Waiting for pod pod-secrets-9e8da5b5-8949-4bcb-aea5-1687a268e361 to disappear
May  6 20:53:32.491: INFO: Pod pod-secrets-9e8da5b5-8949-4bcb-aea5-1687a268e361 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:53:32.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4576" for this suite.
May  6 20:53:38.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:53:38.606: INFO: namespace secrets-4576 deletion completed in 6.111323428s

• [SLOW TEST:8.183 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:53:38.606: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-65b2ef00-b69b-464b-9614-a16d2c48b849
STEP: Creating a pod to test consume secrets
May  6 20:53:38.643: INFO: Waiting up to 5m0s for pod "pod-secrets-a6dff0bc-cff0-490d-a18b-381528a8ceb0" in namespace "secrets-8004" to be "success or failure"
May  6 20:53:38.647: INFO: Pod "pod-secrets-a6dff0bc-cff0-490d-a18b-381528a8ceb0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.553247ms
May  6 20:53:40.650: INFO: Pod "pod-secrets-a6dff0bc-cff0-490d-a18b-381528a8ceb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006830014s
STEP: Saw pod success
May  6 20:53:40.650: INFO: Pod "pod-secrets-a6dff0bc-cff0-490d-a18b-381528a8ceb0" satisfied condition "success or failure"
May  6 20:53:40.656: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-a6dff0bc-cff0-490d-a18b-381528a8ceb0 container secret-volume-test: <nil>
STEP: delete the pod
May  6 20:53:40.679: INFO: Waiting for pod pod-secrets-a6dff0bc-cff0-490d-a18b-381528a8ceb0 to disappear
May  6 20:53:40.682: INFO: Pod pod-secrets-a6dff0bc-cff0-490d-a18b-381528a8ceb0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:53:40.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8004" for this suite.
May  6 20:53:46.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:53:46.803: INFO: namespace secrets-8004 deletion completed in 6.115984424s

• [SLOW TEST:8.197 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:53:46.803: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 20:53:46.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24aa1352-a7ed-4189-8934-257d5c97678c" in namespace "downward-api-6984" to be "success or failure"
May  6 20:53:46.842: INFO: Pod "downwardapi-volume-24aa1352-a7ed-4189-8934-257d5c97678c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.399389ms
May  6 20:53:48.846: INFO: Pod "downwardapi-volume-24aa1352-a7ed-4189-8934-257d5c97678c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005984488s
STEP: Saw pod success
May  6 20:53:48.846: INFO: Pod "downwardapi-volume-24aa1352-a7ed-4189-8934-257d5c97678c" satisfied condition "success or failure"
May  6 20:53:48.848: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-24aa1352-a7ed-4189-8934-257d5c97678c container client-container: <nil>
STEP: delete the pod
May  6 20:53:48.867: INFO: Waiting for pod downwardapi-volume-24aa1352-a7ed-4189-8934-257d5c97678c to disappear
May  6 20:53:48.870: INFO: Pod downwardapi-volume-24aa1352-a7ed-4189-8934-257d5c97678c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:53:48.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6984" for this suite.
May  6 20:53:54.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:53:54.985: INFO: namespace downward-api-6984 deletion completed in 6.110942899s

• [SLOW TEST:8.181 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:53:54.985: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
May  6 20:53:55.098: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8871" to be "success or failure"
May  6 20:53:55.101: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055344ms
May  6 20:53:57.104: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006560276s
STEP: Saw pod success
May  6 20:53:57.104: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May  6 20:53:57.108: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May  6 20:53:57.125: INFO: Waiting for pod pod-host-path-test to disappear
May  6 20:53:57.128: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:53:57.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8871" for this suite.
May  6 20:54:03.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:54:03.237: INFO: namespace hostpath-8871 deletion completed in 6.105360239s

• [SLOW TEST:8.253 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:54:03.238: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:54:05.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5111" for this suite.
May  6 20:54:49.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:54:49.418: INFO: namespace kubelet-test-5111 deletion completed in 44.126746302s

• [SLOW TEST:46.181 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:54:49.419: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-070ae46a-1047-4257-9900-61102e927c84
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-070ae46a-1047-4257-9900-61102e927c84
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 20:54:53.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3099" for this suite.
May  6 20:55:05.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 20:55:05.621: INFO: namespace configmap-3099 deletion completed in 12.119192335s

• [SLOW TEST:16.202 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 20:55:05.621: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7138
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-7138
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7138
May  6 20:55:05.659: INFO: Found 0 stateful pods, waiting for 1
May  6 20:55:15.664: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May  6 20:55:15.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:55:15.818: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:55:15.818: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:55:15.818: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:55:15.822: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  6 20:55:25.826: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:55:25.826: INFO: Waiting for statefulset status.replicas updated to 0
May  6 20:55:25.839: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:25.839: INFO: ss-0  kube-node-2-kubelet.kubernetes-cluster.mesos  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:05 +0000 UTC  }]
May  6 20:55:25.839: INFO: 
May  6 20:55:25.839: INFO: StatefulSet ss has not reached scale 3, at 1
May  6 20:55:26.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997129535s
May  6 20:55:27.848: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993758865s
May  6 20:55:28.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988052503s
May  6 20:55:29.856: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983612605s
May  6 20:55:30.860: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980162992s
May  6 20:55:31.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97607087s
May  6 20:55:32.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971844229s
May  6 20:55:33.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968331256s
May  6 20:55:34.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.612249ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7138
May  6 20:55:35.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:55:36.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 20:55:36.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 20:55:36.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 20:55:36.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:55:36.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  6 20:55:36.178: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 20:55:36.178: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 20:55:36.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:55:36.328: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  6 20:55:36.328: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 20:55:36.328: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 20:55:36.332: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:55:36.332: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 20:55:36.332: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May  6 20:55:36.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:55:36.487: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:55:36.487: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:55:36.487: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:55:36.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:55:36.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:55:36.629: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:55:36.629: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:55:36.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 20:55:36.784: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 20:55:36.784: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 20:55:36.784: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 20:55:36.784: INFO: Waiting for statefulset status.replicas updated to 0
May  6 20:55:36.787: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  6 20:55:46.794: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:55:46.794: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:55:46.794: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  6 20:55:46.804: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:46.804: INFO: ss-0  kube-node-2-kubelet.kubernetes-cluster.mesos  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:05 +0000 UTC  }]
May  6 20:55:46.804: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:46.804: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:46.804: INFO: 
May  6 20:55:46.804: INFO: StatefulSet ss has not reached scale 0, at 3
May  6 20:55:47.808: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:47.808: INFO: ss-0  kube-node-2-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:05 +0000 UTC  }]
May  6 20:55:47.808: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:47.808: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:47.808: INFO: 
May  6 20:55:47.808: INFO: StatefulSet ss has not reached scale 0, at 3
May  6 20:55:48.812: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:48.812: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:48.812: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:48.812: INFO: 
May  6 20:55:48.812: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 20:55:49.815: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:49.815: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:49.815: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:49.815: INFO: 
May  6 20:55:49.815: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 20:55:50.820: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:50.820: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:50.820: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:50.820: INFO: 
May  6 20:55:50.820: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 20:55:51.823: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:51.823: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:51.823: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:51.823: INFO: 
May  6 20:55:51.823: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 20:55:52.826: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:52.826: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:52.826: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:52.826: INFO: 
May  6 20:55:52.826: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 20:55:53.830: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:53.830: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:53.830: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:53.830: INFO: 
May  6 20:55:53.830: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 20:55:54.834: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:54.834: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:54.834: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:54.834: INFO: 
May  6 20:55:54.834: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 20:55:55.838: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
May  6 20:55:55.838: INFO: ss-1  kube-node-1-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:55.838: INFO: ss-2  kube-node-0-kubelet.kubernetes-cluster.mesos  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 20:55:25 +0000 UTC  }]
May  6 20:55:55.838: INFO: 
May  6 20:55:55.838: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7138
May  6 20:55:56.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:55:56.932: INFO: rc: 1
May  6 20:55:56.932: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May  6 20:56:06.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:56:06.996: INFO: rc: 1
May  6 20:56:06.996: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:56:16.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:56:17.063: INFO: rc: 1
May  6 20:56:17.063: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:56:27.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:56:27.130: INFO: rc: 1
May  6 20:56:27.130: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:56:37.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:56:37.200: INFO: rc: 1
May  6 20:56:37.200: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:56:47.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:56:47.264: INFO: rc: 1
May  6 20:56:47.264: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:56:57.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:56:57.334: INFO: rc: 1
May  6 20:56:57.334: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:57:07.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:57:07.397: INFO: rc: 1
May  6 20:57:07.397: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:57:17.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:57:17.463: INFO: rc: 1
May  6 20:57:17.463: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:57:27.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:57:27.528: INFO: rc: 1
May  6 20:57:27.529: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:57:37.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:57:37.597: INFO: rc: 1
May  6 20:57:37.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:57:47.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:57:47.664: INFO: rc: 1
May  6 20:57:47.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:57:57.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:57:57.734: INFO: rc: 1
May  6 20:57:57.734: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:58:07.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:58:07.802: INFO: rc: 1
May  6 20:58:07.802: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:58:17.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:58:17.869: INFO: rc: 1
May  6 20:58:17.869: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:58:27.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:58:27.937: INFO: rc: 1
May  6 20:58:27.937: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:58:37.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:58:38.002: INFO: rc: 1
May  6 20:58:38.003: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:58:48.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:58:48.067: INFO: rc: 1
May  6 20:58:48.067: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:58:58.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:58:58.133: INFO: rc: 1
May  6 20:58:58.133: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:59:08.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:59:08.197: INFO: rc: 1
May  6 20:59:08.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:59:18.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:59:18.265: INFO: rc: 1
May  6 20:59:18.265: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:59:28.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:59:28.338: INFO: rc: 1
May  6 20:59:28.338: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:59:38.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:59:38.410: INFO: rc: 1
May  6 20:59:38.410: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:59:48.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:59:48.481: INFO: rc: 1
May  6 20:59:48.481: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 20:59:58.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 20:59:58.598: INFO: rc: 1
May  6 20:59:58.598: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 21:00:08.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:00:08.667: INFO: rc: 1
May  6 21:00:08.667: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 21:00:18.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:00:18.736: INFO: rc: 1
May  6 21:00:18.736: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 21:00:28.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:00:28.802: INFO: rc: 1
May  6 21:00:28.802: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 21:00:38.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:00:38.868: INFO: rc: 1
May  6 21:00:38.868: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 21:00:48.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:00:48.936: INFO: rc: 1
May  6 21:00:48.936: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
May  6 21:00:58.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-7138 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:00:59.002: INFO: rc: 1
May  6 21:00:59.002: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
May  6 21:00:59.002: INFO: Scaling statefulset ss to 0
May  6 21:00:59.011: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 21:00:59.014: INFO: Deleting all statefulset in ns statefulset-7138
May  6 21:00:59.016: INFO: Scaling statefulset ss to 0
May  6 21:00:59.025: INFO: Waiting for statefulset status.replicas updated to 0
May  6 21:00:59.027: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:00:59.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7138" for this suite.
May  6 21:01:05.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:01:05.162: INFO: namespace statefulset-7138 deletion completed in 6.116013193s

• [SLOW TEST:359.541 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:01:05.162: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 21:01:05.189: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:01:08.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4645" for this suite.
May  6 21:01:14.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:01:15.014: INFO: namespace init-container-4645 deletion completed in 6.118940528s

• [SLOW TEST:9.852 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:01:15.014: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May  6 21:01:15.342: INFO: Pod name wrapped-volume-race-ad1941ff-e12a-4ae7-bea7-f17fbbc54665: Found 0 pods out of 5
May  6 21:01:20.347: INFO: Pod name wrapped-volume-race-ad1941ff-e12a-4ae7-bea7-f17fbbc54665: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ad1941ff-e12a-4ae7-bea7-f17fbbc54665 in namespace emptydir-wrapper-7626, will wait for the garbage collector to delete the pods
May  6 21:01:20.425: INFO: Deleting ReplicationController wrapped-volume-race-ad1941ff-e12a-4ae7-bea7-f17fbbc54665 took: 9.546883ms
May  6 21:01:20.825: INFO: Terminating ReplicationController wrapped-volume-race-ad1941ff-e12a-4ae7-bea7-f17fbbc54665 pods took: 400.203153ms
STEP: Creating RC which spawns configmap-volume pods
May  6 21:01:58.842: INFO: Pod name wrapped-volume-race-88852317-b7f5-4984-8ce0-828b2b2fedba: Found 0 pods out of 5
May  6 21:02:03.849: INFO: Pod name wrapped-volume-race-88852317-b7f5-4984-8ce0-828b2b2fedba: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-88852317-b7f5-4984-8ce0-828b2b2fedba in namespace emptydir-wrapper-7626, will wait for the garbage collector to delete the pods
May  6 21:02:03.930: INFO: Deleting ReplicationController wrapped-volume-race-88852317-b7f5-4984-8ce0-828b2b2fedba took: 9.738182ms
May  6 21:02:04.330: INFO: Terminating ReplicationController wrapped-volume-race-88852317-b7f5-4984-8ce0-828b2b2fedba pods took: 400.162552ms
STEP: Creating RC which spawns configmap-volume pods
May  6 21:02:37.246: INFO: Pod name wrapped-volume-race-44c9ccd7-5ecc-4304-9f5c-9f89c2339808: Found 0 pods out of 5
May  6 21:02:42.252: INFO: Pod name wrapped-volume-race-44c9ccd7-5ecc-4304-9f5c-9f89c2339808: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-44c9ccd7-5ecc-4304-9f5c-9f89c2339808 in namespace emptydir-wrapper-7626, will wait for the garbage collector to delete the pods
May  6 21:02:42.333: INFO: Deleting ReplicationController wrapped-volume-race-44c9ccd7-5ecc-4304-9f5c-9f89c2339808 took: 8.951943ms
May  6 21:02:42.733: INFO: Terminating ReplicationController wrapped-volume-race-44c9ccd7-5ecc-4304-9f5c-9f89c2339808 pods took: 400.160087ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:03:19.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7626" for this suite.
May  6 21:03:27.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:03:27.511: INFO: namespace emptydir-wrapper-7626 deletion completed in 8.116220379s

• [SLOW TEST:132.498 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:03:27.512: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 21:03:27.550: INFO: Waiting up to 5m0s for pod "downwardapi-volume-005b31e0-30f6-4ff2-ba0d-2b8c8a3ba44e" in namespace "projected-8485" to be "success or failure"
May  6 21:03:27.553: INFO: Pod "downwardapi-volume-005b31e0-30f6-4ff2-ba0d-2b8c8a3ba44e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.143151ms
May  6 21:03:29.556: INFO: Pod "downwardapi-volume-005b31e0-30f6-4ff2-ba0d-2b8c8a3ba44e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006854909s
STEP: Saw pod success
May  6 21:03:29.556: INFO: Pod "downwardapi-volume-005b31e0-30f6-4ff2-ba0d-2b8c8a3ba44e" satisfied condition "success or failure"
May  6 21:03:29.560: INFO: Trying to get logs from node kube-node-1-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-005b31e0-30f6-4ff2-ba0d-2b8c8a3ba44e container client-container: <nil>
STEP: delete the pod
May  6 21:03:29.588: INFO: Waiting for pod downwardapi-volume-005b31e0-30f6-4ff2-ba0d-2b8c8a3ba44e to disappear
May  6 21:03:29.590: INFO: Pod downwardapi-volume-005b31e0-30f6-4ff2-ba0d-2b8c8a3ba44e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:03:29.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8485" for this suite.
May  6 21:03:35.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:03:35.720: INFO: namespace projected-8485 deletion completed in 6.12444746s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:03:35.720: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3335.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3335.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3335.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3335.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3335.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3335.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 21:03:37.794: INFO: DNS probes using dns-3335/dns-test-01b6db90-4c2f-413c-a340-2b323fafa522 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:03:37.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3335" for this suite.
May  6 21:03:43.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:03:43.931: INFO: namespace dns-3335 deletion completed in 6.119930547s

• [SLOW TEST:8.212 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:03:43.932: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
May  6 21:03:43.967: INFO: Waiting up to 5m0s for pod "var-expansion-4c8e721b-2b66-459e-b9e9-a8d0acb30867" in namespace "var-expansion-2683" to be "success or failure"
May  6 21:03:43.972: INFO: Pod "var-expansion-4c8e721b-2b66-459e-b9e9-a8d0acb30867": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413815ms
May  6 21:03:45.975: INFO: Pod "var-expansion-4c8e721b-2b66-459e-b9e9-a8d0acb30867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007227856s
STEP: Saw pod success
May  6 21:03:45.975: INFO: Pod "var-expansion-4c8e721b-2b66-459e-b9e9-a8d0acb30867" satisfied condition "success or failure"
May  6 21:03:45.978: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod var-expansion-4c8e721b-2b66-459e-b9e9-a8d0acb30867 container dapi-container: <nil>
STEP: delete the pod
May  6 21:03:46.004: INFO: Waiting for pod var-expansion-4c8e721b-2b66-459e-b9e9-a8d0acb30867 to disappear
May  6 21:03:46.007: INFO: Pod var-expansion-4c8e721b-2b66-459e-b9e9-a8d0acb30867 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:03:46.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2683" for this suite.
May  6 21:03:52.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:03:52.136: INFO: namespace var-expansion-2683 deletion completed in 6.123584257s

• [SLOW TEST:8.204 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:03:52.136: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
May  6 21:03:58.237: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0506 21:03:58.237210      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:03:58.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9959" for this suite.
May  6 21:04:04.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:04:04.361: INFO: namespace gc-9959 deletion completed in 6.120167702s

• [SLOW TEST:12.225 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:04:04.361: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
May  6 21:04:04.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-8233'
May  6 21:04:04.689: INFO: stderr: ""
May  6 21:04:04.689: INFO: stdout: "pod/pause created\n"
May  6 21:04:04.689: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  6 21:04:04.689: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8233" to be "running and ready"
May  6 21:04:04.693: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53923ms
May  6 21:04:06.696: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007023685s
May  6 21:04:06.696: INFO: Pod "pause" satisfied condition "running and ready"
May  6 21:04:06.696: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
May  6 21:04:06.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 label pods pause testing-label=testing-label-value --namespace=kubectl-8233'
May  6 21:04:06.775: INFO: stderr: ""
May  6 21:04:06.775: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May  6 21:04:06.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pod pause -L testing-label --namespace=kubectl-8233'
May  6 21:04:06.838: INFO: stderr: ""
May  6 21:04:06.838: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May  6 21:04:06.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 label pods pause testing-label- --namespace=kubectl-8233'
May  6 21:04:06.908: INFO: stderr: ""
May  6 21:04:06.908: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May  6 21:04:06.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pod pause -L testing-label --namespace=kubectl-8233'
May  6 21:04:06.970: INFO: stderr: ""
May  6 21:04:06.970: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
May  6 21:04:06.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-8233'
May  6 21:04:07.046: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 21:04:07.046: INFO: stdout: "pod \"pause\" force deleted\n"
May  6 21:04:07.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get rc,svc -l name=pause --no-headers --namespace=kubectl-8233'
May  6 21:04:07.118: INFO: stderr: "No resources found in kubectl-8233 namespace.\n"
May  6 21:04:07.118: INFO: stdout: ""
May  6 21:04:07.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -l name=pause --namespace=kubectl-8233 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 21:04:07.183: INFO: stderr: ""
May  6 21:04:07.183: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:04:07.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8233" for this suite.
May  6 21:04:13.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:04:13.304: INFO: namespace kubectl-8233 deletion completed in 6.116541926s

• [SLOW TEST:8.943 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:04:13.304: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:04:13.333: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May  6 21:04:15.362: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:04:15.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7591" for this suite.
May  6 21:04:21.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:04:21.497: INFO: namespace replication-controller-7591 deletion completed in 6.126350116s

• [SLOW TEST:8.193 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:04:21.497: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0506 21:04:31.558947      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  6 21:04:31.558: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:04:31.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-756" for this suite.
May  6 21:04:37.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:04:37.685: INFO: namespace gc-756 deletion completed in 6.12305583s

• [SLOW TEST:16.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:04:37.686: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May  6 21:04:37.725: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7350 /api/v1/namespaces/watch-7350/configmaps/e2e-watch-test-watch-closed 5944c986-693a-45d8-a9e7-c204b231ea17 65890 0 2020-05-06 21:04:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  6 21:04:37.726: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7350 /api/v1/namespaces/watch-7350/configmaps/e2e-watch-test-watch-closed 5944c986-693a-45d8-a9e7-c204b231ea17 65891 0 2020-05-06 21:04:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May  6 21:04:37.741: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7350 /api/v1/namespaces/watch-7350/configmaps/e2e-watch-test-watch-closed 5944c986-693a-45d8-a9e7-c204b231ea17 65892 0 2020-05-06 21:04:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 21:04:37.741: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7350 /api/v1/namespaces/watch-7350/configmaps/e2e-watch-test-watch-closed 5944c986-693a-45d8-a9e7-c204b231ea17 65893 0 2020-05-06 21:04:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:04:37.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7350" for this suite.
May  6 21:04:43.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:04:43.857: INFO: namespace watch-7350 deletion completed in 6.110876805s

• [SLOW TEST:6.171 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:04:43.857: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:04:43.883: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:04:45.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3534" for this suite.
May  6 21:05:29.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:05:30.114: INFO: namespace pods-3534 deletion completed in 44.131721806s

• [SLOW TEST:46.257 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:05:30.114: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:05:30.159: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May  6 21:05:30.171: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:30.171: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:30.171: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:30.174: INFO: Number of nodes with available pods: 0
May  6 21:05:30.174: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:05:31.179: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:31.179: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:31.179: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:31.182: INFO: Number of nodes with available pods: 0
May  6 21:05:31.182: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:05:32.178: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:32.178: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:32.178: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:32.182: INFO: Number of nodes with available pods: 3
May  6 21:05:32.182: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May  6 21:05:32.207: INFO: Wrong image for pod: daemon-set-5h5ls. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:32.207: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:32.207: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:32.211: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:32.211: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:32.211: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:33.215: INFO: Wrong image for pod: daemon-set-5h5ls. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:33.215: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:33.215: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:33.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:33.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:33.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:34.215: INFO: Wrong image for pod: daemon-set-5h5ls. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:34.215: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:34.215: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:34.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:34.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:34.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:35.215: INFO: Wrong image for pod: daemon-set-5h5ls. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:35.215: INFO: Pod daemon-set-5h5ls is not available
May  6 21:05:35.215: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:35.215: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:35.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:35.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:35.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:36.215: INFO: Wrong image for pod: daemon-set-5h5ls. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:36.215: INFO: Pod daemon-set-5h5ls is not available
May  6 21:05:36.215: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:36.215: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:36.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:36.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:36.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:37.214: INFO: Wrong image for pod: daemon-set-5h5ls. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:37.214: INFO: Pod daemon-set-5h5ls is not available
May  6 21:05:37.214: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:37.214: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:37.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:37.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:37.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:38.249: INFO: Wrong image for pod: daemon-set-5h5ls. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:38.249: INFO: Pod daemon-set-5h5ls is not available
May  6 21:05:38.249: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:38.249: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:38.253: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:38.253: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:38.253: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:39.214: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:39.214: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:39.214: INFO: Pod daemon-set-wvc6b is not available
May  6 21:05:39.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:39.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:39.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:40.216: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:40.216: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:40.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:40.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:40.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:41.215: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:41.215: INFO: Wrong image for pod: daemon-set-h2thr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:41.215: INFO: Pod daemon-set-h2thr is not available
May  6 21:05:41.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:41.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:41.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:42.283: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:42.283: INFO: Pod daemon-set-lhz9k is not available
May  6 21:05:42.287: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:42.287: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:42.287: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:43.214: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:43.217: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:43.217: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:43.217: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:44.214: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:44.214: INFO: Pod daemon-set-d6vvt is not available
May  6 21:05:44.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:44.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:44.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:45.215: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:45.215: INFO: Pod daemon-set-d6vvt is not available
May  6 21:05:45.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:45.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:45.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:46.214: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:46.214: INFO: Pod daemon-set-d6vvt is not available
May  6 21:05:46.217: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:46.217: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:46.217: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:47.214: INFO: Wrong image for pod: daemon-set-d6vvt. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May  6 21:05:47.214: INFO: Pod daemon-set-d6vvt is not available
May  6 21:05:47.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:47.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:47.219: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:48.214: INFO: Pod daemon-set-9r6n9 is not available
May  6 21:05:48.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:48.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:48.218: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May  6 21:05:48.225: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:48.225: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:48.225: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:48.228: INFO: Number of nodes with available pods: 2
May  6 21:05:48.228: INFO: Node kube-node-2-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:05:49.231: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:49.231: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:49.231: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:05:49.235: INFO: Number of nodes with available pods: 3
May  6 21:05:49.235: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3545, will wait for the garbage collector to delete the pods
May  6 21:05:49.310: INFO: Deleting DaemonSet.extensions daemon-set took: 8.0444ms
May  6 21:05:49.710: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.157449ms
May  6 21:05:58.714: INFO: Number of nodes with available pods: 0
May  6 21:05:58.714: INFO: Number of running nodes: 0, number of available pods: 0
May  6 21:05:58.716: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3545/daemonsets","resourceVersion":"66240"},"items":null}

May  6 21:05:58.719: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3545/pods","resourceVersion":"66240"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:05:58.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3545" for this suite.
May  6 21:06:04.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:06:04.845: INFO: namespace daemonsets-3545 deletion completed in 6.111535952s

• [SLOW TEST:34.731 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:06:04.845: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-w56tv in namespace proxy-2187
I0506 21:06:04.886334      22 runners.go:184] Created replication controller with name: proxy-service-w56tv, namespace: proxy-2187, replica count: 1
I0506 21:06:05.936629      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0506 21:06:06.936780      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 21:06:07.936896      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 21:06:08.937045      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 21:06:09.937176      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 21:06:10.937310      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 21:06:11.937442      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 21:06:12.937612      22 runners.go:184] proxy-service-w56tv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 21:06:12.941: INFO: setup took 8.068646797s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May  6 21:06:12.947: INFO: (0) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.678378ms)
May  6 21:06:12.947: INFO: (0) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.742574ms)
May  6 21:06:12.948: INFO: (0) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 6.507636ms)
May  6 21:06:12.948: INFO: (0) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.56363ms)
May  6 21:06:12.948: INFO: (0) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 6.491083ms)
May  6 21:06:12.949: INFO: (0) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 7.757029ms)
May  6 21:06:12.949: INFO: (0) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 8.012704ms)
May  6 21:06:12.949: INFO: (0) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 7.90644ms)
May  6 21:06:12.949: INFO: (0) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 7.943948ms)
May  6 21:06:12.951: INFO: (0) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 9.559775ms)
May  6 21:06:12.952: INFO: (0) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 10.568227ms)
May  6 21:06:12.953: INFO: (0) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 11.94146ms)
May  6 21:06:12.953: INFO: (0) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 11.881654ms)
May  6 21:06:12.953: INFO: (0) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 12.315088ms)
May  6 21:06:12.954: INFO: (0) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 12.624354ms)
May  6 21:06:12.954: INFO: (0) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 12.608821ms)
May  6 21:06:12.958: INFO: (1) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 4.308091ms)
May  6 21:06:12.958: INFO: (1) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 4.424093ms)
May  6 21:06:12.958: INFO: (1) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 4.632294ms)
May  6 21:06:12.959: INFO: (1) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.675759ms)
May  6 21:06:12.959: INFO: (1) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 4.907943ms)
May  6 21:06:12.959: INFO: (1) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.19186ms)
May  6 21:06:12.959: INFO: (1) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.378947ms)
May  6 21:06:12.959: INFO: (1) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 5.67257ms)
May  6 21:06:12.959: INFO: (1) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 5.541194ms)
May  6 21:06:12.960: INFO: (1) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.803582ms)
May  6 21:06:12.960: INFO: (1) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.748459ms)
May  6 21:06:12.960: INFO: (1) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 6.259901ms)
May  6 21:06:12.960: INFO: (1) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 6.267756ms)
May  6 21:06:12.961: INFO: (1) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 6.942628ms)
May  6 21:06:12.961: INFO: (1) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 7.416012ms)
May  6 21:06:12.962: INFO: (1) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 8.02986ms)
May  6 21:06:12.965: INFO: (2) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 3.224888ms)
May  6 21:06:12.966: INFO: (2) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 3.571402ms)
May  6 21:06:12.967: INFO: (2) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.410874ms)
May  6 21:06:12.967: INFO: (2) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.227814ms)
May  6 21:06:12.967: INFO: (2) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.381885ms)
May  6 21:06:12.967: INFO: (2) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 5.341928ms)
May  6 21:06:12.967: INFO: (2) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.348578ms)
May  6 21:06:12.968: INFO: (2) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 5.505884ms)
May  6 21:06:12.968: INFO: (2) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 5.637062ms)
May  6 21:06:12.968: INFO: (2) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.537678ms)
May  6 21:06:12.968: INFO: (2) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 5.679219ms)
May  6 21:06:12.968: INFO: (2) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 6.004327ms)
May  6 21:06:12.969: INFO: (2) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 6.613848ms)
May  6 21:06:12.969: INFO: (2) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 7.036234ms)
May  6 21:06:12.970: INFO: (2) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.488879ms)
May  6 21:06:12.970: INFO: (2) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 7.748028ms)
May  6 21:06:12.974: INFO: (3) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 3.413775ms)
May  6 21:06:12.974: INFO: (3) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.549301ms)
May  6 21:06:12.974: INFO: (3) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 3.98888ms)
May  6 21:06:12.974: INFO: (3) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 3.74194ms)
May  6 21:06:12.975: INFO: (3) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 4.958117ms)
May  6 21:06:12.976: INFO: (3) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.901854ms)
May  6 21:06:12.976: INFO: (3) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 5.291427ms)
May  6 21:06:12.976: INFO: (3) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 5.966209ms)
May  6 21:06:12.976: INFO: (3) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 5.590713ms)
May  6 21:06:12.976: INFO: (3) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.856357ms)
May  6 21:06:12.976: INFO: (3) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.649491ms)
May  6 21:06:12.977: INFO: (3) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 5.712515ms)
May  6 21:06:12.977: INFO: (3) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 6.291673ms)
May  6 21:06:12.977: INFO: (3) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 6.672027ms)
May  6 21:06:12.978: INFO: (3) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 6.920311ms)
May  6 21:06:12.978: INFO: (3) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 8.132844ms)
May  6 21:06:12.982: INFO: (4) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.561211ms)
May  6 21:06:12.982: INFO: (4) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 3.871608ms)
May  6 21:06:12.982: INFO: (4) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 3.984439ms)
May  6 21:06:12.983: INFO: (4) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.541756ms)
May  6 21:06:12.983: INFO: (4) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.715985ms)
May  6 21:06:12.984: INFO: (4) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 5.299852ms)
May  6 21:06:12.984: INFO: (4) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.366196ms)
May  6 21:06:12.984: INFO: (4) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 5.509976ms)
May  6 21:06:12.985: INFO: (4) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.132866ms)
May  6 21:06:12.985: INFO: (4) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 6.032434ms)
May  6 21:06:12.985: INFO: (4) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 6.334374ms)
May  6 21:06:12.985: INFO: (4) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 6.450163ms)
May  6 21:06:12.985: INFO: (4) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 6.699764ms)
May  6 21:06:12.986: INFO: (4) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 7.019224ms)
May  6 21:06:12.987: INFO: (4) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 8.050361ms)
May  6 21:06:12.987: INFO: (4) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 8.248241ms)
May  6 21:06:12.990: INFO: (5) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 2.921713ms)
May  6 21:06:12.991: INFO: (5) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 3.197287ms)
May  6 21:06:12.991: INFO: (5) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.738014ms)
May  6 21:06:12.991: INFO: (5) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 3.741115ms)
May  6 21:06:12.991: INFO: (5) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.7685ms)
May  6 21:06:12.992: INFO: (5) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 3.629501ms)
May  6 21:06:12.992: INFO: (5) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.453145ms)
May  6 21:06:12.992: INFO: (5) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 4.644128ms)
May  6 21:06:12.993: INFO: (5) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.214456ms)
May  6 21:06:12.993: INFO: (5) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 5.184741ms)
May  6 21:06:12.993: INFO: (5) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 6.224386ms)
May  6 21:06:12.994: INFO: (5) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 6.104041ms)
May  6 21:06:12.994: INFO: (5) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 6.954902ms)
May  6 21:06:12.995: INFO: (5) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 7.519419ms)
May  6 21:06:12.995: INFO: (5) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 7.308897ms)
May  6 21:06:12.995: INFO: (5) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 8.176755ms)
May  6 21:06:13.000: INFO: (6) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.110262ms)
May  6 21:06:13.000: INFO: (6) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 4.489979ms)
May  6 21:06:13.000: INFO: (6) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.732043ms)
May  6 21:06:13.000: INFO: (6) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.607597ms)
May  6 21:06:13.001: INFO: (6) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 5.193289ms)
May  6 21:06:13.001: INFO: (6) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 4.951304ms)
May  6 21:06:13.001: INFO: (6) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.343237ms)
May  6 21:06:13.002: INFO: (6) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 6.339341ms)
May  6 21:06:13.002: INFO: (6) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 6.533958ms)
May  6 21:06:13.003: INFO: (6) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.619114ms)
May  6 21:06:13.003: INFO: (6) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.572741ms)
May  6 21:06:13.003: INFO: (6) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 7.433037ms)
May  6 21:06:13.004: INFO: (6) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 7.770277ms)
May  6 21:06:13.004: INFO: (6) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 7.8517ms)
May  6 21:06:13.004: INFO: (6) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 8.740353ms)
May  6 21:06:13.005: INFO: (6) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 9.273756ms)
May  6 21:06:13.009: INFO: (7) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 3.865213ms)
May  6 21:06:13.009: INFO: (7) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 3.751702ms)
May  6 21:06:13.010: INFO: (7) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.64031ms)
May  6 21:06:13.010: INFO: (7) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 4.640095ms)
May  6 21:06:13.010: INFO: (7) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.745119ms)
May  6 21:06:13.010: INFO: (7) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 5.507113ms)
May  6 21:06:13.011: INFO: (7) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 5.770353ms)
May  6 21:06:13.011: INFO: (7) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.86037ms)
May  6 21:06:13.011: INFO: (7) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.819834ms)
May  6 21:06:13.011: INFO: (7) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 6.02166ms)
May  6 21:06:13.011: INFO: (7) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 6.042274ms)
May  6 21:06:13.011: INFO: (7) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 6.293274ms)
May  6 21:06:13.012: INFO: (7) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 6.712101ms)
May  6 21:06:13.012: INFO: (7) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 6.784671ms)
May  6 21:06:13.013: INFO: (7) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 7.921111ms)
May  6 21:06:13.013: INFO: (7) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.929969ms)
May  6 21:06:13.016: INFO: (8) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 3.107403ms)
May  6 21:06:13.017: INFO: (8) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.442254ms)
May  6 21:06:13.017: INFO: (8) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 3.295632ms)
May  6 21:06:13.017: INFO: (8) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 3.76721ms)
May  6 21:06:13.017: INFO: (8) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 3.551235ms)
May  6 21:06:13.017: INFO: (8) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 3.345783ms)
May  6 21:06:13.018: INFO: (8) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.210572ms)
May  6 21:06:13.018: INFO: (8) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.051788ms)
May  6 21:06:13.018: INFO: (8) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 3.927934ms)
May  6 21:06:13.018: INFO: (8) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 4.203465ms)
May  6 21:06:13.050: INFO: (8) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 35.967633ms)
May  6 21:06:13.050: INFO: (8) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 36.6919ms)
May  6 21:06:13.050: INFO: (8) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 36.068836ms)
May  6 21:06:13.050: INFO: (8) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 36.378845ms)
May  6 21:06:13.051: INFO: (8) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 37.026159ms)
May  6 21:06:13.051: INFO: (8) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 37.253807ms)
May  6 21:06:13.055: INFO: (9) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 3.835324ms)
May  6 21:06:13.055: INFO: (9) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 3.918603ms)
May  6 21:06:13.055: INFO: (9) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 4.620822ms)
May  6 21:06:13.056: INFO: (9) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.49811ms)
May  6 21:06:13.056: INFO: (9) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.595801ms)
May  6 21:06:13.057: INFO: (9) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 5.548996ms)
May  6 21:06:13.057: INFO: (9) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 5.532349ms)
May  6 21:06:13.057: INFO: (9) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 5.628267ms)
May  6 21:06:13.057: INFO: (9) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.670886ms)
May  6 21:06:13.057: INFO: (9) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.939596ms)
May  6 21:06:13.058: INFO: (9) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 6.523679ms)
May  6 21:06:13.058: INFO: (9) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 6.917984ms)
May  6 21:06:13.058: INFO: (9) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 7.224451ms)
May  6 21:06:13.059: INFO: (9) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 7.643949ms)
May  6 21:06:13.059: INFO: (9) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.84432ms)
May  6 21:06:13.060: INFO: (9) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 8.687748ms)
May  6 21:06:13.063: INFO: (10) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 3.217177ms)
May  6 21:06:13.064: INFO: (10) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.234747ms)
May  6 21:06:13.064: INFO: (10) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 4.088282ms)
May  6 21:06:13.064: INFO: (10) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.214099ms)
May  6 21:06:13.065: INFO: (10) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 5.509764ms)
May  6 21:06:13.065: INFO: (10) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 5.550475ms)
May  6 21:06:13.065: INFO: (10) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.627017ms)
May  6 21:06:13.066: INFO: (10) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 5.51885ms)
May  6 21:06:13.066: INFO: (10) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.509788ms)
May  6 21:06:13.066: INFO: (10) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.214691ms)
May  6 21:06:13.066: INFO: (10) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 6.188082ms)
May  6 21:06:13.067: INFO: (10) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 6.721707ms)
May  6 21:06:13.067: INFO: (10) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 7.027172ms)
May  6 21:06:13.067: INFO: (10) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.330236ms)
May  6 21:06:13.068: INFO: (10) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 8.464248ms)
May  6 21:06:13.068: INFO: (10) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 8.441037ms)
May  6 21:06:13.072: INFO: (11) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 3.24756ms)
May  6 21:06:13.073: INFO: (11) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 4.378646ms)
May  6 21:06:13.074: INFO: (11) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.354878ms)
May  6 21:06:13.074: INFO: (11) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 5.466646ms)
May  6 21:06:13.075: INFO: (11) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 6.622702ms)
May  6 21:06:13.075: INFO: (11) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 6.612065ms)
May  6 21:06:13.075: INFO: (11) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.53839ms)
May  6 21:06:13.075: INFO: (11) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 6.765403ms)
May  6 21:06:13.075: INFO: (11) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 6.591349ms)
May  6 21:06:13.076: INFO: (11) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.775418ms)
May  6 21:06:13.076: INFO: (11) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 7.194013ms)
May  6 21:06:13.076: INFO: (11) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 7.81562ms)
May  6 21:06:13.077: INFO: (11) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 7.904898ms)
May  6 21:06:13.077: INFO: (11) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.880521ms)
May  6 21:06:13.077: INFO: (11) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 8.336431ms)
May  6 21:06:13.077: INFO: (11) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 8.54206ms)
May  6 21:06:13.081: INFO: (12) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.530942ms)
May  6 21:06:13.081: INFO: (12) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 4.18296ms)
May  6 21:06:13.081: INFO: (12) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 4.314268ms)
May  6 21:06:13.081: INFO: (12) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 4.31304ms)
May  6 21:06:13.082: INFO: (12) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.964835ms)
May  6 21:06:13.082: INFO: (12) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.116826ms)
May  6 21:06:13.082: INFO: (12) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.102283ms)
May  6 21:06:13.083: INFO: (12) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.476181ms)
May  6 21:06:13.083: INFO: (12) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.644629ms)
May  6 21:06:13.083: INFO: (12) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.724173ms)
May  6 21:06:13.084: INFO: (12) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 6.465589ms)
May  6 21:06:13.084: INFO: (12) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 7.197417ms)
May  6 21:06:13.084: INFO: (12) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 7.212444ms)
May  6 21:06:13.084: INFO: (12) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 7.251845ms)
May  6 21:06:13.085: INFO: (12) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 7.772922ms)
May  6 21:06:13.086: INFO: (12) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 8.396047ms)
May  6 21:06:13.089: INFO: (13) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 3.696174ms)
May  6 21:06:13.090: INFO: (13) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 4.489357ms)
May  6 21:06:13.090: INFO: (13) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 4.456807ms)
May  6 21:06:13.090: INFO: (13) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 4.651171ms)
May  6 21:06:13.091: INFO: (13) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.907524ms)
May  6 21:06:13.092: INFO: (13) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 6.081172ms)
May  6 21:06:13.092: INFO: (13) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.148436ms)
May  6 21:06:13.092: INFO: (13) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 6.085444ms)
May  6 21:06:13.092: INFO: (13) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 6.092503ms)
May  6 21:06:13.092: INFO: (13) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 6.19546ms)
May  6 21:06:13.092: INFO: (13) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 6.796063ms)
May  6 21:06:13.093: INFO: (13) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 6.952556ms)
May  6 21:06:13.093: INFO: (13) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.450655ms)
May  6 21:06:13.094: INFO: (13) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 8.077461ms)
May  6 21:06:13.094: INFO: (13) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 8.155382ms)
May  6 21:06:13.095: INFO: (13) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 8.988772ms)
May  6 21:06:13.099: INFO: (14) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 3.976602ms)
May  6 21:06:13.100: INFO: (14) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 5.113949ms)
May  6 21:06:13.101: INFO: (14) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.229449ms)
May  6 21:06:13.101: INFO: (14) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.547303ms)
May  6 21:06:13.101: INFO: (14) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.779474ms)
May  6 21:06:13.102: INFO: (14) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 6.52799ms)
May  6 21:06:13.102: INFO: (14) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.716301ms)
May  6 21:06:13.102: INFO: (14) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 7.496205ms)
May  6 21:06:13.102: INFO: (14) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 7.308097ms)
May  6 21:06:13.102: INFO: (14) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 7.207538ms)
May  6 21:06:13.103: INFO: (14) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 7.537948ms)
May  6 21:06:13.103: INFO: (14) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 8.061518ms)
May  6 21:06:13.104: INFO: (14) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 8.480222ms)
May  6 21:06:13.104: INFO: (14) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 8.534773ms)
May  6 21:06:13.104: INFO: (14) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 8.54883ms)
May  6 21:06:13.105: INFO: (14) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 9.878468ms)
May  6 21:06:13.108: INFO: (15) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 2.93139ms)
May  6 21:06:13.109: INFO: (15) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 3.769186ms)
May  6 21:06:13.109: INFO: (15) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.786509ms)
May  6 21:06:13.109: INFO: (15) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 3.619709ms)
May  6 21:06:13.110: INFO: (15) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.550647ms)
May  6 21:06:13.110: INFO: (15) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 4.324558ms)
May  6 21:06:13.110: INFO: (15) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.90995ms)
May  6 21:06:13.110: INFO: (15) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.811418ms)
May  6 21:06:13.110: INFO: (15) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.097803ms)
May  6 21:06:13.111: INFO: (15) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 5.302639ms)
May  6 21:06:13.111: INFO: (15) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 5.957273ms)
May  6 21:06:13.112: INFO: (15) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 6.314586ms)
May  6 21:06:13.112: INFO: (15) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 6.331418ms)
May  6 21:06:13.112: INFO: (15) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 7.235644ms)
May  6 21:06:13.113: INFO: (15) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 6.928554ms)
May  6 21:06:13.114: INFO: (15) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.839283ms)
May  6 21:06:13.117: INFO: (16) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 3.346466ms)
May  6 21:06:13.117: INFO: (16) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 3.849484ms)
May  6 21:06:13.118: INFO: (16) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 3.91721ms)
May  6 21:06:13.118: INFO: (16) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 4.076167ms)
May  6 21:06:13.118: INFO: (16) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.776025ms)
May  6 21:06:13.118: INFO: (16) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 4.724723ms)
May  6 21:06:13.119: INFO: (16) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 5.134239ms)
May  6 21:06:13.119: INFO: (16) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 5.227139ms)
May  6 21:06:13.119: INFO: (16) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 5.410808ms)
May  6 21:06:13.119: INFO: (16) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 5.576028ms)
May  6 21:06:13.119: INFO: (16) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.651978ms)
May  6 21:06:13.149: INFO: (16) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 34.821145ms)
May  6 21:06:13.149: INFO: (16) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 35.384565ms)
May  6 21:06:13.149: INFO: (16) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 35.392251ms)
May  6 21:06:13.150: INFO: (16) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 36.222067ms)
May  6 21:06:13.150: INFO: (16) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 36.471305ms)
May  6 21:06:13.154: INFO: (17) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 3.261825ms)
May  6 21:06:13.154: INFO: (17) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 3.787576ms)
May  6 21:06:13.155: INFO: (17) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.338398ms)
May  6 21:06:13.155: INFO: (17) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.417548ms)
May  6 21:06:13.155: INFO: (17) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 4.575701ms)
May  6 21:06:13.156: INFO: (17) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.158118ms)
May  6 21:06:13.156: INFO: (17) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.163164ms)
May  6 21:06:13.156: INFO: (17) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.433123ms)
May  6 21:06:13.156: INFO: (17) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.492617ms)
May  6 21:06:13.156: INFO: (17) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 5.361994ms)
May  6 21:06:13.157: INFO: (17) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 6.050375ms)
May  6 21:06:13.157: INFO: (17) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 6.243693ms)
May  6 21:06:13.157: INFO: (17) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 6.980743ms)
May  6 21:06:13.158: INFO: (17) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 7.39983ms)
May  6 21:06:13.158: INFO: (17) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 7.525344ms)
May  6 21:06:13.158: INFO: (17) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 7.997937ms)
May  6 21:06:13.162: INFO: (18) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 3.572517ms)
May  6 21:06:13.163: INFO: (18) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 4.6365ms)
May  6 21:06:13.163: INFO: (18) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 4.7358ms)
May  6 21:06:13.163: INFO: (18) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 4.673025ms)
May  6 21:06:13.163: INFO: (18) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 4.729516ms)
May  6 21:06:13.164: INFO: (18) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.254201ms)
May  6 21:06:13.164: INFO: (18) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 5.289039ms)
May  6 21:06:13.164: INFO: (18) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.524998ms)
May  6 21:06:13.164: INFO: (18) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.499369ms)
May  6 21:06:13.164: INFO: (18) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.482206ms)
May  6 21:06:13.165: INFO: (18) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 6.566253ms)
May  6 21:06:13.165: INFO: (18) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 6.542167ms)
May  6 21:06:13.166: INFO: (18) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 7.067382ms)
May  6 21:06:13.166: INFO: (18) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 7.182174ms)
May  6 21:06:13.166: INFO: (18) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 7.261639ms)
May  6 21:06:13.167: INFO: (18) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 8.45912ms)
May  6 21:06:13.171: INFO: (19) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:460/proxy/: tls baz (200; 3.617503ms)
May  6 21:06:13.171: INFO: (19) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:443/proxy/tlsrewritem... (200; 3.987021ms)
May  6 21:06:13.172: INFO: (19) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv/proxy/rewriteme">test</a> (200; 4.514074ms)
May  6 21:06:13.172: INFO: (19) /api/v1/namespaces/proxy-2187/pods/https:proxy-service-w56tv-nhpwv:462/proxy/: tls qux (200; 5.079827ms)
May  6 21:06:13.173: INFO: (19) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 5.243459ms)
May  6 21:06:13.173: INFO: (19) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.386832ms)
May  6 21:06:13.173: INFO: (19) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:162/proxy/: bar (200; 5.990908ms)
May  6 21:06:13.173: INFO: (19) /api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/http:proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">... (200; 6.090139ms)
May  6 21:06:13.173: INFO: (19) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:1080/proxy/rewriteme">test<... (200; 5.922854ms)
May  6 21:06:13.174: INFO: (19) /api/v1/namespaces/proxy-2187/pods/proxy-service-w56tv-nhpwv:160/proxy/: foo (200; 6.34008ms)
May  6 21:06:13.174: INFO: (19) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname2/proxy/: bar (200; 6.541994ms)
May  6 21:06:13.174: INFO: (19) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname2/proxy/: tls qux (200; 7.13643ms)
May  6 21:06:13.174: INFO: (19) /api/v1/namespaces/proxy-2187/services/proxy-service-w56tv:portname1/proxy/: foo (200; 7.205279ms)
May  6 21:06:13.175: INFO: (19) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname1/proxy/: foo (200; 7.322772ms)
May  6 21:06:13.175: INFO: (19) /api/v1/namespaces/proxy-2187/services/https:proxy-service-w56tv:tlsportname1/proxy/: tls baz (200; 7.891249ms)
May  6 21:06:13.176: INFO: (19) /api/v1/namespaces/proxy-2187/services/http:proxy-service-w56tv:portname2/proxy/: bar (200; 8.640849ms)
STEP: deleting ReplicationController proxy-service-w56tv in namespace proxy-2187, will wait for the garbage collector to delete the pods
May  6 21:06:13.237: INFO: Deleting ReplicationController proxy-service-w56tv took: 8.443992ms
May  6 21:06:13.337: INFO: Terminating ReplicationController proxy-service-w56tv pods took: 100.182507ms
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:06:17.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2187" for this suite.
May  6 21:06:23.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:06:24.058: INFO: namespace proxy-2187 deletion completed in 6.117133778s

• [SLOW TEST:19.213 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:06:24.058: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:06:35.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5386" for this suite.
May  6 21:06:41.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:06:41.254: INFO: namespace resourcequota-5386 deletion completed in 6.116072704s

• [SLOW TEST:17.196 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:06:41.255: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May  6 21:06:41.591: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  6 21:06:43.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396001, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396001, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396001, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396001, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:06:46.615: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:06:46.619: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:06:47.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3774" for this suite.
May  6 21:06:53.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:06:54.026: INFO: namespace crd-webhook-3774 deletion completed in 6.118929034s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.784 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:06:54.039: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May  6 21:06:54.065: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 21:06:54.077: INFO: Waiting for terminating namespaces to be deleted...
May  6 21:06:54.081: INFO: 
Logging pods the kubelet thinks is on node kube-node-0-kubelet.kubernetes-cluster.mesos before test
May  6 21:06:54.091: INFO: calico-node-xjmdx from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.091: INFO: 	Container calico-node ready: true, restart count 0
May  6 21:06:54.091: INFO: sonobuoy-e2e-job-76e28147b51c42fc from sonobuoy started at 2020-05-06 19:47:59 +0000 UTC (2 container statuses recorded)
May  6 21:06:54.091: INFO: 	Container e2e ready: true, restart count 0
May  6 21:06:54.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 21:06:54.091: INFO: kube-proxy-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.091: INFO: 	Container kp ready: true, restart count 0
May  6 21:06:54.091: INFO: local-dns-dispatcher-kube-node-0-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.091: INFO: 	Container coredns ready: true, restart count 0
May  6 21:06:54.091: INFO: sonobuoy from sonobuoy started at 2020-05-06 19:47:58 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.091: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 21:06:54.091: INFO: 
Logging pods the kubelet thinks is on node kube-node-1-kubelet.kubernetes-cluster.mesos before test
May  6 21:06:54.104: INFO: kube-proxy-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.104: INFO: 	Container kp ready: true, restart count 0
May  6 21:06:54.104: INFO: local-dns-dispatcher-kube-node-1-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.104: INFO: 	Container coredns ready: true, restart count 0
May  6 21:06:54.104: INFO: metrics-server-859cc6f97b-mpsmb from kube-system started at 2020-05-06 16:32:55 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.104: INFO: 	Container metrics-server ready: true, restart count 0
May  6 21:06:54.104: INFO: calico-node-tsjfr from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.104: INFO: 	Container calico-node ready: true, restart count 0
May  6 21:06:54.104: INFO: coredns-7fb4bb54b-p8tfs from kube-system started at 2020-05-06 17:52:40 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.104: INFO: 	Container coredns ready: true, restart count 0
May  6 21:06:54.104: INFO: calico-kube-controllers-788d6b9876-q8w7w from kube-system started at 2020-05-06 18:24:18 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.104: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 21:06:54.104: INFO: 
Logging pods the kubelet thinks is on node kube-node-2-kubelet.kubernetes-cluster.mesos before test
May  6 21:06:54.115: INFO: local-dns-dispatcher-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.116: INFO: 	Container coredns ready: true, restart count 0
May  6 21:06:54.116: INFO: calico-node-tb2vk from kube-system started at 2020-05-06 16:32:28 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.116: INFO: 	Container calico-node ready: true, restart count 0
May  6 21:06:54.116: INFO: coredns-7fb4bb54b-xtdwq from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.116: INFO: 	Container coredns ready: true, restart count 0
May  6 21:06:54.116: INFO: kube-proxy-kube-node-2-kubelet.kubernetes-cluster.mesos from kube-system started at 2020-05-06 16:32:27 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.116: INFO: 	Container kp ready: true, restart count 0
May  6 21:06:54.116: INFO: kubernetes-dashboard-68c7899b54-cq697 from kube-system started at 2020-05-06 16:32:58 +0000 UTC (1 container statuses recorded)
May  6 21:06:54.116: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node kube-node-0-kubelet.kubernetes-cluster.mesos
STEP: verifying the node has the label node kube-node-1-kubelet.kubernetes-cluster.mesos
STEP: verifying the node has the label node kube-node-2-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod calico-kube-controllers-788d6b9876-q8w7w requesting resource cpu=0m on Node kube-node-1-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod calico-node-tb2vk requesting resource cpu=250m on Node kube-node-2-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod calico-node-tsjfr requesting resource cpu=250m on Node kube-node-1-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod calico-node-xjmdx requesting resource cpu=250m on Node kube-node-0-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod coredns-7fb4bb54b-p8tfs requesting resource cpu=100m on Node kube-node-1-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod coredns-7fb4bb54b-xtdwq requesting resource cpu=100m on Node kube-node-2-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod kube-proxy-kube-node-0-kubelet.kubernetes-cluster.mesos requesting resource cpu=0m on Node kube-node-0-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod kube-proxy-kube-node-1-kubelet.kubernetes-cluster.mesos requesting resource cpu=0m on Node kube-node-1-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod kube-proxy-kube-node-2-kubelet.kubernetes-cluster.mesos requesting resource cpu=0m on Node kube-node-2-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod kubernetes-dashboard-68c7899b54-cq697 requesting resource cpu=0m on Node kube-node-2-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod local-dns-dispatcher-kube-node-0-kubelet.kubernetes-cluster.mesos requesting resource cpu=100m on Node kube-node-0-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod local-dns-dispatcher-kube-node-1-kubelet.kubernetes-cluster.mesos requesting resource cpu=100m on Node kube-node-1-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod local-dns-dispatcher-kube-node-2-kubelet.kubernetes-cluster.mesos requesting resource cpu=100m on Node kube-node-2-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod metrics-server-859cc6f97b-mpsmb requesting resource cpu=0m on Node kube-node-1-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-node-0-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.164: INFO: Pod sonobuoy-e2e-job-76e28147b51c42fc requesting resource cpu=0m on Node kube-node-0-kubelet.kubernetes-cluster.mesos
STEP: Starting Pods to consume most of the cluster CPU.
May  6 21:06:54.164: INFO: Creating a pod which consumes cpu=1155m on Node kube-node-0-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.171: INFO: Creating a pod which consumes cpu=1085m on Node kube-node-1-kubelet.kubernetes-cluster.mesos
May  6 21:06:54.176: INFO: Creating a pod which consumes cpu=1085m on Node kube-node-2-kubelet.kubernetes-cluster.mesos
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddeb513b-5663-4848-8b4d-8a4a2e5dc8c8.160c8c77ac9f3be7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4603/filler-pod-ddeb513b-5663-4848-8b4d-8a4a2e5dc8c8 to kube-node-0-kubelet.kubernetes-cluster.mesos]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddeb513b-5663-4848-8b4d-8a4a2e5dc8c8.160c8c77d6d310f2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddeb513b-5663-4848-8b4d-8a4a2e5dc8c8.160c8c77d9272b88], Reason = [Created], Message = [Created container filler-pod-ddeb513b-5663-4848-8b4d-8a4a2e5dc8c8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddeb513b-5663-4848-8b4d-8a4a2e5dc8c8.160c8c77e08831c8], Reason = [Started], Message = [Started container filler-pod-ddeb513b-5663-4848-8b4d-8a4a2e5dc8c8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2bc006c-46de-4902-b004-5df23e34e0a3.160c8c77ace0eafd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4603/filler-pod-f2bc006c-46de-4902-b004-5df23e34e0a3 to kube-node-1-kubelet.kubernetes-cluster.mesos]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2bc006c-46de-4902-b004-5df23e34e0a3.160c8c77d5e14d87], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2bc006c-46de-4902-b004-5df23e34e0a3.160c8c77d7abc1aa], Reason = [Created], Message = [Created container filler-pod-f2bc006c-46de-4902-b004-5df23e34e0a3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2bc006c-46de-4902-b004-5df23e34e0a3.160c8c77dd54f484], Reason = [Started], Message = [Started container filler-pod-f2bc006c-46de-4902-b004-5df23e34e0a3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcabfbec-dc03-4935-919e-ad4a5644786e.160c8c77ad26613f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4603/filler-pod-fcabfbec-dc03-4935-919e-ad4a5644786e to kube-node-2-kubelet.kubernetes-cluster.mesos]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcabfbec-dc03-4935-919e-ad4a5644786e.160c8c77d516ea24], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcabfbec-dc03-4935-919e-ad4a5644786e.160c8c77d790f3a5], Reason = [Created], Message = [Created container filler-pod-fcabfbec-dc03-4935-919e-ad4a5644786e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcabfbec-dc03-4935-919e-ad4a5644786e.160c8c77dd0c49b7], Reason = [Started], Message = [Started container filler-pod-fcabfbec-dc03-4935-919e-ad4a5644786e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.160c8c782588fd00], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.160c8c7826704921], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node kube-node-1-kubelet.kubernetes-cluster.mesos
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-node-2-kubelet.kubernetes-cluster.mesos
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-node-0-kubelet.kubernetes-cluster.mesos
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:06:57.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4603" for this suite.
May  6 21:07:03.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:07:03.426: INFO: namespace sched-pred-4603 deletion completed in 6.116304725s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:9.387 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:07:03.426: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2899
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2899
STEP: Creating statefulset with conflicting port in namespace statefulset-2899
STEP: Waiting until pod test-pod will start running in namespace statefulset-2899
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2899
May  6 21:07:05.480: INFO: Observed stateful pod in namespace: statefulset-2899, name: ss-0, uid: c97a006e-47b5-46a1-939a-1272868b41db, status phase: Pending. Waiting for statefulset controller to delete.
May  6 21:07:05.540: INFO: Observed stateful pod in namespace: statefulset-2899, name: ss-0, uid: c97a006e-47b5-46a1-939a-1272868b41db, status phase: Failed. Waiting for statefulset controller to delete.
May  6 21:07:05.547: INFO: Observed stateful pod in namespace: statefulset-2899, name: ss-0, uid: c97a006e-47b5-46a1-939a-1272868b41db, status phase: Failed. Waiting for statefulset controller to delete.
May  6 21:07:05.552: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2899
STEP: Removing pod with conflicting port in namespace statefulset-2899
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2899 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 21:07:07.573: INFO: Deleting all statefulset in ns statefulset-2899
May  6 21:07:07.575: INFO: Scaling statefulset ss to 0
May  6 21:07:17.589: INFO: Waiting for statefulset status.replicas updated to 0
May  6 21:07:17.592: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:07:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2899" for this suite.
May  6 21:07:23.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:07:23.728: INFO: namespace statefulset-2899 deletion completed in 6.118221672s

• [SLOW TEST:20.302 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:07:23.728: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:07:24.319: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 21:07:26.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396044, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396044, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396044, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396044, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:07:29.342: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:07:29.346: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8765-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:07:30.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3454" for this suite.
May  6 21:07:36.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:07:36.709: INFO: namespace webhook-3454 deletion completed in 6.119814682s
STEP: Destroying namespace "webhook-3454-markers" for this suite.
May  6 21:07:42.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:07:42.826: INFO: namespace webhook-3454-markers deletion completed in 6.117614725s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.112 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:07:42.840: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 21:07:42.868: INFO: PodSpec: initContainers in spec.initContainers
May  6 21:08:30.043: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-274c098e-7924-4087-bc6f-43010939c881", GenerateName:"", Namespace:"init-container-361", SelfLink:"/api/v1/namespaces/init-container-361/pods/pod-init-274c098e-7924-4087-bc6f-43010939c881", UID:"d1551502-5ba2-4a52-9148-1627236269d1", ResourceVersion:"67134", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63724396062, loc:(*time.Location)(0x78a2900)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"868083131"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.32.240/32", "cni.projectcalico.org/podIPs":"192.168.32.240/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8mz8j", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002afbfc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8mz8j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8mz8j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8mz8j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027cd608), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-node-2-kubelet.kubernetes-cluster.mesos", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0039006c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027cd680)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027cd6a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027cd6a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027cd6ac), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396062, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396062, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396062, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396062, loc:(*time.Location)(0x78a2900)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"9.0.1.5", PodIP:"192.168.32.240", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.32.240"}}, StartTime:(*v1.Time)(0xc0034d5e40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00328a1c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00328a230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e22dff4c40fe5d206cea99eb411e6dbc9d525f2944f80d58f3654ede55831176", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0034d5e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0034d5e60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0027cd70c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:08:30.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-361" for this suite.
May  6 21:08:42.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:08:42.164: INFO: namespace init-container-361 deletion completed in 12.114966069s

• [SLOW TEST:59.323 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:08:42.164: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 in namespace container-probe-7910
May  6 21:08:44.204: INFO: Started pod liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 in namespace container-probe-7910
STEP: checking the pod's current state and verifying that restartCount is present
May  6 21:08:44.207: INFO: Initial restart count of pod liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 is 0
May  6 21:08:54.226: INFO: Restart count of pod container-probe-7910/liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 is now 1 (10.018663285s elapsed)
May  6 21:09:16.264: INFO: Restart count of pod container-probe-7910/liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 is now 2 (32.056479466s elapsed)
May  6 21:09:36.299: INFO: Restart count of pod container-probe-7910/liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 is now 3 (52.091189578s elapsed)
May  6 21:09:56.333: INFO: Restart count of pod container-probe-7910/liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 is now 4 (1m12.125854389s elapsed)
May  6 21:10:58.447: INFO: Restart count of pod container-probe-7910/liveness-37e90a88-3f6e-456e-9613-b70239aae4c0 is now 5 (2m14.23968941s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:10:58.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7910" for this suite.
May  6 21:11:04.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:11:04.582: INFO: namespace container-probe-7910 deletion completed in 6.119241766s

• [SLOW TEST:142.418 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:11:04.582: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May  6 21:11:04.612: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
May  6 21:11:07.984: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:11:19.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1998" for this suite.
May  6 21:11:25.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:11:25.430: INFO: namespace crd-publish-openapi-1998 deletion completed in 6.11978471s

• [SLOW TEST:20.847 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:11:25.437: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:11:25.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:11:28.952: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:11:29.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6396" for this suite.
May  6 21:11:35.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:11:35.136: INFO: namespace webhook-6396 deletion completed in 6.116816755s
STEP: Destroying namespace "webhook-6396-markers" for this suite.
May  6 21:11:41.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:11:41.250: INFO: namespace webhook-6396-markers deletion completed in 6.114731554s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.828 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:11:41.264: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
May  6 21:11:43.313: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-161989487 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May  6 21:11:48.386: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:11:48.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2810" for this suite.
May  6 21:11:54.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:11:54.517: INFO: namespace pods-2810 deletion completed in 6.120688622s

• [SLOW TEST:13.253 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:11:54.517: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:11:54.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 21:11:56.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396314, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396314, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396314, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396314, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:11:59.909: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:11:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-414" for this suite.
May  6 21:12:05.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:12:06.075: INFO: namespace webhook-414 deletion completed in 6.113355204s
STEP: Destroying namespace "webhook-414-markers" for this suite.
May  6 21:12:12.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:12:12.185: INFO: namespace webhook-414-markers deletion completed in 6.109563124s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:12:12.200: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:12:13.006: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 21:12:15.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396333, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396333, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396333, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396333, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:12:18.030: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:12:18.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1695" for this suite.
May  6 21:12:24.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:12:24.159: INFO: namespace webhook-1695 deletion completed in 6.117860457s
STEP: Destroying namespace "webhook-1695-markers" for this suite.
May  6 21:12:30.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:12:30.295: INFO: namespace webhook-1695-markers deletion completed in 6.135824124s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.115 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:12:30.315: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
May  6 21:12:30.870: INFO: created pod pod-service-account-defaultsa
May  6 21:12:30.870: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  6 21:12:30.876: INFO: created pod pod-service-account-mountsa
May  6 21:12:30.876: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  6 21:12:30.881: INFO: created pod pod-service-account-nomountsa
May  6 21:12:30.881: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  6 21:12:30.887: INFO: created pod pod-service-account-defaultsa-mountspec
May  6 21:12:30.887: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  6 21:12:30.894: INFO: created pod pod-service-account-mountsa-mountspec
May  6 21:12:30.894: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  6 21:12:30.900: INFO: created pod pod-service-account-nomountsa-mountspec
May  6 21:12:30.900: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  6 21:12:30.905: INFO: created pod pod-service-account-defaultsa-nomountspec
May  6 21:12:30.905: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  6 21:12:30.911: INFO: created pod pod-service-account-mountsa-nomountspec
May  6 21:12:30.911: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  6 21:12:30.915: INFO: created pod pod-service-account-nomountsa-nomountspec
May  6 21:12:30.915: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:12:30.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-227" for this suite.
May  6 21:12:36.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:12:37.046: INFO: namespace svcaccounts-227 deletion completed in 6.127142187s

• [SLOW TEST:6.730 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:12:37.046: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:12:44.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5504" for this suite.
May  6 21:12:50.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:12:50.202: INFO: namespace resourcequota-5504 deletion completed in 6.115073888s

• [SLOW TEST:13.156 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:12:50.202: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  6 21:12:50.237: INFO: Waiting up to 5m0s for pod "pod-175d65da-db20-4df1-ae02-99f0fd928ea8" in namespace "emptydir-7025" to be "success or failure"
May  6 21:12:50.242: INFO: Pod "pod-175d65da-db20-4df1-ae02-99f0fd928ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101209ms
May  6 21:12:52.245: INFO: Pod "pod-175d65da-db20-4df1-ae02-99f0fd928ea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007528962s
STEP: Saw pod success
May  6 21:12:52.245: INFO: Pod "pod-175d65da-db20-4df1-ae02-99f0fd928ea8" satisfied condition "success or failure"
May  6 21:12:52.248: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-175d65da-db20-4df1-ae02-99f0fd928ea8 container test-container: <nil>
STEP: delete the pod
May  6 21:12:52.267: INFO: Waiting for pod pod-175d65da-db20-4df1-ae02-99f0fd928ea8 to disappear
May  6 21:12:52.271: INFO: Pod pod-175d65da-db20-4df1-ae02-99f0fd928ea8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:12:52.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7025" for this suite.
May  6 21:12:58.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:12:58.391: INFO: namespace emptydir-7025 deletion completed in 6.116918138s

• [SLOW TEST:8.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:12:58.391: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-d97200a8-42a3-44aa-82c3-957ed2ab08f3
STEP: Creating a pod to test consume secrets
May  6 21:12:58.429: INFO: Waiting up to 5m0s for pod "pod-secrets-11b0e04b-b756-454a-b207-c3b686c09b6b" in namespace "secrets-1269" to be "success or failure"
May  6 21:12:58.432: INFO: Pod "pod-secrets-11b0e04b-b756-454a-b207-c3b686c09b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899211ms
May  6 21:13:00.435: INFO: Pod "pod-secrets-11b0e04b-b756-454a-b207-c3b686c09b6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006269134s
STEP: Saw pod success
May  6 21:13:00.435: INFO: Pod "pod-secrets-11b0e04b-b756-454a-b207-c3b686c09b6b" satisfied condition "success or failure"
May  6 21:13:00.438: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-11b0e04b-b756-454a-b207-c3b686c09b6b container secret-volume-test: <nil>
STEP: delete the pod
May  6 21:13:00.457: INFO: Waiting for pod pod-secrets-11b0e04b-b756-454a-b207-c3b686c09b6b to disappear
May  6 21:13:00.459: INFO: Pod pod-secrets-11b0e04b-b756-454a-b207-c3b686c09b6b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:13:00.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1269" for this suite.
May  6 21:13:06.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:13:06.744: INFO: namespace secrets-1269 deletion completed in 6.281024895s

• [SLOW TEST:8.353 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:13:06.744: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May  6 21:13:06.772: INFO: namespace kubectl-1439
May  6 21:13:06.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-1439'
May  6 21:13:06.957: INFO: stderr: ""
May  6 21:13:06.957: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  6 21:13:07.961: INFO: Selector matched 1 pods for map[app:redis]
May  6 21:13:07.961: INFO: Found 0 / 1
May  6 21:13:08.961: INFO: Selector matched 1 pods for map[app:redis]
May  6 21:13:08.961: INFO: Found 1 / 1
May  6 21:13:08.961: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  6 21:13:08.964: INFO: Selector matched 1 pods for map[app:redis]
May  6 21:13:08.964: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 21:13:08.964: INFO: wait on redis-master startup in kubectl-1439 
May  6 21:13:08.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 logs redis-master-lq9tj redis-master --namespace=kubectl-1439'
May  6 21:13:09.040: INFO: stderr: ""
May  6 21:13:09.040: INFO: stdout: "1:C 06 May 2020 21:13:07.748 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 06 May 2020 21:13:07.748 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 06 May 2020 21:13:07.748 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 06 May 2020 21:13:07.749 * Running mode=standalone, port=6379.\n1:M 06 May 2020 21:13:07.749 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 May 2020 21:13:07.749 # Server initialized\n1:M 06 May 2020 21:13:07.749 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 May 2020 21:13:07.749 * Ready to accept connections\n"
STEP: exposing RC
May  6 21:13:09.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1439'
May  6 21:13:09.120: INFO: stderr: ""
May  6 21:13:09.120: INFO: stdout: "service/rm2 exposed\n"
May  6 21:13:09.123: INFO: Service rm2 in namespace kubectl-1439 found.
STEP: exposing service
May  6 21:13:11.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1439'
May  6 21:13:11.215: INFO: stderr: ""
May  6 21:13:11.215: INFO: stdout: "service/rm3 exposed\n"
May  6 21:13:11.218: INFO: Service rm3 in namespace kubectl-1439 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:13:13.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1439" for this suite.
May  6 21:13:25.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:13:25.340: INFO: namespace kubectl-1439 deletion completed in 12.113057859s

• [SLOW TEST:18.596 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:13:25.341: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:13:25.367: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  6 21:13:25.374: INFO: Pod name sample-pod: Found 0 pods out of 1
May  6 21:13:30.377: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  6 21:13:30.377: INFO: Creating deployment "test-rolling-update-deployment"
May  6 21:13:30.381: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  6 21:13:30.386: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  6 21:13:32.392: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  6 21:13:32.395: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 21:13:32.411: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8185 /apis/apps/v1/namespaces/deployment-8185/deployments/test-rolling-update-deployment fc8387de-df57-4747-9186-837e1d786626 68432 1 2020-05-06 21:13:30 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004507b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-06 21:13:30 +0000 UTC,LastTransitionTime:2020-05-06 21:13:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-05-06 21:13:31 +0000 UTC,LastTransitionTime:2020-05-06 21:13:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 21:13:32.414: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-8185 /apis/apps/v1/namespaces/deployment-8185/replicasets/test-rolling-update-deployment-55d946486 e75cd55d-c80a-4fa6-a2dd-2c564a7b5635 68421 1 2020-05-06 21:13:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment fc8387de-df57-4747-9186-837e1d786626 0xc0006f76c0 0xc0006f76c1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0006f7748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 21:13:32.414: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  6 21:13:32.414: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8185 /apis/apps/v1/namespaces/deployment-8185/replicasets/test-rolling-update-controller 9abe7e34-83b9-451b-b2bb-b6a952acf71f 68430 2 2020-05-06 21:13:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment fc8387de-df57-4747-9186-837e1d786626 0xc0006f7547 0xc0006f7548}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0006f75f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 21:13:32.418: INFO: Pod "test-rolling-update-deployment-55d946486-zjnxc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-zjnxc test-rolling-update-deployment-55d946486- deployment-8185 /api/v1/namespaces/deployment-8185/pods/test-rolling-update-deployment-55d946486-zjnxc 89e823af-0933-437b-ba4c-e0d1891e8a7d 68420 0 2020-05-06 21:13:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:192.168.80.70/32 cni.projectcalico.org/podIPs:192.168.80.70/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 e75cd55d-c80a-4fa6-a2dd-2c564a7b5635 0xc0006f7dc0 0xc0006f7dc1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pqjw8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pqjw8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pqjw8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:13:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:13:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:13:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:13:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.2.4,PodIP:192.168.80.70,StartTime:2020-05-06 21:13:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:13:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://5a87c60a71a4423406d2dbb7d118245afa559d5f7ea3ef60902c9b4b90f399e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.80.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:13:32.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8185" for this suite.
May  6 21:13:38.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:13:38.539: INFO: namespace deployment-8185 deletion completed in 6.117074995s

• [SLOW TEST:13.198 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:13:38.539: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May  6 21:13:38.565: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:13:42.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3903" for this suite.
May  6 21:14:10.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:14:10.473: INFO: namespace init-container-3903 deletion completed in 28.161207776s

• [SLOW TEST:31.934 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:14:10.473: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 21:14:10.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-637f7cb5-058c-4c22-926f-95101d051ea5" in namespace "downward-api-5153" to be "success or failure"
May  6 21:14:10.561: INFO: Pod "downwardapi-volume-637f7cb5-058c-4c22-926f-95101d051ea5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452001ms
May  6 21:14:12.565: INFO: Pod "downwardapi-volume-637f7cb5-058c-4c22-926f-95101d051ea5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009110107s
STEP: Saw pod success
May  6 21:14:12.565: INFO: Pod "downwardapi-volume-637f7cb5-058c-4c22-926f-95101d051ea5" satisfied condition "success or failure"
May  6 21:14:12.567: INFO: Trying to get logs from node kube-node-1-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-637f7cb5-058c-4c22-926f-95101d051ea5 container client-container: <nil>
STEP: delete the pod
May  6 21:14:12.596: INFO: Waiting for pod downwardapi-volume-637f7cb5-058c-4c22-926f-95101d051ea5 to disappear
May  6 21:14:12.599: INFO: Pod downwardapi-volume-637f7cb5-058c-4c22-926f-95101d051ea5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:14:12.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5153" for this suite.
May  6 21:14:18.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:14:18.740: INFO: namespace downward-api-5153 deletion completed in 6.136675194s

• [SLOW TEST:8.266 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:14:18.740: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  6 21:14:18.799: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:18.800: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:18.800: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:18.802: INFO: Number of nodes with available pods: 0
May  6 21:14:18.802: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:19.806: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:19.806: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:19.806: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:19.810: INFO: Number of nodes with available pods: 1
May  6 21:14:19.810: INFO: Node kube-node-1-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:20.806: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:20.806: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:20.806: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:20.810: INFO: Number of nodes with available pods: 3
May  6 21:14:20.810: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May  6 21:14:20.825: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:20.825: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:20.825: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:20.828: INFO: Number of nodes with available pods: 2
May  6 21:14:20.828: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:21.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:21.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:21.833: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:21.836: INFO: Number of nodes with available pods: 2
May  6 21:14:21.836: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:22.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:22.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:22.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:22.835: INFO: Number of nodes with available pods: 2
May  6 21:14:22.835: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:23.833: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:23.833: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:23.833: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:23.836: INFO: Number of nodes with available pods: 2
May  6 21:14:23.836: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:24.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:24.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:24.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:24.835: INFO: Number of nodes with available pods: 2
May  6 21:14:24.835: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:25.833: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:25.833: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:25.833: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:25.837: INFO: Number of nodes with available pods: 2
May  6 21:14:25.837: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:26.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:26.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:26.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:26.835: INFO: Number of nodes with available pods: 2
May  6 21:14:26.835: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:27.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:27.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:27.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:27.835: INFO: Number of nodes with available pods: 2
May  6 21:14:27.835: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:28.834: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:28.834: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:28.834: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:28.837: INFO: Number of nodes with available pods: 2
May  6 21:14:28.837: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:14:29.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-0-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:29.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-1-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:29.832: INFO: DaemonSet pods can't tolerate node kube-control-plane-2-instance.kubernetes-cluster.mesos with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoExecute TimeAdded:<nil>}], skip checking this node
May  6 21:14:29.835: INFO: Number of nodes with available pods: 3
May  6 21:14:29.835: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8259, will wait for the garbage collector to delete the pods
May  6 21:14:29.901: INFO: Deleting DaemonSet.extensions daemon-set took: 8.055053ms
May  6 21:14:30.301: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.162212ms
May  6 21:14:38.704: INFO: Number of nodes with available pods: 0
May  6 21:14:38.704: INFO: Number of running nodes: 0, number of available pods: 0
May  6 21:14:38.707: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8259/daemonsets","resourceVersion":"68772"},"items":null}

May  6 21:14:38.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8259/pods","resourceVersion":"68772"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:14:38.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8259" for this suite.
May  6 21:14:44.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:14:44.846: INFO: namespace daemonsets-8259 deletion completed in 6.11414849s

• [SLOW TEST:26.106 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:14:44.846: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 21:14:44.878: INFO: Waiting up to 5m0s for pod "downward-api-5ccb0bed-7274-4a60-9070-bb3f909f405a" in namespace "downward-api-9901" to be "success or failure"
May  6 21:14:44.880: INFO: Pod "downward-api-5ccb0bed-7274-4a60-9070-bb3f909f405a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20547ms
May  6 21:14:46.884: INFO: Pod "downward-api-5ccb0bed-7274-4a60-9070-bb3f909f405a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005811505s
STEP: Saw pod success
May  6 21:14:46.884: INFO: Pod "downward-api-5ccb0bed-7274-4a60-9070-bb3f909f405a" satisfied condition "success or failure"
May  6 21:14:46.887: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downward-api-5ccb0bed-7274-4a60-9070-bb3f909f405a container dapi-container: <nil>
STEP: delete the pod
May  6 21:14:46.913: INFO: Waiting for pod downward-api-5ccb0bed-7274-4a60-9070-bb3f909f405a to disappear
May  6 21:14:46.915: INFO: Pod downward-api-5ccb0bed-7274-4a60-9070-bb3f909f405a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:14:46.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9901" for this suite.
May  6 21:14:52.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:14:53.030: INFO: namespace downward-api-9901 deletion completed in 6.111078581s

• [SLOW TEST:8.184 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:14:53.030: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:14:53.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3420" for this suite.
May  6 21:14:59.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:14:59.205: INFO: namespace resourcequota-3420 deletion completed in 6.118588301s

• [SLOW TEST:6.175 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:14:59.205: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8327
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8327
STEP: creating replication controller externalsvc in namespace services-8327
I0506 21:14:59.291156      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-8327, replica count: 2
I0506 21:15:02.341477      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May  6 21:15:02.359: INFO: Creating new exec pod
May  6 21:15:04.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=services-8327 execpodzf9j6 -- /bin/sh -x -c nslookup clusterip-service'
May  6 21:15:04.613: INFO: stderr: "+ nslookup clusterip-service\n"
May  6 21:15:04.613: INFO: stdout: "Server:\t\t9.0.4.4\nAddress:\t9.0.4.4#53\n\nclusterip-service.services-8327.svc.cluster.local\tcanonical name = externalsvc.services-8327.svc.cluster.local.\nName:\texternalsvc.services-8327.svc.cluster.local\nAddress: 10.100.198.200\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8327, will wait for the garbage collector to delete the pods
May  6 21:15:04.675: INFO: Deleting ReplicationController externalsvc took: 8.226159ms
May  6 21:15:05.075: INFO: Terminating ReplicationController externalsvc pods took: 400.181924ms
May  6 21:15:17.897: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:15:17.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8327" for this suite.
May  6 21:15:23.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:15:24.047: INFO: namespace services-8327 deletion completed in 6.117462957s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.841 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:15:24.047: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-2268
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2268 to expose endpoints map[]
May  6 21:15:24.086: INFO: Get endpoints failed (4.34667ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May  6 21:15:25.089: INFO: successfully validated that service multi-endpoint-test in namespace services-2268 exposes endpoints map[] (1.007802016s elapsed)
STEP: Creating pod pod1 in namespace services-2268
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2268 to expose endpoints map[pod1:[100]]
May  6 21:15:26.108: INFO: successfully validated that service multi-endpoint-test in namespace services-2268 exposes endpoints map[pod1:[100]] (1.012547351s elapsed)
STEP: Creating pod pod2 in namespace services-2268
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2268 to expose endpoints map[pod1:[100] pod2:[101]]
May  6 21:15:28.140: INFO: successfully validated that service multi-endpoint-test in namespace services-2268 exposes endpoints map[pod1:[100] pod2:[101]] (2.027415457s elapsed)
STEP: Deleting pod pod1 in namespace services-2268
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2268 to expose endpoints map[pod2:[101]]
May  6 21:15:29.160: INFO: successfully validated that service multi-endpoint-test in namespace services-2268 exposes endpoints map[pod2:[101]] (1.014673632s elapsed)
STEP: Deleting pod pod2 in namespace services-2268
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2268 to expose endpoints map[]
May  6 21:15:30.173: INFO: successfully validated that service multi-endpoint-test in namespace services-2268 exposes endpoints map[] (1.006484401s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:15:30.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2268" for this suite.
May  6 21:15:42.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:15:42.317: INFO: namespace services-2268 deletion completed in 12.116053665s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.271 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:15:42.318: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
May  6 21:15:42.360: INFO: Waiting up to 5m0s for pod "client-containers-ec4297e6-3272-438e-981f-e09bfcb74066" in namespace "containers-1946" to be "success or failure"
May  6 21:15:42.365: INFO: Pod "client-containers-ec4297e6-3272-438e-981f-e09bfcb74066": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501878ms
May  6 21:15:44.369: INFO: Pod "client-containers-ec4297e6-3272-438e-981f-e09bfcb74066": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008395476s
STEP: Saw pod success
May  6 21:15:44.369: INFO: Pod "client-containers-ec4297e6-3272-438e-981f-e09bfcb74066" satisfied condition "success or failure"
May  6 21:15:44.371: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod client-containers-ec4297e6-3272-438e-981f-e09bfcb74066 container test-container: <nil>
STEP: delete the pod
May  6 21:15:44.390: INFO: Waiting for pod client-containers-ec4297e6-3272-438e-981f-e09bfcb74066 to disappear
May  6 21:15:44.393: INFO: Pod client-containers-ec4297e6-3272-438e-981f-e09bfcb74066 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:15:44.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1946" for this suite.
May  6 21:15:50.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:15:50.547: INFO: namespace containers-1946 deletion completed in 6.150586154s

• [SLOW TEST:8.229 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:15:50.547: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-wrrg
STEP: Creating a pod to test atomic-volume-subpath
May  6 21:15:50.588: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wrrg" in namespace "subpath-9614" to be "success or failure"
May  6 21:15:50.590: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340423ms
May  6 21:15:52.593: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 2.005559867s
May  6 21:15:54.597: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 4.009334339s
May  6 21:15:56.600: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 6.012349606s
May  6 21:15:58.604: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 8.016014658s
May  6 21:16:00.608: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 10.019856143s
May  6 21:16:02.611: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 12.023061771s
May  6 21:16:04.615: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 14.026568886s
May  6 21:16:06.618: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 16.030471088s
May  6 21:16:08.622: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 18.033916615s
May  6 21:16:10.625: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Running", Reason="", readiness=true. Elapsed: 20.037026719s
May  6 21:16:12.629: INFO: Pod "pod-subpath-test-configmap-wrrg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041030701s
STEP: Saw pod success
May  6 21:16:12.629: INFO: Pod "pod-subpath-test-configmap-wrrg" satisfied condition "success or failure"
May  6 21:16:12.632: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-subpath-test-configmap-wrrg container test-container-subpath-configmap-wrrg: <nil>
STEP: delete the pod
May  6 21:16:12.652: INFO: Waiting for pod pod-subpath-test-configmap-wrrg to disappear
May  6 21:16:12.654: INFO: Pod pod-subpath-test-configmap-wrrg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wrrg
May  6 21:16:12.654: INFO: Deleting pod "pod-subpath-test-configmap-wrrg" in namespace "subpath-9614"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:16:12.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9614" for this suite.
May  6 21:16:18.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:16:18.772: INFO: namespace subpath-9614 deletion completed in 6.111738768s

• [SLOW TEST:28.225 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:16:18.772: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  6 21:16:18.805: INFO: Waiting up to 5m0s for pod "pod-09f239dd-f500-4d1d-bf67-f0518a98f66c" in namespace "emptydir-2492" to be "success or failure"
May  6 21:16:18.808: INFO: Pod "pod-09f239dd-f500-4d1d-bf67-f0518a98f66c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.444309ms
May  6 21:16:20.811: INFO: Pod "pod-09f239dd-f500-4d1d-bf67-f0518a98f66c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005588384s
STEP: Saw pod success
May  6 21:16:20.811: INFO: Pod "pod-09f239dd-f500-4d1d-bf67-f0518a98f66c" satisfied condition "success or failure"
May  6 21:16:20.813: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-09f239dd-f500-4d1d-bf67-f0518a98f66c container test-container: <nil>
STEP: delete the pod
May  6 21:16:20.832: INFO: Waiting for pod pod-09f239dd-f500-4d1d-bf67-f0518a98f66c to disappear
May  6 21:16:20.835: INFO: Pod pod-09f239dd-f500-4d1d-bf67-f0518a98f66c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:16:20.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2492" for this suite.
May  6 21:16:26.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:16:26.949: INFO: namespace emptydir-2492 deletion completed in 6.110775495s

• [SLOW TEST:8.177 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:16:26.949: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:16:28.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2770" for this suite.
May  6 21:16:39.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:16:39.136: INFO: namespace containers-2770 deletion completed in 10.13489761s

• [SLOW TEST:12.186 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:16:39.136: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:16:39.166: INFO: Creating deployment "webserver-deployment"
May  6 21:16:39.171: INFO: Waiting for observed generation 1
May  6 21:16:41.176: INFO: Waiting for all required pods to come up
May  6 21:16:41.181: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May  6 21:16:41.181: INFO: Waiting for deployment "webserver-deployment" to complete
May  6 21:16:41.187: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  6 21:16:41.194: INFO: Updating deployment webserver-deployment
May  6 21:16:41.194: INFO: Waiting for observed generation 2
May  6 21:16:43.200: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  6 21:16:43.203: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  6 21:16:43.205: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  6 21:16:43.213: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  6 21:16:43.213: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  6 21:16:43.216: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  6 21:16:43.221: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  6 21:16:43.221: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  6 21:16:43.228: INFO: Updating deployment webserver-deployment
May  6 21:16:43.228: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  6 21:16:43.234: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  6 21:16:43.239: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 21:16:43.259: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7984 /apis/apps/v1/namespaces/deployment-7984/deployments/webserver-deployment 73772b13-ea4a-4303-b888-432f721f4caa 69677 3 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035d29a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-05-06 21:16:41 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-06 21:16:43 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May  6 21:16:43.267: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-7984 /apis/apps/v1/namespaces/deployment-7984/replicasets/webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 69664 3 2020-05-06 21:16:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 73772b13-ea4a-4303-b888-432f721f4caa 0xc0035d2eb7 0xc0035d2eb8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035d2f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 21:16:43.267: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  6 21:16:43.268: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-7984 /apis/apps/v1/namespaces/deployment-7984/replicasets/webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 69661 3 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 73772b13-ea4a-4303-b888-432f721f4caa 0xc0035d2df7 0xc0035d2df8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035d2e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May  6 21:16:43.290: INFO: Pod "webserver-deployment-595b5b9587-25p2l" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-25p2l webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-25p2l 95d1ed58-ff53-4a6e-8241-324a3d9ac356 69556 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.32.201/32 cni.projectcalico.org/podIPs:192.168.32.201/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c057 0xc00347c058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:192.168.32.201,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://aa5d83245f9a6a4b905e95eebdd0cd07447a12a8772432e3263f803a44ab7396,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.32.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.290: INFO: Pod "webserver-deployment-595b5b9587-7h44v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7h44v webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-7h44v 42f60a84-e4c0-42e6-aabd-428dcc4f30ef 69538 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.80.76/32 cni.projectcalico.org/podIPs:192.168.80.76/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c1e0 0xc00347c1e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.2.4,PodIP:192.168.80.76,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://53cac01300357370411dbfb1553a8a4ac2c959d8dd762e7c28be1f3b351a6e3a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.80.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.290: INFO: Pod "webserver-deployment-595b5b9587-7wndq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7wndq webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-7wndq 6a058da4-87b1-4a43-a783-7fcaeff751de 69531 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.43.198/32 cni.projectcalico.org/podIPs:192.168.43.198/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c340 0xc00347c341}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.4.4,PodIP:192.168.43.198,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3334b0f55aa92b3fd27472d305d079d036784c04b9dc2fa390daf7a80034da37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.43.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.290: INFO: Pod "webserver-deployment-595b5b9587-cmpmm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-cmpmm webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-cmpmm d1b92f2e-e191-40e5-a74d-688dfbda7e8b 69675 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c4c0 0xc00347c4c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.290: INFO: Pod "webserver-deployment-595b5b9587-k8j8v" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-k8j8v webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-k8j8v bf84280d-514b-4090-a7d9-15afb72dd6bf 69697 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c5c0 0xc00347c5c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.290: INFO: Pod "webserver-deployment-595b5b9587-kpgbq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kpgbq webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-kpgbq 8b7526cb-49a1-4232-ae66-de0845fc7c0f 69541 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.80.75/32 cni.projectcalico.org/podIPs:192.168.80.75/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c6e0 0xc00347c6e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.2.4,PodIP:192.168.80.75,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://bab68222b5bb05e713be8bf03f6666b69db5c2ed887ebb3fe2459b2da6909bc5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.80.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.290: INFO: Pod "webserver-deployment-595b5b9587-lzbth" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lzbth webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-lzbth d5f6cfd4-6670-4124-baca-aeec8c419267 69689 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c840 0xc00347c841}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:,StartTime:2020-05-06 21:16:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.291: INFO: Pod "webserver-deployment-595b5b9587-mcfs4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mcfs4 webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-mcfs4 cf46b3ec-6f8d-4e53-a4c1-bd533d6c0737 69547 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.32.195/32 cni.projectcalico.org/podIPs:192.168.32.195/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347c980 0xc00347c981}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:192.168.32.195,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d029c5557f5ccf4ddfa745fc0406851ae4043f8f90319e92abfe6c8a94e29b57,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.32.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.291: INFO: Pod "webserver-deployment-595b5b9587-smkvb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-smkvb webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-smkvb 854da1ae-03bb-4531-be3a-64bfd3c41843 69544 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.80.78/32 cni.projectcalico.org/podIPs:192.168.80.78/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347cb20 0xc00347cb21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.2.4,PodIP:192.168.80.78,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://37825c6cea4cce4ed52b2b560ee395c9398680995cf25a61e19740f93c4627a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.80.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.291: INFO: Pod "webserver-deployment-595b5b9587-t2b92" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-t2b92 webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-t2b92 6c785c28-a313-4a1e-bde0-188eebd3788a 69553 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.32.199/32 cni.projectcalico.org/podIPs:192.168.32.199/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347cc80 0xc00347cc81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:192.168.32.199,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2034f8e80e31a9cdc5e191ada9e0552ff2c8033d214acaf821ff65c593d7d165,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.32.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.291: INFO: Pod "webserver-deployment-595b5b9587-tfrnt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tfrnt webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-tfrnt 9a34995b-d34a-4489-8700-871e90ac5605 69684 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347cde0 0xc00347cde1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.291: INFO: Pod "webserver-deployment-595b5b9587-x64jg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-x64jg webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-x64jg ac7c7bbc-b270-4c4a-997f-5b1a09784098 69695 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347cee0 0xc00347cee1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.4.4,PodIP:,StartTime:2020-05-06 21:16:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.291: INFO: Pod "webserver-deployment-595b5b9587-xz4ww" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xz4ww webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-xz4ww 094af84e-af7d-4c26-96f0-bf6a4faf986b 69686 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347d020 0xc00347d021}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-595b5b9587-zcsmw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zcsmw webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-zcsmw 914f5bf6-6dce-40ce-b92b-7914a4fe37b2 69706 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347d120 0xc00347d121}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.2.4,PodIP:,StartTime:2020-05-06 21:16:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-595b5b9587-zdwcw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zdwcw webserver-deployment-595b5b9587- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-595b5b9587-zdwcw 229d98c4-58c0-4055-86e9-34e039eb2fa2 69549 0 2020-05-06 21:16:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.32.202/32 cni.projectcalico.org/podIPs:192.168.32.202/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 a08c0f5f-39eb-476f-b3ff-05c1da03ad8e 0xc00347d260 0xc00347d261}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:192.168.32.202,StartTime:2020-05-06 21:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-06 21:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ad99a18a96356ec7fa86943695b9d3e8507a9167af67ed0b4f3808d17cc71c75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.32.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-c7997dcc8-29ngl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-29ngl webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-29ngl fd7e3f6c-9290-4535-b888-38c2653570d4 69632 0 2020-05-06 21:16:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.80.79/32 cni.projectcalico.org/podIPs:192.168.80.79/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347d3e0 0xc00347d3e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.2.4,PodIP:,StartTime:2020-05-06 21:16:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-c7997dcc8-bkxk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bkxk2 webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-bkxk2 d7a7e85a-1c5f-43e7-911a-4639aeb848f0 69705 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347d540 0xc00347d541}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-c7997dcc8-c9h9p" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-c9h9p webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-c9h9p e8620d86-e21d-49e9-a473-4be816f6e06c 69704 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347d660 0xc00347d661}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-c7997dcc8-j6rj9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-j6rj9 webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-j6rj9 eba72518-8020-4bcc-abd6-bd2102f34d9e 69649 0 2020-05-06 21:16:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.80.77/32 cni.projectcalico.org/podIPs:192.168.80.77/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347d790 0xc00347d791}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-1-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.2.4,PodIP:,StartTime:2020-05-06 21:16:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-c7997dcc8-lr547" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lr547 webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-lr547 defdd2c7-6b2a-4288-aa6f-36eaac33d96a 69651 0 2020-05-06 21:16:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.32.200/32 cni.projectcalico.org/podIPs:192.168.32.200/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347d900 0xc00347d901}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:,StartTime:2020-05-06 21:16:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.292: INFO: Pod "webserver-deployment-c7997dcc8-lrdw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lrdw9 webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-lrdw9 df32b4e7-5085-4756-9be1-0502819e5427 69638 0 2020-05-06 21:16:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.43.215/32 cni.projectcalico.org/podIPs:192.168.43.215/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347da70 0xc00347da71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.4.4,PodIP:,StartTime:2020-05-06 21:16:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.293: INFO: Pod "webserver-deployment-c7997dcc8-ntk5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ntk5h webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-ntk5h 2ce7dca8-4758-4fce-99ae-cd75bb4de770 69635 0 2020-05-06 21:16:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.43.208/32 cni.projectcalico.org/podIPs:192.168.43.208/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347dbd0 0xc00347dbd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.4.4,PodIP:,StartTime:2020-05-06 21:16:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.293: INFO: Pod "webserver-deployment-c7997dcc8-p9zfb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p9zfb webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-p9zfb 2a0aced9-c498-4cea-8170-ca51695d3d95 69701 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347dd30 0xc00347dd31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.293: INFO: Pod "webserver-deployment-c7997dcc8-psnpk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-psnpk webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-psnpk 16245801-8be2-435f-a8e6-38a27cf6f7dd 69708 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347de40 0xc00347de41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.293: INFO: Pod "webserver-deployment-c7997dcc8-pw59c" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pw59c webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-pw59c a3a7f866-7f9b-4518-97f1-c87cc6bb3e1f 69690 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00347df50 0xc00347df51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-0-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.293: INFO: Pod "webserver-deployment-c7997dcc8-v4g9z" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v4g9z webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-v4g9z 1e301b5b-6b54-427c-8085-339705d765b0 69709 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00316c060 0xc00316c061}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:16:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:,StartTime:2020-05-06 21:16:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 21:16:43.293: INFO: Pod "webserver-deployment-c7997dcc8-wh6sz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wh6sz webserver-deployment-c7997dcc8- deployment-7984 /api/v1/namespaces/deployment-7984/pods/webserver-deployment-c7997dcc8-wh6sz 97c96843-c2b9-406f-836a-89fd6d984663 69698 0 2020-05-06 21:16:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 97c822b5-9bfa-4d56-a34d-3a395cb1c9bc 0xc00316c1c0 0xc00316c1c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c22qb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c22qb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c22qb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:16:43.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7984" for this suite.
May  6 21:16:51.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:16:51.459: INFO: namespace deployment-7984 deletion completed in 8.152831151s

• [SLOW TEST:12.323 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:16:51.459: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  6 21:16:55.534: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 21:16:55.537: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 21:16:57.537: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 21:16:57.540: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 21:16:59.537: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 21:16:59.541: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:16:59.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1112" for this suite.
May  6 21:17:27.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:17:27.659: INFO: namespace container-lifecycle-hook-1112 deletion completed in 28.114815763s

• [SLOW TEST:36.200 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:17:27.659: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-9fdc51b6-ae3a-42d1-a257-f79adb0a6074
STEP: Creating a pod to test consume secrets
May  6 21:17:27.698: INFO: Waiting up to 5m0s for pod "pod-secrets-e64d3af1-d909-4181-95a7-9963ac0926dd" in namespace "secrets-6215" to be "success or failure"
May  6 21:17:27.701: INFO: Pod "pod-secrets-e64d3af1-d909-4181-95a7-9963ac0926dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362382ms
May  6 21:17:29.704: INFO: Pod "pod-secrets-e64d3af1-d909-4181-95a7-9963ac0926dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005782401s
STEP: Saw pod success
May  6 21:17:29.704: INFO: Pod "pod-secrets-e64d3af1-d909-4181-95a7-9963ac0926dd" satisfied condition "success or failure"
May  6 21:17:29.708: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-e64d3af1-d909-4181-95a7-9963ac0926dd container secret-env-test: <nil>
STEP: delete the pod
May  6 21:17:29.728: INFO: Waiting for pod pod-secrets-e64d3af1-d909-4181-95a7-9963ac0926dd to disappear
May  6 21:17:29.731: INFO: Pod pod-secrets-e64d3af1-d909-4181-95a7-9963ac0926dd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:17:29.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6215" for this suite.
May  6 21:17:35.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:17:35.850: INFO: namespace secrets-6215 deletion completed in 6.11414789s

• [SLOW TEST:8.191 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:17:35.850: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-a62f2e1b-0765-4294-a460-fb30dca6ad3e
STEP: Creating a pod to test consume configMaps
May  6 21:17:35.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-24ee1b9f-7bd0-4de3-b319-407d01123bfd" in namespace "configmap-2998" to be "success or failure"
May  6 21:17:35.893: INFO: Pod "pod-configmaps-24ee1b9f-7bd0-4de3-b319-407d01123bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48671ms
May  6 21:17:37.897: INFO: Pod "pod-configmaps-24ee1b9f-7bd0-4de3-b319-407d01123bfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007292557s
STEP: Saw pod success
May  6 21:17:37.897: INFO: Pod "pod-configmaps-24ee1b9f-7bd0-4de3-b319-407d01123bfd" satisfied condition "success or failure"
May  6 21:17:37.900: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-24ee1b9f-7bd0-4de3-b319-407d01123bfd container configmap-volume-test: <nil>
STEP: delete the pod
May  6 21:17:37.918: INFO: Waiting for pod pod-configmaps-24ee1b9f-7bd0-4de3-b319-407d01123bfd to disappear
May  6 21:17:37.920: INFO: Pod pod-configmaps-24ee1b9f-7bd0-4de3-b319-407d01123bfd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:17:37.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2998" for this suite.
May  6 21:17:43.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:17:44.039: INFO: namespace configmap-2998 deletion completed in 6.114962763s

• [SLOW TEST:8.189 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:17:44.039: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-a724fb45-4e63-4c9a-a83e-7babec0f5e43
STEP: Creating a pod to test consume secrets
May  6 21:17:44.075: INFO: Waiting up to 5m0s for pod "pod-secrets-dcdae94d-a36b-44cc-bc1b-1d368f5d2adb" in namespace "secrets-8117" to be "success or failure"
May  6 21:17:44.077: INFO: Pod "pod-secrets-dcdae94d-a36b-44cc-bc1b-1d368f5d2adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131231ms
May  6 21:17:46.081: INFO: Pod "pod-secrets-dcdae94d-a36b-44cc-bc1b-1d368f5d2adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006162819s
STEP: Saw pod success
May  6 21:17:46.081: INFO: Pod "pod-secrets-dcdae94d-a36b-44cc-bc1b-1d368f5d2adb" satisfied condition "success or failure"
May  6 21:17:46.084: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-dcdae94d-a36b-44cc-bc1b-1d368f5d2adb container secret-volume-test: <nil>
STEP: delete the pod
May  6 21:17:46.102: INFO: Waiting for pod pod-secrets-dcdae94d-a36b-44cc-bc1b-1d368f5d2adb to disappear
May  6 21:17:46.105: INFO: Pod pod-secrets-dcdae94d-a36b-44cc-bc1b-1d368f5d2adb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:17:46.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8117" for this suite.
May  6 21:17:52.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:17:52.225: INFO: namespace secrets-8117 deletion completed in 6.11537786s

• [SLOW TEST:8.186 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:17:52.225: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-5wvq
STEP: Creating a pod to test atomic-volume-subpath
May  6 21:17:52.265: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5wvq" in namespace "subpath-8602" to be "success or failure"
May  6 21:17:52.267: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358619ms
May  6 21:17:54.270: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005467592s
May  6 21:17:56.274: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 4.009020874s
May  6 21:17:58.277: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 6.012212307s
May  6 21:18:00.281: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.015768485s
May  6 21:18:02.285: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 10.019889827s
May  6 21:18:04.288: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 12.022881068s
May  6 21:18:06.291: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 14.025850885s
May  6 21:18:08.295: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 16.030069652s
May  6 21:18:10.298: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 18.033190204s
May  6 21:18:12.302: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Running", Reason="", readiness=true. Elapsed: 20.036977894s
May  6 21:18:14.306: INFO: Pod "pod-subpath-test-projected-5wvq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040978327s
STEP: Saw pod success
May  6 21:18:14.306: INFO: Pod "pod-subpath-test-projected-5wvq" satisfied condition "success or failure"
May  6 21:18:14.309: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-subpath-test-projected-5wvq container test-container-subpath-projected-5wvq: <nil>
STEP: delete the pod
May  6 21:18:14.330: INFO: Waiting for pod pod-subpath-test-projected-5wvq to disappear
May  6 21:18:14.334: INFO: Pod pod-subpath-test-projected-5wvq no longer exists
STEP: Deleting pod pod-subpath-test-projected-5wvq
May  6 21:18:14.334: INFO: Deleting pod "pod-subpath-test-projected-5wvq" in namespace "subpath-8602"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:18:14.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8602" for this suite.
May  6 21:18:20.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:18:20.463: INFO: namespace subpath-8602 deletion completed in 6.121207207s

• [SLOW TEST:28.238 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:18:20.464: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-8c2a4e82-598a-41d8-8a3a-d6821768f9e5
STEP: Creating a pod to test consume configMaps
May  6 21:18:20.501: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-128751f9-5a70-436a-866c-a190e407b2da" in namespace "projected-2866" to be "success or failure"
May  6 21:18:20.505: INFO: Pod "pod-projected-configmaps-128751f9-5a70-436a-866c-a190e407b2da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.989626ms
May  6 21:18:22.510: INFO: Pod "pod-projected-configmaps-128751f9-5a70-436a-866c-a190e407b2da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008327745s
STEP: Saw pod success
May  6 21:18:22.510: INFO: Pod "pod-projected-configmaps-128751f9-5a70-436a-866c-a190e407b2da" satisfied condition "success or failure"
May  6 21:18:22.512: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-configmaps-128751f9-5a70-436a-866c-a190e407b2da container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  6 21:18:22.537: INFO: Waiting for pod pod-projected-configmaps-128751f9-5a70-436a-866c-a190e407b2da to disappear
May  6 21:18:22.539: INFO: Pod pod-projected-configmaps-128751f9-5a70-436a-866c-a190e407b2da no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:18:22.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2866" for this suite.
May  6 21:18:28.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:18:28.657: INFO: namespace projected-2866 deletion completed in 6.113729465s

• [SLOW TEST:8.193 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:18:28.657: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May  6 21:18:28.691: INFO: Waiting up to 5m0s for pod "downward-api-3853cb05-37ed-4fb0-8bc1-4639dd2f4d86" in namespace "downward-api-7812" to be "success or failure"
May  6 21:18:28.694: INFO: Pod "downward-api-3853cb05-37ed-4fb0-8bc1-4639dd2f4d86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871666ms
May  6 21:18:30.698: INFO: Pod "downward-api-3853cb05-37ed-4fb0-8bc1-4639dd2f4d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006791872s
STEP: Saw pod success
May  6 21:18:30.698: INFO: Pod "downward-api-3853cb05-37ed-4fb0-8bc1-4639dd2f4d86" satisfied condition "success or failure"
May  6 21:18:30.701: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downward-api-3853cb05-37ed-4fb0-8bc1-4639dd2f4d86 container dapi-container: <nil>
STEP: delete the pod
May  6 21:18:30.722: INFO: Waiting for pod downward-api-3853cb05-37ed-4fb0-8bc1-4639dd2f4d86 to disappear
May  6 21:18:30.724: INFO: Pod downward-api-3853cb05-37ed-4fb0-8bc1-4639dd2f4d86 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:18:30.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7812" for this suite.
May  6 21:18:36.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:18:36.852: INFO: namespace downward-api-7812 deletion completed in 6.122546797s

• [SLOW TEST:8.196 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:18:36.853: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
May  6 21:18:36.887: INFO: Waiting up to 5m0s for pod "var-expansion-50742006-d137-44e2-9d15-ffc9597021f8" in namespace "var-expansion-2309" to be "success or failure"
May  6 21:18:36.890: INFO: Pod "var-expansion-50742006-d137-44e2-9d15-ffc9597021f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.462487ms
May  6 21:18:38.894: INFO: Pod "var-expansion-50742006-d137-44e2-9d15-ffc9597021f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00712235s
STEP: Saw pod success
May  6 21:18:38.894: INFO: Pod "var-expansion-50742006-d137-44e2-9d15-ffc9597021f8" satisfied condition "success or failure"
May  6 21:18:38.897: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod var-expansion-50742006-d137-44e2-9d15-ffc9597021f8 container dapi-container: <nil>
STEP: delete the pod
May  6 21:18:38.915: INFO: Waiting for pod var-expansion-50742006-d137-44e2-9d15-ffc9597021f8 to disappear
May  6 21:18:38.918: INFO: Pod var-expansion-50742006-d137-44e2-9d15-ffc9597021f8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:18:38.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2309" for this suite.
May  6 21:18:44.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:18:45.038: INFO: namespace var-expansion-2309 deletion completed in 6.115555777s

• [SLOW TEST:8.185 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:18:45.038: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:18:45.541: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:18:48.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:18:48.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4583" for this suite.
May  6 21:18:54.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:18:54.899: INFO: namespace webhook-4583 deletion completed in 6.112064881s
STEP: Destroying namespace "webhook-4583-markers" for this suite.
May  6 21:19:00.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:19:01.014: INFO: namespace webhook-4583-markers deletion completed in 6.114566407s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.992 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:19:01.030: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:19:25.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5230" for this suite.
May  6 21:19:31.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:19:31.368: INFO: namespace container-runtime-5230 deletion completed in 6.119536605s

• [SLOW TEST:30.338 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:19:31.368: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May  6 21:19:31.407: INFO: Waiting up to 5m0s for pod "pod-9ed5c775-f8a9-4770-abc3-a937e1e5751b" in namespace "emptydir-5376" to be "success or failure"
May  6 21:19:31.410: INFO: Pod "pod-9ed5c775-f8a9-4770-abc3-a937e1e5751b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928583ms
May  6 21:19:33.414: INFO: Pod "pod-9ed5c775-f8a9-4770-abc3-a937e1e5751b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006785823s
STEP: Saw pod success
May  6 21:19:33.414: INFO: Pod "pod-9ed5c775-f8a9-4770-abc3-a937e1e5751b" satisfied condition "success or failure"
May  6 21:19:33.416: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-9ed5c775-f8a9-4770-abc3-a937e1e5751b container test-container: <nil>
STEP: delete the pod
May  6 21:19:33.436: INFO: Waiting for pod pod-9ed5c775-f8a9-4770-abc3-a937e1e5751b to disappear
May  6 21:19:33.439: INFO: Pod pod-9ed5c775-f8a9-4770-abc3-a937e1e5751b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:19:33.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5376" for this suite.
May  6 21:19:39.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:19:39.555: INFO: namespace emptydir-5376 deletion completed in 6.111581759s

• [SLOW TEST:8.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:19:39.555: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-2qdv
STEP: Creating a pod to test atomic-volume-subpath
May  6 21:19:39.596: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2qdv" in namespace "subpath-402" to be "success or failure"
May  6 21:19:39.598: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268809ms
May  6 21:19:41.601: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005611074s
May  6 21:19:43.605: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 4.009894437s
May  6 21:19:45.609: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 6.013239126s
May  6 21:19:47.612: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 8.016600566s
May  6 21:19:49.616: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 10.020483524s
May  6 21:19:51.619: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 12.023463099s
May  6 21:19:53.649: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 14.053221163s
May  6 21:19:55.653: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 16.057779765s
May  6 21:19:57.656: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 18.060649904s
May  6 21:19:59.660: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Running", Reason="", readiness=true. Elapsed: 20.064188202s
May  6 21:20:01.663: INFO: Pod "pod-subpath-test-downwardapi-2qdv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06735401s
STEP: Saw pod success
May  6 21:20:01.663: INFO: Pod "pod-subpath-test-downwardapi-2qdv" satisfied condition "success or failure"
May  6 21:20:01.666: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-subpath-test-downwardapi-2qdv container test-container-subpath-downwardapi-2qdv: <nil>
STEP: delete the pod
May  6 21:20:01.686: INFO: Waiting for pod pod-subpath-test-downwardapi-2qdv to disappear
May  6 21:20:01.693: INFO: Pod pod-subpath-test-downwardapi-2qdv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2qdv
May  6 21:20:01.693: INFO: Deleting pod "pod-subpath-test-downwardapi-2qdv" in namespace "subpath-402"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:20:01.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-402" for this suite.
May  6 21:20:07.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:20:07.814: INFO: namespace subpath-402 deletion completed in 6.112139515s

• [SLOW TEST:28.259 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:20:07.814: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 21:20:07.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5924'
May  6 21:20:07.918: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  6 21:20:07.918: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
May  6 21:20:07.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5924'
May  6 21:20:07.997: INFO: stderr: ""
May  6 21:20:07.997: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:20:07.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5924" for this suite.
May  6 21:20:14.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:20:14.120: INFO: namespace kubectl-5924 deletion completed in 6.119245302s

• [SLOW TEST:6.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:20:14.120: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:20:14.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 21:20:16.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396814, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396814, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396814, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724396814, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:20:19.694: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:20:19.699: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:20:20.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-785" for this suite.
May  6 21:20:26.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:20:27.010: INFO: namespace webhook-785 deletion completed in 6.121746267s
STEP: Destroying namespace "webhook-785-markers" for this suite.
May  6 21:20:33.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:20:33.122: INFO: namespace webhook-785-markers deletion completed in 6.111866564s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.016 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:20:33.136: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1182.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1182.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1182.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1182.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 21:20:35.190: INFO: DNS probes using dns-test-211c1eaa-f7ce-4cce-983c-e7b60c1b4f53 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1182.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1182.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1182.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1182.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 21:20:37.229: INFO: File wheezy_udp@dns-test-service-3.dns-1182.svc.cluster.local from pod  dns-1182/dns-test-953bcd1f-6de5-4fe4-8c04-b5cccbef4f2e contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 21:20:37.236: INFO: File jessie_udp@dns-test-service-3.dns-1182.svc.cluster.local from pod  dns-1182/dns-test-953bcd1f-6de5-4fe4-8c04-b5cccbef4f2e contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 21:20:37.236: INFO: Lookups using dns-1182/dns-test-953bcd1f-6de5-4fe4-8c04-b5cccbef4f2e failed for: [wheezy_udp@dns-test-service-3.dns-1182.svc.cluster.local jessie_udp@dns-test-service-3.dns-1182.svc.cluster.local]

May  6 21:20:42.244: INFO: DNS probes using dns-test-953bcd1f-6de5-4fe4-8c04-b5cccbef4f2e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1182.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1182.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1182.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1182.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 21:20:44.300: INFO: DNS probes using dns-test-f313d3fe-f5c8-4d86-bb2e-7752d570564f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:20:44.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1182" for this suite.
May  6 21:20:50.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:20:50.467: INFO: namespace dns-1182 deletion completed in 6.120034355s

• [SLOW TEST:17.332 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:20:50.468: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9187.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9187.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9187.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9187.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9187.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 87.120.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.120.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.120.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.120.87_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9187.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9187.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9187.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9187.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9187.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9187.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 87.120.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.120.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.120.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.120.87_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  6 21:20:52.531: INFO: Unable to read wheezy_udp@dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.534: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.538: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.542: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.564: INFO: Unable to read jessie_udp@dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.568: INFO: Unable to read jessie_tcp@dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.571: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.574: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local from pod dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72: the server could not find the requested resource (get pods dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72)
May  6 21:20:52.594: INFO: Lookups using dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72 failed for: [wheezy_udp@dns-test-service.dns-9187.svc.cluster.local wheezy_tcp@dns-test-service.dns-9187.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local jessie_udp@dns-test-service.dns-9187.svc.cluster.local jessie_tcp@dns-test-service.dns-9187.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9187.svc.cluster.local]

May  6 21:20:57.663: INFO: DNS probes using dns-9187/dns-test-50974ac7-90b7-4d31-8da9-14e2651cce72 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:20:57.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9187" for this suite.
May  6 21:21:03.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:21:03.857: INFO: namespace dns-9187 deletion completed in 6.116105999s

• [SLOW TEST:13.390 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:21:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May  6 21:21:03.894: INFO: Waiting up to 5m0s for pod "pod-0d7dcd18-12bc-46a6-b177-32e6b27ea887" in namespace "emptydir-8121" to be "success or failure"
May  6 21:21:03.896: INFO: Pod "pod-0d7dcd18-12bc-46a6-b177-32e6b27ea887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283129ms
May  6 21:21:05.900: INFO: Pod "pod-0d7dcd18-12bc-46a6-b177-32e6b27ea887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005708266s
STEP: Saw pod success
May  6 21:21:05.900: INFO: Pod "pod-0d7dcd18-12bc-46a6-b177-32e6b27ea887" satisfied condition "success or failure"
May  6 21:21:05.903: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-0d7dcd18-12bc-46a6-b177-32e6b27ea887 container test-container: <nil>
STEP: delete the pod
May  6 21:21:05.921: INFO: Waiting for pod pod-0d7dcd18-12bc-46a6-b177-32e6b27ea887 to disappear
May  6 21:21:05.924: INFO: Pod pod-0d7dcd18-12bc-46a6-b177-32e6b27ea887 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:21:05.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8121" for this suite.
May  6 21:21:11.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:21:12.042: INFO: namespace emptydir-8121 deletion completed in 6.114172683s

• [SLOW TEST:8.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:21:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4870
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May  6 21:21:12.080: INFO: Found 0 stateful pods, waiting for 3
May  6 21:21:22.084: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 21:21:22.084: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 21:21:22.084: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  6 21:21:22.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-4870 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 21:21:22.241: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 21:21:22.241: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 21:21:22.241: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May  6 21:21:32.271: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May  6 21:21:42.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-4870 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:21:42.440: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 21:21:42.440: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 21:21:42.440: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 21:21:42.456: INFO: Waiting for StatefulSet statefulset-4870/ss2 to complete update
May  6 21:21:42.456: INFO: Waiting for Pod statefulset-4870/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 21:21:42.456: INFO: Waiting for Pod statefulset-4870/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 21:21:42.456: INFO: Waiting for Pod statefulset-4870/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 21:21:52.464: INFO: Waiting for StatefulSet statefulset-4870/ss2 to complete update
May  6 21:21:52.464: INFO: Waiting for Pod statefulset-4870/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 21:21:52.464: INFO: Waiting for Pod statefulset-4870/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May  6 21:22:02.464: INFO: Waiting for StatefulSet statefulset-4870/ss2 to complete update
STEP: Rolling back to a previous revision
May  6 21:22:12.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-4870 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 21:22:12.603: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 21:22:12.603: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 21:22:12.603: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 21:22:22.637: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May  6 21:22:32.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec --namespace=statefulset-4870 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 21:22:32.806: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 21:22:32.806: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 21:22:32.806: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May  6 21:22:52.826: INFO: Deleting all statefulset in ns statefulset-4870
May  6 21:22:52.828: INFO: Scaling statefulset ss2 to 0
May  6 21:23:22.841: INFO: Waiting for statefulset status.replicas updated to 0
May  6 21:23:22.844: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:23:22.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4870" for this suite.
May  6 21:23:28.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:23:28.975: INFO: namespace statefulset-4870 deletion completed in 6.114393969s

• [SLOW TEST:136.933 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:23:28.975: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May  6 21:23:29.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 create -f - --namespace=kubectl-7451'
May  6 21:23:29.193: INFO: stderr: ""
May  6 21:23:29.193: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  6 21:23:29.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7451'
May  6 21:23:29.261: INFO: stderr: ""
May  6 21:23:29.261: INFO: stdout: "update-demo-nautilus-th9lm update-demo-nautilus-wl7jf "
May  6 21:23:29.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-th9lm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7451'
May  6 21:23:29.324: INFO: stderr: ""
May  6 21:23:29.324: INFO: stdout: ""
May  6 21:23:29.324: INFO: update-demo-nautilus-th9lm is created but not running
May  6 21:23:34.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7451'
May  6 21:23:34.397: INFO: stderr: ""
May  6 21:23:34.397: INFO: stdout: "update-demo-nautilus-th9lm update-demo-nautilus-wl7jf "
May  6 21:23:34.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-th9lm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7451'
May  6 21:23:34.461: INFO: stderr: ""
May  6 21:23:34.462: INFO: stdout: "true"
May  6 21:23:34.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-th9lm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7451'
May  6 21:23:34.524: INFO: stderr: ""
May  6 21:23:34.524: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 21:23:34.524: INFO: validating pod update-demo-nautilus-th9lm
May  6 21:23:34.529: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 21:23:34.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 21:23:34.529: INFO: update-demo-nautilus-th9lm is verified up and running
May  6 21:23:34.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-wl7jf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7451'
May  6 21:23:34.588: INFO: stderr: ""
May  6 21:23:34.588: INFO: stdout: "true"
May  6 21:23:34.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods update-demo-nautilus-wl7jf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7451'
May  6 21:23:34.650: INFO: stderr: ""
May  6 21:23:34.650: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  6 21:23:34.650: INFO: validating pod update-demo-nautilus-wl7jf
May  6 21:23:34.656: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 21:23:34.656: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 21:23:34.656: INFO: update-demo-nautilus-wl7jf is verified up and running
STEP: using delete to clean up resources
May  6 21:23:34.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete --grace-period=0 --force -f - --namespace=kubectl-7451'
May  6 21:23:34.729: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 21:23:34.729: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  6 21:23:34.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7451'
May  6 21:23:34.796: INFO: stderr: "No resources found in kubectl-7451 namespace.\n"
May  6 21:23:34.796: INFO: stdout: ""
May  6 21:23:34.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 get pods -l name=update-demo --namespace=kubectl-7451 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 21:23:34.860: INFO: stderr: ""
May  6 21:23:34.860: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:23:34.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7451" for this suite.
May  6 21:24:02.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:24:02.977: INFO: namespace kubectl-7451 deletion completed in 28.113296767s

• [SLOW TEST:34.002 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:24:02.978: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  6 21:24:03.011: INFO: Waiting up to 5m0s for pod "pod-24af0e72-804c-42f7-8c5a-f62defd0d913" in namespace "emptydir-3366" to be "success or failure"
May  6 21:24:03.014: INFO: Pod "pod-24af0e72-804c-42f7-8c5a-f62defd0d913": Phase="Pending", Reason="", readiness=false. Elapsed: 3.126497ms
May  6 21:24:05.017: INFO: Pod "pod-24af0e72-804c-42f7-8c5a-f62defd0d913": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006169637s
STEP: Saw pod success
May  6 21:24:05.017: INFO: Pod "pod-24af0e72-804c-42f7-8c5a-f62defd0d913" satisfied condition "success or failure"
May  6 21:24:05.020: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-24af0e72-804c-42f7-8c5a-f62defd0d913 container test-container: <nil>
STEP: delete the pod
May  6 21:24:05.045: INFO: Waiting for pod pod-24af0e72-804c-42f7-8c5a-f62defd0d913 to disappear
May  6 21:24:05.048: INFO: Pod pod-24af0e72-804c-42f7-8c5a-f62defd0d913 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:24:05.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3366" for this suite.
May  6 21:24:11.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:24:11.164: INFO: namespace emptydir-3366 deletion completed in 6.112294469s

• [SLOW TEST:8.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:24:11.164: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
May  6 21:24:11.189: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 21:25:11.202: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:25:11.205: INFO: Starting informer...
STEP: Starting pod...
May  6 21:25:11.418: INFO: Pod is running on kube-node-2-kubelet.kubernetes-cluster.mesos. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May  6 21:25:11.430: INFO: Pod wasn't evicted. Proceeding
May  6 21:25:11.430: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May  6 21:26:26.451: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:26:26.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2092" for this suite.
May  6 21:26:54.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:26:54.572: INFO: namespace taint-single-pod-2092 deletion completed in 28.116106445s

• [SLOW TEST:163.408 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:26:54.572: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:26:55.097: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:26:58.115: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:26:58.119: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8061-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:26:59.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2614" for this suite.
May  6 21:27:05.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:27:05.326: INFO: namespace webhook-2614 deletion completed in 6.122989122s
STEP: Destroying namespace "webhook-2614-markers" for this suite.
May  6 21:27:11.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:27:11.461: INFO: namespace webhook-2614-markers deletion completed in 6.135053043s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.902 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:27:11.474: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
May  6 21:27:11.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 cluster-info'
May  6 21:27:11.650: INFO: stderr: ""
May  6 21:27:11.650: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:27:11.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3925" for this suite.
May  6 21:27:17.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:27:17.768: INFO: namespace kubectl-3925 deletion completed in 6.113652247s

• [SLOW TEST:6.294 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:27:17.768: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:27:18.238: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 21:27:20.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724397238, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724397238, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724397238, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724397238, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:27:23.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:27:33.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1604" for this suite.
May  6 21:27:39.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:27:39.500: INFO: namespace webhook-1604 deletion completed in 6.141061291s
STEP: Destroying namespace "webhook-1604-markers" for this suite.
May  6 21:27:45.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:27:45.617: INFO: namespace webhook-1604-markers deletion completed in 6.116991586s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.863 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:27:45.631: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:27:46.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:27:49.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:27:49.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5066" for this suite.
May  6 21:27:55.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:27:55.349: INFO: namespace webhook-5066 deletion completed in 6.125029168s
STEP: Destroying namespace "webhook-5066-markers" for this suite.
May  6 21:28:01.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:28:01.465: INFO: namespace webhook-5066-markers deletion completed in 6.116184564s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.847 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:28:01.478: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May  6 21:28:03.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 exec pod-sharedvolume-5622e7f1-e1dd-467d-b68c-942159e3c4cd -c busybox-main-container --namespace=emptydir-592 -- cat /usr/share/volumeshare/shareddata.txt'
May  6 21:28:03.668: INFO: stderr: ""
May  6 21:28:03.668: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:28:03.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-592" for this suite.
May  6 21:28:09.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:28:09.805: INFO: namespace emptydir-592 deletion completed in 6.131604415s

• [SLOW TEST:8.326 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:28:09.805: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May  6 21:28:09.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2756'
May  6 21:28:09.902: INFO: stderr: ""
May  6 21:28:09.902: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
May  6 21:28:09.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 delete pods e2e-test-httpd-pod --namespace=kubectl-2756'
May  6 21:28:17.857: INFO: stderr: ""
May  6 21:28:17.857: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:28:17.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2756" for this suite.
May  6 21:28:23.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:28:23.977: INFO: namespace kubectl-2756 deletion completed in 6.116102418s

• [SLOW TEST:14.172 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:28:23.977: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-356db6cc-967b-49af-b554-0e39e15e076e
STEP: Creating a pod to test consume configMaps
May  6 21:28:24.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-1be600bb-4d71-4590-a456-53e8fab5e91e" in namespace "configmap-2795" to be "success or failure"
May  6 21:28:24.016: INFO: Pod "pod-configmaps-1be600bb-4d71-4590-a456-53e8fab5e91e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.614726ms
May  6 21:28:26.019: INFO: Pod "pod-configmaps-1be600bb-4d71-4590-a456-53e8fab5e91e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005747392s
STEP: Saw pod success
May  6 21:28:26.019: INFO: Pod "pod-configmaps-1be600bb-4d71-4590-a456-53e8fab5e91e" satisfied condition "success or failure"
May  6 21:28:26.022: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-1be600bb-4d71-4590-a456-53e8fab5e91e container configmap-volume-test: <nil>
STEP: delete the pod
May  6 21:28:26.046: INFO: Waiting for pod pod-configmaps-1be600bb-4d71-4590-a456-53e8fab5e91e to disappear
May  6 21:28:26.049: INFO: Pod pod-configmaps-1be600bb-4d71-4590-a456-53e8fab5e91e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:28:26.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2795" for this suite.
May  6 21:28:32.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:28:32.163: INFO: namespace configmap-2795 deletion completed in 6.10945796s

• [SLOW TEST:8.186 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:28:32.163: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-a94734fe-671c-44b2-ba0e-8dd1a407898c
STEP: Creating a pod to test consume secrets
May  6 21:28:32.201: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-635bcdf1-06cf-4104-a80d-3534ae7645e8" in namespace "projected-9586" to be "success or failure"
May  6 21:28:32.204: INFO: Pod "pod-projected-secrets-635bcdf1-06cf-4104-a80d-3534ae7645e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.572569ms
May  6 21:28:34.208: INFO: Pod "pod-projected-secrets-635bcdf1-06cf-4104-a80d-3534ae7645e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006433745s
STEP: Saw pod success
May  6 21:28:34.208: INFO: Pod "pod-projected-secrets-635bcdf1-06cf-4104-a80d-3534ae7645e8" satisfied condition "success or failure"
May  6 21:28:34.210: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-projected-secrets-635bcdf1-06cf-4104-a80d-3534ae7645e8 container secret-volume-test: <nil>
STEP: delete the pod
May  6 21:28:34.230: INFO: Waiting for pod pod-projected-secrets-635bcdf1-06cf-4104-a80d-3534ae7645e8 to disappear
May  6 21:28:34.233: INFO: Pod pod-projected-secrets-635bcdf1-06cf-4104-a80d-3534ae7645e8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:28:34.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9586" for this suite.
May  6 21:28:40.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:28:40.357: INFO: namespace projected-9586 deletion completed in 6.121180861s

• [SLOW TEST:8.194 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:28:40.357: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-48bd5312-84fa-4410-b473-fcedc535fce9 in namespace container-probe-5351
May  6 21:28:42.396: INFO: Started pod busybox-48bd5312-84fa-4410-b473-fcedc535fce9 in namespace container-probe-5351
STEP: checking the pod's current state and verifying that restartCount is present
May  6 21:28:42.399: INFO: Initial restart count of pod busybox-48bd5312-84fa-4410-b473-fcedc535fce9 is 0
May  6 21:29:30.483: INFO: Restart count of pod container-probe-5351/busybox-48bd5312-84fa-4410-b473-fcedc535fce9 is now 1 (48.084135833s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:29:30.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5351" for this suite.
May  6 21:29:36.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:29:36.626: INFO: namespace container-probe-5351 deletion completed in 6.116166025s

• [SLOW TEST:56.269 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:29:36.626: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  6 21:29:40.806: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 21:29:40.813: INFO: Pod pod-with-poststart-http-hook still exists
May  6 21:29:42.814: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 21:29:42.817: INFO: Pod pod-with-poststart-http-hook still exists
May  6 21:29:44.814: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 21:29:44.817: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:29:44.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-573" for this suite.
May  6 21:30:12.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:30:12.933: INFO: namespace container-lifecycle-hook-573 deletion completed in 28.111804494s

• [SLOW TEST:36.307 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:30:12.934: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-566714c1-f869-4997-b432-17cc15233bb3
STEP: Creating secret with name s-test-opt-upd-2e1462a4-18cb-4df6-ba02-76477c14756d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-566714c1-f869-4997-b432-17cc15233bb3
STEP: Updating secret s-test-opt-upd-2e1462a4-18cb-4df6-ba02-76477c14756d
STEP: Creating secret with name s-test-opt-create-82717f2c-a7c3-4f3b-967c-a43e10a87e3b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:30:17.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6442" for this suite.
May  6 21:30:29.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:30:29.176: INFO: namespace projected-6442 deletion completed in 12.113320103s

• [SLOW TEST:16.242 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:30:29.176: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-s5tf
STEP: Creating a pod to test atomic-volume-subpath
May  6 21:30:29.216: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-s5tf" in namespace "subpath-584" to be "success or failure"
May  6 21:30:29.220: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818621ms
May  6 21:30:31.223: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006667884s
May  6 21:30:33.227: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 4.010678746s
May  6 21:30:35.231: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 6.014407782s
May  6 21:30:37.234: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 8.017733761s
May  6 21:30:39.238: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 10.021401426s
May  6 21:30:41.241: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 12.024492191s
May  6 21:30:43.244: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 14.028002261s
May  6 21:30:45.248: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 16.031928738s
May  6 21:30:47.252: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 18.035057685s
May  6 21:30:49.255: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Running", Reason="", readiness=true. Elapsed: 20.038536785s
May  6 21:30:51.258: INFO: Pod "pod-subpath-test-secret-s5tf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041578393s
STEP: Saw pod success
May  6 21:30:51.258: INFO: Pod "pod-subpath-test-secret-s5tf" satisfied condition "success or failure"
May  6 21:30:51.261: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-subpath-test-secret-s5tf container test-container-subpath-secret-s5tf: <nil>
STEP: delete the pod
May  6 21:30:51.284: INFO: Waiting for pod pod-subpath-test-secret-s5tf to disappear
May  6 21:30:51.286: INFO: Pod pod-subpath-test-secret-s5tf no longer exists
STEP: Deleting pod pod-subpath-test-secret-s5tf
May  6 21:30:51.286: INFO: Deleting pod "pod-subpath-test-secret-s5tf" in namespace "subpath-584"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:30:51.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-584" for this suite.
May  6 21:30:57.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:30:57.429: INFO: namespace subpath-584 deletion completed in 6.131374527s

• [SLOW TEST:28.253 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:30:57.429: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:31:08.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5960" for this suite.
May  6 21:31:14.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:31:14.612: INFO: namespace resourcequota-5960 deletion completed in 6.118119108s

• [SLOW TEST:17.183 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:31:14.612: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May  6 21:31:16.661: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:31:16.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-200" for this suite.
May  6 21:31:22.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:31:22.912: INFO: namespace container-runtime-200 deletion completed in 6.232047219s

• [SLOW TEST:8.300 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:31:22.912: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-fec6b638-f55e-46cd-9cf0-35adcf33a495 in namespace container-probe-6222
May  6 21:31:24.952: INFO: Started pod test-webserver-fec6b638-f55e-46cd-9cf0-35adcf33a495 in namespace container-probe-6222
STEP: checking the pod's current state and verifying that restartCount is present
May  6 21:31:24.955: INFO: Initial restart count of pod test-webserver-fec6b638-f55e-46cd-9cf0-35adcf33a495 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:35:25.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6222" for this suite.
May  6 21:35:31.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:35:31.516: INFO: namespace container-probe-6222 deletion completed in 6.116681954s

• [SLOW TEST:248.604 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:35:31.517: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-803a011d-13fb-4c71-8d04-e09c730c6365
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-803a011d-13fb-4c71-8d04-e09c730c6365
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:35:37.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3616" for this suite.
May  6 21:35:49.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:35:49.728: INFO: namespace projected-3616 deletion completed in 12.114405226s

• [SLOW TEST:18.211 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:35:49.728: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:35:49.789: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May  6 21:35:49.796: INFO: Number of nodes with available pods: 0
May  6 21:35:49.796: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May  6 21:35:49.811: INFO: Number of nodes with available pods: 0
May  6 21:35:49.811: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:50.815: INFO: Number of nodes with available pods: 0
May  6 21:35:50.815: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:51.816: INFO: Number of nodes with available pods: 1
May  6 21:35:51.816: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May  6 21:35:51.829: INFO: Number of nodes with available pods: 1
May  6 21:35:51.829: INFO: Number of running nodes: 0, number of available pods: 1
May  6 21:35:52.832: INFO: Number of nodes with available pods: 0
May  6 21:35:52.832: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May  6 21:35:52.839: INFO: Number of nodes with available pods: 0
May  6 21:35:52.839: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:53.845: INFO: Number of nodes with available pods: 0
May  6 21:35:53.845: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:54.842: INFO: Number of nodes with available pods: 0
May  6 21:35:54.842: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:55.842: INFO: Number of nodes with available pods: 0
May  6 21:35:55.842: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:56.842: INFO: Number of nodes with available pods: 0
May  6 21:35:56.842: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:57.845: INFO: Number of nodes with available pods: 0
May  6 21:35:57.845: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:58.842: INFO: Number of nodes with available pods: 0
May  6 21:35:58.842: INFO: Node kube-node-0-kubelet.kubernetes-cluster.mesos is running more than one daemon pod
May  6 21:35:59.842: INFO: Number of nodes with available pods: 1
May  6 21:35:59.842: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5951, will wait for the garbage collector to delete the pods
May  6 21:35:59.908: INFO: Deleting DaemonSet.extensions daemon-set took: 7.547696ms
May  6 21:36:00.308: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.169312ms
May  6 21:36:08.711: INFO: Number of nodes with available pods: 0
May  6 21:36:08.711: INFO: Number of running nodes: 0, number of available pods: 0
May  6 21:36:08.714: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5951/daemonsets","resourceVersion":"74761"},"items":null}

May  6 21:36:08.717: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5951/pods","resourceVersion":"74761"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:36:08.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5951" for this suite.
May  6 21:36:14.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:36:14.851: INFO: namespace daemonsets-5951 deletion completed in 6.111040906s

• [SLOW TEST:25.123 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:36:14.851: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-173, will wait for the garbage collector to delete the pods
May  6 21:36:16.945: INFO: Deleting Job.batch foo took: 8.050834ms
May  6 21:36:17.345: INFO: Terminating Job.batch foo pods took: 400.189747ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:36:57.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-173" for this suite.
May  6 21:37:03.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:37:04.068: INFO: namespace job-173 deletion completed in 6.115864858s

• [SLOW TEST:49.217 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:37:04.068: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:37:04.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6285" for this suite.
May  6 21:37:10.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:37:10.219: INFO: namespace services-6285 deletion completed in 6.118680461s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.151 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:37:10.219: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-5769/configmap-test-d5b41922-422f-4db8-8c33-55e440ad5e4e
STEP: Creating a pod to test consume configMaps
May  6 21:37:10.262: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b4e911a-1bc5-4bd7-af68-84fc0ebb691d" in namespace "configmap-5769" to be "success or failure"
May  6 21:37:10.285: INFO: Pod "pod-configmaps-2b4e911a-1bc5-4bd7-af68-84fc0ebb691d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.653668ms
May  6 21:37:12.288: INFO: Pod "pod-configmaps-2b4e911a-1bc5-4bd7-af68-84fc0ebb691d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025786036s
STEP: Saw pod success
May  6 21:37:12.288: INFO: Pod "pod-configmaps-2b4e911a-1bc5-4bd7-af68-84fc0ebb691d" satisfied condition "success or failure"
May  6 21:37:12.291: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-2b4e911a-1bc5-4bd7-af68-84fc0ebb691d container env-test: <nil>
STEP: delete the pod
May  6 21:37:12.317: INFO: Waiting for pod pod-configmaps-2b4e911a-1bc5-4bd7-af68-84fc0ebb691d to disappear
May  6 21:37:12.321: INFO: Pod pod-configmaps-2b4e911a-1bc5-4bd7-af68-84fc0ebb691d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:37:12.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5769" for this suite.
May  6 21:37:18.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:37:18.444: INFO: namespace configmap-5769 deletion completed in 6.118182117s

• [SLOW TEST:8.225 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:37:18.444: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
May  6 21:37:18.474: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 21:38:18.486: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:38:18.489: INFO: Starting informer...
STEP: Starting pods...
May  6 21:38:18.703: INFO: Pod1 is running on kube-node-2-kubelet.kubernetes-cluster.mesos. Tainting Node
May  6 21:38:20.917: INFO: Pod2 is running on kube-node-2-kubelet.kubernetes-cluster.mesos. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May  6 21:38:37.855: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  6 21:38:57.853: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:38:57.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7523" for this suite.
May  6 21:39:03.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:39:03.987: INFO: namespace taint-multiple-pods-7523 deletion completed in 6.118641272s

• [SLOW TEST:105.543 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:39:03.987: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May  6 21:39:04.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7cf1745-6f79-43f0-b1dd-ec29d55b2a46" in namespace "downward-api-6676" to be "success or failure"
May  6 21:39:04.022: INFO: Pod "downwardapi-volume-c7cf1745-6f79-43f0-b1dd-ec29d55b2a46": Phase="Pending", Reason="", readiness=false. Elapsed: 3.248571ms
May  6 21:39:06.026: INFO: Pod "downwardapi-volume-c7cf1745-6f79-43f0-b1dd-ec29d55b2a46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006520167s
STEP: Saw pod success
May  6 21:39:06.026: INFO: Pod "downwardapi-volume-c7cf1745-6f79-43f0-b1dd-ec29d55b2a46" satisfied condition "success or failure"
May  6 21:39:06.029: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod downwardapi-volume-c7cf1745-6f79-43f0-b1dd-ec29d55b2a46 container client-container: <nil>
STEP: delete the pod
May  6 21:39:06.053: INFO: Waiting for pod downwardapi-volume-c7cf1745-6f79-43f0-b1dd-ec29d55b2a46 to disappear
May  6 21:39:06.056: INFO: Pod downwardapi-volume-c7cf1745-6f79-43f0-b1dd-ec29d55b2a46 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:39:06.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6676" for this suite.
May  6 21:39:12.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:39:12.168: INFO: namespace downward-api-6676 deletion completed in 6.10809662s

• [SLOW TEST:8.180 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:39:12.168: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-64cb9ea2-7f78-497e-b5ea-51477bf69e37
STEP: Creating a pod to test consume configMaps
May  6 21:39:12.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d22c113-af22-4647-afad-2dd165425944" in namespace "configmap-2219" to be "success or failure"
May  6 21:39:12.206: INFO: Pod "pod-configmaps-8d22c113-af22-4647-afad-2dd165425944": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924987ms
May  6 21:39:14.209: INFO: Pod "pod-configmaps-8d22c113-af22-4647-afad-2dd165425944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006142735s
STEP: Saw pod success
May  6 21:39:14.209: INFO: Pod "pod-configmaps-8d22c113-af22-4647-afad-2dd165425944" satisfied condition "success or failure"
May  6 21:39:14.213: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-configmaps-8d22c113-af22-4647-afad-2dd165425944 container configmap-volume-test: <nil>
STEP: delete the pod
May  6 21:39:14.231: INFO: Waiting for pod pod-configmaps-8d22c113-af22-4647-afad-2dd165425944 to disappear
May  6 21:39:14.234: INFO: Pod pod-configmaps-8d22c113-af22-4647-afad-2dd165425944 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:39:14.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2219" for this suite.
May  6 21:39:20.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:39:20.365: INFO: namespace configmap-2219 deletion completed in 6.127105962s

• [SLOW TEST:8.197 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:39:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6723c2d4-8c02-4aeb-8bcb-ff13a41c7fd8
STEP: Creating a pod to test consume secrets
May  6 21:39:20.403: INFO: Waiting up to 5m0s for pod "pod-secrets-982639e0-debb-4462-aee5-72fa631a77eb" in namespace "secrets-2335" to be "success or failure"
May  6 21:39:20.406: INFO: Pod "pod-secrets-982639e0-debb-4462-aee5-72fa631a77eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.953128ms
May  6 21:39:22.409: INFO: Pod "pod-secrets-982639e0-debb-4462-aee5-72fa631a77eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006360774s
STEP: Saw pod success
May  6 21:39:22.409: INFO: Pod "pod-secrets-982639e0-debb-4462-aee5-72fa631a77eb" satisfied condition "success or failure"
May  6 21:39:22.412: INFO: Trying to get logs from node kube-node-2-kubelet.kubernetes-cluster.mesos pod pod-secrets-982639e0-debb-4462-aee5-72fa631a77eb container secret-volume-test: <nil>
STEP: delete the pod
May  6 21:39:22.430: INFO: Waiting for pod pod-secrets-982639e0-debb-4462-aee5-72fa631a77eb to disappear
May  6 21:39:22.436: INFO: Pod pod-secrets-982639e0-debb-4462-aee5-72fa631a77eb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:39:22.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2335" for this suite.
May  6 21:39:28.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:39:28.554: INFO: namespace secrets-2335 deletion completed in 6.114569943s

• [SLOW TEST:8.189 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:39:28.555: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:39:28.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 version'
May  6 21:39:28.644: INFO: stderr: ""
May  6 21:39:28.644: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:44:51Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9+d2iq.1\", GitCommit:\"d8fe1dd1a7fec3081ac1d6a6451c3d06b9381963\", GitTreeState:\"clean\", BuildDate:\"2020-05-06T13:10:35Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:39:28.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8093" for this suite.
May  6 21:39:34.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:39:34.764: INFO: namespace kubectl-8093 deletion completed in 6.115314229s

• [SLOW TEST:6.210 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:39:34.765: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May  6 21:40:05.319: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0506 21:40:05.319345      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:40:05.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8417" for this suite.
May  6 21:40:11.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:40:11.430: INFO: namespace gc-8417 deletion completed in 6.108092577s

• [SLOW TEST:36.666 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:40:11.431: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-b1b599b0-07a4-4f12-a245-8d4865b42f74
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:40:13.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4564" for this suite.
May  6 21:40:25.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:40:25.606: INFO: namespace configmap-4564 deletion completed in 12.110223841s

• [SLOW TEST:14.175 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:40:25.606: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May  6 21:40:25.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9852 /api/v1/namespaces/watch-9852/configmaps/e2e-watch-test-resource-version 1ba5c905-af2a-4be1-b7b8-1cd3364ea633 75716 0 2020-05-06 21:40:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  6 21:40:25.657: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9852 /api/v1/namespaces/watch-9852/configmaps/e2e-watch-test-resource-version 1ba5c905-af2a-4be1-b7b8-1cd3364ea633 75717 0 2020-05-06 21:40:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:40:25.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9852" for this suite.
May  6 21:40:31.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:40:31.772: INFO: namespace watch-9852 deletion completed in 6.112069322s

• [SLOW TEST:6.166 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:40:31.773: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May  6 21:40:32.137: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 21:40:34.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724398032, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724398032, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63724398032, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63724398032, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May  6 21:40:37.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May  6 21:40:37.177: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:40:37.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5943" for this suite.
May  6 21:40:43.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:40:43.308: INFO: namespace webhook-5943 deletion completed in 6.114538362s
STEP: Destroying namespace "webhook-5943-markers" for this suite.
May  6 21:40:49.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:40:49.455: INFO: namespace webhook-5943-markers deletion completed in 6.147154884s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.697 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:40:49.469: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
May  6 21:40:51.512: INFO: Pod pod-hostip-f1cb215f-df4a-45fe-bc1f-dc450c16e3d0 has hostIP: 9.0.1.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:40:51.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2325" for this suite.
May  6 21:41:03.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:41:03.629: INFO: namespace pods-2325 deletion completed in 12.113689752s

• [SLOW TEST:14.160 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:41:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:41:03.657: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May  6 21:41:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 create -f -'
May  6 21:41:07.334: INFO: stderr: ""
May  6 21:41:07.334: INFO: stdout: "e2e-test-crd-publish-openapi-5366-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  6 21:41:07.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 delete e2e-test-crd-publish-openapi-5366-crds test-foo'
May  6 21:41:07.427: INFO: stderr: ""
May  6 21:41:07.428: INFO: stdout: "e2e-test-crd-publish-openapi-5366-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  6 21:41:07.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 apply -f -'
May  6 21:41:07.670: INFO: stderr: ""
May  6 21:41:07.670: INFO: stdout: "e2e-test-crd-publish-openapi-5366-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  6 21:41:07.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 delete e2e-test-crd-publish-openapi-5366-crds test-foo'
May  6 21:41:07.737: INFO: stderr: ""
May  6 21:41:07.737: INFO: stdout: "e2e-test-crd-publish-openapi-5366-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May  6 21:41:07.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 create -f -'
May  6 21:41:07.914: INFO: rc: 1
May  6 21:41:07.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 apply -f -'
May  6 21:41:08.088: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May  6 21:41:08.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 create -f -'
May  6 21:41:08.221: INFO: rc: 1
May  6 21:41:08.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 --namespace=crd-publish-openapi-376 apply -f -'
May  6 21:41:08.359: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May  6 21:41:08.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-5366-crds'
May  6 21:41:08.506: INFO: stderr: ""
May  6 21:41:08.506: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5366-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May  6 21:41:08.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-5366-crds.metadata'
May  6 21:41:08.688: INFO: stderr: ""
May  6 21:41:08.688: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5366-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  6 21:41:08.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-5366-crds.spec'
May  6 21:41:08.869: INFO: stderr: ""
May  6 21:41:08.869: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5366-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  6 21:41:08.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-5366-crds.spec.bars'
May  6 21:41:09.049: INFO: stderr: ""
May  6 21:41:09.049: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5366-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May  6 21:41:09.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-161989487 explain e2e-test-crd-publish-openapi-5366-crds.spec.bars2'
May  6 21:41:09.184: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:41:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-376" for this suite.
May  6 21:41:18.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:41:18.706: INFO: namespace crd-publish-openapi-376 deletion completed in 6.178267341s

• [SLOW TEST:15.077 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May  6 21:41:18.706: INFO: >>> kubeConfig: /tmp/kubeconfig-161989487
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May  6 21:41:18.750: INFO: Creating deployment "test-recreate-deployment"
May  6 21:41:18.753: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  6 21:41:18.758: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May  6 21:41:20.765: INFO: Waiting deployment "test-recreate-deployment" to complete
May  6 21:41:20.768: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  6 21:41:20.774: INFO: Updating deployment test-recreate-deployment
May  6 21:41:20.774: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May  6 21:41:20.820: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2474 /apis/apps/v1/namespaces/deployment-2474/deployments/test-recreate-deployment 6ec670a6-18a8-4760-948a-ae47e2b85690 76005 2 2020-05-06 21:41:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0047d93c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-06 21:41:20 +0000 UTC,LastTransitionTime:2020-05-06 21:41:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-05-06 21:41:20 +0000 UTC,LastTransitionTime:2020-05-06 21:41:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  6 21:41:20.823: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-2474 /apis/apps/v1/namespaces/deployment-2474/replicasets/test-recreate-deployment-5f94c574ff a1470c01-6f2d-4f14-9541-1f38e0ebaeff 76003 1 2020-05-06 21:41:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6ec670a6-18a8-4760-948a-ae47e2b85690 0xc0047d9a17 0xc0047d9a18}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0047d9ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 21:41:20.823: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  6 21:41:20.823: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-2474 /apis/apps/v1/namespaces/deployment-2474/replicasets/test-recreate-deployment-68fc85c7bb 6162a4f9-a00a-4a48-a278-9e19926c0d9e 75992 2 2020-05-06 21:41:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6ec670a6-18a8-4760-948a-ae47e2b85690 0xc0047d9b47 0xc0047d9b48}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0047d9bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 21:41:20.826: INFO: Pod "test-recreate-deployment-5f94c574ff-s7lz4" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-s7lz4 test-recreate-deployment-5f94c574ff- deployment-2474 /api/v1/namespaces/deployment-2474/pods/test-recreate-deployment-5f94c574ff-s7lz4 79a63a1b-f69c-4b4c-93c5-ba10a5a61a77 76006 0 2020-05-06 21:41:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff a1470c01-6f2d-4f14-9541-1f38e0ebaeff 0xc00475ee97 0xc00475ee98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zh44g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zh44g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zh44g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-2-kubelet.kubernetes-cluster.mesos,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:41:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:41:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:41:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-06 21:41:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:9.0.1.5,PodIP:,StartTime:2020-05-06 21:41:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May  6 21:41:20.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2474" for this suite.
May  6 21:41:26.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  6 21:41:26.938: INFO: namespace deployment-2474 deletion completed in 6.10896292s

• [SLOW TEST:8.232 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSMay  6 21:41:26.939: INFO: Running AfterSuite actions on all nodes
May  6 21:41:26.939: INFO: Running AfterSuite actions on node 1
May  6 21:41:26.939: INFO: Skipping dumping logs from cluster

Ran 276 of 4732 Specs in 6804.993 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4456 Skipped
PASS

Ginkgo ran 1 suite in 1h53m26.072555653s
Test Suite Passed
