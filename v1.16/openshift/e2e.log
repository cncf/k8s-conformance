Dec 12 03:04:26.720: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I1212 03:04:26.720254    1956 e2e.go:92] Starting e2e run "02702353-672b-4a57-aa2b-3bd319a2d11e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576119864 - Will randomize all specs
Will run 15 of 4731 specs

Dec 12 03:04:26.738: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:04:26.743: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Dec 12 03:04:26.853: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 12 03:04:26.932: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 12 03:04:26.932: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Dec 12 03:04:26.932: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 12 03:04:26.957: INFO: e2e test version: v1.16.4-1+8aa1d9dd63e9a4
Dec 12 03:04:26.971: INFO: kube-apiserver version: v1.16.2
Dec 12 03:04:26.971: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:04:26.991: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:04:26.994: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
Dec 12 03:04:27.190: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 12 03:04:27.204: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Dec 12 03:04:27.277: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 03:04:27.295: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-17.ec2.internal before test
Dec 12 03:04:27.344: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-12-12 02:55:39 +0000 UTC (3 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:04:27.344: INFO: ovs-6kvjd from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:04:27.344: INFO: openshift-state-metrics-5466cb9dcd-5sdmv from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 12 03:04:27.344: INFO: community-operators-57df479757-26s9p from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container community-operators ready: true, restart count 0
Dec 12 03:04:27.344: INFO: node-ca-zxkbz from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:04:27.344: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-12-12 02:56:19 +0000 UTC (7 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:04:27.344: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:04:27.344: INFO: telemeter-client-7476f789b5-xk24h from openshift-monitoring started at 2019-12-12 02:54:54 +0000 UTC (3 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container reload ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 12 03:04:27.344: INFO: multus-9ljmq from openshift-multus started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:04:27.344: INFO: sdn-q6qf9 from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:04:27.344: INFO: redhat-operators-7cc8cf78cb-l7qp4 from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container redhat-operators ready: true, restart count 0
Dec 12 03:04:27.344: INFO: router-default-6dd7776f8d-px9mm from openshift-ingress started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container router ready: true, restart count 0
Dec 12 03:04:27.344: INFO: kube-state-metrics-55f44c57bf-lqq5l from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 12 03:04:27.344: INFO: node-exporter-6mcgp from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:04:27.344: INFO: certified-operators-784f95ccfc-28hmb from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container certified-operators ready: true, restart count 0
Dec 12 03:04:27.344: INFO: thanos-querier-6648b6577b-dfq6v from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:04:27.344: INFO: prometheus-adapter-7969f64c9-xg8hs from openshift-monitoring started at 2019-12-12 02:54:58 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:04:27.344: INFO: tuned-b6d4c from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:04:27.344: INFO: dns-default-9gg8k from openshift-dns started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:04:27.344: INFO: machine-config-daemon-fhxhd from openshift-machine-config-operator started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.344: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:04:27.344: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-2.ec2.internal before test
Dec 12 03:04:27.374: INFO: sdn-h9mg9 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:04:27.374: INFO: multus-xprhq from openshift-multus started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:04:27.374: INFO: dns-default-jxhlm from openshift-dns started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:04:27.374: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:04:27.374: INFO: router-default-6dd7776f8d-8q6vk from openshift-ingress started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container router ready: true, restart count 0
Dec 12 03:04:27.374: INFO: node-exporter-n8p7f from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.374: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:04:27.374: INFO: tuned-fgjfd from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:04:27.374: INFO: machine-config-daemon-d8nnk from openshift-machine-config-operator started at 2019-12-12 02:48:34 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:04:27.374: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:04:27.374: INFO: image-registry-8444c9fb8d-sth4j from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container registry ready: true, restart count 0
Dec 12 03:04:27.374: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-12-12 02:55:22 +0000 UTC (3 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:04:27.374: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:04:27.374: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:04:27.374: INFO: node-ca-6cs8f from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:04:27.374: INFO: ovs-99f97 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.374: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:04:27.374: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-151-48.ec2.internal before test
Dec 12 03:04:27.405: INFO: sdn-k452z from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container sdn ready: true, restart count 0
Dec 12 03:04:27.405: INFO: dns-default-d5h9f from openshift-dns started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:04:27.405: INFO: machine-config-daemon-hjf2v from openshift-machine-config-operator started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:04:27.405: INFO: node-ca-fmkp2 from openshift-image-registry started at 2019-12-12 02:50:05 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:04:27.405: INFO: prometheus-adapter-7969f64c9-n4qhv from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:04:27.405: INFO: grafana-5bcb6c54b5-tcpmz from openshift-monitoring started at 2019-12-12 02:55:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container grafana ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 12 03:04:27.405: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-12-12 02:56:33 +0000 UTC (7 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:04:27.405: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:04:27.405: INFO: multus-msl6m from openshift-multus started at 2019-12-12 02:49:00 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:04:27.405: INFO: tuned-qh2kj from openshift-cluster-node-tuning-operator started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:04:27.405: INFO: ovs-fdb6v from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:04:27.405: INFO: node-exporter-wrwwf from openshift-monitoring started at 2019-12-12 02:49:02 +0000 UTC (2 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:04:27.405: INFO: thanos-querier-6648b6577b-5ztxj from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:04:27.405: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:04:27.405: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:04:27.406: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:04:27.406: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:04:27.406: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (3 container statuses recorded)
Dec 12 03:04:27.406: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:04:27.406: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:04:27.406: INFO: 	Container config-reloader ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c1842b84-8dcc-45d8-9d5f-e8b765836c86 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c1842b84-8dcc-45d8-9d5f-e8b765836c86 off the node ip-10-0-133-2.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c1842b84-8dcc-45d8-9d5f-e8b765836c86
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:09:47.731: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-311" for this suite.
Dec 12 03:09:55.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:09:59.326: INFO: namespace sched-pred-311 deletion completed in 11.566170659s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:332.332 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:09:59.329: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:09:59.592: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 12 03:09:59.634: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:09:59.635: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:09:59.635: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:09:59.654: INFO: Number of nodes with available pods: 0
Dec 12 03:09:59.654: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:00.685: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:00.685: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:00.685: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:00.701: INFO: Number of nodes with available pods: 0
Dec 12 03:10:00.701: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:01.685: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:01.685: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:01.685: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:01.702: INFO: Number of nodes with available pods: 0
Dec 12 03:10:01.702: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:02.685: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:02.685: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:02.685: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:02.702: INFO: Number of nodes with available pods: 0
Dec 12 03:10:02.702: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:03.686: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:03.687: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:03.687: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:03.704: INFO: Number of nodes with available pods: 0
Dec 12 03:10:03.704: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:04.689: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:04.689: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:04.689: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:04.705: INFO: Number of nodes with available pods: 0
Dec 12 03:10:04.705: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:05.686: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:05.686: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:05.686: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:05.705: INFO: Number of nodes with available pods: 0
Dec 12 03:10:05.705: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:06.684: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:06.685: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:06.685: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:06.700: INFO: Number of nodes with available pods: 0
Dec 12 03:10:06.700: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:07.685: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:07.685: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:07.685: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:07.700: INFO: Number of nodes with available pods: 0
Dec 12 03:10:07.700: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:08.685: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:08.685: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:08.685: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:08.703: INFO: Number of nodes with available pods: 0
Dec 12 03:10:08.703: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:09.684: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:09.684: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:09.684: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:09.701: INFO: Number of nodes with available pods: 0
Dec 12 03:10:09.701: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:10.685: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:10.685: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:10.685: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:10.702: INFO: Number of nodes with available pods: 0
Dec 12 03:10:10.702: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:11.688: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:11.688: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:11.688: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:11.705: INFO: Number of nodes with available pods: 0
Dec 12 03:10:11.705: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:12.690: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:12.690: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:12.690: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:12.710: INFO: Number of nodes with available pods: 0
Dec 12 03:10:12.710: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:13.684: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:13.684: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:13.684: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:13.702: INFO: Number of nodes with available pods: 2
Dec 12 03:10:13.702: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:10:14.689: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:14.689: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:14.689: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:14.706: INFO: Number of nodes with available pods: 2
Dec 12 03:10:14.706: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:10:15.689: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:15.689: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:15.689: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:15.706: INFO: Number of nodes with available pods: 3
Dec 12 03:10:15.706: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 12 03:10:15.836: INFO: Wrong image for pod: daemon-set-jmwws. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:15.836: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:15.836: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:15.858: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:15.858: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:15.858: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:16.882: INFO: Wrong image for pod: daemon-set-jmwws. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:16.882: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:16.882: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:16.914: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:16.914: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:16.914: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:17.875: INFO: Wrong image for pod: daemon-set-jmwws. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:17.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:17.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:17.904: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:17.904: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:17.904: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:18.874: INFO: Wrong image for pod: daemon-set-jmwws. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:18.874: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:18.874: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:18.903: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:18.903: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:18.903: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:19.876: INFO: Wrong image for pod: daemon-set-jmwws. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:19.876: INFO: Pod daemon-set-jmwws is not available
Dec 12 03:10:19.876: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:19.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:19.907: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:19.907: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:19.907: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:20.875: INFO: Wrong image for pod: daemon-set-jmwws. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:20.875: INFO: Pod daemon-set-jmwws is not available
Dec 12 03:10:20.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:20.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:20.904: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:20.904: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:20.904: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:21.876: INFO: Wrong image for pod: daemon-set-jmwws. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:21.876: INFO: Pod daemon-set-jmwws is not available
Dec 12 03:10:21.876: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:21.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:21.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:21.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:21.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:22.874: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:22.874: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:22.874: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:22.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:22.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:22.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:23.878: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:23.878: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:23.878: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:23.908: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:23.908: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:23.908: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:24.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:24.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:24.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:24.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:24.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:24.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:25.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:25.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:25.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:25.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:25.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:25.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:26.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:26.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:26.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:26.907: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:26.907: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:26.907: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:27.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:27.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:27.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:27.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:27.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:27.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:28.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:28.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:28.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:28.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:28.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:28.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:29.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:29.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:29.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:29.904: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:29.904: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:29.904: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:30.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:30.876: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:30.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:30.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:30.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:30.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:31.876: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:31.876: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:31.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:31.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:31.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:31.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:32.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:32.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:32.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:32.904: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:32.904: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:32.904: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:33.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:33.875: INFO: Pod daemon-set-mbh94 is not available
Dec 12 03:10:33.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:33.904: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:33.904: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:33.904: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:34.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:34.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:34.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:34.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:34.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:35.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:35.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:35.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:35.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:35.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:36.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:36.875: INFO: Pod daemon-set-l859v is not available
Dec 12 03:10:36.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:36.909: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:36.909: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:36.909: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:37.875: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:37.875: INFO: Pod daemon-set-l859v is not available
Dec 12 03:10:37.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:37.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:37.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:37.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:38.876: INFO: Wrong image for pod: daemon-set-l859v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:38.876: INFO: Pod daemon-set-l859v is not available
Dec 12 03:10:38.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:38.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:38.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:38.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:39.876: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:39.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:39.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:39.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:39.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:40.876: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:40.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:40.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:40.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:40.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:41.875: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:41.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:41.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:41.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:41.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:42.875: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:42.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:42.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:42.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:42.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:43.874: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:43.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:43.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:43.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:43.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:44.875: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:44.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:44.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:44.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:44.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:45.876: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:45.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:45.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:45.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:45.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:46.875: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:46.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:46.910: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:46.910: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:46.910: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:47.876: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:47.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:47.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:47.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:47.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:48.876: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:48.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:48.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:48.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:48.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:49.875: INFO: Pod daemon-set-djlt9 is not available
Dec 12 03:10:49.875: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:49.905: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:49.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:49.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:50.877: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:50.907: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:50.907: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:50.907: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:51.876: INFO: Wrong image for pod: daemon-set-qxhth. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 12 03:10:51.876: INFO: Pod daemon-set-qxhth is not available
Dec 12 03:10:51.906: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:51.906: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:51.906: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:52.875: INFO: Pod daemon-set-9wksz is not available
Dec 12 03:10:52.904: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:52.904: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:52.904: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 12 03:10:52.922: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:52.922: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:52.922: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:52.937: INFO: Number of nodes with available pods: 2
Dec 12 03:10:52.937: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:53.968: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:53.968: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:53.968: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:53.985: INFO: Number of nodes with available pods: 2
Dec 12 03:10:53.985: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:54.967: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:54.967: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:54.967: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:54.983: INFO: Number of nodes with available pods: 2
Dec 12 03:10:54.983: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:55.968: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:55.968: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:55.968: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:55.984: INFO: Number of nodes with available pods: 2
Dec 12 03:10:55.984: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:56.968: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:56.968: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:56.968: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:56.984: INFO: Number of nodes with available pods: 2
Dec 12 03:10:56.984: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:57.966: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:57.967: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:57.967: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:57.983: INFO: Number of nodes with available pods: 2
Dec 12 03:10:57.983: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:58.981: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:58.981: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:58.981: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:58.997: INFO: Number of nodes with available pods: 2
Dec 12 03:10:58.997: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:10:59.968: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:10:59.968: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:59.968: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:10:59.984: INFO: Number of nodes with available pods: 2
Dec 12 03:10:59.984: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:00.967: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:00.967: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:00.967: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:00.983: INFO: Number of nodes with available pods: 2
Dec 12 03:11:00.983: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:01.969: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:01.969: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:01.969: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:01.986: INFO: Number of nodes with available pods: 2
Dec 12 03:11:01.986: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:02.967: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:02.967: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:02.967: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:02.983: INFO: Number of nodes with available pods: 2
Dec 12 03:11:02.983: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:03.967: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:03.967: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:03.967: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:03.985: INFO: Number of nodes with available pods: 3
Dec 12 03:11:03.985: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9787, will wait for the garbage collector to delete the pods
Dec 12 03:11:04.151: INFO: Deleting DaemonSet.extensions daemon-set took: 20.744347ms
Dec 12 03:11:04.352: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.292528ms
Dec 12 03:11:12.668: INFO: Number of nodes with available pods: 0
Dec 12 03:11:12.668: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 03:11:12.683: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9787/daemonsets","resourceVersion":"21400"},"items":null}

Dec 12 03:11:12.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9787/pods","resourceVersion":"21400"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:11:12.775: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-9787" for this suite.
Dec 12 03:11:18.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:11:21.145: INFO: namespace daemonsets-9787 deletion completed in 8.340451482s

â€¢ [SLOW TEST:81.815 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:11:21.146: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 12 03:11:21.268: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Dec 12 03:11:21.322: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 03:11:21.340: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-17.ec2.internal before test
Dec 12 03:11:21.389: INFO: ovs-6kvjd from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:11:21.389: INFO: openshift-state-metrics-5466cb9dcd-5sdmv from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 12 03:11:21.389: INFO: community-operators-57df479757-26s9p from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container community-operators ready: true, restart count 0
Dec 12 03:11:21.389: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-12-12 02:55:39 +0000 UTC (3 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:11:21.389: INFO: multus-9ljmq from openshift-multus started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:11:21.389: INFO: sdn-q6qf9 from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:11:21.389: INFO: redhat-operators-7cc8cf78cb-l7qp4 from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container redhat-operators ready: true, restart count 0
Dec 12 03:11:21.389: INFO: node-ca-zxkbz from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:11:21.389: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-12-12 02:56:19 +0000 UTC (7 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:11:21.389: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:11:21.389: INFO: telemeter-client-7476f789b5-xk24h from openshift-monitoring started at 2019-12-12 02:54:54 +0000 UTC (3 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container reload ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 12 03:11:21.389: INFO: router-default-6dd7776f8d-px9mm from openshift-ingress started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container router ready: true, restart count 0
Dec 12 03:11:21.389: INFO: kube-state-metrics-55f44c57bf-lqq5l from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 12 03:11:21.389: INFO: node-exporter-6mcgp from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:11:21.389: INFO: tuned-b6d4c from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:11:21.389: INFO: dns-default-9gg8k from openshift-dns started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:11:21.389: INFO: machine-config-daemon-fhxhd from openshift-machine-config-operator started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: certified-operators-784f95ccfc-28hmb from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container certified-operators ready: true, restart count 0
Dec 12 03:11:21.389: INFO: thanos-querier-6648b6577b-dfq6v from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:11:21.389: INFO: prometheus-adapter-7969f64c9-xg8hs from openshift-monitoring started at 2019-12-12 02:54:58 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.389: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:11:21.389: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-2.ec2.internal before test
Dec 12 03:11:21.432: INFO: ovs-99f97 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:11:21.432: INFO: sdn-h9mg9 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:11:21.432: INFO: multus-xprhq from openshift-multus started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:11:21.432: INFO: dns-default-jxhlm from openshift-dns started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:11:21.432: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:11:21.432: INFO: router-default-6dd7776f8d-8q6vk from openshift-ingress started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container router ready: true, restart count 0
Dec 12 03:11:21.432: INFO: tuned-fgjfd from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:11:21.432: INFO: machine-config-daemon-d8nnk from openshift-machine-config-operator started at 2019-12-12 02:48:34 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:11:21.432: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:11:21.432: INFO: image-registry-8444c9fb8d-sth4j from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container registry ready: true, restart count 0
Dec 12 03:11:21.432: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-12-12 02:55:22 +0000 UTC (3 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:11:21.432: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:11:21.432: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:11:21.432: INFO: node-exporter-n8p7f from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.432: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:11:21.432: INFO: node-ca-6cs8f from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.432: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:11:21.432: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-151-48.ec2.internal before test
Dec 12 03:11:21.488: INFO: dns-default-d5h9f from openshift-dns started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.488: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:11:21.488: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:11:21.488: INFO: machine-config-daemon-hjf2v from openshift-machine-config-operator started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.488: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:11:21.488: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:11:21.488: INFO: node-ca-fmkp2 from openshift-image-registry started at 2019-12-12 02:50:05 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.488: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:11:21.488: INFO: prometheus-adapter-7969f64c9-n4qhv from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.488: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:11:21.488: INFO: grafana-5bcb6c54b5-tcpmz from openshift-monitoring started at 2019-12-12 02:55:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.488: INFO: 	Container grafana ready: true, restart count 0
Dec 12 03:11:21.488: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 12 03:11:21.488: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-12-12 02:56:33 +0000 UTC (7 container statuses recorded)
Dec 12 03:11:21.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.488: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:11:21.488: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:11:21.489: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:11:21.489: INFO: multus-msl6m from openshift-multus started at 2019-12-12 02:49:00 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.489: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:11:21.489: INFO: tuned-qh2kj from openshift-cluster-node-tuning-operator started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.489: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:11:21.489: INFO: ovs-fdb6v from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.489: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:11:21.489: INFO: node-exporter-wrwwf from openshift-monitoring started at 2019-12-12 02:49:02 +0000 UTC (2 container statuses recorded)
Dec 12 03:11:21.489: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:11:21.489: INFO: thanos-querier-6648b6577b-5ztxj from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:11:21.489: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:11:21.489: INFO: sdn-k452z from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:11:21.489: INFO: 	Container sdn ready: true, restart count 0
Dec 12 03:11:21.489: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (3 container statuses recorded)
Dec 12 03:11:21.489: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:11:21.489: INFO: 	Container config-reloader ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15df810ba658fc76], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match node selector, 3 node(s) were unschedulable.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15df810ba71424f6], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match node selector, 3 node(s) were unschedulable.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:11:22.663: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-3271" for this suite.
Dec 12 03:11:28.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:11:31.033: INFO: namespace sched-pred-3271 deletion completed in 8.326525863s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:9.888 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:11:31.037: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 12 03:11:31.337: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:31.337: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:31.337: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:31.354: INFO: Number of nodes with available pods: 0
Dec 12 03:11:31.354: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:32.402: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:32.402: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:32.402: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:32.418: INFO: Number of nodes with available pods: 0
Dec 12 03:11:32.418: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:33.400: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:33.400: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:33.400: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:33.419: INFO: Number of nodes with available pods: 0
Dec 12 03:11:33.419: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:34.401: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:34.402: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:34.402: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:34.419: INFO: Number of nodes with available pods: 0
Dec 12 03:11:34.419: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:35.396: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:35.396: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:35.396: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:35.412: INFO: Number of nodes with available pods: 0
Dec 12 03:11:35.412: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:36.398: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:36.398: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:36.398: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:36.415: INFO: Number of nodes with available pods: 0
Dec 12 03:11:36.415: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:37.397: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:37.397: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:37.397: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:37.414: INFO: Number of nodes with available pods: 0
Dec 12 03:11:37.414: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:38.397: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:38.397: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:38.397: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:38.413: INFO: Number of nodes with available pods: 0
Dec 12 03:11:38.413: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:39.398: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:39.398: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:39.398: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:39.418: INFO: Number of nodes with available pods: 1
Dec 12 03:11:39.418: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:11:40.402: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:40.402: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:40.402: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:40.418: INFO: Number of nodes with available pods: 3
Dec 12 03:11:40.418: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 12 03:11:40.489: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:40.489: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:40.489: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:40.509: INFO: Number of nodes with available pods: 2
Dec 12 03:11:40.509: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:41.557: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:41.557: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:41.557: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:41.574: INFO: Number of nodes with available pods: 2
Dec 12 03:11:41.574: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:42.552: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:42.552: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:42.552: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:42.568: INFO: Number of nodes with available pods: 2
Dec 12 03:11:42.568: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:43.553: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:43.553: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:43.553: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:43.568: INFO: Number of nodes with available pods: 2
Dec 12 03:11:43.568: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:44.553: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:44.553: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:44.553: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:44.570: INFO: Number of nodes with available pods: 2
Dec 12 03:11:44.570: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:45.555: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:45.555: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:45.555: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:45.572: INFO: Number of nodes with available pods: 2
Dec 12 03:11:45.572: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:46.555: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:46.555: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:46.555: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:46.573: INFO: Number of nodes with available pods: 2
Dec 12 03:11:46.573: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:47.555: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:47.555: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:47.555: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:47.572: INFO: Number of nodes with available pods: 2
Dec 12 03:11:47.572: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:48.554: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:48.554: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:48.554: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:48.570: INFO: Number of nodes with available pods: 2
Dec 12 03:11:48.570: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:11:49.552: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:11:49.552: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:49.552: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:11:49.568: INFO: Number of nodes with available pods: 3
Dec 12 03:11:49.568: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4123, will wait for the garbage collector to delete the pods
Dec 12 03:11:49.689: INFO: Deleting DaemonSet.extensions daemon-set took: 21.220139ms
Dec 12 03:11:49.890: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.923132ms
Dec 12 03:12:02.706: INFO: Number of nodes with available pods: 0
Dec 12 03:12:02.706: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 03:12:02.729: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4123/daemonsets","resourceVersion":"21857"},"items":null}

Dec 12 03:12:02.748: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4123/pods","resourceVersion":"21857"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:12:02.829: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-4123" for this suite.
Dec 12 03:12:08.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:12:11.239: INFO: namespace daemonsets-4123 deletion completed in 8.373563037s

â€¢ [SLOW TEST:40.202 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:12:11.242: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Dec 12 03:12:11.402: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 12 03:13:11.677: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:13:11.699: INFO: Starting informer...
STEP: Starting pods...
Dec 12 03:13:11.769: INFO: Pod1 is running on ip-10-0-133-2.ec2.internal. Tainting Node
Dec 12 03:13:21.871: INFO: Pod2 is running on ip-10-0-133-2.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 12 03:13:32.764: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 12 03:13:52.642: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:13:52.695: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-868" for this suite.
Dec 12 03:13:58.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:14:01.044: INFO: namespace taint-multiple-pods-868 deletion completed in 8.318696431s

â€¢ [SLOW TEST:109.802 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:14:01.045: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 12 03:14:02.167: INFO: Pod name wrapped-volume-race-97f52f72-70b3-4bdd-8c2b-30af99739ca5: Found 0 pods out of 5
Dec 12 03:14:07.203: INFO: Pod name wrapped-volume-race-97f52f72-70b3-4bdd-8c2b-30af99739ca5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-97f52f72-70b3-4bdd-8c2b-30af99739ca5 in namespace emptydir-wrapper-9502, will wait for the garbage collector to delete the pods
Dec 12 03:14:15.415: INFO: Deleting ReplicationController wrapped-volume-race-97f52f72-70b3-4bdd-8c2b-30af99739ca5 took: 21.778336ms
Dec 12 03:14:15.517: INFO: Terminating ReplicationController wrapped-volume-race-97f52f72-70b3-4bdd-8c2b-30af99739ca5 pods took: 102.247436ms
STEP: Creating RC which spawns configmap-volume pods
Dec 12 03:14:59.275: INFO: Pod name wrapped-volume-race-f01232c2-60a4-4e8e-9e4a-bc78e4f8dfe8: Found 0 pods out of 5
Dec 12 03:15:04.311: INFO: Pod name wrapped-volume-race-f01232c2-60a4-4e8e-9e4a-bc78e4f8dfe8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f01232c2-60a4-4e8e-9e4a-bc78e4f8dfe8 in namespace emptydir-wrapper-9502, will wait for the garbage collector to delete the pods
Dec 12 03:15:10.505: INFO: Deleting ReplicationController wrapped-volume-race-f01232c2-60a4-4e8e-9e4a-bc78e4f8dfe8 took: 22.499115ms
Dec 12 03:15:10.705: INFO: Terminating ReplicationController wrapped-volume-race-f01232c2-60a4-4e8e-9e4a-bc78e4f8dfe8 pods took: 200.25847ms
STEP: Creating RC which spawns configmap-volume pods
Dec 12 03:15:49.364: INFO: Pod name wrapped-volume-race-6bcc6633-ea42-4843-902e-a06bd31e6d4f: Found 0 pods out of 5
Dec 12 03:15:54.398: INFO: Pod name wrapped-volume-race-6bcc6633-ea42-4843-902e-a06bd31e6d4f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6bcc6633-ea42-4843-902e-a06bd31e6d4f in namespace emptydir-wrapper-9502, will wait for the garbage collector to delete the pods
Dec 12 03:16:00.592: INFO: Deleting ReplicationController wrapped-volume-race-6bcc6633-ea42-4843-902e-a06bd31e6d4f took: 22.408381ms
Dec 12 03:16:00.793: INFO: Terminating ReplicationController wrapped-volume-race-6bcc6633-ea42-4843-902e-a06bd31e6d4f pods took: 200.660617ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:16:40.347: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9502" for this suite.
Dec 12 03:16:48.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:16:50.757: INFO: namespace emptydir-wrapper-9502 deletion completed in 10.37875003s

â€¢ [SLOW TEST:169.711 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:16:50.760: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:16:50.989: INFO: Create a RollingUpdate DaemonSet
Dec 12 03:16:51.011: INFO: Check that daemon pods launch on every node of the cluster
Dec 12 03:16:51.029: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:51.029: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:51.029: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:51.047: INFO: Number of nodes with available pods: 0
Dec 12 03:16:51.047: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:52.092: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:52.092: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:52.092: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:52.111: INFO: Number of nodes with available pods: 0
Dec 12 03:16:52.111: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:53.091: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:53.091: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:53.091: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:53.107: INFO: Number of nodes with available pods: 0
Dec 12 03:16:53.107: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:54.096: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:54.096: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:54.096: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:54.113: INFO: Number of nodes with available pods: 0
Dec 12 03:16:54.113: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:55.091: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:55.091: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:55.091: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:55.108: INFO: Number of nodes with available pods: 0
Dec 12 03:16:55.108: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:56.090: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:56.090: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:56.090: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:56.106: INFO: Number of nodes with available pods: 0
Dec 12 03:16:56.106: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:57.096: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:57.096: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:57.096: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:57.113: INFO: Number of nodes with available pods: 0
Dec 12 03:16:57.113: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:58.096: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:58.097: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:58.097: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:58.115: INFO: Number of nodes with available pods: 0
Dec 12 03:16:58.115: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:16:59.093: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:16:59.093: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:59.093: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:16:59.112: INFO: Number of nodes with available pods: 1
Dec 12 03:16:59.112: INFO: Node ip-10-0-133-2.ec2.internal is running more than one daemon pod
Dec 12 03:17:00.090: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:00.090: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:00.090: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:00.107: INFO: Number of nodes with available pods: 3
Dec 12 03:17:00.107: INFO: Number of running nodes: 3, number of available pods: 3
Dec 12 03:17:00.107: INFO: Update the DaemonSet to trigger a rollout
Dec 12 03:17:00.155: INFO: Updating DaemonSet daemon-set
Dec 12 03:17:13.237: INFO: Roll back the DaemonSet before rollout is complete
Dec 12 03:17:13.272: INFO: Updating DaemonSet daemon-set
Dec 12 03:17:13.272: INFO: Make sure DaemonSet rollback is complete
Dec 12 03:17:13.289: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:13.289: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:13.307: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:13.307: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:13.308: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:14.326: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:14.326: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:14.359: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:14.359: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:14.359: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:15.325: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:15.325: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:15.356: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:15.356: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:15.356: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:16.325: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:16.325: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:16.355: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:16.355: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:16.355: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:17.328: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:17.328: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:17.357: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:17.357: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:17.357: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:18.325: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:18.325: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:18.356: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:18.356: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:18.356: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:19.325: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:19.325: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:19.355: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:19.355: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:19.355: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:20.326: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:20.326: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:20.356: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:20.356: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:20.356: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:21.325: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:21.325: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:21.355: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:21.355: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:21.355: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:22.325: INFO: Wrong image for pod: daemon-set-xdvvp. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 12 03:17:22.326: INFO: Pod daemon-set-xdvvp is not available
Dec 12 03:17:22.357: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:22.357: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:22.357: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:23.326: INFO: Pod daemon-set-bn8r2 is not available
Dec 12 03:17:23.358: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:17:23.358: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:17:23.358: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7887, will wait for the garbage collector to delete the pods
Dec 12 03:17:23.489: INFO: Deleting DaemonSet.extensions daemon-set took: 26.206732ms
Dec 12 03:17:23.693: INFO: Terminating DaemonSet.extensions daemon-set pods took: 204.752579ms
Dec 12 03:17:32.711: INFO: Number of nodes with available pods: 0
Dec 12 03:17:32.711: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 03:17:32.728: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7887/daemonsets","resourceVersion":"24265"},"items":null}

Dec 12 03:17:32.743: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7887/pods","resourceVersion":"24265"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:17:32.821: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-7887" for this suite.
Dec 12 03:17:38.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:17:41.207: INFO: namespace daemonsets-7887 deletion completed in 8.355625132s

â€¢ [SLOW TEST:50.446 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:17:41.209: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Dec 12 03:17:41.357: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 12 03:18:41.601: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:18:41.619: INFO: Starting informer...
STEP: Starting pod...
Dec 12 03:18:41.676: INFO: Pod is running on ip-10-0-133-2.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 12 03:18:41.748: INFO: Pod wasn't evicted. Proceeding
Dec 12 03:18:41.748: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 12 03:19:56.800: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:19:56.801: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2302" for this suite.
Dec 12 03:20:24.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:20:27.226: INFO: namespace taint-single-pod-2302 deletion completed in 30.377928935s

â€¢ [SLOW TEST:166.017 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:20:27.231: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 12 03:20:27.360: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Dec 12 03:20:27.417: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 03:20:27.436: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-17.ec2.internal before test
Dec 12 03:20:27.490: INFO: thanos-querier-6648b6577b-dfq6v from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:20:27.490: INFO: prometheus-adapter-7969f64c9-xg8hs from openshift-monitoring started at 2019-12-12 02:54:58 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:20:27.490: INFO: tuned-b6d4c from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:20:27.490: INFO: dns-default-9gg8k from openshift-dns started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:20:27.490: INFO: machine-config-daemon-fhxhd from openshift-machine-config-operator started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: certified-operators-784f95ccfc-28hmb from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container certified-operators ready: true, restart count 0
Dec 12 03:20:27.490: INFO: ovs-6kvjd from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:20:27.490: INFO: openshift-state-metrics-5466cb9dcd-5sdmv from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 12 03:20:27.490: INFO: community-operators-57df479757-26s9p from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container community-operators ready: true, restart count 0
Dec 12 03:20:27.490: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-12-12 02:55:39 +0000 UTC (3 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:20:27.490: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-12-12 02:56:19 +0000 UTC (7 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:20:27.490: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:20:27.490: INFO: telemeter-client-7476f789b5-xk24h from openshift-monitoring started at 2019-12-12 02:54:54 +0000 UTC (3 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container reload ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 12 03:20:27.490: INFO: multus-9ljmq from openshift-multus started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:20:27.490: INFO: sdn-q6qf9 from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:20:27.490: INFO: redhat-operators-7cc8cf78cb-l7qp4 from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container redhat-operators ready: true, restart count 0
Dec 12 03:20:27.490: INFO: node-ca-zxkbz from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:20:27.490: INFO: router-default-6dd7776f8d-px9mm from openshift-ingress started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container router ready: true, restart count 0
Dec 12 03:20:27.490: INFO: kube-state-metrics-55f44c57bf-lqq5l from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:20:27.490: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:20:27.490: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 12 03:20:27.490: INFO: node-exporter-6mcgp from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.491: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.491: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:20:27.491: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-2.ec2.internal before test
Dec 12 03:20:27.519: INFO: machine-config-daemon-8lzxl from openshift-machine-config-operator started at 2019-12-12 03:18:52 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:20:27.519: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:20:27.519: INFO: node-ca-5jctk from openshift-image-registry started at 2019-12-12 03:18:52 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:20:27.519: INFO: ovs-99f97 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:20:27.519: INFO: sdn-h9mg9 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:20:27.519: INFO: multus-xprhq from openshift-multus started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:20:27.519: INFO: dns-default-fz8bj from openshift-dns started at 2019-12-12 03:18:52 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:20:27.519: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:20:27.519: INFO: tuned-fgjfd from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:20:27.519: INFO: node-exporter-n8p7f from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.519: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.519: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:20:27.519: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-151-48.ec2.internal before test
Dec 12 03:20:27.570: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (3 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:20:27.570: INFO: router-default-6dd7776f8d-85842 from openshift-ingress started at 2019-12-12 03:13:22 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container router ready: true, restart count 0
Dec 12 03:20:27.570: INFO: image-registry-8444c9fb8d-mk2h4 from openshift-image-registry started at 2019-12-12 03:13:22 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container registry ready: true, restart count 0
Dec 12 03:20:27.570: INFO: dns-default-d5h9f from openshift-dns started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:20:27.570: INFO: machine-config-daemon-hjf2v from openshift-machine-config-operator started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: node-ca-fmkp2 from openshift-image-registry started at 2019-12-12 02:50:05 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:20:27.570: INFO: prometheus-adapter-7969f64c9-n4qhv from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:20:27.570: INFO: grafana-5bcb6c54b5-tcpmz from openshift-monitoring started at 2019-12-12 02:55:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container grafana ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-12-12 03:13:32 +0000 UTC (3 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:20:27.570: INFO: multus-msl6m from openshift-multus started at 2019-12-12 02:49:00 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:20:27.570: INFO: tuned-qh2kj from openshift-cluster-node-tuning-operator started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:20:27.570: INFO: ovs-fdb6v from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:20:27.570: INFO: node-exporter-wrwwf from openshift-monitoring started at 2019-12-12 02:49:02 +0000 UTC (2 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:20:27.570: INFO: thanos-querier-6648b6577b-5ztxj from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:20:27.570: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-12-12 02:56:33 +0000 UTC (7 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:20:27.570: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:20:27.570: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:20:27.570: INFO: sdn-k452z from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:20:27.570: INFO: 	Container sdn ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-25116452-8411-4f03-9dc4-3b1e54485c3d 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-25116452-8411-4f03-9dc4-3b1e54485c3d off the node ip-10-0-133-2.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-25116452-8411-4f03-9dc4-3b1e54485c3d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:21:15.942: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-1042" for this suite.
Dec 12 03:21:38.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:21:40.341: INFO: namespace sched-pred-1042 deletion completed in 24.370169201s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:73.110 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:21:40.342: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:21:40.602: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 12 03:21:40.646: INFO: Number of nodes with available pods: 0
Dec 12 03:21:40.646: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 12 03:21:40.722: INFO: Number of nodes with available pods: 0
Dec 12 03:21:40.722: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:41.742: INFO: Number of nodes with available pods: 0
Dec 12 03:21:41.742: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:42.742: INFO: Number of nodes with available pods: 0
Dec 12 03:21:42.742: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:43.738: INFO: Number of nodes with available pods: 0
Dec 12 03:21:43.738: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:44.739: INFO: Number of nodes with available pods: 0
Dec 12 03:21:44.739: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:45.741: INFO: Number of nodes with available pods: 0
Dec 12 03:21:45.741: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:46.741: INFO: Number of nodes with available pods: 0
Dec 12 03:21:46.741: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:47.740: INFO: Number of nodes with available pods: 0
Dec 12 03:21:47.740: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:48.738: INFO: Number of nodes with available pods: 0
Dec 12 03:21:48.738: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:49.741: INFO: Number of nodes with available pods: 1
Dec 12 03:21:49.741: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 12 03:21:49.810: INFO: Number of nodes with available pods: 0
Dec 12 03:21:49.810: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 12 03:21:49.845: INFO: Number of nodes with available pods: 0
Dec 12 03:21:49.845: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:50.862: INFO: Number of nodes with available pods: 0
Dec 12 03:21:50.862: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:51.863: INFO: Number of nodes with available pods: 0
Dec 12 03:21:51.863: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:52.862: INFO: Number of nodes with available pods: 0
Dec 12 03:21:52.862: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:53.866: INFO: Number of nodes with available pods: 0
Dec 12 03:21:53.866: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:54.864: INFO: Number of nodes with available pods: 0
Dec 12 03:21:54.864: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:55.861: INFO: Number of nodes with available pods: 0
Dec 12 03:21:55.861: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:56.864: INFO: Number of nodes with available pods: 0
Dec 12 03:21:56.864: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:57.862: INFO: Number of nodes with available pods: 0
Dec 12 03:21:57.862: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:58.867: INFO: Number of nodes with available pods: 0
Dec 12 03:21:58.867: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:21:59.862: INFO: Number of nodes with available pods: 0
Dec 12 03:21:59.862: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:22:00.863: INFO: Number of nodes with available pods: 0
Dec 12 03:22:00.863: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:22:01.863: INFO: Number of nodes with available pods: 1
Dec 12 03:22:01.863: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7651, will wait for the garbage collector to delete the pods
Dec 12 03:22:01.986: INFO: Deleting DaemonSet.extensions daemon-set took: 21.410829ms
Dec 12 03:22:02.186: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.2161ms
Dec 12 03:22:09.203: INFO: Number of nodes with available pods: 0
Dec 12 03:22:09.203: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 03:22:09.218: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7651/daemonsets","resourceVersion":"25905"},"items":null}

Dec 12 03:22:09.233: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7651/pods","resourceVersion":"25905"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:22:09.333: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-7651" for this suite.
Dec 12 03:22:15.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:22:17.730: INFO: namespace daemonsets-7651 deletion completed in 8.365186212s

â€¢ [SLOW TEST:37.389 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:22:17.730: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:22:24.214: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-201" for this suite.
Dec 12 03:22:30.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:22:32.607: INFO: namespace namespaces-201 deletion completed in 8.336696704s
STEP: Destroying namespace "nsdeletetest-1643" for this suite.
Dec 12 03:22:32.623: INFO: Namespace nsdeletetest-1643 was already deleted
STEP: Destroying namespace "nsdeletetest-805" for this suite.
Dec 12 03:22:38.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:22:40.921: INFO: namespace nsdeletetest-805 deletion completed in 8.298046199s

â€¢ [SLOW TEST:23.190 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:22:40.922: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:23:19.476: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-5533" for this suite.
Dec 12 03:23:25.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:23:27.848: INFO: namespace namespaces-5533 deletion completed in 8.317668719s
STEP: Destroying namespace "nsdeletetest-5160" for this suite.
Dec 12 03:23:27.866: INFO: Namespace nsdeletetest-5160 was already deleted
STEP: Destroying namespace "nsdeletetest-4901" for this suite.
Dec 12 03:23:33.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:23:36.225: INFO: namespace nsdeletetest-4901 deletion completed in 8.358912815s

â€¢ [SLOW TEST:55.302 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:23:36.227: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 12 03:23:36.360: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Dec 12 03:23:36.413: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 03:23:36.435: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-17.ec2.internal before test
Dec 12 03:23:36.487: INFO: ovs-6kvjd from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:23:36.487: INFO: openshift-state-metrics-5466cb9dcd-5sdmv from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 12 03:23:36.487: INFO: community-operators-57df479757-26s9p from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container community-operators ready: true, restart count 0
Dec 12 03:23:36.487: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-12-12 02:55:39 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:23:36.487: INFO: multus-9ljmq from openshift-multus started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:23:36.487: INFO: sdn-q6qf9 from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:23:36.487: INFO: redhat-operators-7cc8cf78cb-l7qp4 from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container redhat-operators ready: true, restart count 0
Dec 12 03:23:36.487: INFO: node-ca-zxkbz from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:23:36.487: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-12-12 02:56:19 +0000 UTC (7 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:23:36.487: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:23:36.487: INFO: telemeter-client-7476f789b5-xk24h from openshift-monitoring started at 2019-12-12 02:54:54 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container reload ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 12 03:23:36.487: INFO: router-default-6dd7776f8d-px9mm from openshift-ingress started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container router ready: true, restart count 0
Dec 12 03:23:36.487: INFO: kube-state-metrics-55f44c57bf-lqq5l from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:23:36.487: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 12 03:23:36.487: INFO: node-exporter-6mcgp from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.487: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.488: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:23:36.488: INFO: tuned-b6d4c from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.488: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:23:36.488: INFO: dns-default-9gg8k from openshift-dns started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.488: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:23:36.488: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:23:36.488: INFO: machine-config-daemon-fhxhd from openshift-machine-config-operator started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.488: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:23:36.488: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:36.488: INFO: certified-operators-784f95ccfc-28hmb from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.488: INFO: 	Container certified-operators ready: true, restart count 0
Dec 12 03:23:36.488: INFO: thanos-querier-6648b6577b-dfq6v from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:23:36.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.488: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:36.488: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:36.488: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:23:36.488: INFO: prometheus-adapter-7969f64c9-xg8hs from openshift-monitoring started at 2019-12-12 02:54:58 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.488: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:23:36.488: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-2.ec2.internal before test
Dec 12 03:23:36.518: INFO: node-exporter-n8p7f from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.518: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:23:36.518: INFO: tuned-fgjfd from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:23:36.518: INFO: machine-config-daemon-8lzxl from openshift-machine-config-operator started at 2019-12-12 03:18:52 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:23:36.518: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:36.518: INFO: node-ca-5jctk from openshift-image-registry started at 2019-12-12 03:18:52 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:23:36.518: INFO: ovs-99f97 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:23:36.518: INFO: sdn-h9mg9 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:23:36.518: INFO: multus-xprhq from openshift-multus started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:23:36.518: INFO: dns-default-fz8bj from openshift-dns started at 2019-12-12 03:18:52 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.518: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:23:36.518: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:23:36.518: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-151-48.ec2.internal before test
Dec 12 03:23:36.562: INFO: sdn-k452z from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container sdn ready: true, restart count 0
Dec 12 03:23:36.562: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:23:36.562: INFO: router-default-6dd7776f8d-85842 from openshift-ingress started at 2019-12-12 03:13:22 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container router ready: true, restart count 0
Dec 12 03:23:36.562: INFO: image-registry-8444c9fb8d-mk2h4 from openshift-image-registry started at 2019-12-12 03:13:22 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container registry ready: true, restart count 0
Dec 12 03:23:36.562: INFO: dns-default-d5h9f from openshift-dns started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:23:36.562: INFO: machine-config-daemon-hjf2v from openshift-machine-config-operator started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: node-ca-fmkp2 from openshift-image-registry started at 2019-12-12 02:50:05 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:23:36.562: INFO: prometheus-adapter-7969f64c9-n4qhv from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:23:36.562: INFO: grafana-5bcb6c54b5-tcpmz from openshift-monitoring started at 2019-12-12 02:55:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container grafana ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-12-12 03:13:32 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:23:36.562: INFO: multus-msl6m from openshift-multus started at 2019-12-12 02:49:00 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:23:36.562: INFO: tuned-qh2kj from openshift-cluster-node-tuning-operator started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:23:36.562: INFO: ovs-fdb6v from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:23:36.562: INFO: node-exporter-wrwwf from openshift-monitoring started at 2019-12-12 02:49:02 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:23:36.562: INFO: thanos-querier-6648b6577b-5ztxj from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:23:36.562: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-12-12 02:56:33 +0000 UTC (7 container statuses recorded)
Dec 12 03:23:36.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:23:36.562: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:23:36.562: INFO: 	Container thanos-sidecar ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node ip-10-0-133-17.ec2.internal
STEP: verifying the node has the label node ip-10-0-133-2.ec2.internal
STEP: verifying the node has the label node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod tuned-b6d4c requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod tuned-fgjfd requesting resource cpu=10m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod tuned-qh2kj requesting resource cpu=10m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod dns-default-9gg8k requesting resource cpu=110m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod dns-default-d5h9f requesting resource cpu=110m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod dns-default-fz8bj requesting resource cpu=110m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod image-registry-8444c9fb8d-mk2h4 requesting resource cpu=100m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod node-ca-5jctk requesting resource cpu=10m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod node-ca-fmkp2 requesting resource cpu=10m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod node-ca-zxkbz requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod router-default-6dd7776f8d-85842 requesting resource cpu=100m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod router-default-6dd7776f8d-px9mm requesting resource cpu=100m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod machine-config-daemon-8lzxl requesting resource cpu=40m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod machine-config-daemon-fhxhd requesting resource cpu=40m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod machine-config-daemon-hjf2v requesting resource cpu=40m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod certified-operators-784f95ccfc-28hmb requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod community-operators-57df479757-26s9p requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod redhat-operators-7cc8cf78cb-l7qp4 requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod grafana-5bcb6c54b5-tcpmz requesting resource cpu=100m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod kube-state-metrics-55f44c57bf-lqq5l requesting resource cpu=30m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod node-exporter-6mcgp requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod node-exporter-n8p7f requesting resource cpu=10m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod node-exporter-wrwwf requesting resource cpu=10m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod openshift-state-metrics-5466cb9dcd-5sdmv requesting resource cpu=120m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod prometheus-adapter-7969f64c9-n4qhv requesting resource cpu=10m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod prometheus-adapter-7969f64c9-xg8hs requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod prometheus-k8s-0 requesting resource cpu=430m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod prometheus-k8s-1 requesting resource cpu=430m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod telemeter-client-7476f789b5-xk24h requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod thanos-querier-6648b6577b-5ztxj requesting resource cpu=40m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod thanos-querier-6648b6577b-dfq6v requesting resource cpu=40m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod multus-9ljmq requesting resource cpu=10m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod multus-msl6m requesting resource cpu=10m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod multus-xprhq requesting resource cpu=10m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod ovs-6kvjd requesting resource cpu=200m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.842: INFO: Pod ovs-99f97 requesting resource cpu=200m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod ovs-fdb6v requesting resource cpu=200m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod sdn-h9mg9 requesting resource cpu=100m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.842: INFO: Pod sdn-k452z requesting resource cpu=100m on Node ip-10-0-151-48.ec2.internal
Dec 12 03:23:36.842: INFO: Pod sdn-q6qf9 requesting resource cpu=100m on Node ip-10-0-133-17.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Dec 12 03:23:36.842: INFO: Creating a pod which consumes cpu=1498m on Node ip-10-0-133-17.ec2.internal
Dec 12 03:23:36.879: INFO: Creating a pod which consumes cpu=2107m on Node ip-10-0-133-2.ec2.internal
Dec 12 03:23:36.910: INFO: Creating a pod which consumes cpu=1421m on Node ip-10-0-151-48.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225.15df81b6d6615b88], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3114/filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225 to ip-10-0-133-17.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225.15df81b8ba23133d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225.15df81b921502bc3], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225.15df81b929d20bed], Reason = [Created], Message = [Created container filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225.15df81b92b134358], Reason = [Started], Message = [Started container filler-pod-02dcdf13-611f-4739-bad1-3585c97a8225]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10.15df81b6d9af1f4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3114/filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10 to ip-10-0-151-48.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10.15df81b8a01c58f4], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10.15df81b90b08ae96], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10.15df81b913a4b8d6], Reason = [Created], Message = [Created container filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10.15df81b914e5f182], Reason = [Started], Message = [Started container filler-pod-065f51b3-c057-4110-9e6b-3afdb412db10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89ca4815-9076-450a-81fb-d36853ec5f71.15df81b6d800b470], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3114/filler-pod-89ca4815-9076-450a-81fb-d36853ec5f71 to ip-10-0-133-2.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89ca4815-9076-450a-81fb-d36853ec5f71.15df81b8b9137948], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89ca4815-9076-450a-81fb-d36853ec5f71.15df81b8c12d1a2f], Reason = [Created], Message = [Created container filler-pod-89ca4815-9076-450a-81fb-d36853ec5f71]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89ca4815-9076-450a-81fb-d36853ec5f71.15df81b8c263953b], Reason = [Started], Message = [Started container filler-pod-89ca4815-9076-450a-81fb-d36853ec5f71]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15df81b934d1d822], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) were unschedulable.]
STEP: removing the label node off the node ip-10-0-133-17.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-133-2.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-151-48.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:23:48.230: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-3114" for this suite.
Dec 12 03:23:54.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:23:56.546: INFO: namespace sched-pred-3114 deletion completed in 8.287032711s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:20.319 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:23:56.548: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 12 03:23:56.693: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Dec 12 03:23:56.746: INFO: Waiting for terminating namespaces to be deleted...
Dec 12 03:23:56.764: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-17.ec2.internal before test
Dec 12 03:23:56.802: INFO: telemeter-client-7476f789b5-xk24h from openshift-monitoring started at 2019-12-12 02:54:54 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container reload ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 12 03:23:56.802: INFO: multus-9ljmq from openshift-multus started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:23:56.802: INFO: sdn-q6qf9 from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:23:56.802: INFO: redhat-operators-7cc8cf78cb-l7qp4 from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container redhat-operators ready: true, restart count 0
Dec 12 03:23:56.802: INFO: node-ca-zxkbz from openshift-image-registry started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:23:56.802: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-12-12 02:56:19 +0000 UTC (7 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:23:56.802: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:23:56.802: INFO: router-default-6dd7776f8d-px9mm from openshift-ingress started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container router ready: true, restart count 0
Dec 12 03:23:56.802: INFO: kube-state-metrics-55f44c57bf-lqq5l from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 12 03:23:56.802: INFO: node-exporter-6mcgp from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:23:56.802: INFO: prometheus-adapter-7969f64c9-xg8hs from openshift-monitoring started at 2019-12-12 02:54:58 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:23:56.802: INFO: tuned-b6d4c from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:23:56.802: INFO: dns-default-9gg8k from openshift-dns started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:23:56.802: INFO: machine-config-daemon-fhxhd from openshift-machine-config-operator started at 2019-12-12 02:48:33 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: certified-operators-784f95ccfc-28hmb from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container certified-operators ready: true, restart count 0
Dec 12 03:23:56.802: INFO: thanos-querier-6648b6577b-dfq6v from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:23:56.802: INFO: ovs-6kvjd from openshift-sdn started at 2019-12-12 02:47:21 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:23:56.802: INFO: openshift-state-metrics-5466cb9dcd-5sdmv from openshift-monitoring started at 2019-12-12 02:49:01 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 12 03:23:56.802: INFO: community-operators-57df479757-26s9p from openshift-marketplace started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container community-operators ready: true, restart count 0
Dec 12 03:23:56.802: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-12-12 02:55:39 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:56.802: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:23:56.802: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-133-2.ec2.internal before test
Dec 12 03:23:56.823: INFO: ovs-99f97 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:23:56.823: INFO: sdn-h9mg9 from openshift-sdn started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container sdn ready: true, restart count 1
Dec 12 03:23:56.823: INFO: multus-xprhq from openshift-multus started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:23:56.823: INFO: dns-default-fz8bj from openshift-dns started at 2019-12-12 03:18:52 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:23:56.823: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:23:56.823: INFO: tuned-fgjfd from openshift-cluster-node-tuning-operator started at 2019-12-12 02:47:25 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:23:56.823: INFO: node-exporter-n8p7f from openshift-monitoring started at 2019-12-12 02:49:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.823: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:23:56.823: INFO: machine-config-daemon-8lzxl from openshift-machine-config-operator started at 2019-12-12 03:18:52 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:23:56.823: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:56.823: INFO: node-ca-5jctk from openshift-image-registry started at 2019-12-12 03:18:52 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.823: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:23:56.823: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-151-48.ec2.internal before test
Dec 12 03:23:56.858: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:23:56.858: INFO: router-default-6dd7776f8d-85842 from openshift-ingress started at 2019-12-12 03:13:22 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container router ready: true, restart count 0
Dec 12 03:23:56.858: INFO: image-registry-8444c9fb8d-mk2h4 from openshift-image-registry started at 2019-12-12 03:13:22 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container registry ready: true, restart count 0
Dec 12 03:23:56.858: INFO: dns-default-d5h9f from openshift-dns started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container dns ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 12 03:23:56.858: INFO: machine-config-daemon-hjf2v from openshift-machine-config-operator started at 2019-12-12 02:50:05 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: node-ca-fmkp2 from openshift-image-registry started at 2019-12-12 02:50:05 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container node-ca ready: true, restart count 0
Dec 12 03:23:56.858: INFO: prometheus-adapter-7969f64c9-n4qhv from openshift-monitoring started at 2019-12-12 02:54:57 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 12 03:23:56.858: INFO: grafana-5bcb6c54b5-tcpmz from openshift-monitoring started at 2019-12-12 02:55:03 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container grafana ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-12-12 03:13:32 +0000 UTC (3 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container alertmanager ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container config-reloader ready: true, restart count 0
Dec 12 03:23:56.858: INFO: multus-msl6m from openshift-multus started at 2019-12-12 02:49:00 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container kube-multus ready: true, restart count 0
Dec 12 03:23:56.858: INFO: tuned-qh2kj from openshift-cluster-node-tuning-operator started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container tuned ready: true, restart count 0
Dec 12 03:23:56.858: INFO: ovs-fdb6v from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container openvswitch ready: true, restart count 0
Dec 12 03:23:56.858: INFO: node-exporter-wrwwf from openshift-monitoring started at 2019-12-12 02:49:02 +0000 UTC (2 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container node-exporter ready: true, restart count 0
Dec 12 03:23:56.858: INFO: thanos-querier-6648b6577b-5ztxj from openshift-monitoring started at 2019-12-12 02:54:55 +0000 UTC (4 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container thanos-querier ready: true, restart count 0
Dec 12 03:23:56.858: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-12-12 02:56:33 +0000 UTC (7 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container prometheus ready: true, restart count 1
Dec 12 03:23:56.858: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec 12 03:23:56.858: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 12 03:23:56.858: INFO: sdn-k452z from openshift-sdn started at 2019-12-12 02:49:01 +0000 UTC (1 container statuses recorded)
Dec 12 03:23:56.858: INFO: 	Container sdn ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-74b5d643-9b3c-40f6-b1cd-bf1f40af947b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-74b5d643-9b3c-40f6-b1cd-bf1f40af947b off the node ip-10-0-133-2.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-74b5d643-9b3c-40f6-b1cd-bf1f40af947b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:24:15.142: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-4972" for this suite.
Dec 12 03:24:25.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:24:27.512: INFO: namespace sched-pred-4972 deletion completed in 12.340233104s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:30.964 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:24:27.517: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 12 03:24:27.785: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:27.785: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:27.785: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:27.801: INFO: Number of nodes with available pods: 0
Dec 12 03:24:27.801: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:28.846: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:28.846: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:28.846: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:28.862: INFO: Number of nodes with available pods: 0
Dec 12 03:24:28.862: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:29.844: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:29.844: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:29.844: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:29.861: INFO: Number of nodes with available pods: 0
Dec 12 03:24:29.861: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:30.844: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:30.844: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:30.844: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:30.860: INFO: Number of nodes with available pods: 0
Dec 12 03:24:30.860: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:31.844: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:31.844: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:31.844: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:31.860: INFO: Number of nodes with available pods: 0
Dec 12 03:24:31.860: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:32.845: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:32.845: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:32.845: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:32.861: INFO: Number of nodes with available pods: 0
Dec 12 03:24:32.861: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:33.844: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:33.844: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:33.844: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:33.860: INFO: Number of nodes with available pods: 0
Dec 12 03:24:33.860: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:34.844: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:34.844: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:34.844: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:34.860: INFO: Number of nodes with available pods: 0
Dec 12 03:24:34.860: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:35.845: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:35.845: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:35.845: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:35.862: INFO: Number of nodes with available pods: 0
Dec 12 03:24:35.862: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:36.847: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:36.847: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:36.847: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:36.863: INFO: Number of nodes with available pods: 3
Dec 12 03:24:36.863: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 12 03:24:36.937: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:36.937: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:36.937: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:36.954: INFO: Number of nodes with available pods: 2
Dec 12 03:24:36.954: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:37.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:37.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:37.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:38.013: INFO: Number of nodes with available pods: 2
Dec 12 03:24:38.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:38.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:38.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:38.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:39.014: INFO: Number of nodes with available pods: 2
Dec 12 03:24:39.014: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:39.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:39.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:39.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:40.013: INFO: Number of nodes with available pods: 2
Dec 12 03:24:40.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:40.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:40.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:40.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:41.012: INFO: Number of nodes with available pods: 2
Dec 12 03:24:41.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:41.998: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:41.998: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:41.998: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:42.016: INFO: Number of nodes with available pods: 2
Dec 12 03:24:42.016: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:42.996: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:42.996: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:42.996: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:43.013: INFO: Number of nodes with available pods: 2
Dec 12 03:24:43.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:43.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:43.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:43.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:44.013: INFO: Number of nodes with available pods: 2
Dec 12 03:24:44.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:44.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:44.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:44.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:45.014: INFO: Number of nodes with available pods: 2
Dec 12 03:24:45.014: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:45.996: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:45.996: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:45.996: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:46.012: INFO: Number of nodes with available pods: 2
Dec 12 03:24:46.012: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:47.000: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:47.000: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:47.000: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:47.015: INFO: Number of nodes with available pods: 2
Dec 12 03:24:47.015: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:47.996: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:47.996: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:47.996: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:48.012: INFO: Number of nodes with available pods: 2
Dec 12 03:24:48.012: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:48.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:48.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:48.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:49.013: INFO: Number of nodes with available pods: 2
Dec 12 03:24:49.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:49.999: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:49.999: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:49.999: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:50.016: INFO: Number of nodes with available pods: 2
Dec 12 03:24:50.016: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:50.996: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:50.996: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:50.996: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:51.012: INFO: Number of nodes with available pods: 2
Dec 12 03:24:51.012: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:51.996: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:51.996: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:51.996: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:52.013: INFO: Number of nodes with available pods: 2
Dec 12 03:24:52.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:52.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:52.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:52.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:53.013: INFO: Number of nodes with available pods: 2
Dec 12 03:24:53.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:53.998: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:53.999: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:53.999: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:54.015: INFO: Number of nodes with available pods: 2
Dec 12 03:24:54.015: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:54.996: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:54.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:54.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:55.012: INFO: Number of nodes with available pods: 2
Dec 12 03:24:55.012: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:55.998: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:55.998: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:55.998: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:56.018: INFO: Number of nodes with available pods: 2
Dec 12 03:24:56.018: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:56.997: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:56.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:56.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:57.014: INFO: Number of nodes with available pods: 2
Dec 12 03:24:57.014: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:57.996: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:57.997: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:57.997: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:58.012: INFO: Number of nodes with available pods: 2
Dec 12 03:24:58.013: INFO: Node ip-10-0-133-17.ec2.internal is running more than one daemon pod
Dec 12 03:24:58.998: INFO: DaemonSet pods can't tolerate node ip-10-0-134-128.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:38 +0000 UTC}], skip checking this node
Dec 12 03:24:58.998: INFO: DaemonSet pods can't tolerate node ip-10-0-137-11.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:58.998: INFO: DaemonSet pods can't tolerate node ip-10-0-159-82.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-12-12 02:58:39 +0000 UTC}], skip checking this node
Dec 12 03:24:59.014: INFO: Number of nodes with available pods: 3
Dec 12 03:24:59.014: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8484, will wait for the garbage collector to delete the pods
Dec 12 03:24:59.116: INFO: Deleting DaemonSet.extensions daemon-set took: 20.736848ms
Dec 12 03:24:59.316: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.302611ms
Dec 12 03:25:09.332: INFO: Number of nodes with available pods: 0
Dec 12 03:25:09.332: INFO: Number of running nodes: 0, number of available pods: 0
Dec 12 03:25:09.348: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8484/daemonsets","resourceVersion":"27413"},"items":null}

Dec 12 03:25:09.363: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8484/pods","resourceVersion":"27413"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:09.438: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-8484" for this suite.
Dec 12 03:25:15.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:25:17.785: INFO: namespace daemonsets-8484 deletion completed in 8.315729299s

â€¢ [SLOW TEST:50.268 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec 12 03:25:17.790: INFO: Running AfterSuite actions on all nodes
Dec 12 03:25:17.790: INFO: Running AfterSuite actions on node 1
Dec 12 03:25:17.790: INFO: Dumping logs locally to: /tmp/artifacts
Dec 12 03:25:17.790: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 15 of 4731 Specs in 1251.059 seconds
SUCCESS! -- 15 Passed | 0 Failed | 0 Pending | 4716 Skipped
PASS

Ginkgo ran 1 suite in 20m52.870275529s
Test Suite Passed
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576121117 - Will randomize all specs
Will run 4731 specs

Running in parallel across 4 nodes

Dec 12 03:25:19.840: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:25:19.843: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Dec 12 03:25:19.995: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 12 03:25:20.059: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 12 03:25:20.060: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Dec 12 03:25:20.060: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 12 03:25:20.082: INFO: e2e test version: v1.16.4-1+8aa1d9dd63e9a4
Dec 12 03:25:20.101: INFO: kube-apiserver version: v1.16.2
Dec 12 03:25:20.102: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:25:20.120: INFO: Cluster IP family: ipv4

S
------------------------------
Dec 12 03:25:20.104: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:25:20.177: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSS
------------------------------
Dec 12 03:25:20.135: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:25:20.212: INFO: Cluster IP family: ipv4

SS
------------------------------
Dec 12 03:25:20.140: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:25:20.217: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:20.319: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
Dec 12 03:25:20.577: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Dec 12 03:25:20.641: INFO: Waiting up to 5m0s for pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301" in namespace "containers-3132" to be "success or failure"
Dec 12 03:25:20.659: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Pending", Reason="", readiness=false. Elapsed: 17.83582ms
Dec 12 03:25:22.676: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03438322s
Dec 12 03:25:24.693: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051585556s
Dec 12 03:25:26.710: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069026063s
Dec 12 03:25:28.727: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085846397s
Dec 12 03:25:30.744: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102485334s
Dec 12 03:25:32.762: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Pending", Reason="", readiness=false. Elapsed: 12.120463947s
Dec 12 03:25:34.779: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.13714517s
STEP: Saw pod success
Dec 12 03:25:34.779: INFO: Pod "client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301" satisfied condition "success or failure"
Dec 12 03:25:34.794: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301 container test-container: <nil>
STEP: delete the pod
Dec 12 03:25:34.848: INFO: Waiting for pod client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301 to disappear
Dec 12 03:25:34.863: INFO: Pod client-containers-7e2cca00-ca88-4906-9e75-708a58d2c301 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:34.863: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-3132" for this suite.
Dec 12 03:25:40.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:25:44.224: INFO: namespace containers-3132 deletion completed in 9.318085816s


â€¢ [SLOW TEST:23.904 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:20.363: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
Dec 12 03:25:20.626: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-d269e93c-b906-49c5-b8fb-147528b15d1b
STEP: Creating a pod to test consume secrets
Dec 12 03:25:20.698: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07" in namespace "projected-1325" to be "success or failure"
Dec 12 03:25:20.716: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Pending", Reason="", readiness=false. Elapsed: 17.78688ms
Dec 12 03:25:22.734: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035314381s
Dec 12 03:25:24.750: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05194736s
Dec 12 03:25:26.768: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070070176s
Dec 12 03:25:28.785: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086914883s
Dec 12 03:25:30.801: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Pending", Reason="", readiness=false. Elapsed: 10.103226872s
Dec 12 03:25:32.819: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.120799975s
Dec 12 03:25:34.835: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.136750585s
STEP: Saw pod success
Dec 12 03:25:34.835: INFO: Pod "pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07" satisfied condition "success or failure"
Dec 12 03:25:34.851: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:25:34.901: INFO: Waiting for pod pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07 to disappear
Dec 12 03:25:34.916: INFO: Pod pod-projected-secrets-f04d12ad-3b78-4cd4-8a06-9b5abf091c07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:34.917: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1325" for this suite.
Dec 12 03:25:41.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:25:44.263: INFO: namespace projected-1325 deletion completed in 9.30155172s


â€¢ [SLOW TEST:23.900 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:20.127: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
Dec 12 03:25:20.265: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:25:20.280: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 12 03:25:27.097: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-9972 create -f -'
Dec 12 03:25:28.037: INFO: stderr: ""
Dec 12 03:25:28.037: INFO: stdout: "e2e-test-crd-publish-openapi-5629-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 12 03:25:28.037: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-9972 delete e2e-test-crd-publish-openapi-5629-crds test-cr'
Dec 12 03:25:28.208: INFO: stderr: ""
Dec 12 03:25:28.208: INFO: stdout: "e2e-test-crd-publish-openapi-5629-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 12 03:25:28.208: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-9972 apply -f -'
Dec 12 03:25:28.818: INFO: stderr: ""
Dec 12 03:25:28.818: INFO: stdout: "e2e-test-crd-publish-openapi-5629-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 12 03:25:28.818: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-9972 delete e2e-test-crd-publish-openapi-5629-crds test-cr'
Dec 12 03:25:29.004: INFO: stderr: ""
Dec 12 03:25:29.004: INFO: stdout: "e2e-test-crd-publish-openapi-5629-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 12 03:25:29.004: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-5629-crds'
Dec 12 03:25:29.501: INFO: stderr: ""
Dec 12 03:25:29.501: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5629-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:36.390: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9972" for this suite.
Dec 12 03:25:42.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:25:44.884: INFO: namespace crd-publish-openapi-9972 deletion completed in 8.449913138s


â€¢ [SLOW TEST:24.757 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:20.244: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replicaset
Dec 12 03:25:20.423: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:25:20.439: INFO: Creating ReplicaSet my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5
Dec 12 03:25:20.522: INFO: Pod name my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5: Found 0 pods out of 1
Dec 12 03:25:25.539: INFO: Pod name my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5: Found 1 pods out of 1
Dec 12 03:25:25.539: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5" is running
Dec 12 03:25:33.575: INFO: Pod "my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5-sxbrz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:25:20 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:25:20 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:25:20 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:25:20 +0000 UTC Reason: Message:}])
Dec 12 03:25:33.575: INFO: Trying to dial the pod
Dec 12 03:25:38.631: INFO: Controller my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5: Got expected result from replica 1 [my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5-sxbrz]: "my-hostname-basic-6fbf6c34-5f56-4993-a89d-7b2956be29b5-sxbrz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:38.631: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-4007" for this suite.
Dec 12 03:25:44.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:25:47.020: INFO: namespace replicaset-4007 deletion completed in 8.345217992s


â€¢ [SLOW TEST:26.777 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:44.334: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:25:44.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4" in namespace "projected-1624" to be "success or failure"
Dec 12 03:25:44.522: INFO: Pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.320101ms
Dec 12 03:25:46.538: INFO: Pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035000211s
Dec 12 03:25:48.555: INFO: Pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051551038s
Dec 12 03:25:50.572: INFO: Pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068362735s
Dec 12 03:25:52.589: INFO: Pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085660511s
Dec 12 03:25:54.606: INFO: Pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102592722s
STEP: Saw pod success
Dec 12 03:25:54.606: INFO: Pod "downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4" satisfied condition "success or failure"
Dec 12 03:25:54.622: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4 container client-container: <nil>
STEP: delete the pod
Dec 12 03:25:54.669: INFO: Waiting for pod downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4 to disappear
Dec 12 03:25:54.685: INFO: Pod downwardapi-volume-d9ee4a91-b505-48a8-8320-c67393e4a0f4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:54.685: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1624" for this suite.
Dec 12 03:26:00.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:03.091: INFO: namespace projected-1624 deletion completed in 8.364532338s


â€¢ [SLOW TEST:18.757 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:47.053: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 12 03:25:47.194: INFO: Waiting up to 5m0s for pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e" in namespace "emptydir-6153" to be "success or failure"
Dec 12 03:25:47.211: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.674845ms
Dec 12 03:25:49.227: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033179313s
Dec 12 03:25:51.244: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049915286s
Dec 12 03:25:53.260: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066311524s
Dec 12 03:25:55.276: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082517432s
Dec 12 03:25:57.294: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.099746192s
Dec 12 03:25:59.310: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.116303384s
STEP: Saw pod success
Dec 12 03:25:59.310: INFO: Pod "pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e" satisfied condition "success or failure"
Dec 12 03:25:59.326: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e container test-container: <nil>
STEP: delete the pod
Dec 12 03:25:59.380: INFO: Waiting for pod pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e to disappear
Dec 12 03:25:59.396: INFO: Pod pod-1b691502-82f9-4bc5-9775-dd88afa2bc6e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:59.396: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6153" for this suite.
Dec 12 03:26:05.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:07.822: INFO: namespace emptydir-6153 deletion completed in 8.382468787s


â€¢ [SLOW TEST:20.769 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:44.905: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:26:01.320: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-841" for this suite.
Dec 12 03:26:07.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:09.699: INFO: namespace resourcequota-841 deletion completed in 8.335956908s


â€¢ [SLOW TEST:24.795 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:25:44.281: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:25:45.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:25:47.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717945, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:25:49.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717945, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:25:51.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717945, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:25:53.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717945, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:25:55.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717945, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711717944, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:25:58.072: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:25:58.292: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-7862" for this suite.
Dec 12 03:26:04.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:06.648: INFO: namespace webhook-7862 deletion completed in 8.326145096s
STEP: Destroying namespace "webhook-7862-markers" for this suite.
Dec 12 03:26:12.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:14.976: INFO: namespace webhook-7862-markers deletion completed in 8.327960609s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:30.762 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:26:09.703: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Dec 12 03:26:09.874: INFO: Waiting up to 5m0s for pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512" in namespace "var-expansion-9461" to be "success or failure"
Dec 12 03:26:09.890: INFO: Pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512": Phase="Pending", Reason="", readiness=false. Elapsed: 16.095383ms
Dec 12 03:26:11.907: INFO: Pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032996253s
Dec 12 03:26:13.924: INFO: Pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049145012s
Dec 12 03:26:15.941: INFO: Pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066179856s
Dec 12 03:26:17.957: INFO: Pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082507362s
Dec 12 03:26:19.975: INFO: Pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.100466174s
STEP: Saw pod success
Dec 12 03:26:19.975: INFO: Pod "var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512" satisfied condition "success or failure"
Dec 12 03:26:19.993: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512 container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:26:20.039: INFO: Waiting for pod var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512 to disappear
Dec 12 03:26:20.054: INFO: Pod var-expansion-1544237c-c601-40f6-a6d3-0f1af43f7512 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:26:20.055: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-9461" for this suite.
Dec 12 03:26:26.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:28.419: INFO: namespace var-expansion-9461 deletion completed in 8.320492685s


â€¢ [SLOW TEST:18.715 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:26:15.067: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Dec 12 03:26:15.230: INFO: Waiting up to 5m0s for pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1" in namespace "var-expansion-9785" to be "success or failure"
Dec 12 03:26:15.246: INFO: Pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.591835ms
Dec 12 03:26:17.263: INFO: Pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032211267s
Dec 12 03:26:19.279: INFO: Pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048486378s
Dec 12 03:26:21.296: INFO: Pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065583655s
Dec 12 03:26:23.312: INFO: Pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082060473s
Dec 12 03:26:25.330: INFO: Pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099461227s
STEP: Saw pod success
Dec 12 03:26:25.330: INFO: Pod "var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1" satisfied condition "success or failure"
Dec 12 03:26:25.346: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1 container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:26:25.412: INFO: Waiting for pod var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1 to disappear
Dec 12 03:26:25.428: INFO: Pod var-expansion-1541dbf4-b5a4-41d2-ba80-610b376c66c1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:26:25.428: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-9785" for this suite.
Dec 12 03:26:31.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:33.777: INFO: namespace var-expansion-9785 deletion completed in 8.306123176s


â€¢ [SLOW TEST:18.710 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:26:07.844: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9069
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 03:26:07.964: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Dec 12 03:26:44.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.0.41:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9069 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:26:44.433: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:26:44.730: INFO: Found all expected endpoints: [netserver-0]
Dec 12 03:26:44.763: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.19:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9069 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:26:44.763: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:26:45.017: INFO: Found all expected endpoints: [netserver-1]
Dec 12 03:26:45.034: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9069 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:26:45.034: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:26:45.295: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:26:45.295: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-9069" for this suite.
Dec 12 03:26:51.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:26:53.904: INFO: namespace pod-network-test-9069 deletion completed in 8.553258842s


â€¢ [SLOW TEST:46.060 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:26:03.118: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:26:13.429: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-4269" for this suite.
Dec 12 03:26:59.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:01.888: INFO: namespace kubelet-test-4269 deletion completed in 48.41158326s


â€¢ [SLOW TEST:58.771 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:26:33.840: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 12 03:26:44.655: INFO: Successfully updated pod "annotationupdatea57333c7-8cc4-4cf4-a0ed-b7e7b4f0e47d"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:26:46.706: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7798" for this suite.
Dec 12 03:27:04.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:07.189: INFO: namespace projected-7798 deletion completed in 20.402355247s


â€¢ [SLOW TEST:33.350 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:26:28.504: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 12 03:26:48.883: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 12 03:26:48.909: INFO: Pod pod-with-prestop-http-hook still exists
Dec 12 03:26:50.919: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 12 03:26:50.938: INFO: Pod pod-with-prestop-http-hook still exists
Dec 12 03:26:52.921: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 12 03:26:52.938: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:26:52.957: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6764" for this suite.
Dec 12 03:27:05.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:07.469: INFO: namespace container-lifecycle-hook-6764 deletion completed in 14.468757759s


â€¢ [SLOW TEST:38.966 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:07.633: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Dec 12 03:27:07.755: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix175625112/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:07.904: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9503" for this suite.
Dec 12 03:27:13.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:16.368: INFO: namespace kubectl-9503 deletion completed in 8.432707748s


â€¢ [SLOW TEST:8.735 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:26:53.963: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:10.465: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5943" for this suite.
Dec 12 03:27:16.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:18.894: INFO: namespace resourcequota-5943 deletion completed in 8.385826314s


â€¢ [SLOW TEST:24.932 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:07.256: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:27:07.498: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3" in namespace "projected-966" to be "success or failure"
Dec 12 03:27:07.515: INFO: Pod "downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.405645ms
Dec 12 03:27:09.533: INFO: Pod "downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035020246s
Dec 12 03:27:11.550: INFO: Pod "downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051745761s
Dec 12 03:27:13.567: INFO: Pod "downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069034683s
Dec 12 03:27:15.584: INFO: Pod "downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.08626358s
STEP: Saw pod success
Dec 12 03:27:15.584: INFO: Pod "downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3" satisfied condition "success or failure"
Dec 12 03:27:15.602: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3 container client-container: <nil>
STEP: delete the pod
Dec 12 03:27:15.652: INFO: Waiting for pod downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3 to disappear
Dec 12 03:27:15.668: INFO: Pod downwardapi-volume-a414369d-5888-4daa-ba1f-04903cf78bc3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:15.668: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-966" for this suite.
Dec 12 03:27:21.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:24.027: INFO: namespace projected-966 deletion completed in 8.316704056s


â€¢ [SLOW TEST:16.771 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:02.150: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-78fv4 in namespace proxy-3761
I1212 03:27:02.363205    1994 runners.go:184] Created replication controller with name: proxy-service-78fv4, namespace: proxy-3761, replica count: 1
I1212 03:27:03.413934    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:04.414116    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:05.420917    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:06.421913    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:07.425914    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:08.426171    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:09.426348    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:10.426555    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:11.430799    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:12.431174    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:13.431426    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:14.431660    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:15.431901    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:16.432079    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:17.433153    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1212 03:27:18.434051    1994 runners.go:184] proxy-service-78fv4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 03:27:18.450: INFO: setup took 16.143764559s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 12 03:27:18.470: INFO: (0) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 19.697966ms)
Dec 12 03:27:18.470: INFO: (0) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 19.506515ms)
Dec 12 03:27:18.470: INFO: (0) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 20.00639ms)
Dec 12 03:27:18.471: INFO: (0) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 20.594596ms)
Dec 12 03:27:18.471: INFO: (0) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 21.363011ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 32.615256ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 32.610193ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 32.407188ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 32.940898ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 33.429234ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 33.056837ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 32.681009ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 32.727992ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 32.588613ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 32.695524ms)
Dec 12 03:27:18.483: INFO: (0) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 32.651559ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 26.453512ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 26.497803ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 26.467545ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 26.671376ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 26.420122ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 26.648862ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 26.483774ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 26.561904ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 26.513388ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 26.482846ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 26.63008ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 26.57733ms)
Dec 12 03:27:18.510: INFO: (1) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 26.580147ms)
Dec 12 03:27:18.522: INFO: (1) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 38.893734ms)
Dec 12 03:27:18.523: INFO: (1) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 39.122828ms)
Dec 12 03:27:18.523: INFO: (1) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 38.978693ms)
Dec 12 03:27:18.541: INFO: (2) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 17.839166ms)
Dec 12 03:27:18.541: INFO: (2) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 18.170642ms)
Dec 12 03:27:18.542: INFO: (2) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 18.764994ms)
Dec 12 03:27:18.544: INFO: (2) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 20.710131ms)
Dec 12 03:27:18.544: INFO: (2) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 20.742787ms)
Dec 12 03:27:18.544: INFO: (2) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 20.880517ms)
Dec 12 03:27:18.544: INFO: (2) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 20.755233ms)
Dec 12 03:27:18.544: INFO: (2) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 21.31408ms)
Dec 12 03:27:18.544: INFO: (2) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 21.469965ms)
Dec 12 03:27:18.544: INFO: (2) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 21.655418ms)
Dec 12 03:27:18.545: INFO: (2) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.891945ms)
Dec 12 03:27:18.545: INFO: (2) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 21.778963ms)
Dec 12 03:27:18.546: INFO: (2) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 23.417307ms)
Dec 12 03:27:18.546: INFO: (2) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 23.425259ms)
Dec 12 03:27:18.546: INFO: (2) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 23.530235ms)
Dec 12 03:27:18.546: INFO: (2) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 23.547356ms)
Dec 12 03:27:18.564: INFO: (3) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 17.10929ms)
Dec 12 03:27:18.565: INFO: (3) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 18.642775ms)
Dec 12 03:27:18.565: INFO: (3) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 18.808582ms)
Dec 12 03:27:18.565: INFO: (3) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 18.662355ms)
Dec 12 03:27:18.565: INFO: (3) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 18.742694ms)
Dec 12 03:27:18.565: INFO: (3) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 18.833581ms)
Dec 12 03:27:18.566: INFO: (3) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 19.158934ms)
Dec 12 03:27:18.567: INFO: (3) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 20.035302ms)
Dec 12 03:27:18.567: INFO: (3) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 20.690944ms)
Dec 12 03:27:18.568: INFO: (3) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 21.155197ms)
Dec 12 03:27:18.568: INFO: (3) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 21.245179ms)
Dec 12 03:27:18.568: INFO: (3) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 21.572224ms)
Dec 12 03:27:18.569: INFO: (3) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 22.598209ms)
Dec 12 03:27:18.569: INFO: (3) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 22.777578ms)
Dec 12 03:27:18.569: INFO: (3) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 22.939059ms)
Dec 12 03:27:18.571: INFO: (3) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 23.989726ms)
Dec 12 03:27:18.588: INFO: (4) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 16.679496ms)
Dec 12 03:27:18.589: INFO: (4) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 17.732696ms)
Dec 12 03:27:18.590: INFO: (4) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 18.50763ms)
Dec 12 03:27:18.590: INFO: (4) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 18.717387ms)
Dec 12 03:27:18.590: INFO: (4) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 18.802389ms)
Dec 12 03:27:18.590: INFO: (4) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 19.314205ms)
Dec 12 03:27:18.591: INFO: (4) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 18.988263ms)
Dec 12 03:27:18.592: INFO: (4) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 20.653281ms)
Dec 12 03:27:18.592: INFO: (4) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 20.160675ms)
Dec 12 03:27:18.593: INFO: (4) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 21.247086ms)
Dec 12 03:27:18.593: INFO: (4) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 22.500854ms)
Dec 12 03:27:18.593: INFO: (4) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.564599ms)
Dec 12 03:27:18.597: INFO: (4) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 25.461084ms)
Dec 12 03:27:18.597: INFO: (4) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 25.833943ms)
Dec 12 03:27:18.598: INFO: (4) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 26.070703ms)
Dec 12 03:27:18.598: INFO: (4) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 25.767112ms)
Dec 12 03:27:18.615: INFO: (5) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 16.976156ms)
Dec 12 03:27:18.616: INFO: (5) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 17.595031ms)
Dec 12 03:27:18.616: INFO: (5) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 18.182993ms)
Dec 12 03:27:18.616: INFO: (5) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 18.13964ms)
Dec 12 03:27:18.618: INFO: (5) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 20.156037ms)
Dec 12 03:27:18.618: INFO: (5) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 20.027335ms)
Dec 12 03:27:18.618: INFO: (5) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 20.357289ms)
Dec 12 03:27:18.618: INFO: (5) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 20.457272ms)
Dec 12 03:27:18.618: INFO: (5) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 20.526254ms)
Dec 12 03:27:18.618: INFO: (5) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 20.567546ms)
Dec 12 03:27:18.618: INFO: (5) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 20.639049ms)
Dec 12 03:27:18.619: INFO: (5) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 21.653945ms)
Dec 12 03:27:18.621: INFO: (5) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 22.95496ms)
Dec 12 03:27:18.621: INFO: (5) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 22.992478ms)
Dec 12 03:27:18.621: INFO: (5) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 23.303527ms)
Dec 12 03:27:18.621: INFO: (5) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 23.485192ms)
Dec 12 03:27:18.639: INFO: (6) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 18.105656ms)
Dec 12 03:27:18.640: INFO: (6) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 18.479741ms)
Dec 12 03:27:18.640: INFO: (6) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 18.899427ms)
Dec 12 03:27:18.641: INFO: (6) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 19.833454ms)
Dec 12 03:27:18.642: INFO: (6) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 20.670916ms)
Dec 12 03:27:18.643: INFO: (6) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 21.680293ms)
Dec 12 03:27:18.643: INFO: (6) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 21.697958ms)
Dec 12 03:27:18.643: INFO: (6) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 21.676874ms)
Dec 12 03:27:18.644: INFO: (6) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 22.138122ms)
Dec 12 03:27:18.644: INFO: (6) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 22.222259ms)
Dec 12 03:27:18.644: INFO: (6) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 22.201192ms)
Dec 12 03:27:18.644: INFO: (6) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 22.416295ms)
Dec 12 03:27:18.645: INFO: (6) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 23.203115ms)
Dec 12 03:27:18.649: INFO: (6) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 27.750472ms)
Dec 12 03:27:18.649: INFO: (6) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 27.859182ms)
Dec 12 03:27:18.649: INFO: (6) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 27.900202ms)
Dec 12 03:27:18.668: INFO: (7) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 18.020587ms)
Dec 12 03:27:18.668: INFO: (7) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 18.269289ms)
Dec 12 03:27:18.669: INFO: (7) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 19.253781ms)
Dec 12 03:27:18.669: INFO: (7) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 19.351931ms)
Dec 12 03:27:18.669: INFO: (7) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 19.475185ms)
Dec 12 03:27:18.670: INFO: (7) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 20.151369ms)
Dec 12 03:27:18.670: INFO: (7) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 20.202642ms)
Dec 12 03:27:18.670: INFO: (7) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 20.405345ms)
Dec 12 03:27:18.670: INFO: (7) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 20.722242ms)
Dec 12 03:27:18.671: INFO: (7) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 21.782726ms)
Dec 12 03:27:18.671: INFO: (7) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 21.780668ms)
Dec 12 03:27:18.673: INFO: (7) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 23.320171ms)
Dec 12 03:27:18.673: INFO: (7) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 23.396233ms)
Dec 12 03:27:18.673: INFO: (7) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 23.43336ms)
Dec 12 03:27:18.673: INFO: (7) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 23.585491ms)
Dec 12 03:27:18.674: INFO: (7) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 24.155083ms)
Dec 12 03:27:18.691: INFO: (8) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 16.955314ms)
Dec 12 03:27:18.693: INFO: (8) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 19.554698ms)
Dec 12 03:27:18.693: INFO: (8) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 19.553663ms)
Dec 12 03:27:18.693: INFO: (8) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 19.310041ms)
Dec 12 03:27:18.694: INFO: (8) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 19.782621ms)
Dec 12 03:27:18.695: INFO: (8) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 21.236394ms)
Dec 12 03:27:18.696: INFO: (8) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.42094ms)
Dec 12 03:27:18.696: INFO: (8) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 21.971134ms)
Dec 12 03:27:18.696: INFO: (8) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 22.387441ms)
Dec 12 03:27:18.697: INFO: (8) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 23.097638ms)
Dec 12 03:27:18.697: INFO: (8) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 22.868138ms)
Dec 12 03:27:18.698: INFO: (8) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 23.36558ms)
Dec 12 03:27:18.698: INFO: (8) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 23.503176ms)
Dec 12 03:27:18.698: INFO: (8) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 23.965536ms)
Dec 12 03:27:18.699: INFO: (8) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 24.25286ms)
Dec 12 03:27:18.699: INFO: (8) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 24.395868ms)
Dec 12 03:27:18.717: INFO: (9) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 17.771168ms)
Dec 12 03:27:18.718: INFO: (9) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 18.604223ms)
Dec 12 03:27:18.718: INFO: (9) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 18.500773ms)
Dec 12 03:27:18.718: INFO: (9) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 18.995836ms)
Dec 12 03:27:18.718: INFO: (9) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 19.465828ms)
Dec 12 03:27:18.718: INFO: (9) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 19.26576ms)
Dec 12 03:27:18.720: INFO: (9) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 20.163859ms)
Dec 12 03:27:18.721: INFO: (9) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 21.481522ms)
Dec 12 03:27:18.721: INFO: (9) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 21.478813ms)
Dec 12 03:27:18.721: INFO: (9) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.394202ms)
Dec 12 03:27:18.721: INFO: (9) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 21.83751ms)
Dec 12 03:27:18.721: INFO: (9) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 21.585897ms)
Dec 12 03:27:18.721: INFO: (9) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.682945ms)
Dec 12 03:27:18.726: INFO: (9) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 26.535127ms)
Dec 12 03:27:18.726: INFO: (9) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 26.535782ms)
Dec 12 03:27:18.726: INFO: (9) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 26.212659ms)
Dec 12 03:27:18.744: INFO: (10) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 17.764771ms)
Dec 12 03:27:18.745: INFO: (10) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 18.328824ms)
Dec 12 03:27:18.746: INFO: (10) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 20.048354ms)
Dec 12 03:27:18.746: INFO: (10) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 19.773352ms)
Dec 12 03:27:18.747: INFO: (10) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 20.298081ms)
Dec 12 03:27:18.747: INFO: (10) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 20.89416ms)
Dec 12 03:27:18.747: INFO: (10) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 20.222071ms)
Dec 12 03:27:18.747: INFO: (10) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 21.045369ms)
Dec 12 03:27:18.749: INFO: (10) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 22.711194ms)
Dec 12 03:27:18.749: INFO: (10) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 22.249563ms)
Dec 12 03:27:18.749: INFO: (10) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 22.153548ms)
Dec 12 03:27:18.749: INFO: (10) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 22.410481ms)
Dec 12 03:27:18.749: INFO: (10) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 22.597623ms)
Dec 12 03:27:18.749: INFO: (10) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 23.109424ms)
Dec 12 03:27:18.750: INFO: (10) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 23.538026ms)
Dec 12 03:27:18.751: INFO: (10) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 24.072021ms)
Dec 12 03:27:18.770: INFO: (11) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 19.100214ms)
Dec 12 03:27:18.770: INFO: (11) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 19.154036ms)
Dec 12 03:27:18.772: INFO: (11) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 21.139878ms)
Dec 12 03:27:18.772: INFO: (11) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 21.007854ms)
Dec 12 03:27:18.774: INFO: (11) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 22.630474ms)
Dec 12 03:27:18.776: INFO: (11) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 24.908278ms)
Dec 12 03:27:18.776: INFO: (11) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 25.118354ms)
Dec 12 03:27:18.776: INFO: (11) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 25.313033ms)
Dec 12 03:27:18.776: INFO: (11) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 25.577158ms)
Dec 12 03:27:18.776: INFO: (11) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 25.584347ms)
Dec 12 03:27:18.776: INFO: (11) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 25.426412ms)
Dec 12 03:27:18.776: INFO: (11) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 25.464312ms)
Dec 12 03:27:18.779: INFO: (11) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 27.7676ms)
Dec 12 03:27:18.779: INFO: (11) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 28.017308ms)
Dec 12 03:27:18.779: INFO: (11) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 27.774454ms)
Dec 12 03:27:18.780: INFO: (11) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 28.755118ms)
Dec 12 03:27:18.797: INFO: (12) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 17.655975ms)
Dec 12 03:27:18.797: INFO: (12) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 17.704368ms)
Dec 12 03:27:18.798: INFO: (12) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 18.183947ms)
Dec 12 03:27:18.798: INFO: (12) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 18.483976ms)
Dec 12 03:27:18.798: INFO: (12) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 18.49749ms)
Dec 12 03:27:18.799: INFO: (12) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 18.816222ms)
Dec 12 03:27:18.800: INFO: (12) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 20.668046ms)
Dec 12 03:27:18.800: INFO: (12) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 20.693487ms)
Dec 12 03:27:18.801: INFO: (12) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 21.41689ms)
Dec 12 03:27:18.801: INFO: (12) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 21.298858ms)
Dec 12 03:27:18.802: INFO: (12) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 21.856725ms)
Dec 12 03:27:18.802: INFO: (12) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 21.791097ms)
Dec 12 03:27:18.802: INFO: (12) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 21.90805ms)
Dec 12 03:27:18.802: INFO: (12) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 22.755226ms)
Dec 12 03:27:18.802: INFO: (12) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 22.672902ms)
Dec 12 03:27:18.802: INFO: (12) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 22.746622ms)
Dec 12 03:27:18.820: INFO: (13) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 17.187352ms)
Dec 12 03:27:18.820: INFO: (13) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 17.849671ms)
Dec 12 03:27:18.821: INFO: (13) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 17.802828ms)
Dec 12 03:27:18.821: INFO: (13) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 18.160709ms)
Dec 12 03:27:18.821: INFO: (13) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 18.362147ms)
Dec 12 03:27:18.822: INFO: (13) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 19.285257ms)
Dec 12 03:27:18.823: INFO: (13) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 19.574833ms)
Dec 12 03:27:18.823: INFO: (13) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 20.284111ms)
Dec 12 03:27:18.824: INFO: (13) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 20.736759ms)
Dec 12 03:27:18.824: INFO: (13) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 20.830563ms)
Dec 12 03:27:18.824: INFO: (13) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 20.974883ms)
Dec 12 03:27:18.824: INFO: (13) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 20.923726ms)
Dec 12 03:27:18.824: INFO: (13) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 21.046992ms)
Dec 12 03:27:18.824: INFO: (13) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 21.296281ms)
Dec 12 03:27:18.825: INFO: (13) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 21.391473ms)
Dec 12 03:27:18.826: INFO: (13) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 22.471558ms)
Dec 12 03:27:18.843: INFO: (14) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 16.608348ms)
Dec 12 03:27:18.845: INFO: (14) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 18.728283ms)
Dec 12 03:27:18.845: INFO: (14) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 18.602561ms)
Dec 12 03:27:18.845: INFO: (14) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 18.860313ms)
Dec 12 03:27:18.845: INFO: (14) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 19.298041ms)
Dec 12 03:27:18.845: INFO: (14) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 19.554474ms)
Dec 12 03:27:18.846: INFO: (14) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 20.354689ms)
Dec 12 03:27:18.848: INFO: (14) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 21.900269ms)
Dec 12 03:27:18.848: INFO: (14) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 22.046979ms)
Dec 12 03:27:18.848: INFO: (14) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 22.165674ms)
Dec 12 03:27:18.848: INFO: (14) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 22.091657ms)
Dec 12 03:27:18.848: INFO: (14) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 22.416096ms)
Dec 12 03:27:18.848: INFO: (14) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 22.656631ms)
Dec 12 03:27:18.849: INFO: (14) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 23.190495ms)
Dec 12 03:27:18.849: INFO: (14) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 23.323345ms)
Dec 12 03:27:18.849: INFO: (14) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 23.343322ms)
Dec 12 03:27:18.866: INFO: (15) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 16.552407ms)
Dec 12 03:27:18.867: INFO: (15) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 17.498627ms)
Dec 12 03:27:18.868: INFO: (15) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 19.069272ms)
Dec 12 03:27:18.869: INFO: (15) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 19.478621ms)
Dec 12 03:27:18.869: INFO: (15) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 19.580697ms)
Dec 12 03:27:18.869: INFO: (15) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 19.56129ms)
Dec 12 03:27:18.870: INFO: (15) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 20.265498ms)
Dec 12 03:27:18.870: INFO: (15) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 20.310539ms)
Dec 12 03:27:18.870: INFO: (15) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.079778ms)
Dec 12 03:27:18.871: INFO: (15) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 21.204941ms)
Dec 12 03:27:18.871: INFO: (15) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.282947ms)
Dec 12 03:27:18.871: INFO: (15) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 21.851296ms)
Dec 12 03:27:18.872: INFO: (15) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 22.834347ms)
Dec 12 03:27:18.872: INFO: (15) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 22.925509ms)
Dec 12 03:27:18.873: INFO: (15) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 23.224945ms)
Dec 12 03:27:18.874: INFO: (15) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 24.424799ms)
Dec 12 03:27:18.893: INFO: (16) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 18.519514ms)
Dec 12 03:27:18.893: INFO: (16) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 18.628906ms)
Dec 12 03:27:18.893: INFO: (16) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 19.147801ms)
Dec 12 03:27:18.893: INFO: (16) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 19.087374ms)
Dec 12 03:27:18.893: INFO: (16) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 19.193864ms)
Dec 12 03:27:18.894: INFO: (16) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 19.990142ms)
Dec 12 03:27:18.895: INFO: (16) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 20.619585ms)
Dec 12 03:27:18.895: INFO: (16) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 20.653346ms)
Dec 12 03:27:18.895: INFO: (16) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 21.222215ms)
Dec 12 03:27:18.895: INFO: (16) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 21.146868ms)
Dec 12 03:27:18.896: INFO: (16) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 22.046608ms)
Dec 12 03:27:18.896: INFO: (16) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 22.028987ms)
Dec 12 03:27:18.896: INFO: (16) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 22.305898ms)
Dec 12 03:27:18.896: INFO: (16) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 22.346655ms)
Dec 12 03:27:18.896: INFO: (16) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 22.416587ms)
Dec 12 03:27:18.897: INFO: (16) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 22.800455ms)
Dec 12 03:27:18.915: INFO: (17) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 17.490787ms)
Dec 12 03:27:18.915: INFO: (17) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 17.609133ms)
Dec 12 03:27:18.915: INFO: (17) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 17.422997ms)
Dec 12 03:27:18.915: INFO: (17) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 17.291234ms)
Dec 12 03:27:18.917: INFO: (17) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 19.547699ms)
Dec 12 03:27:18.918: INFO: (17) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 20.060242ms)
Dec 12 03:27:18.918: INFO: (17) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 20.763879ms)
Dec 12 03:27:18.918: INFO: (17) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 19.98032ms)
Dec 12 03:27:18.918: INFO: (17) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 20.820357ms)
Dec 12 03:27:18.918: INFO: (17) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 20.472441ms)
Dec 12 03:27:18.918: INFO: (17) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 20.207779ms)
Dec 12 03:27:18.920: INFO: (17) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 21.648856ms)
Dec 12 03:27:18.920: INFO: (17) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 22.402554ms)
Dec 12 03:27:18.920: INFO: (17) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 22.061448ms)
Dec 12 03:27:18.921: INFO: (17) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 22.916016ms)
Dec 12 03:27:18.921: INFO: (17) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 23.164128ms)
Dec 12 03:27:18.939: INFO: (18) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 18.033999ms)
Dec 12 03:27:18.941: INFO: (18) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 19.407802ms)
Dec 12 03:27:18.941: INFO: (18) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 19.74023ms)
Dec 12 03:27:18.942: INFO: (18) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 20.548724ms)
Dec 12 03:27:18.942: INFO: (18) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 20.460794ms)
Dec 12 03:27:18.945: INFO: (18) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 23.236948ms)
Dec 12 03:27:18.945: INFO: (18) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 23.418651ms)
Dec 12 03:27:18.945: INFO: (18) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 23.287907ms)
Dec 12 03:27:18.945: INFO: (18) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 23.445057ms)
Dec 12 03:27:18.945: INFO: (18) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 23.764205ms)
Dec 12 03:27:18.945: INFO: (18) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 24.04587ms)
Dec 12 03:27:18.947: INFO: (18) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 25.360598ms)
Dec 12 03:27:18.947: INFO: (18) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 25.465018ms)
Dec 12 03:27:18.947: INFO: (18) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 25.808232ms)
Dec 12 03:27:18.948: INFO: (18) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 26.445045ms)
Dec 12 03:27:18.950: INFO: (18) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 28.875783ms)
Dec 12 03:27:18.971: INFO: (19) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:443/proxy/tlsrewritem... (200; 20.97541ms)
Dec 12 03:27:18.971: INFO: (19) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:460/proxy/: tls baz (200; 20.931333ms)
Dec 12 03:27:18.972: INFO: (19) /api/v1/namespaces/proxy-3761/pods/https:proxy-service-78fv4-9mgfh:462/proxy/: tls qux (200; 21.086836ms)
Dec 12 03:27:18.972: INFO: (19) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 21.850105ms)
Dec 12 03:27:18.973: INFO: (19) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">test<... (200; 22.388198ms)
Dec 12 03:27:18.973: INFO: (19) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:160/proxy/: foo (200; 22.239013ms)
Dec 12 03:27:18.975: INFO: (19) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname1/proxy/: foo (200; 24.468938ms)
Dec 12 03:27:18.975: INFO: (19) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname1/proxy/: tls baz (200; 24.529337ms)
Dec 12 03:27:18.975: INFO: (19) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh/proxy/rewriteme">test</a> (200; 24.817856ms)
Dec 12 03:27:18.976: INFO: (19) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 25.672759ms)
Dec 12 03:27:18.976: INFO: (19) /api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/: <a href="/api/v1/namespaces/proxy-3761/pods/http:proxy-service-78fv4-9mgfh:1080/proxy/rewriteme">... (200; 25.698801ms)
Dec 12 03:27:18.978: INFO: (19) /api/v1/namespaces/proxy-3761/services/https:proxy-service-78fv4:tlsportname2/proxy/: tls qux (200; 27.75287ms)
Dec 12 03:27:18.980: INFO: (19) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname2/proxy/: bar (200; 29.806757ms)
Dec 12 03:27:18.981: INFO: (19) /api/v1/namespaces/proxy-3761/services/http:proxy-service-78fv4:portname1/proxy/: foo (200; 30.908361ms)
Dec 12 03:27:18.981: INFO: (19) /api/v1/namespaces/proxy-3761/pods/proxy-service-78fv4-9mgfh:162/proxy/: bar (200; 30.833936ms)
Dec 12 03:27:18.988: INFO: (19) /api/v1/namespaces/proxy-3761/services/proxy-service-78fv4:portname2/proxy/: bar (200; 37.699012ms)
STEP: deleting ReplicationController proxy-service-78fv4 in namespace proxy-3761, will wait for the garbage collector to delete the pods
Dec 12 03:27:19.085: INFO: Deleting ReplicationController proxy-service-78fv4 took: 24.150995ms
Dec 12 03:27:19.286: INFO: Terminating ReplicationController proxy-service-78fv4 pods took: 200.258731ms
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:21.386: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-3761" for this suite.
Dec 12 03:27:29.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:30.692: INFO: namespace proxy-3761 deletion completed in 9.261660475s


â€¢ [SLOW TEST:28.541 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:16.455: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1212 03:27:22.763706    1995 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 03:27:22.763: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:22.763: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-7226" for this suite.
Dec 12 03:27:28.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:31.168: INFO: namespace gc-7226 deletion completed in 8.387226907s


â€¢ [SLOW TEST:14.713 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:18.896: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6bb3a767-a8eb-4b6e-a954-1e54fad50b86
STEP: Creating a pod to test consume secrets
Dec 12 03:27:19.063: INFO: Waiting up to 5m0s for pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271" in namespace "secrets-4808" to be "success or failure"
Dec 12 03:27:19.078: INFO: Pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271": Phase="Pending", Reason="", readiness=false. Elapsed: 15.654302ms
Dec 12 03:27:21.095: INFO: Pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032320047s
Dec 12 03:27:23.112: INFO: Pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049060257s
Dec 12 03:27:25.131: INFO: Pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068298263s
Dec 12 03:27:27.157: INFO: Pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093913544s
Dec 12 03:27:29.173: INFO: Pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.110666562s
STEP: Saw pod success
Dec 12 03:27:29.173: INFO: Pod "pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271" satisfied condition "success or failure"
Dec 12 03:27:29.190: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:27:29.234: INFO: Waiting for pod pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271 to disappear
Dec 12 03:27:29.251: INFO: Pod pod-secrets-78015623-8992-44a8-a6f6-4a465cbc2271 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:29.251: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-4808" for this suite.
Dec 12 03:27:35.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:37.656: INFO: namespace secrets-4808 deletion completed in 8.360814072s


â€¢ [SLOW TEST:18.759 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:24.041: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-ae645cf1-3f6b-41a0-bf00-5b8cc6276b5f
STEP: Creating a pod to test consume secrets
Dec 12 03:27:24.227: INFO: Waiting up to 5m0s for pod "pod-secrets-42490489-7992-4582-a145-d45475125260" in namespace "secrets-6981" to be "success or failure"
Dec 12 03:27:24.244: INFO: Pod "pod-secrets-42490489-7992-4582-a145-d45475125260": Phase="Pending", Reason="", readiness=false. Elapsed: 16.998724ms
Dec 12 03:27:26.262: INFO: Pod "pod-secrets-42490489-7992-4582-a145-d45475125260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034788794s
Dec 12 03:27:28.281: INFO: Pod "pod-secrets-42490489-7992-4582-a145-d45475125260": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053862986s
Dec 12 03:27:30.298: INFO: Pod "pod-secrets-42490489-7992-4582-a145-d45475125260": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070760058s
Dec 12 03:27:32.316: INFO: Pod "pod-secrets-42490489-7992-4582-a145-d45475125260": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088776635s
Dec 12 03:27:34.334: INFO: Pod "pod-secrets-42490489-7992-4582-a145-d45475125260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.106908234s
STEP: Saw pod success
Dec 12 03:27:34.334: INFO: Pod "pod-secrets-42490489-7992-4582-a145-d45475125260" satisfied condition "success or failure"
Dec 12 03:27:34.352: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-42490489-7992-4582-a145-d45475125260 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:27:34.399: INFO: Waiting for pod pod-secrets-42490489-7992-4582-a145-d45475125260 to disappear
Dec 12 03:27:34.415: INFO: Pod pod-secrets-42490489-7992-4582-a145-d45475125260 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:34.415: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-6981" for this suite.
Dec 12 03:27:40.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:42.819: INFO: namespace secrets-6981 deletion completed in 8.360969526s


â€¢ [SLOW TEST:18.778 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:31.173: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:27:31.384: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1" in namespace "projected-5592" to be "success or failure"
Dec 12 03:27:31.400: INFO: Pod "downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.376334ms
Dec 12 03:27:33.417: INFO: Pod "downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033340854s
Dec 12 03:27:35.434: INFO: Pod "downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050418649s
Dec 12 03:27:37.454: INFO: Pod "downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070411932s
Dec 12 03:27:39.471: INFO: Pod "downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.087430952s
STEP: Saw pod success
Dec 12 03:27:39.471: INFO: Pod "downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1" satisfied condition "success or failure"
Dec 12 03:27:39.488: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1 container client-container: <nil>
STEP: delete the pod
Dec 12 03:27:39.544: INFO: Waiting for pod downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1 to disappear
Dec 12 03:27:39.561: INFO: Pod downwardapi-volume-ab34b0ae-10f6-45f1-bdc5-95f069f412a1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:39.561: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5592" for this suite.
Dec 12 03:27:45.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:48.029: INFO: namespace projected-5592 deletion completed in 8.423126762s


â€¢ [SLOW TEST:16.856 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:30.742: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-744f827c-b7c8-4981-904f-777d6770e039
STEP: Creating a pod to test consume configMaps
Dec 12 03:27:30.924: INFO: Waiting up to 5m0s for pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54" in namespace "configmap-7712" to be "success or failure"
Dec 12 03:27:30.940: INFO: Pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54": Phase="Pending", Reason="", readiness=false. Elapsed: 16.694062ms
Dec 12 03:27:32.957: INFO: Pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033054927s
Dec 12 03:27:34.973: INFO: Pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049244938s
Dec 12 03:27:36.991: INFO: Pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067123018s
Dec 12 03:27:39.008: INFO: Pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084256467s
Dec 12 03:27:41.026: INFO: Pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102425482s
STEP: Saw pod success
Dec 12 03:27:41.026: INFO: Pod "pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54" satisfied condition "success or failure"
Dec 12 03:27:41.042: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:27:41.088: INFO: Waiting for pod pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54 to disappear
Dec 12 03:27:41.104: INFO: Pod pod-configmaps-5808230c-ea07-4bb6-a3ec-ec028e1edd54 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:41.104: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7712" for this suite.
Dec 12 03:27:47.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:27:49.484: INFO: namespace configmap-7712 deletion completed in 8.335045003s


â€¢ [SLOW TEST:18.742 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:42.887: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1829.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1829.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1829.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1829.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:28:05.138: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local from pod dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b: the server could not find the requested resource (get pods dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b)
Dec 12 03:28:05.155: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local from pod dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b: the server could not find the requested resource (get pods dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b)
Dec 12 03:28:05.190: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1829.svc.cluster.local from pod dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b: the server could not find the requested resource (get pods dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b)
Dec 12 03:28:05.240: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local from pod dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b: the server could not find the requested resource (get pods dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b)
Dec 12 03:28:05.257: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local from pod dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b: the server could not find the requested resource (get pods dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b)
Dec 12 03:28:05.273: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1829.svc.cluster.local from pod dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b: the server could not find the requested resource (get pods dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b)
Dec 12 03:28:05.290: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1829.svc.cluster.local from pod dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b: the server could not find the requested resource (get pods dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b)
Dec 12 03:28:05.324: INFO: Lookups using dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1829.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1829.svc.cluster.local jessie_udp@dns-test-service-2.dns-1829.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1829.svc.cluster.local]

Dec 12 03:28:10.537: INFO: DNS probes using dns-1829/dns-test-a66cdc33-5b82-4840-8602-82a8e66fd89b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:28:10.596: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-1829" for this suite.
Dec 12 03:28:16.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:28:19.038: INFO: namespace dns-1829 deletion completed in 8.41228886s


â€¢ [SLOW TEST:36.150 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:48.057: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-1918
STEP: creating replication controller nodeport-test in namespace services-1918
I1212 03:27:48.244021    1995 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-1918, replica count: 2
I1212 03:27:51.294488    1995 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:54.294700    1995 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:27:57.294928    1995 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 03:27:57.294: INFO: Creating new exec pod
Dec 12 03:28:08.426: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-1918 execpodb5msn -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 12 03:28:11.287: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 12 03:28:11.287: INFO: stdout: ""
Dec 12 03:28:11.288: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-1918 execpodb5msn -- /bin/sh -x -c nc -zv -t -w 2 172.30.95.122 80'
Dec 12 03:28:11.739: INFO: stderr: "+ nc -zv -t -w 2 172.30.95.122 80\nConnection to 172.30.95.122 80 port [tcp/http] succeeded!\n"
Dec 12 03:28:11.739: INFO: stdout: ""
Dec 12 03:28:11.740: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-1918 execpodb5msn -- /bin/sh -x -c nc -zv -t -w 2 10.0.133.17 32237'
Dec 12 03:28:12.269: INFO: stderr: "+ nc -zv -t -w 2 10.0.133.17 32237\nConnection to 10.0.133.17 32237 port [tcp/32237] succeeded!\n"
Dec 12 03:28:12.269: INFO: stdout: ""
Dec 12 03:28:12.269: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-1918 execpodb5msn -- /bin/sh -x -c nc -zv -t -w 2 10.0.133.2 32237'
Dec 12 03:28:12.654: INFO: stderr: "+ nc -zv -t -w 2 10.0.133.2 32237\nConnection to 10.0.133.2 32237 port [tcp/32237] succeeded!\n"
Dec 12 03:28:12.654: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:28:12.654: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-1918" for this suite.
Dec 12 03:28:18.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:28:21.059: INFO: namespace services-1918 deletion completed in 8.362109572s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:33.002 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:37.669: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:27:37.794: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-1772'
Dec 12 03:27:38.372: INFO: stderr: ""
Dec 12 03:27:38.372: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 12 03:27:38.372: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-1772'
Dec 12 03:27:38.816: INFO: stderr: ""
Dec 12 03:27:38.816: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 12 03:27:39.834: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:39.834: INFO: Found 0 / 1
Dec 12 03:27:40.833: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:40.833: INFO: Found 0 / 1
Dec 12 03:27:41.833: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:41.833: INFO: Found 0 / 1
Dec 12 03:27:42.835: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:42.835: INFO: Found 0 / 1
Dec 12 03:27:43.833: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:43.833: INFO: Found 0 / 1
Dec 12 03:27:44.833: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:44.833: INFO: Found 0 / 1
Dec 12 03:27:45.833: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:45.833: INFO: Found 0 / 1
Dec 12 03:27:46.837: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:46.837: INFO: Found 0 / 1
Dec 12 03:27:47.833: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:47.833: INFO: Found 1 / 1
Dec 12 03:27:47.833: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 12 03:27:47.849: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:27:47.849: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 12 03:27:47.849: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe pod redis-master-g72p5 --namespace=kubectl-1772'
Dec 12 03:27:48.157: INFO: stderr: ""
Dec 12 03:27:48.157: INFO: stdout: "Name:         redis-master-g72p5\nNamespace:    kubectl-1772\nPriority:     0\nNode:         ip-10-0-133-2.ec2.internal/10.0.133.2\nStart Time:   Thu, 12 Dec 2019 03:27:38 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"openshift-sdn\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.128.2.55\"\n                    ],\n                    \"dns\": {},\n                    \"default-route\": [\n                        \"10.128.2.1\"\n                    ]\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           10.128.2.55\nIPs:\n  IP:           10.128.2.55\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://6a1e3bb8660551d01ae117d58984d5582355529adbd785a73fc8c3f4587af429\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 12 Dec 2019 03:27:46 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-k59rw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-k59rw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-k59rw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                 Message\n  ----    ------     ----       ----                                 -------\n  Normal  Scheduled  <unknown>  default-scheduler                    Successfully assigned kubectl-1772/redis-master-g72p5 to ip-10-0-133-2.ec2.internal\n  Normal  Pulled     2s         kubelet, ip-10-0-133-2.ec2.internal  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s         kubelet, ip-10-0-133-2.ec2.internal  Created container redis-master\n  Normal  Started    2s         kubelet, ip-10-0-133-2.ec2.internal  Started container redis-master\n"
Dec 12 03:27:48.158: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe rc redis-master --namespace=kubectl-1772'
Dec 12 03:27:48.437: INFO: stderr: ""
Dec 12 03:27:48.437: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1772\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  10s   replication-controller  Created pod: redis-master-g72p5\n"
Dec 12 03:27:48.437: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe service redis-master --namespace=kubectl-1772'
Dec 12 03:27:48.706: INFO: stderr: ""
Dec 12 03:27:48.706: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1772\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.30.185.115\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.128.2.55:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 12 03:27:48.750: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe node ip-10-0-133-17.ec2.internal'
Dec 12 03:27:49.095: INFO: stderr: ""
Dec 12 03:27:49.095: INFO: stdout: "Name:               ip-10-0-133-17.ec2.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-133-17\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.openshift.io/os_id=rhcos\nAnnotations:        machine.openshift.io/machine: openshift-machine-api/ci-op-8rty34r5-fb0a9-wsl2j-worker-us-east-1a-zm8h7\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-6092aaa81c698c37333c1d179420189f\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-6092aaa81c698c37333c1d179420189f\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 12 Dec 2019 02:47:21 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 12 Dec 2019 03:27:44 +0000   Thu, 12 Dec 2019 02:47:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 12 Dec 2019 03:27:44 +0000   Thu, 12 Dec 2019 02:47:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 12 Dec 2019 03:27:44 +0000   Thu, 12 Dec 2019 02:47:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 12 Dec 2019 03:27:44 +0000   Thu, 12 Dec 2019 02:48:32 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.133.17\n  Hostname:     ip-10-0-133-17.ec2.internal\n  InternalDNS:  ip-10-0-133-17.ec2.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         4\n ephemeral-storage:           125277164Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      16419384Ki\n pods:                        250\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         3500m\n ephemeral-storage:           115455434152\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      15804984Ki\n pods:                        250\nSystem Info:\n Machine ID:                              66ed17d6e6b44f50820e7421356e5138\n System UUID:                             ec2960c7-efc2-1b55-18ba-c9b41ea5af88\n Boot ID:                                 e0a2cd8b-ff8d-4957-aa9e-e680701a5380\n Kernel Version:                          4.18.0-147.0.3.el8_1.x86_64\n OS Image:                                Red Hat Enterprise Linux CoreOS 44.81.201912111801.0 (Ootpa)\n Operating System:                        linux\n Architecture:                            amd64\n Container Runtime Version:               cri-o://1.16.1-4.dev.rhaos4.3.git0c5c314.el8\n Kubelet Version:                         v1.16.2\n Kube-Proxy Version:                      v1.16.2\nProviderID:                               aws:///us-east-1a/i-0caceb7a3d82ef6cf\nNon-terminated Pods:                      (19 in total)\n  Namespace                               Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                        ------------  ----------  ---------------  -------------  ---\n  openshift-cluster-node-tuning-operator  tuned-b6d4c                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         40m\n  openshift-dns                           dns-default-9gg8k                           110m (3%)     0 (0%)      70Mi (0%)        512Mi (3%)     39m\n  openshift-image-registry                node-ca-zxkbz                               10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         38m\n  openshift-ingress                       router-default-6dd7776f8d-px9mm             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         38m\n  openshift-machine-config-operator       machine-config-daemon-fhxhd                 40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         39m\n  openshift-marketplace                   certified-operators-784f95ccfc-28hmb        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         38m\n  openshift-marketplace                   community-operators-57df479757-26s9p        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         38m\n  openshift-marketplace                   redhat-operators-7cc8cf78cb-l7qp4           10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         38m\n  openshift-monitoring                    alertmanager-main-0                         100m (2%)     100m (2%)   225Mi (1%)       25Mi (0%)      32m\n  openshift-monitoring                    kube-state-metrics-55f44c57bf-lqq5l         30m (0%)      0 (0%)      120Mi (0%)       0 (0%)         38m\n  openshift-monitoring                    node-exporter-6mcgp                         10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         38m\n  openshift-monitoring                    openshift-state-metrics-5466cb9dcd-5sdmv    120m (3%)     0 (0%)      190Mi (1%)       0 (0%)         38m\n  openshift-monitoring                    prometheus-adapter-7969f64c9-xg8hs          10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         32m\n  openshift-monitoring                    prometheus-k8s-1                            430m (12%)    200m (5%)   1134Mi (7%)      50Mi (0%)      31m\n  openshift-monitoring                    telemeter-client-7476f789b5-xk24h           10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         32m\n  openshift-monitoring                    thanos-querier-6648b6577b-dfq6v             40m (1%)      0 (0%)      72Mi (0%)        0 (0%)         32m\n  openshift-multus                        multus-9ljmq                                10m (0%)      0 (0%)      150Mi (0%)       0 (0%)         40m\n  openshift-sdn                           ovs-6kvjd                                   200m (5%)     0 (0%)      400Mi (2%)       0 (0%)         40m\n  openshift-sdn                           sdn-q6qf9                                   100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         40m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         1360m (38%)   300m (8%)\n  memory                      3337Mi (21%)  587Mi (3%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type    Reason                   Age                From                                  Message\n  ----    ------                   ----               ----                                  -------\n  Normal  NodeHasSufficientMemory  40m (x8 over 40m)  kubelet, ip-10-0-133-17.ec2.internal  Node ip-10-0-133-17.ec2.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasSufficientPID     40m (x8 over 40m)  kubelet, ip-10-0-133-17.ec2.internal  Node ip-10-0-133-17.ec2.internal status is now: NodeHasSufficientPID\n"
Dec 12 03:27:49.096: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe namespace kubectl-1772'
Dec 12 03:27:49.344: INFO: stderr: ""
Dec 12 03:27:49.344: INFO: stdout: "Name:         kubectl-1772\nLabels:       e2e-framework=kubectl\n              e2e-run=2aee741d-ff32-46bf-9dff-7e64639f72f8\nAnnotations:  openshift.io/sa.scc.mcs: s0:c31,c25\n              openshift.io/sa.scc.supplemental-groups: 1000980000/10000\n              openshift.io/sa.scc.uid-range: 1000980000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:27:49.344: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1772" for this suite.
Dec 12 03:28:19.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:28:21.757: INFO: namespace kubectl-1772 deletion completed in 32.366519399s


â€¢ [SLOW TEST:44.087 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:27:49.496: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4714.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4714.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4714.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4714.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:28:05.790: INFO: DNS probes using dns-test-bbaf87d7-7d35-41c2-9415-b4477d7bbc52 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4714.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4714.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4714.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4714.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:28:15.970: INFO: DNS probes using dns-test-91798a87-6918-48f0-bdad-938a90168e43 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4714.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4714.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4714.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4714.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:28:26.174: INFO: DNS probes using dns-test-9c0521f4-2a28-482e-9399-b19f325c8fb3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:28:26.227: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-4714" for this suite.
Dec 12 03:28:32.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:28:34.575: INFO: namespace dns-4714 deletion completed in 8.304885925s


â€¢ [SLOW TEST:45.079 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:28:19.043: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-dfc10ad8-0874-454b-b348-c6d137c87caa
STEP: Creating a pod to test consume secrets
Dec 12 03:28:19.212: INFO: Waiting up to 5m0s for pod "pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10" in namespace "secrets-9451" to be "success or failure"
Dec 12 03:28:19.228: INFO: Pod "pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10": Phase="Pending", Reason="", readiness=false. Elapsed: 15.859685ms
Dec 12 03:28:21.247: INFO: Pod "pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034889651s
Dec 12 03:28:23.263: INFO: Pod "pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051181833s
Dec 12 03:28:25.280: INFO: Pod "pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068412596s
Dec 12 03:28:27.297: INFO: Pod "pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.085619459s
STEP: Saw pod success
Dec 12 03:28:27.297: INFO: Pod "pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10" satisfied condition "success or failure"
Dec 12 03:28:27.313: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10 container secret-env-test: <nil>
STEP: delete the pod
Dec 12 03:28:27.360: INFO: Waiting for pod pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10 to disappear
Dec 12 03:28:27.376: INFO: Pod pod-secrets-c5e0033b-cded-4d82-b9f1-eb2ae6ae0c10 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:28:27.376: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9451" for this suite.
Dec 12 03:28:33.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:28:35.804: INFO: namespace secrets-9451 deletion completed in 8.383389878s


â€¢ [SLOW TEST:16.761 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:28:21.775: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:28:22.574: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:24.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:26.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:28.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:30.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:32.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:34.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:36.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:38.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718102, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:28:41.616: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:28:41.633: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:28:42.443: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-3363" for this suite.
Dec 12 03:28:48.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:28:50.870: INFO: namespace crd-webhook-3363 deletion completed in 8.379633399s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


â€¢ [SLOW TEST:29.163 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:28:35.827: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:28:36.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508" in namespace "projected-1892" to be "success or failure"
Dec 12 03:28:36.017: INFO: Pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508": Phase="Pending", Reason="", readiness=false. Elapsed: 16.082322ms
Dec 12 03:28:38.034: INFO: Pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033272062s
Dec 12 03:28:40.051: INFO: Pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050348799s
Dec 12 03:28:42.068: INFO: Pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067682608s
Dec 12 03:28:44.084: INFO: Pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083969779s
Dec 12 03:28:46.102: INFO: Pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102012274s
STEP: Saw pod success
Dec 12 03:28:46.102: INFO: Pod "downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508" satisfied condition "success or failure"
Dec 12 03:28:46.119: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508 container client-container: <nil>
STEP: delete the pod
Dec 12 03:28:46.166: INFO: Waiting for pod downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508 to disappear
Dec 12 03:28:46.182: INFO: Pod downwardapi-volume-3b5df859-c05b-414f-bd47-45849718a508 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:28:46.182: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1892" for this suite.
Dec 12 03:28:52.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:28:54.552: INFO: namespace projected-1892 deletion completed in 8.325972365s


â€¢ [SLOW TEST:18.725 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:28:50.969: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 12 03:28:51.166: INFO: Waiting up to 5m0s for pod "pod-059f3f10-24e4-4954-8b51-36d4dce3a5df" in namespace "emptydir-9308" to be "success or failure"
Dec 12 03:28:51.183: INFO: Pod "pod-059f3f10-24e4-4954-8b51-36d4dce3a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 16.423729ms
Dec 12 03:28:53.200: INFO: Pod "pod-059f3f10-24e4-4954-8b51-36d4dce3a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033382948s
Dec 12 03:28:55.220: INFO: Pod "pod-059f3f10-24e4-4954-8b51-36d4dce3a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053319147s
Dec 12 03:28:57.237: INFO: Pod "pod-059f3f10-24e4-4954-8b51-36d4dce3a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070308822s
Dec 12 03:28:59.253: INFO: Pod "pod-059f3f10-24e4-4954-8b51-36d4dce3a5df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.086683197s
STEP: Saw pod success
Dec 12 03:28:59.253: INFO: Pod "pod-059f3f10-24e4-4954-8b51-36d4dce3a5df" satisfied condition "success or failure"
Dec 12 03:28:59.270: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-059f3f10-24e4-4954-8b51-36d4dce3a5df container test-container: <nil>
STEP: delete the pod
Dec 12 03:28:59.320: INFO: Waiting for pod pod-059f3f10-24e4-4954-8b51-36d4dce3a5df to disappear
Dec 12 03:28:59.336: INFO: Pod pod-059f3f10-24e4-4954-8b51-36d4dce3a5df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:28:59.336: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-9308" for this suite.
Dec 12 03:29:05.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:07.784: INFO: namespace emptydir-9308 deletion completed in 8.403800901s


â€¢ [SLOW TEST:16.815 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:28:21.157: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 12 03:28:49.498: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:28:49.515: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 03:28:51.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:28:51.532: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 03:28:53.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:28:53.532: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 03:28:55.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:28:55.531: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 03:28:57.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:28:57.533: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 03:28:59.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:28:59.532: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 03:29:01.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:29:01.532: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 12 03:29:03.515: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 12 03:29:03.532: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:03.532: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3029" for this suite.
Dec 12 03:29:15.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:18.006: INFO: namespace container-lifecycle-hook-3029 deletion completed in 14.429945756s


â€¢ [SLOW TEST:56.849 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:28:54.554: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:28:55.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:57.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:28:59.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:29:01.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:29:03.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718135, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:29:06.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:06.858: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-8357" for this suite.
Dec 12 03:29:12.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:15.253: INFO: namespace webhook-8357 deletion completed in 8.348126977s
STEP: Destroying namespace "webhook-8357-markers" for this suite.
Dec 12 03:29:21.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:23.580: INFO: namespace webhook-8357-markers deletion completed in 8.326363118s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:29.094 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:07.791: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:24.282: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5897" for this suite.
Dec 12 03:29:30.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:32.705: INFO: namespace resourcequota-5897 deletion completed in 8.379409164s


â€¢ [SLOW TEST:24.914 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:28:34.605: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Dec 12 03:28:34.772: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-2728'
Dec 12 03:28:35.373: INFO: stderr: ""
Dec 12 03:28:35.373: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 03:28:35.373: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2728'
Dec 12 03:28:35.557: INFO: stderr: ""
Dec 12 03:28:35.557: INFO: stdout: "update-demo-nautilus-2vbzr update-demo-nautilus-t5z89 "
Dec 12 03:28:35.557: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2vbzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:28:35.788: INFO: stderr: ""
Dec 12 03:28:35.788: INFO: stdout: ""
Dec 12 03:28:35.788: INFO: update-demo-nautilus-2vbzr is created but not running
Dec 12 03:28:40.789: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2728'
Dec 12 03:28:40.959: INFO: stderr: ""
Dec 12 03:28:40.959: INFO: stdout: "update-demo-nautilus-2vbzr update-demo-nautilus-t5z89 "
Dec 12 03:28:40.959: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2vbzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:28:41.156: INFO: stderr: ""
Dec 12 03:28:41.156: INFO: stdout: ""
Dec 12 03:28:41.156: INFO: update-demo-nautilus-2vbzr is created but not running
Dec 12 03:28:46.157: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2728'
Dec 12 03:28:46.329: INFO: stderr: ""
Dec 12 03:28:46.329: INFO: stdout: "update-demo-nautilus-2vbzr update-demo-nautilus-t5z89 "
Dec 12 03:28:46.329: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2vbzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:28:46.501: INFO: stderr: ""
Dec 12 03:28:46.501: INFO: stdout: "true"
Dec 12 03:28:46.501: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-2vbzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:28:46.657: INFO: stderr: ""
Dec 12 03:28:46.657: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:28:46.657: INFO: validating pod update-demo-nautilus-2vbzr
Dec 12 03:28:46.677: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:28:46.677: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:28:46.677: INFO: update-demo-nautilus-2vbzr is verified up and running
Dec 12 03:28:46.677: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-t5z89 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:28:46.840: INFO: stderr: ""
Dec 12 03:28:46.840: INFO: stdout: "true"
Dec 12 03:28:46.840: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-t5z89 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:28:46.993: INFO: stderr: ""
Dec 12 03:28:46.993: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:28:46.993: INFO: validating pod update-demo-nautilus-t5z89
Dec 12 03:28:47.013: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:28:47.013: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:28:47.013: INFO: update-demo-nautilus-t5z89 is verified up and running
STEP: rolling-update to new replication controller
Dec 12 03:28:47.016: INFO: scanned /tmp/home for discovery docs: <nil>
Dec 12 03:28:47.016: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2728'
Dec 12 03:29:23.602: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 12 03:29:23.602: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 03:29:23.602: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2728'
Dec 12 03:29:23.770: INFO: stderr: ""
Dec 12 03:29:23.770: INFO: stdout: "update-demo-kitten-hjd26 update-demo-kitten-v6l8n "
Dec 12 03:29:23.771: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-hjd26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:29:23.932: INFO: stderr: ""
Dec 12 03:29:23.932: INFO: stdout: "true"
Dec 12 03:29:23.932: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-hjd26 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:29:24.091: INFO: stderr: ""
Dec 12 03:29:24.091: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 12 03:29:24.091: INFO: validating pod update-demo-kitten-hjd26
Dec 12 03:29:24.111: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 12 03:29:24.111: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 12 03:29:24.111: INFO: update-demo-kitten-hjd26 is verified up and running
Dec 12 03:29:24.111: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-v6l8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:29:24.276: INFO: stderr: ""
Dec 12 03:29:24.276: INFO: stdout: "true"
Dec 12 03:29:24.276: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-v6l8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2728'
Dec 12 03:29:24.447: INFO: stderr: ""
Dec 12 03:29:24.447: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 12 03:29:24.447: INFO: validating pod update-demo-kitten-v6l8n
Dec 12 03:29:24.467: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 12 03:29:24.468: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 12 03:29:24.468: INFO: update-demo-kitten-v6l8n is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:24.468: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2728" for this suite.
Dec 12 03:29:42.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:44.884: INFO: namespace kubectl-2728 deletion completed in 20.370908987s


â€¢ [SLOW TEST:70.279 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:23.650: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 12 03:29:23.803: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-2798'
Dec 12 03:29:24.359: INFO: stderr: ""
Dec 12 03:29:24.359: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 12 03:29:25.377: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:25.377: INFO: Found 0 / 1
Dec 12 03:29:26.376: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:26.376: INFO: Found 0 / 1
Dec 12 03:29:27.377: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:27.377: INFO: Found 0 / 1
Dec 12 03:29:28.377: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:28.377: INFO: Found 0 / 1
Dec 12 03:29:29.376: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:29.376: INFO: Found 0 / 1
Dec 12 03:29:30.377: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:30.377: INFO: Found 0 / 1
Dec 12 03:29:31.376: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:31.376: INFO: Found 0 / 1
Dec 12 03:29:32.378: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:32.378: INFO: Found 0 / 1
Dec 12 03:29:33.378: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:33.378: INFO: Found 1 / 1
Dec 12 03:29:33.378: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 12 03:29:33.394: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:33.394: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 12 03:29:33.394: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig patch pod redis-master-tnwh8 --namespace=kubectl-2798 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 12 03:29:33.611: INFO: stderr: ""
Dec 12 03:29:33.611: INFO: stdout: "pod/redis-master-tnwh8 patched\n"
STEP: checking annotations
Dec 12 03:29:33.629: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:29:33.629: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:33.629: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2798" for this suite.
Dec 12 03:29:45.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:48.059: INFO: namespace kubectl-2798 deletion completed in 14.387161778s


â€¢ [SLOW TEST:24.410 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:18.046: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 12 03:29:18.223: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:42.698: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-2897" for this suite.
Dec 12 03:29:48.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:51.150: INFO: namespace pods-2897 deletion completed in 8.407811356s


â€¢ [SLOW TEST:33.104 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:44.899: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:29:45.050: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version'
Dec 12 03:29:45.268: INFO: stderr: ""
Dec 12 03:29:45.268: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.4-1+8aa1d9dd63e9a4\", GitCommit:\"8aa1d9dd63e9a489da916580b42612a3ea286c37\", GitTreeState:\"clean\", BuildDate:\"2019-12-12T02:59:52Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2\", GitCommit:\"40f2586\", GitTreeState:\"clean\", BuildDate:\"2019-12-12T02:08:27Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:45.268: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6305" for this suite.
Dec 12 03:29:51.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:29:53.628: INFO: namespace kubectl-6305 deletion completed in 8.321995161s


â€¢ [SLOW TEST:8.729 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:53.764: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 12 03:29:54.028: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4954 /api/v1/namespaces/watch-4954/configmaps/e2e-watch-test-resource-version 029b523c-5f98-4d21-adca-0f5b404fac24 32393 0 2019-12-12 03:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 03:29:54.028: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4954 /api/v1/namespaces/watch-4954/configmaps/e2e-watch-test-resource-version 029b523c-5f98-4d21-adca-0f5b404fac24 32395 0 2019-12-12 03:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:54.028: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-4954" for this suite.
Dec 12 03:30:00.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:02.373: INFO: namespace watch-4954 deletion completed in 8.328595213s


â€¢ [SLOW TEST:8.609 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:32.748: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:54.945: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-56" for this suite.
Dec 12 03:30:01.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:03.434: INFO: namespace job-56 deletion completed in 8.44410542s


â€¢ [SLOW TEST:30.686 seconds]
[sig-apps] Job
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:48.131: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 03:29:57.488: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:29:57.531: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-9913" for this suite.
Dec 12 03:30:03.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:05.898: INFO: namespace container-runtime-9913 deletion completed in 8.323519154s


â€¢ [SLOW TEST:17.767 seconds]
[k8s.io] Container Runtime
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:02.577: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-157f6f25-3cf8-4e99-a708-df95937813bb
STEP: Creating a pod to test consume secrets
Dec 12 03:30:02.765: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250" in namespace "projected-6265" to be "success or failure"
Dec 12 03:30:02.782: INFO: Pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250": Phase="Pending", Reason="", readiness=false. Elapsed: 16.30557ms
Dec 12 03:30:04.797: INFO: Pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031978451s
Dec 12 03:30:06.816: INFO: Pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050180986s
Dec 12 03:30:08.832: INFO: Pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066479618s
Dec 12 03:30:10.849: INFO: Pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083371296s
Dec 12 03:30:12.866: INFO: Pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.100389123s
STEP: Saw pod success
Dec 12 03:30:12.866: INFO: Pod "pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250" satisfied condition "success or failure"
Dec 12 03:30:12.882: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:30:12.927: INFO: Waiting for pod pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250 to disappear
Dec 12 03:30:12.942: INFO: Pod pod-projected-secrets-83758c6e-f2f7-4bfb-ae5f-87d9c3ff1250 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:30:12.942: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6265" for this suite.
Dec 12 03:30:19.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:21.320: INFO: namespace projected-6265 deletion completed in 8.333643464s


â€¢ [SLOW TEST:18.743 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:03.460: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 12 03:30:03.595: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:30:14.064: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-1224" for this suite.
Dec 12 03:30:20.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:22.481: INFO: namespace init-container-1224 deletion completed in 8.371309945s


â€¢ [SLOW TEST:19.021 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:29:51.205: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6949
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6949
I1212 03:29:51.473162    1995 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6949, replica count: 2
I1212 03:29:54.523690    1995 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:29:57.523935    1995 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:30:00.524124    1995 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 03:30:00.524: INFO: Creating new exec pod
Dec 12 03:30:11.591: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-6949 execpodsrqb4 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 12 03:30:14.504: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 12 03:30:14.504: INFO: stdout: ""
Dec 12 03:30:14.505: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-6949 execpodsrqb4 -- /bin/sh -x -c nc -zv -t -w 2 172.30.235.5 80'
Dec 12 03:30:14.908: INFO: stderr: "+ nc -zv -t -w 2 172.30.235.5 80\nConnection to 172.30.235.5 80 port [tcp/http] succeeded!\n"
Dec 12 03:30:14.908: INFO: stdout: ""
Dec 12 03:30:14.908: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:30:14.952: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6949" for this suite.
Dec 12 03:30:21.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:23.390: INFO: namespace services-6949 deletion completed in 8.394436567s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:32.185 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:21.343: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Dec 12 03:30:21.457: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-1267'
Dec 12 03:30:21.856: INFO: stderr: ""
Dec 12 03:30:21.856: INFO: stdout: "pod/pause created\n"
Dec 12 03:30:21.856: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 12 03:30:21.856: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1267" to be "running and ready"
Dec 12 03:30:21.872: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.600305ms
Dec 12 03:30:23.888: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031827652s
Dec 12 03:30:25.904: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048086014s
Dec 12 03:30:27.922: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065491024s
Dec 12 03:30:29.938: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081717681s
Dec 12 03:30:31.955: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.098611045s
Dec 12 03:30:31.955: INFO: Pod "pause" satisfied condition "running and ready"
Dec 12 03:30:31.955: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 12 03:30:31.955: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=kubectl-1267'
Dec 12 03:30:32.164: INFO: stderr: ""
Dec 12 03:30:32.164: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 12 03:30:32.164: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=kubectl-1267'
Dec 12 03:30:32.353: INFO: stderr: ""
Dec 12 03:30:32.354: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 12 03:30:32.354: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label- --namespace=kubectl-1267'
Dec 12 03:30:32.569: INFO: stderr: ""
Dec 12 03:30:32.569: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 12 03:30:32.569: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=kubectl-1267'
Dec 12 03:30:32.787: INFO: stderr: ""
Dec 12 03:30:32.787: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   \n"
[AfterEach] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Dec 12 03:30:32.787: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-1267'
Dec 12 03:30:32.987: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:30:32.987: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 12 03:30:32.987: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=kubectl-1267'
Dec 12 03:30:33.192: INFO: stderr: "No resources found in kubectl-1267 namespace.\n"
Dec 12 03:30:33.192: INFO: stdout: ""
Dec 12 03:30:33.192: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=pause --namespace=kubectl-1267 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 03:30:33.364: INFO: stderr: ""
Dec 12 03:30:33.364: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:30:33.364: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1267" for this suite.
Dec 12 03:30:39.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:41.790: INFO: namespace kubectl-1267 deletion completed in 8.383594459s


â€¢ [SLOW TEST:20.447 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:23.415: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:30:24.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-64d485d9bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:30:26.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:30:28.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:30:30.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:30:32.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718224, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:30:35.232: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:30:35.248: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:30:36.433: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-4121" for this suite.
Dec 12 03:30:44.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:46.875: INFO: namespace crd-webhook-4121 deletion completed in 10.397106957s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


â€¢ [SLOW TEST:23.529 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:05.960: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1212 03:30:46.238588    1997 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 03:30:46.238: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:30:46.238: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-404" for this suite.
Dec 12 03:30:54.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:30:56.597: INFO: namespace gc-404 deletion completed in 10.341474645s


â€¢ [SLOW TEST:50.638 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:41.810: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 03:30:41.976: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-4231'
Dec 12 03:30:42.229: INFO: stderr: ""
Dec 12 03:30:42.229: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 12 03:30:52.280: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod e2e-test-httpd-pod --namespace=kubectl-4231 -o json'
Dec 12 03:30:52.471: INFO: stderr: ""
Dec 12 03:30:52.471: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.2.91\\\"\\n    ],\\n    \\\"dns\\\": {},\\n    \\\"default-route\\\": [\\n        \\\"10.128.2.1\\\"\\n    ]\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2019-12-12T03:30:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4231\",\n        \"resourceVersion\": \"33447\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4231/pods/e2e-test-httpd-pod\",\n        \"uid\": \"6fdf80e3-e139-43a3-9158-dfcb82da16c7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-tk668\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-133-2.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c35,c30\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-tk668\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-tk668\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T03:30:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T03:30:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T03:30:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-12T03:30:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://4abb40669027373b12a44a6527a306278340853ec65bea42c3b4e0dbba04a331\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-12T03:30:50Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.133.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.2.91\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.128.2.91\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-12T03:30:42Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 12 03:30:52.472: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig replace -f - --namespace=kubectl-4231'
Dec 12 03:30:52.889: INFO: stderr: ""
Dec 12 03:30:52.889: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Dec 12 03:30:52.905: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-httpd-pod --namespace=kubectl-4231'
Dec 12 03:31:02.650: INFO: stderr: ""
Dec 12 03:31:02.650: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:31:02.650: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4231" for this suite.
Dec 12 03:31:08.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:31:11.007: INFO: namespace kubectl-4231 deletion completed in 8.311352427s


â€¢ [SLOW TEST:29.197 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:56.599: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 03:30:56.720: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5065'
Dec 12 03:30:56.892: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 03:30:56.892: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Dec 12 03:30:56.909: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete jobs e2e-test-httpd-job --namespace=kubectl-5065'
Dec 12 03:30:57.103: INFO: stderr: ""
Dec 12 03:30:57.103: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:30:57.103: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5065" for this suite.
Dec 12 03:31:09.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:31:11.457: INFO: namespace kubectl-5065 deletion completed in 14.335516968s


â€¢ [SLOW TEST:14.858 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:31:11.463: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-7e69a029-bd4d-4a1a-92dd-92bb015462b9
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:31:11.631: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-5386" for this suite.
Dec 12 03:31:17.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:31:19.983: INFO: namespace secrets-5386 deletion completed in 8.334089501s


â€¢ [SLOW TEST:8.519 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:31:11.084: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 12 03:31:11.235: INFO: Waiting up to 5m0s for pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1" in namespace "emptydir-4492" to be "success or failure"
Dec 12 03:31:11.258: INFO: Pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.543441ms
Dec 12 03:31:13.275: INFO: Pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039247702s
Dec 12 03:31:15.291: INFO: Pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055729177s
Dec 12 03:31:17.309: INFO: Pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073976109s
Dec 12 03:31:19.327: INFO: Pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091196109s
Dec 12 03:31:21.343: INFO: Pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.1073683s
STEP: Saw pod success
Dec 12 03:31:21.343: INFO: Pod "pod-e7017042-c872-4c40-ad9e-692f890f0ed1" satisfied condition "success or failure"
Dec 12 03:31:21.359: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-e7017042-c872-4c40-ad9e-692f890f0ed1 container test-container: <nil>
STEP: delete the pod
Dec 12 03:31:21.402: INFO: Waiting for pod pod-e7017042-c872-4c40-ad9e-692f890f0ed1 to disappear
Dec 12 03:31:21.418: INFO: Pod pod-e7017042-c872-4c40-ad9e-692f890f0ed1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:31:21.418: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4492" for this suite.
Dec 12 03:31:27.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:31:29.768: INFO: namespace emptydir-4492 deletion completed in 8.307729259s


â€¢ [SLOW TEST:18.685 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:46.957: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 12 03:30:47.089: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:31:00.525: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-3861" for this suite.
Dec 12 03:31:28.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:31:30.976: INFO: namespace init-container-3861 deletion completed in 30.406140719s


â€¢ [SLOW TEST:44.019 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:31:29.804: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 12 03:31:40.608: INFO: Successfully updated pod "pod-update-activedeadlineseconds-38985723-bcec-4234-a95c-276d014b9c9a"
Dec 12 03:31:40.608: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-38985723-bcec-4234-a95c-276d014b9c9a" in namespace "pods-5520" to be "terminated due to deadline exceeded"
Dec 12 03:31:40.625: INFO: Pod "pod-update-activedeadlineseconds-38985723-bcec-4234-a95c-276d014b9c9a": Phase="Running", Reason="", readiness=true. Elapsed: 16.268186ms
Dec 12 03:31:42.642: INFO: Pod "pod-update-activedeadlineseconds-38985723-bcec-4234-a95c-276d014b9c9a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.033406144s
Dec 12 03:31:42.642: INFO: Pod "pod-update-activedeadlineseconds-38985723-bcec-4234-a95c-276d014b9c9a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:31:42.642: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-5520" for this suite.
Dec 12 03:31:48.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:31:51.066: INFO: namespace pods-5520 deletion completed in 8.359232273s


â€¢ [SLOW TEST:21.262 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:31:30.988: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3912.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3912.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 206.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.206_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3912.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3912.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3912.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3912.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3912.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3912.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 206.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.190.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.190.206_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:31:41.329: INFO: Unable to read wheezy_udp@dns-test-service.dns-3912.svc.cluster.local from pod dns-3912/dns-test-5463c043-31ae-4365-81c1-8f88e8d249de: the server could not find the requested resource (get pods dns-test-5463c043-31ae-4365-81c1-8f88e8d249de)
Dec 12 03:31:41.349: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3912.svc.cluster.local from pod dns-3912/dns-test-5463c043-31ae-4365-81c1-8f88e8d249de: the server could not find the requested resource (get pods dns-test-5463c043-31ae-4365-81c1-8f88e8d249de)
Dec 12 03:31:41.546: INFO: Unable to read jessie_udp@dns-test-service.dns-3912.svc.cluster.local from pod dns-3912/dns-test-5463c043-31ae-4365-81c1-8f88e8d249de: the server could not find the requested resource (get pods dns-test-5463c043-31ae-4365-81c1-8f88e8d249de)
Dec 12 03:31:41.603: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3912.svc.cluster.local from pod dns-3912/dns-test-5463c043-31ae-4365-81c1-8f88e8d249de: the server could not find the requested resource (get pods dns-test-5463c043-31ae-4365-81c1-8f88e8d249de)
Dec 12 03:31:41.733: INFO: Lookups using dns-3912/dns-test-5463c043-31ae-4365-81c1-8f88e8d249de failed for: [wheezy_udp@dns-test-service.dns-3912.svc.cluster.local wheezy_tcp@dns-test-service.dns-3912.svc.cluster.local jessie_udp@dns-test-service.dns-3912.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3912.svc.cluster.local]

Dec 12 03:31:47.137: INFO: DNS probes using dns-3912/dns-test-5463c043-31ae-4365-81c1-8f88e8d249de succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:31:47.249: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-3912" for this suite.
Dec 12 03:31:53.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:31:55.656: INFO: namespace dns-3912 deletion completed in 8.376467763s


â€¢ [SLOW TEST:24.668 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:31:55.763: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:32:07.048: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-3531" for this suite.
Dec 12 03:32:13.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:32:15.501: INFO: namespace resourcequota-3531 deletion completed in 8.408292875s


â€¢ [SLOW TEST:19.738 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:31:51.244: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:31:51.459: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 12 03:31:56.477: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 12 03:32:00.518: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 12 03:32:10.663: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2615 /apis/apps/v1/namespaces/deployment-2615/deployments/test-cleanup-deployment 08a45e28-7c69-4ce6-b397-7a2be2e4192d 34510 1 2019-12-12 03:32:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035ed7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-12 03:32:00 +0000 UTC,LastTransitionTime:2019-12-12 03:32:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2019-12-12 03:32:09 +0000 UTC,LastTransitionTime:2019-12-12 03:32:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 12 03:32:10.679: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-2615 /apis/apps/v1/namespaces/deployment-2615/replicasets/test-cleanup-deployment-65db99849b 18668176-86bb-43d8-b1cc-d6548f535032 34499 1 2019-12-12 03:32:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 08a45e28-7c69-4ce6-b397-7a2be2e4192d 0xc0035edbe7 0xc0035edbe8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0035edc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:32:10.697: INFO: Pod "test-cleanup-deployment-65db99849b-sqj9l" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-sqj9l test-cleanup-deployment-65db99849b- deployment-2615 /api/v1/namespaces/deployment-2615/pods/test-cleanup-deployment-65db99849b-sqj9l fa39dbec-b309-4646-9187-5a29e610d8b2 34498 0 2019-12-12 03:32:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.98"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 18668176-86bb-43d8-b1cc-d6548f535032 0xc00361ce47 0xc00361ce48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qkmwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qkmwv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qkmwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-hslxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:32:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:32:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:10.128.2.98,StartTime:2019-12-12 03:32:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:32:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://933ac1ab9939256c890be363cf5591436775fda3741cbce7150d9ec146f27212,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:32:10.697: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-2615" for this suite.
Dec 12 03:32:16.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:32:19.227: INFO: namespace deployment-2615 deletion completed in 8.485654066s


â€¢ [SLOW TEST:27.983 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:30:22.496: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:30:22.670: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-34b01b83-8f6d-41d0-a1f1-7c08078db04d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-34b01b83-8f6d-41d0-a1f1-7c08078db04d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:31:55.759: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6776" for this suite.
Dec 12 03:32:23.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:32:26.222: INFO: namespace projected-6776 deletion completed in 30.416559196s


â€¢ [SLOW TEST:123.725 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:32:15.736: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:32:23.004: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5756" for this suite.
Dec 12 03:32:29.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:32:31.414: INFO: namespace resourcequota-5756 deletion completed in 8.365732241s


â€¢ [SLOW TEST:15.678 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:32:19.523: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-1ed04cce-d4f1-4c1f-bb6c-0cad5e5070e6
STEP: Creating a pod to test consume configMaps
Dec 12 03:32:19.711: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094" in namespace "projected-6013" to be "success or failure"
Dec 12 03:32:19.729: INFO: Pod "pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 17.338878ms
Dec 12 03:32:21.746: INFO: Pod "pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034461445s
Dec 12 03:32:23.766: INFO: Pod "pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054358132s
Dec 12 03:32:25.785: INFO: Pod "pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074015439s
Dec 12 03:32:27.804: INFO: Pod "pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.092363854s
STEP: Saw pod success
Dec 12 03:32:27.804: INFO: Pod "pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094" satisfied condition "success or failure"
Dec 12 03:32:27.819: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:32:27.866: INFO: Waiting for pod pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094 to disappear
Dec 12 03:32:27.881: INFO: Pod pod-projected-configmaps-0ff33101-1dd7-43ac-9f0f-abcac56e9094 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:32:27.881: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6013" for this suite.
Dec 12 03:32:33.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:32:36.294: INFO: namespace projected-6013 deletion completed in 8.369205802s


â€¢ [SLOW TEST:16.770 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:32:26.600: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-e6b69f74-a019-4ab9-a71e-5929f0f3688b
STEP: Creating a pod to test consume secrets
Dec 12 03:32:26.934: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7" in namespace "projected-2375" to be "success or failure"
Dec 12 03:32:26.958: INFO: Pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 23.926739ms
Dec 12 03:32:28.976: INFO: Pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041965258s
Dec 12 03:32:30.994: INFO: Pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059818862s
Dec 12 03:32:33.013: INFO: Pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079585099s
Dec 12 03:32:35.092: INFO: Pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.157974367s
Dec 12 03:32:37.110: INFO: Pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.176176449s
STEP: Saw pod success
Dec 12 03:32:37.110: INFO: Pod "pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7" satisfied condition "success or failure"
Dec 12 03:32:37.126: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:32:37.173: INFO: Waiting for pod pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7 to disappear
Dec 12 03:32:37.191: INFO: Pod pod-projected-secrets-4e0cbd5f-a991-47df-8b85-280a5fb3a2a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:32:37.191: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2375" for this suite.
Dec 12 03:32:43.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:32:45.678: INFO: namespace projected-2375 deletion completed in 8.441698804s


â€¢ [SLOW TEST:19.077 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:32:31.485: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9552
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9552
I1212 03:32:31.724231    1995 runners.go:184] Created replication controller with name: externalname-service, namespace: services-9552, replica count: 2
I1212 03:32:34.775498    1995 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:32:37.775700    1995 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:32:40.777971    1995 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:32:43.779360    1995 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 03:32:43.779: INFO: Creating new exec pod
Dec 12 03:32:54.874: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-9552 execpodn6ktp -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 12 03:32:58.001: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 12 03:32:58.002: INFO: stdout: ""
Dec 12 03:32:58.002: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-9552 execpodn6ktp -- /bin/sh -x -c nc -zv -t -w 2 172.30.66.106 80'
Dec 12 03:32:58.469: INFO: stderr: "+ nc -zv -t -w 2 172.30.66.106 80\nConnection to 172.30.66.106 80 port [tcp/http] succeeded!\n"
Dec 12 03:32:58.469: INFO: stdout: ""
Dec 12 03:32:58.470: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-9552 execpodn6ktp -- /bin/sh -x -c nc -zv -t -w 2 10.0.133.17 32000'
Dec 12 03:32:58.962: INFO: stderr: "+ nc -zv -t -w 2 10.0.133.17 32000\nConnection to 10.0.133.17 32000 port [tcp/32000] succeeded!\n"
Dec 12 03:32:58.962: INFO: stdout: ""
Dec 12 03:32:58.962: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-9552 execpodn6ktp -- /bin/sh -x -c nc -zv -t -w 2 10.0.133.2 32000'
Dec 12 03:32:59.451: INFO: stderr: "+ nc -zv -t -w 2 10.0.133.2 32000\nConnection to 10.0.133.2 32000 port [tcp/32000] succeeded!\n"
Dec 12 03:32:59.451: INFO: stdout: ""
Dec 12 03:32:59.451: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:32:59.507: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-9552" for this suite.
Dec 12 03:33:05.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:33:07.964: INFO: namespace services-9552 deletion completed in 8.399547271s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:36.478 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:32:45.793: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-hwpn
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 03:32:46.053: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hwpn" in namespace "subpath-9012" to be "success or failure"
Dec 12 03:32:46.069: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Pending", Reason="", readiness=false. Elapsed: 16.712632ms
Dec 12 03:32:48.095: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042738867s
Dec 12 03:32:50.114: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061068299s
Dec 12 03:32:52.131: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078038829s
Dec 12 03:32:54.148: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094977159s
Dec 12 03:32:56.165: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 10.112086657s
Dec 12 03:32:58.194: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 12.140991214s
Dec 12 03:33:00.213: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 14.159999343s
Dec 12 03:33:02.230: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 16.176985016s
Dec 12 03:33:04.248: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 18.194976115s
Dec 12 03:33:06.265: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 20.211973043s
Dec 12 03:33:08.305: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 22.251948924s
Dec 12 03:33:10.321: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 24.268710456s
Dec 12 03:33:12.344: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 26.290998813s
Dec 12 03:33:14.361: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Running", Reason="", readiness=true. Elapsed: 28.30788679s
Dec 12 03:33:16.378: INFO: Pod "pod-subpath-test-projected-hwpn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.325231953s
STEP: Saw pod success
Dec 12 03:33:16.378: INFO: Pod "pod-subpath-test-projected-hwpn" satisfied condition "success or failure"
Dec 12 03:33:16.396: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-subpath-test-projected-hwpn container test-container-subpath-projected-hwpn: <nil>
STEP: delete the pod
Dec 12 03:33:16.442: INFO: Waiting for pod pod-subpath-test-projected-hwpn to disappear
Dec 12 03:33:16.459: INFO: Pod pod-subpath-test-projected-hwpn no longer exists
STEP: Deleting pod pod-subpath-test-projected-hwpn
Dec 12 03:33:16.459: INFO: Deleting pod "pod-subpath-test-projected-hwpn" in namespace "subpath-9012"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:33:16.475: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-9012" for this suite.
Dec 12 03:33:22.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:33:24.964: INFO: namespace subpath-9012 deletion completed in 8.443552137s


â€¢ [SLOW TEST:39.170 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:33:08.281: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:33:08.424: INFO: Creating deployment "webserver-deployment"
Dec 12 03:33:08.444: INFO: Waiting for observed generation 1
Dec 12 03:33:10.482: INFO: Waiting for all required pods to come up
Dec 12 03:33:10.517: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 12 03:33:18.573: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 12 03:33:18.609: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 12 03:33:18.661: INFO: Updating deployment webserver-deployment
Dec 12 03:33:18.661: INFO: Waiting for observed generation 2
Dec 12 03:33:20.695: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 12 03:33:20.711: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 12 03:33:20.727: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 12 03:33:20.783: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 12 03:33:20.783: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 12 03:33:20.802: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 12 03:33:20.834: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 12 03:33:20.834: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 12 03:33:20.871: INFO: Updating deployment webserver-deployment
Dec 12 03:33:20.871: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 12 03:33:20.906: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 12 03:33:22.947: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 12 03:33:22.982: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8869 /apis/apps/v1/namespaces/deployment-8869/deployments/webserver-deployment 1baa6656-d0fd-4763-93d1-ea510c83fd93 35641 3 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004085cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-12 03:33:20 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-12 03:33:21 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 12 03:33:22.999: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8869 /apis/apps/v1/namespaces/deployment-8869/replicasets/webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 35639 3 2019-12-12 03:33:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1baa6656-d0fd-4763-93d1-ea510c83fd93 0xc0074b8ea7 0xc0074b8ea8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0074b8f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:33:22.999: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 12 03:33:22.999: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8869 /apis/apps/v1/namespaces/deployment-8869/replicasets/webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 35599 3 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1baa6656-d0fd-4763-93d1-ea510c83fd93 0xc0074b8de7 0xc0074b8de8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0074b8e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:33:23.046: INFO: Pod "webserver-deployment-595b5b9587-2fwmg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2fwmg webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-2fwmg cae1fea3-2d50-4db0-bfc6-8f85dd34ed10 35589 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0147 0xc00bee0148}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.046: INFO: Pod "webserver-deployment-595b5b9587-42m5w" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-42m5w webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-42m5w 1de6c22d-bc2c-42a4-bd51-4ed6ee931451 35403 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.109"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee02b7 0xc00bee02b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:10.128.2.109,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://42474937d49e1b33ecb2ea40466bf5baa242f1106613e3e41442028dc708fa73,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.047: INFO: Pod "webserver-deployment-595b5b9587-542wc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-542wc webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-542wc f60021e1-85b4-4d55-a307-f10ddeb15a03 35600 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0440 0xc00bee0441}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.047: INFO: Pod "webserver-deployment-595b5b9587-5q6l5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5q6l5 webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-5q6l5 2842d875-bcee-49c6-be5d-23a34ad8e777 35550 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee05a7 0xc00bee05a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.047: INFO: Pod "webserver-deployment-595b5b9587-6lv6n" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6lv6n webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-6lv6n 9e7406a0-e12c-4220-9607-8a69c1bc798d 35579 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0710 0xc00bee0711}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.047: INFO: Pod "webserver-deployment-595b5b9587-7l9k7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7l9k7 webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-7l9k7 96e5b3a8-1c97-4e23-bb60-ff041aeff0a0 35440 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.43"
    ],
    "dns": {},
    "default-route": [
        "10.131.0.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0870 0xc00bee0871}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:10.131.0.43,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://953b76a95c05f0410a6d5295e0db2d1f25dff873dd6a9932dc9e89318d120d2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.047: INFO: Pod "webserver-deployment-595b5b9587-9zdg8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9zdg8 webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-9zdg8 989a0227-a6ac-461d-8e36-fa11164ab4e9 35601 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee09f0 0xc00bee09f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.047: INFO: Pod "webserver-deployment-595b5b9587-b2qwn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-b2qwn webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-b2qwn dddcea8b-3782-4af0-b7b0-27a011eb137a 35442 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.30"
    ],
    "dns": {},
    "default-route": [
        "10.129.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0b57 0xc00bee0b58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:10.129.2.30,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://6e9281ec1d47d1241f71feac5899931b6494b933612f0eb1a6f07701142d248b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.048: INFO: Pod "webserver-deployment-595b5b9587-ct2bx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ct2bx webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-ct2bx 6670b0ff-ec28-4789-a376-f4af7fbb3bf0 35588 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0ce0 0xc00bee0ce1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.048: INFO: Pod "webserver-deployment-595b5b9587-fhfkm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fhfkm webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-fhfkm 6c6f004c-246f-42ff-85be-7eb3468c04c0 35603 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0e40 0xc00bee0e41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.048: INFO: Pod "webserver-deployment-595b5b9587-fxlvq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fxlvq webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-fxlvq d456d531-6415-413e-a77e-63018a361b03 35439 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.28"
    ],
    "dns": {},
    "default-route": [
        "10.129.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee0fa0 0xc00bee0fa1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:10.129.2.28,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://e05e5c7d85954033ad7b7037ce6ed28a301de09e20e50d7d5ee1c313117c976d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.048: INFO: Pod "webserver-deployment-595b5b9587-hp2d8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hp2d8 webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-hp2d8 1b4a418e-7f8c-4411-8473-05c197988cca 35544 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee1120 0xc00bee1121}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.048: INFO: Pod "webserver-deployment-595b5b9587-lnbkm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lnbkm webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-lnbkm 2d38382e-9a03-419f-bf74-b0b959f447ae 35563 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee1287 0xc00bee1288}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.049: INFO: Pod "webserver-deployment-595b5b9587-mcw46" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mcw46 webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-mcw46 61e090ac-75b5-4b8d-9a1e-d7900518045f 35618 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee13f7 0xc00bee13f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.049: INFO: Pod "webserver-deployment-595b5b9587-pzrks" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pzrks webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-pzrks ced149ac-5fa4-4768-8476-f2b49868132c 35449 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.31"
    ],
    "dns": {},
    "default-route": [
        "10.129.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee1560 0xc00bee1561}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:10.129.2.31,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://43daa291695b713c07eef2d95a90c87db99e461a774bbcbd0e2809d79b29a98b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.049: INFO: Pod "webserver-deployment-595b5b9587-rxqr9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rxqr9 webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-rxqr9 415fa7df-5720-4df9-8626-96df99390334 35401 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.105"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee16e0 0xc00bee16e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:10.128.2.105,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://15f8483d3539e4cee6a3fe057c41753c4d4712f15acb7770316b3ebe7c0ae9d2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.049: INFO: Pod "webserver-deployment-595b5b9587-vzp87" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vzp87 webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-vzp87 f00cc724-60d6-4cdf-adf6-95aeb3db35e7 35566 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee1860 0xc00bee1861}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.049: INFO: Pod "webserver-deployment-595b5b9587-wpxpr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wpxpr webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-wpxpr 15b4959f-4760-44d4-8401-2a610897b20c 35452 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.106"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee19c0 0xc00bee19c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:10.128.2.106,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://9b02cd0487adc9689c1148aa2d436de9aa7a05e41933e4175e7263ca722ce28b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.049: INFO: Pod "webserver-deployment-595b5b9587-xzhzv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xzhzv webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-xzhzv d2df0350-ee29-41b0-a6af-0e528b331012 35587 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee1b40 0xc00bee1b41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.050: INFO: Pod "webserver-deployment-595b5b9587-zsgpw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zsgpw webserver-deployment-595b5b9587- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-595b5b9587-zsgpw 9487e518-86df-435f-8eb8-4f498ae94300 35445 0 2019-12-12 03:33:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.29"
    ],
    "dns": {},
    "default-route": [
        "10.129.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3e858c52-dd23-4c5b-8548-6a5d7f088a7d 0xc00bee1ca7 0xc00bee1ca8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:10.129.2.29,StartTime:2019-12-12 03:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:33:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://8f89f602eddc87c4b3ffd035448a419fe368d22512d9429cf4a3c0b0801db756,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.050: INFO: Pod "webserver-deployment-c7997dcc8-2ms8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2ms8p webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-2ms8p f94ef671-be33-434b-9d3d-aebf51df06c0 35630 0 2019-12-12 03:33:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc00bee1e30 0xc00bee1e31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.050: INFO: Pod "webserver-deployment-c7997dcc8-4c8jz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4c8jz webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-4c8jz 7c44fa64-ade5-4c9a-ab6d-4090b596cc92 35608 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc00bee1fb0 0xc00bee1fb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.050: INFO: Pod "webserver-deployment-c7997dcc8-6hw8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6hw8s webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-6hw8s 0f8ea081-bfa5-4950-bbbf-b4348491ce75 35506 0 2019-12-12 03:33:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005130130 0xc005130131}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.050: INFO: Pod "webserver-deployment-c7997dcc8-fb4zz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fb4zz webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-fb4zz 181a3e12-84cb-43f0-a71e-7c511bddf2eb 35625 0 2019-12-12 03:33:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc0051302b0 0xc0051302b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.051: INFO: Pod "webserver-deployment-c7997dcc8-l2s7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-l2s7z webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-l2s7z e35382be-8a34-47e9-9f9c-cb4487f7bcbe 35627 0 2019-12-12 03:33:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005130430 0xc005130431}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.051: INFO: Pod "webserver-deployment-c7997dcc8-lbzwn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lbzwn webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-lbzwn 7a07a827-3460-4b87-976e-643379d8878d 35565 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc0051305b0 0xc0051305b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:,StartTime:2019-12-12 03:33:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.051: INFO: Pod "webserver-deployment-c7997dcc8-lwkgf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lwkgf webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-lwkgf 62f2471d-e20d-4f9f-8424-81f63cb219c7 35500 0 2019-12-12 03:33:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005130730 0xc005130731}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:,StartTime:2019-12-12 03:33:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.051: INFO: Pod "webserver-deployment-c7997dcc8-nkp9l" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nkp9l webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-nkp9l 3b82954c-cd5f-4e11-abaf-5a56baf6664c 35607 0 2019-12-12 03:33:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc0051308b0 0xc0051308b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.051: INFO: Pod "webserver-deployment-c7997dcc8-pfh9x" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pfh9x webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-pfh9x ae9a3bec-bfd4-4da9-bb79-38dd807ca9a9 35501 0 2019-12-12 03:33:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005130a30 0xc005130a31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.052: INFO: Pod "webserver-deployment-c7997dcc8-t7sp4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-t7sp4 webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-t7sp4 839132e4-5865-4a90-b405-8a23fa3eaf55 35514 0 2019-12-12 03:33:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005130bb0 0xc005130bb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-48.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.48,PodIP:,StartTime:2019-12-12 03:33:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.052: INFO: Pod "webserver-deployment-c7997dcc8-tpfj2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tpfj2 webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-tpfj2 17e1366c-932a-4e73-be0d-8fbbc0ad5516 35638 0 2019-12-12 03:33:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005130d30 0xc005130d31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-17.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.17,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.052: INFO: Pod "webserver-deployment-c7997dcc8-twsvp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-twsvp webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-twsvp e6f1931e-5ee8-4155-8db3-63f55c3c4efc 35490 0 2019-12-12 03:33:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005130eb0 0xc005130eb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 12 03:33:23.052: INFO: Pod "webserver-deployment-c7997dcc8-wmlpg" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wmlpg webserver-deployment-c7997dcc8- deployment-8869 /api/v1/namespaces/deployment-8869/pods/webserver-deployment-c7997dcc8-wmlpg a2e8ccc6-25d7-4585-bdcd-09de135f2022 35626 0 2019-12-12 03:33:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 54899458-46cd-4db4-8665-5cf955c5eb04 0xc005131030 0xc005131031}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zm9qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zm9qd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zm9qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wd98b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:33:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:33:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:33:23.052: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-8869" for this suite.
Dec 12 03:33:31.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:33:33.476: INFO: namespace deployment-8869 deletion completed in 10.405834212s


â€¢ [SLOW TEST:25.195 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:32:36.522: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:32:36.665: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:32:45.041: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8125" for this suite.
Dec 12 03:33:39.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:33:41.436: INFO: namespace pods-8125 deletion completed in 56.34869664s


â€¢ [SLOW TEST:64.913 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:33:25.048: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:33:36.396: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-9502" for this suite.
Dec 12 03:33:42.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:33:44.859: INFO: namespace resourcequota-9502 deletion completed in 8.411579512s


â€¢ [SLOW TEST:19.811 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Projected combined
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:33:33.549: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-2e76d1f6-e930-4b4a-9be7-abe7084ae51d
STEP: Creating secret with name secret-projected-all-test-volume-701f53c3-88af-4ed9-a24d-8cdc1d006b7f
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 12 03:33:33.759: INFO: Waiting up to 5m0s for pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f" in namespace "projected-2900" to be "success or failure"
Dec 12 03:33:33.775: INFO: Pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.90486ms
Dec 12 03:33:35.795: INFO: Pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036243349s
Dec 12 03:33:37.813: INFO: Pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05401486s
Dec 12 03:33:39.831: INFO: Pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072071473s
Dec 12 03:33:41.848: INFO: Pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08897849s
Dec 12 03:33:43.864: INFO: Pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.105648428s
STEP: Saw pod success
Dec 12 03:33:43.864: INFO: Pod "projected-volume-169c3618-c778-44ba-9b07-b68727a8327f" satisfied condition "success or failure"
Dec 12 03:33:43.880: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod projected-volume-169c3618-c778-44ba-9b07-b68727a8327f container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 12 03:33:43.925: INFO: Waiting for pod projected-volume-169c3618-c778-44ba-9b07-b68727a8327f to disappear
Dec 12 03:33:43.941: INFO: Pod projected-volume-169c3618-c778-44ba-9b07-b68727a8327f no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:33:43.941: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2900" for this suite.
Dec 12 03:33:52.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:33:54.381: INFO: namespace projected-2900 deletion completed in 10.394796182s


â€¢ [SLOW TEST:20.831 seconds]
[sig-storage] Projected combined
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:31:20.007: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b in namespace container-probe-1406
Dec 12 03:31:30.208: INFO: Started pod liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b in namespace container-probe-1406
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 03:31:30.225: INFO: Initial restart count of pod liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b is 0
Dec 12 03:31:46.390: INFO: Restart count of pod container-probe-1406/liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b is now 1 (16.164927448s elapsed)
Dec 12 03:32:06.576: INFO: Restart count of pod container-probe-1406/liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b is now 2 (36.351058445s elapsed)
Dec 12 03:32:26.759: INFO: Restart count of pod container-probe-1406/liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b is now 3 (56.533988451s elapsed)
Dec 12 03:32:46.937: INFO: Restart count of pod container-probe-1406/liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b is now 4 (1m16.711914753s elapsed)
Dec 12 03:33:47.501: INFO: Restart count of pod container-probe-1406/liveness-4a05d9e2-2110-4e47-bbfe-e7d2f2afb24b is now 5 (2m17.27593622s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:33:47.528: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-1406" for this suite.
Dec 12 03:33:53.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:33:55.954: INFO: namespace container-probe-1406 deletion completed in 8.376658754s


â€¢ [SLOW TEST:155.947 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:33:44.861: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6bbf9a53-85fe-402c-84ca-0f79ae66a490
STEP: Creating a pod to test consume secrets
Dec 12 03:33:45.046: INFO: Waiting up to 5m0s for pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6" in namespace "secrets-9496" to be "success or failure"
Dec 12 03:33:45.063: INFO: Pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.258279ms
Dec 12 03:33:47.080: INFO: Pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033439055s
Dec 12 03:33:49.096: INFO: Pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049877662s
Dec 12 03:33:51.113: INFO: Pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066313813s
Dec 12 03:33:53.130: INFO: Pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08328658s
Dec 12 03:33:55.146: INFO: Pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099988689s
STEP: Saw pod success
Dec 12 03:33:55.146: INFO: Pod "pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6" satisfied condition "success or failure"
Dec 12 03:33:55.167: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:33:55.222: INFO: Waiting for pod pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6 to disappear
Dec 12 03:33:55.239: INFO: Pod pod-secrets-972d6d9b-3b40-4d25-94bd-a4eaad419eb6 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:33:55.239: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9496" for this suite.
Dec 12 03:34:01.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:34:03.654: INFO: namespace secrets-9496 deletion completed in 8.371986711s


â€¢ [SLOW TEST:18.793 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:34:03.736: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1148/secret-test-c2ec959d-e1e4-4fb3-8782-4db11a810423
STEP: Creating a pod to test consume secrets
Dec 12 03:34:03.925: INFO: Waiting up to 5m0s for pod "pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c" in namespace "secrets-1148" to be "success or failure"
Dec 12 03:34:03.941: INFO: Pod "pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.127222ms
Dec 12 03:34:05.959: INFO: Pod "pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033935624s
Dec 12 03:34:07.975: INFO: Pod "pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050542082s
Dec 12 03:34:09.993: INFO: Pod "pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068202223s
Dec 12 03:34:12.010: INFO: Pod "pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.084822014s
STEP: Saw pod success
Dec 12 03:34:12.010: INFO: Pod "pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c" satisfied condition "success or failure"
Dec 12 03:34:12.027: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c container env-test: <nil>
STEP: delete the pod
Dec 12 03:34:12.078: INFO: Waiting for pod pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c to disappear
Dec 12 03:34:12.096: INFO: Pod pod-configmaps-66ecbf29-1ae0-4c5a-a505-016d4be7cb9c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:34:12.096: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1148" for this suite.
Dec 12 03:34:18.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:34:20.530: INFO: namespace secrets-1148 deletion completed in 8.389998656s


â€¢ [SLOW TEST:16.794 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:33:55.988: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-2632
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2632 to expose endpoints map[]
Dec 12 03:33:56.183: INFO: successfully validated that service multi-endpoint-test in namespace services-2632 exposes endpoints map[] (17.929819ms elapsed)
STEP: Creating pod pod1 in namespace services-2632
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2632 to expose endpoints map[pod1:[100]]
Dec 12 03:34:00.417: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.183323115s elapsed, will retry)
Dec 12 03:34:05.594: INFO: successfully validated that service multi-endpoint-test in namespace services-2632 exposes endpoints map[pod1:[100]] (9.360047474s elapsed)
STEP: Creating pod pod2 in namespace services-2632
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2632 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 12 03:34:09.892: INFO: Unexpected endpoints: found map[43f8f47e-4792-4b53-81eb-4c6fc7674153:[100]], expected map[pod1:[100] pod2:[101]] (4.272320197s elapsed, will retry)
Dec 12 03:34:15.158: INFO: successfully validated that service multi-endpoint-test in namespace services-2632 exposes endpoints map[pod1:[100] pod2:[101]] (9.537933616s elapsed)
STEP: Deleting pod pod1 in namespace services-2632
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2632 to expose endpoints map[pod2:[101]]
Dec 12 03:34:15.209: INFO: successfully validated that service multi-endpoint-test in namespace services-2632 exposes endpoints map[pod2:[101]] (32.365633ms elapsed)
STEP: Deleting pod pod2 in namespace services-2632
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2632 to expose endpoints map[]
Dec 12 03:34:15.246: INFO: successfully validated that service multi-endpoint-test in namespace services-2632 exposes endpoints map[] (16.315001ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:34:15.281: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2632" for this suite.
Dec 12 03:34:33.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:34:35.716: INFO: namespace services-2632 deletion completed in 20.391443364s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:39.727 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:34:20.553: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:34:21.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:34:23.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:34:25.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:34:27.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:34:29.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718461, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:34:32.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:34:32.548: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6109-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:34:33.320: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4490" for this suite.
Dec 12 03:34:39.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:34:41.766: INFO: namespace webhook-4490 deletion completed in 8.401379618s
STEP: Destroying namespace "webhook-4490-markers" for this suite.
Dec 12 03:34:47.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:34:50.214: INFO: namespace webhook-4490-markers deletion completed in 8.447889552s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:29.733 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:34:35.764: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1212 03:34:46.245871    1997 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 03:34:46.245: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:34:46.246: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-8239" for this suite.
Dec 12 03:34:52.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:34:54.617: INFO: namespace gc-8239 deletion completed in 8.353116011s


â€¢ [SLOW TEST:18.852 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:33:54.445: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:34:04.745: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-3743" for this suite.
Dec 12 03:34:54.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:34:57.170: INFO: namespace kubelet-test-3743 deletion completed in 52.37804922s


â€¢ [SLOW TEST:62.725 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:34:50.307: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:34:50.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5" in namespace "downward-api-5975" to be "success or failure"
Dec 12 03:34:50.511: INFO: Pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.988594ms
Dec 12 03:34:52.527: INFO: Pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034375101s
Dec 12 03:34:54.544: INFO: Pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051090162s
Dec 12 03:34:56.560: INFO: Pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067523189s
Dec 12 03:34:58.579: INFO: Pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08590547s
Dec 12 03:35:00.595: INFO: Pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102559125s
STEP: Saw pod success
Dec 12 03:35:00.595: INFO: Pod "downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5" satisfied condition "success or failure"
Dec 12 03:35:00.613: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5 container client-container: <nil>
STEP: delete the pod
Dec 12 03:35:00.664: INFO: Waiting for pod downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5 to disappear
Dec 12 03:35:00.682: INFO: Pod downwardapi-volume-0e2a8300-d10b-47ff-9950-dbc1cbc352d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:00.682: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5975" for this suite.
Dec 12 03:35:06.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:35:09.123: INFO: namespace downward-api-5975 deletion completed in 8.396881235s


â€¢ [SLOW TEST:18.816 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:34:57.280: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:34:57.426: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-ac6073c1-c41c-4a27-a5ec-12e4f501bde8
STEP: Creating secret with name s-test-opt-upd-41ca2bf0-0882-4a4f-a2ed-fc87597635ea
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ac6073c1-c41c-4a27-a5ec-12e4f501bde8
STEP: Updating secret s-test-opt-upd-41ca2bf0-0882-4a4f-a2ed-fc87597635ea
STEP: Creating secret with name s-test-opt-create-30d76954-5d0a-490b-a183-ee2d5ebf18c4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:11.771: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1056" for this suite.
Dec 12 03:35:23.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:35:26.236: INFO: namespace projected-1056 deletion completed in 14.417640965s


â€¢ [SLOW TEST:28.956 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:35:09.244: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Dec 12 03:35:09.499: INFO: Waiting up to 5m0s for pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284" in namespace "containers-8536" to be "success or failure"
Dec 12 03:35:09.517: INFO: Pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284": Phase="Pending", Reason="", readiness=false. Elapsed: 17.580993ms
Dec 12 03:35:11.535: INFO: Pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03560082s
Dec 12 03:35:13.552: INFO: Pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052647421s
Dec 12 03:35:15.569: INFO: Pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070139666s
Dec 12 03:35:17.587: INFO: Pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087534681s
Dec 12 03:35:19.604: INFO: Pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104911948s
STEP: Saw pod success
Dec 12 03:35:19.604: INFO: Pod "client-containers-b2405808-3acc-4cfa-84fb-0525d117c284" satisfied condition "success or failure"
Dec 12 03:35:19.621: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod client-containers-b2405808-3acc-4cfa-84fb-0525d117c284 container test-container: <nil>
STEP: delete the pod
Dec 12 03:35:19.667: INFO: Waiting for pod client-containers-b2405808-3acc-4cfa-84fb-0525d117c284 to disappear
Dec 12 03:35:19.682: INFO: Pod client-containers-b2405808-3acc-4cfa-84fb-0525d117c284 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:19.687: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-8536" for this suite.
Dec 12 03:35:25.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:35:28.190: INFO: namespace containers-8536 deletion completed in 8.457482244s


â€¢ [SLOW TEST:18.946 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:33:41.448: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:33:41.596: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-31dbf158-d259-4b6d-a090-b77626fe10a4
STEP: Creating configMap with name cm-test-opt-upd-958915ee-41a8-45f5-af39-7f2fb14dfb26
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-31dbf158-d259-4b6d-a090-b77626fe10a4
STEP: Updating configmap cm-test-opt-upd-958915ee-41a8-45f5-af39-7f2fb14dfb26
STEP: Creating configMap with name cm-test-opt-create-3f729000-4735-4858-bc28-ab562f40b335
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:22.915: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1110" for this suite.
Dec 12 03:35:35.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:35:37.312: INFO: namespace projected-1110 deletion completed in 14.36637726s


â€¢ [SLOW TEST:115.864 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:34:54.674: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7283
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 03:34:54.846: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Dec 12 03:35:29.294: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.135:8080/dial?request=hostName&protocol=http&host=10.128.2.132&port=8080&tries=1'] Namespace:pod-network-test-7283 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:35:29.294: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:35:29.555: INFO: Waiting for endpoints: map[]
Dec 12 03:35:29.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.135:8080/dial?request=hostName&protocol=http&host=10.129.2.42&port=8080&tries=1'] Namespace:pod-network-test-7283 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:35:29.571: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:35:29.798: INFO: Waiting for endpoints: map[]
Dec 12 03:35:29.819: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.135:8080/dial?request=hostName&protocol=http&host=10.131.0.51&port=8080&tries=1'] Namespace:pod-network-test-7283 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:35:29.819: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:35:30.021: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:30.021: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-7283" for this suite.
Dec 12 03:35:36.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:35:38.485: INFO: namespace pod-network-test-7283 deletion completed in 8.42012255s


â€¢ [SLOW TEST:43.810 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:35:37.329: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-03cabe0a-7b1e-47e4-b1f8-1e9b2e96c66b
STEP: Creating a pod to test consume configMaps
Dec 12 03:35:37.515: INFO: Waiting up to 5m0s for pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c" in namespace "configmap-3449" to be "success or failure"
Dec 12 03:35:37.532: INFO: Pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.888735ms
Dec 12 03:35:39.548: INFO: Pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033084005s
Dec 12 03:35:41.568: INFO: Pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052973275s
Dec 12 03:35:43.585: INFO: Pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069951931s
Dec 12 03:35:45.601: INFO: Pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086714855s
Dec 12 03:35:47.618: INFO: Pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.103414293s
STEP: Saw pod success
Dec 12 03:35:47.618: INFO: Pod "pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c" satisfied condition "success or failure"
Dec 12 03:35:47.634: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:35:47.679: INFO: Waiting for pod pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c to disappear
Dec 12 03:35:47.696: INFO: Pod pod-configmaps-4cbd0c30-7747-45d4-9dbd-eef0e465b87c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:47.696: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-3449" for this suite.
Dec 12 03:35:53.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:35:56.063: INFO: namespace configmap-3449 deletion completed in 8.324314223s


â€¢ [SLOW TEST:18.734 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:35:38.622: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 12 03:35:38.783: INFO: Waiting up to 5m0s for pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f" in namespace "emptydir-4749" to be "success or failure"
Dec 12 03:35:38.801: INFO: Pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.745997ms
Dec 12 03:35:40.818: INFO: Pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034922563s
Dec 12 03:35:42.835: INFO: Pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052081925s
Dec 12 03:35:44.852: INFO: Pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06882055s
Dec 12 03:35:46.869: INFO: Pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086172208s
Dec 12 03:35:48.887: INFO: Pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.103968538s
STEP: Saw pod success
Dec 12 03:35:48.887: INFO: Pod "pod-0a008ce6-8e23-4b40-a076-a5356188d51f" satisfied condition "success or failure"
Dec 12 03:35:48.904: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-0a008ce6-8e23-4b40-a076-a5356188d51f container test-container: <nil>
STEP: delete the pod
Dec 12 03:35:48.952: INFO: Waiting for pod pod-0a008ce6-8e23-4b40-a076-a5356188d51f to disappear
Dec 12 03:35:48.970: INFO: Pod pod-0a008ce6-8e23-4b40-a076-a5356188d51f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:48.970: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4749" for this suite.
Dec 12 03:35:55.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:35:57.396: INFO: namespace emptydir-4749 deletion completed in 8.382198537s


â€¢ [SLOW TEST:18.775 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:35:57.425: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:35:57.725: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3403" for this suite.
Dec 12 03:36:03.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:36:06.119: INFO: namespace custom-resource-definition-3403 deletion completed in 8.377230675s


â€¢ [SLOW TEST:8.694 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:36:06.249: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:36:06.401: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:36:06.701: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7054" for this suite.
Dec 12 03:36:12.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:36:15.114: INFO: namespace custom-resource-definition-7054 deletion completed in 8.389800615s


â€¢ [SLOW TEST:8.865 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:35:56.152: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:36:39.601: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-5953" for this suite.
Dec 12 03:36:45.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:36:48.031: INFO: namespace container-runtime-5953 deletion completed in 8.386966416s


â€¢ [SLOW TEST:51.879 seconds]
[k8s.io] Container Runtime
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:36:48.122: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:36:48.338: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-9505" for this suite.
Dec 12 03:36:54.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:36:56.701: INFO: namespace kubelet-test-9505 deletion completed in 8.341246415s


â€¢ [SLOW TEST:8.579 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:35:28.294: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:35:28.417: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-bdc90c16-f12c-4fda-b724-a6faaea9d9d4
STEP: Creating configMap with name cm-test-opt-upd-33cb3032-fe2e-412d-9a47-93a56900ffa8
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bdc90c16-f12c-4fda-b724-a6faaea9d9d4
STEP: Updating configmap cm-test-opt-upd-33cb3032-fe2e-412d-9a47-93a56900ffa8
STEP: Creating configMap with name cm-test-opt-create-b562290f-1a40-407c-949b-b95f4ea58839
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:36:47.472: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-737" for this suite.
Dec 12 03:36:59.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:37:01.857: INFO: namespace configmap-737 deletion completed in 14.354373057s


â€¢ [SLOW TEST:93.562 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:36:56.806: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-a69f8852-41ad-44c1-b32b-07cb624b51f3
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:36:56.943: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7235" for this suite.
Dec 12 03:37:03.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:37:05.334: INFO: namespace configmap-7235 deletion completed in 8.372128706s


â€¢ [SLOW TEST:8.527 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:36:15.133: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 12 03:36:25.856: INFO: Successfully updated pod "adopt-release-t2x8d"
STEP: Checking that the Job readopts the Pod
Dec 12 03:36:25.856: INFO: Waiting up to 15m0s for pod "adopt-release-t2x8d" in namespace "job-1678" to be "adopted"
Dec 12 03:36:25.872: INFO: Pod "adopt-release-t2x8d": Phase="Running", Reason="", readiness=true. Elapsed: 15.919305ms
Dec 12 03:36:25.872: INFO: Pod "adopt-release-t2x8d" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 12 03:36:26.431: INFO: Successfully updated pod "adopt-release-t2x8d"
STEP: Checking that the Job releases the Pod
Dec 12 03:36:26.431: INFO: Waiting up to 15m0s for pod "adopt-release-t2x8d" in namespace "job-1678" to be "released"
Dec 12 03:36:26.451: INFO: Pod "adopt-release-t2x8d": Phase="Running", Reason="", readiness=true. Elapsed: 19.443817ms
Dec 12 03:36:26.451: INFO: Pod "adopt-release-t2x8d" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:36:26.451: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-1678" for this suite.
Dec 12 03:37:14.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:37:16.850: INFO: namespace job-1678 deletion completed in 50.346830238s


â€¢ [SLOW TEST:61.718 seconds]
[sig-apps] Job
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:37:02.022: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-zp9w
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 03:37:02.316: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zp9w" in namespace "subpath-9375" to be "success or failure"
Dec 12 03:37:02.338: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Pending", Reason="", readiness=false. Elapsed: 21.970097ms
Dec 12 03:37:04.355: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039203918s
Dec 12 03:37:06.371: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055683237s
Dec 12 03:37:08.391: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075055626s
Dec 12 03:37:10.408: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092420684s
Dec 12 03:37:12.425: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 10.108908207s
Dec 12 03:37:14.443: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 12.127027652s
Dec 12 03:37:16.460: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 14.14394672s
Dec 12 03:37:18.477: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 16.161063469s
Dec 12 03:37:20.495: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 18.178965062s
Dec 12 03:37:22.514: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 20.197972613s
Dec 12 03:37:24.532: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 22.216016663s
Dec 12 03:37:26.549: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 24.232979396s
Dec 12 03:37:28.565: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 26.249633388s
Dec 12 03:37:30.584: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Running", Reason="", readiness=true. Elapsed: 28.267970864s
Dec 12 03:37:32.604: INFO: Pod "pod-subpath-test-configmap-zp9w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.287985185s
STEP: Saw pod success
Dec 12 03:37:32.604: INFO: Pod "pod-subpath-test-configmap-zp9w" satisfied condition "success or failure"
Dec 12 03:37:32.621: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-subpath-test-configmap-zp9w container test-container-subpath-configmap-zp9w: <nil>
STEP: delete the pod
Dec 12 03:37:32.674: INFO: Waiting for pod pod-subpath-test-configmap-zp9w to disappear
Dec 12 03:37:32.689: INFO: Pod pod-subpath-test-configmap-zp9w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zp9w
Dec 12 03:37:32.689: INFO: Deleting pod "pod-subpath-test-configmap-zp9w" in namespace "subpath-9375"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:37:32.706: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-9375" for this suite.
Dec 12 03:37:38.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:37:41.136: INFO: namespace subpath-9375 deletion completed in 8.38658044s


â€¢ [SLOW TEST:39.113 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:37:16.887: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:37:17.696: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 03:37:19.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:21.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:23.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:25.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718637, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:37:28.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:37:28.899: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-6953" for this suite.
Dec 12 03:37:34.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:37:37.320: INFO: namespace webhook-6953 deletion completed in 8.388710541s
STEP: Destroying namespace "webhook-6953-markers" for this suite.
Dec 12 03:37:43.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:37:45.698: INFO: namespace webhook-6953-markers deletion completed in 8.378013114s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:28.881 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:37:05.409: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:37:05.594: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-180eb44a-15b2-405c-b874-f040e2b40ba8
STEP: Creating secret with name s-test-opt-upd-74e5c0a0-0797-4b77-a93c-3357020dece8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-180eb44a-15b2-405c-b874-f040e2b40ba8
STEP: Updating secret s-test-opt-upd-74e5c0a0-0797-4b77-a93c-3357020dece8
STEP: Creating secret with name s-test-opt-create-c8f90aa4-9ad8-45dc-b1e5-fc3e34e134c6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:37:17.913: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-4817" for this suite.
Dec 12 03:37:46.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:37:48.402: INFO: namespace secrets-4817 deletion completed in 30.446286144s


â€¢ [SLOW TEST:42.993 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:37:41.364: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec 12 03:37:41.548: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Dec 12 03:37:43.159: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:45.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:47.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:49.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:51.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:53.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:55.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:57.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:37:59.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:38:01.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718663, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718662, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:38:03.689: INFO: Waited 484.135789ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:38:06.054: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "aggregator-8320" for this suite.
Dec 12 03:38:12.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:38:14.682: INFO: namespace aggregator-8320 deletion completed in 8.577329776s


â€¢ [SLOW TEST:33.318 seconds]
[sig-api-machinery] Aggregator
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:37:46.067: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:37:46.199: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 12 03:37:59.240: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-6599 create -f -'
Dec 12 03:38:01.391: INFO: stderr: ""
Dec 12 03:38:01.391: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 12 03:38:01.391: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-6599 delete e2e-test-crd-publish-openapi-2466-crds test-cr'
Dec 12 03:38:01.801: INFO: stderr: ""
Dec 12 03:38:01.802: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 12 03:38:01.802: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-6599 apply -f -'
Dec 12 03:38:02.658: INFO: stderr: ""
Dec 12 03:38:02.659: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 12 03:38:02.659: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-6599 delete e2e-test-crd-publish-openapi-2466-crds test-cr'
Dec 12 03:38:03.040: INFO: stderr: ""
Dec 12 03:38:03.041: INFO: stdout: "e2e-test-crd-publish-openapi-2466-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 12 03:38:03.041: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-2466-crds'
Dec 12 03:38:03.885: INFO: stderr: ""
Dec 12 03:38:03.885: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2466-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:38:13.829: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6599" for this suite.
Dec 12 03:38:19.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:38:22.329: INFO: namespace crd-publish-openapi-6599 deletion completed in 8.456042518s


â€¢ [SLOW TEST:36.262 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:38:14.711: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:38:14.897: INFO: Waiting up to 5m0s for pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f" in namespace "downward-api-5509" to be "success or failure"
Dec 12 03:38:14.914: INFO: Pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.837786ms
Dec 12 03:38:16.931: INFO: Pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033987615s
Dec 12 03:38:18.958: INFO: Pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060967423s
Dec 12 03:38:20.975: INFO: Pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077972255s
Dec 12 03:38:23.017: INFO: Pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.120003054s
Dec 12 03:38:25.034: INFO: Pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.137200414s
STEP: Saw pod success
Dec 12 03:38:25.034: INFO: Pod "downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f" satisfied condition "success or failure"
Dec 12 03:38:25.051: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f container client-container: <nil>
STEP: delete the pod
Dec 12 03:38:25.132: INFO: Waiting for pod downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f to disappear
Dec 12 03:38:25.148: INFO: Pod downwardapi-volume-827a176e-0ba7-427f-bd15-36d2a142746f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:38:25.149: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5509" for this suite.
Dec 12 03:38:31.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:38:33.712: INFO: namespace downward-api-5509 deletion completed in 8.517248222s


â€¢ [SLOW TEST:19.001 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:37:48.484: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 12 03:37:48.641: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:38:01.736: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:38:43.367: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8020" for this suite.
Dec 12 03:38:49.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:38:51.734: INFO: namespace crd-publish-openapi-8020 deletion completed in 8.323927267s


â€¢ [SLOW TEST:63.250 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:38:22.338: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 12 03:38:22.471: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-1592'
Dec 12 03:38:23.148: INFO: stderr: ""
Dec 12 03:38:23.149: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 03:38:23.149: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1592'
Dec 12 03:38:23.418: INFO: stderr: ""
Dec 12 03:38:23.418: INFO: stdout: "update-demo-nautilus-r4xv4 update-demo-nautilus-xskmr "
Dec 12 03:38:23.419: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r4xv4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1592'
Dec 12 03:38:23.747: INFO: stderr: ""
Dec 12 03:38:23.748: INFO: stdout: ""
Dec 12 03:38:23.748: INFO: update-demo-nautilus-r4xv4 is created but not running
Dec 12 03:38:28.754: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1592'
Dec 12 03:38:29.091: INFO: stderr: ""
Dec 12 03:38:29.092: INFO: stdout: "update-demo-nautilus-r4xv4 update-demo-nautilus-xskmr "
Dec 12 03:38:29.092: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r4xv4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1592'
Dec 12 03:38:29.477: INFO: stderr: ""
Dec 12 03:38:29.477: INFO: stdout: ""
Dec 12 03:38:29.477: INFO: update-demo-nautilus-r4xv4 is created but not running
Dec 12 03:38:34.477: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1592'
Dec 12 03:38:34.820: INFO: stderr: ""
Dec 12 03:38:34.836: INFO: stdout: "update-demo-nautilus-r4xv4 update-demo-nautilus-xskmr "
Dec 12 03:38:34.843: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r4xv4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1592'
Dec 12 03:38:35.333: INFO: stderr: ""
Dec 12 03:38:35.333: INFO: stdout: "true"
Dec 12 03:38:35.333: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-r4xv4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1592'
Dec 12 03:38:35.808: INFO: stderr: ""
Dec 12 03:38:35.808: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:38:35.808: INFO: validating pod update-demo-nautilus-r4xv4
Dec 12 03:38:35.833: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:38:35.833: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:38:35.833: INFO: update-demo-nautilus-r4xv4 is verified up and running
Dec 12 03:38:35.833: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-xskmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1592'
Dec 12 03:38:36.187: INFO: stderr: ""
Dec 12 03:38:36.187: INFO: stdout: "true"
Dec 12 03:38:36.188: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-xskmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1592'
Dec 12 03:38:36.537: INFO: stderr: ""
Dec 12 03:38:36.537: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:38:36.537: INFO: validating pod update-demo-nautilus-xskmr
Dec 12 03:38:36.558: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:38:36.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:38:36.558: INFO: update-demo-nautilus-xskmr is verified up and running
STEP: using delete to clean up resources
Dec 12 03:38:36.558: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-1592'
Dec 12 03:38:36.888: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:38:36.888: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 12 03:38:36.888: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1592'
Dec 12 03:38:37.259: INFO: stderr: "No resources found in kubectl-1592 namespace.\n"
Dec 12 03:38:37.259: INFO: stdout: ""
Dec 12 03:38:37.259: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=kubectl-1592 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 03:38:37.624: INFO: stderr: ""
Dec 12 03:38:37.624: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:38:37.624: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1592" for this suite.
Dec 12 03:39:05.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:39:08.053: INFO: namespace kubectl-1592 deletion completed in 30.382643221s


â€¢ [SLOW TEST:45.715 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:39:08.131: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 03:39:08.265: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3042'
Dec 12 03:39:08.661: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 03:39:08.662: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Dec 12 03:39:08.701: INFO: scanned /tmp/home for discovery docs: <nil>
Dec 12 03:39:08.701: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3042'
Dec 12 03:39:26.086: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 12 03:39:26.086: INFO: stdout: "Created e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969\nScaling up e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec 12 03:39:26.086: INFO: stdout: "Created e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969\nScaling up e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec 12 03:39:26.086: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3042'
Dec 12 03:39:26.378: INFO: stderr: ""
Dec 12 03:39:26.378: INFO: stdout: "e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969-nth7b "
Dec 12 03:39:26.378: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969-nth7b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3042'
Dec 12 03:39:26.776: INFO: stderr: ""
Dec 12 03:39:26.776: INFO: stdout: "true"
Dec 12 03:39:26.777: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969-nth7b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3042'
Dec 12 03:39:27.087: INFO: stderr: ""
Dec 12 03:39:27.087: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec 12 03:39:27.087: INFO: e2e-test-httpd-rc-80927ad004c855037e37078fdaa67969-nth7b is verified up and running
[AfterEach] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Dec 12 03:39:27.087: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-httpd-rc --namespace=kubectl-3042'
Dec 12 03:39:27.439: INFO: stderr: ""
Dec 12 03:39:27.439: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:39:27.439: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3042" for this suite.
Dec 12 03:39:33.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:39:35.865: INFO: namespace kubectl-3042 deletion completed in 8.378754204s


â€¢ [SLOW TEST:27.734 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:38:33.746: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 12 03:38:33.948: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40028 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 03:38:33.948: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40028 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 12 03:38:43.984: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40128 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 12 03:38:43.986: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40128 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 12 03:38:54.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40234 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 03:38:54.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40234 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 12 03:39:04.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40302 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 03:39:04.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-a 60a5e0a1-f3d0-4856-9841-9a96fd0779bf 40302 0 2019-12-12 03:38:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 12 03:39:14.068: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-b 33b8bcca-96b9-4204-9b85-fb866472038e 40430 0 2019-12-12 03:39:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 03:39:14.068: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-b 33b8bcca-96b9-4204-9b85-fb866472038e 40430 0 2019-12-12 03:39:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 12 03:39:24.090: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-b 33b8bcca-96b9-4204-9b85-fb866472038e 40523 0 2019-12-12 03:39:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 03:39:24.090: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6315 /api/v1/namespaces/watch-6315/configmaps/e2e-watch-test-configmap-b 33b8bcca-96b9-4204-9b85-fb866472038e 40523 0 2019-12-12 03:39:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:39:34.090: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-6315" for this suite.
Dec 12 03:39:40.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:39:42.556: INFO: namespace watch-6315 deletion completed in 8.420470435s


â€¢ [SLOW TEST:68.809 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:35:26.290: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-f8ed1f0a-0656-4366-bb57-259cd700b087 in namespace container-probe-2580
Dec 12 03:35:34.508: INFO: Started pod busybox-f8ed1f0a-0656-4366-bb57-259cd700b087 in namespace container-probe-2580
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 03:35:34.523: INFO: Initial restart count of pod busybox-f8ed1f0a-0656-4366-bb57-259cd700b087 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:39:34.815: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-2580" for this suite.
Dec 12 03:39:40.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:39:43.447: INFO: namespace container-probe-2580 deletion completed in 8.583899769s


â€¢ [SLOW TEST:257.157 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:39:35.949: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 12 03:39:36.101: INFO: Waiting up to 5m0s for pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df" in namespace "emptydir-1292" to be "success or failure"
Dec 12 03:39:36.117: INFO: Pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032549ms
Dec 12 03:39:38.134: INFO: Pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032559915s
Dec 12 03:39:40.150: INFO: Pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048594319s
Dec 12 03:39:42.167: INFO: Pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065640365s
Dec 12 03:39:44.184: INFO: Pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082562161s
Dec 12 03:39:46.201: INFO: Pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09969757s
STEP: Saw pod success
Dec 12 03:39:46.201: INFO: Pod "pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df" satisfied condition "success or failure"
Dec 12 03:39:46.217: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df container test-container: <nil>
STEP: delete the pod
Dec 12 03:39:46.266: INFO: Waiting for pod pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df to disappear
Dec 12 03:39:46.282: INFO: Pod pod-c5e5e1e5-3fbc-4138-a1a9-b42481ad15df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:39:46.282: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1292" for this suite.
Dec 12 03:39:52.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:39:54.736: INFO: namespace emptydir-1292 deletion completed in 8.408415608s


â€¢ [SLOW TEST:18.787 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:39:43.631: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-13ff676f-e2f4-4d9d-96e6-962b19afb2ad
STEP: Creating a pod to test consume configMaps
Dec 12 03:39:43.859: INFO: Waiting up to 5m0s for pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2" in namespace "configmap-8883" to be "success or failure"
Dec 12 03:39:43.878: INFO: Pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.053989ms
Dec 12 03:39:45.895: INFO: Pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03585756s
Dec 12 03:39:47.912: INFO: Pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052803285s
Dec 12 03:39:49.930: INFO: Pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070763771s
Dec 12 03:39:51.948: INFO: Pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088575053s
Dec 12 03:39:53.969: INFO: Pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109631608s
STEP: Saw pod success
Dec 12 03:39:53.969: INFO: Pod "pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2" satisfied condition "success or failure"
Dec 12 03:39:53.985: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:39:54.030: INFO: Waiting for pod pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2 to disappear
Dec 12 03:39:54.046: INFO: Pod pod-configmaps-c64c1cd4-0d48-4332-94b6-6efa45a40dd2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:39:54.046: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8883" for this suite.
Dec 12 03:40:00.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:40:02.471: INFO: namespace configmap-8883 deletion completed in 8.376816074s


â€¢ [SLOW TEST:18.841 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:39:42.701: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:39:53.061: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-8341" for this suite.
Dec 12 03:40:05.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:40:07.542: INFO: namespace replication-controller-8341 deletion completed in 14.434445851s


â€¢ [SLOW TEST:24.841 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:39:54.864: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 12 03:39:55.002: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:40:05.529: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-4655" for this suite.
Dec 12 03:40:11.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:40:13.999: INFO: namespace init-container-4655 deletion completed in 8.421115519s


â€¢ [SLOW TEST:19.135 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:40:02.597: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 03:40:02.733: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3418'
Dec 12 03:40:03.104: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 03:40:03.104: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Dec 12 03:40:03.123: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-httpd-deployment --namespace=kubectl-3418'
Dec 12 03:40:03.465: INFO: stderr: ""
Dec 12 03:40:03.465: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:40:03.465: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3418" for this suite.
Dec 12 03:40:15.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:40:17.888: INFO: namespace kubectl-3418 deletion completed in 14.392091085s


â€¢ [SLOW TEST:15.292 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:40:14.095: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 03:40:24.518: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:40:24.563: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-9779" for this suite.
Dec 12 03:40:30.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:40:32.984: INFO: namespace container-runtime-9779 deletion completed in 8.372440043s


â€¢ [SLOW TEST:18.890 seconds]
[k8s.io] Container Runtime
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:40:17.955: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 12 03:40:18.114: INFO: Waiting up to 5m0s for pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5" in namespace "emptydir-9960" to be "success or failure"
Dec 12 03:40:18.130: INFO: Pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.464603ms
Dec 12 03:40:20.149: INFO: Pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034886635s
Dec 12 03:40:22.166: INFO: Pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052084169s
Dec 12 03:40:24.183: INFO: Pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068923282s
Dec 12 03:40:26.201: INFO: Pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086912945s
Dec 12 03:40:28.218: INFO: Pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104439175s
STEP: Saw pod success
Dec 12 03:40:28.218: INFO: Pod "pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5" satisfied condition "success or failure"
Dec 12 03:40:28.239: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5 container test-container: <nil>
STEP: delete the pod
Dec 12 03:40:28.290: INFO: Waiting for pod pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5 to disappear
Dec 12 03:40:28.306: INFO: Pod pod-2d41a7e8-c300-4238-ad29-a263a9ff47b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:40:28.306: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-9960" for this suite.
Dec 12 03:40:34.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:40:36.723: INFO: namespace emptydir-9960 deletion completed in 8.371372025s


â€¢ [SLOW TEST:18.768 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:40:33.003: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-bd6aad1b-cbf7-4915-bae4-6072106926e8
STEP: Creating a pod to test consume configMaps
Dec 12 03:40:33.175: INFO: Waiting up to 5m0s for pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06" in namespace "configmap-1068" to be "success or failure"
Dec 12 03:40:33.195: INFO: Pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06": Phase="Pending", Reason="", readiness=false. Elapsed: 20.791618ms
Dec 12 03:40:35.213: INFO: Pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038303716s
Dec 12 03:40:37.240: INFO: Pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065124397s
Dec 12 03:40:39.256: INFO: Pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081416471s
Dec 12 03:40:41.272: INFO: Pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097576384s
Dec 12 03:40:43.289: INFO: Pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.113938672s
STEP: Saw pod success
Dec 12 03:40:43.289: INFO: Pod "pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06" satisfied condition "success or failure"
Dec 12 03:40:43.305: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:40:43.366: INFO: Waiting for pod pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06 to disappear
Dec 12 03:40:43.388: INFO: Pod pod-configmaps-91bbf403-2bcf-4c20-8b64-a818f3f15a06 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:40:43.388: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-1068" for this suite.
Dec 12 03:40:49.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:40:51.820: INFO: namespace configmap-1068 deletion completed in 8.387352017s


â€¢ [SLOW TEST:18.817 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:40:07.551: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5958, will wait for the garbage collector to delete the pods
Dec 12 03:40:17.820: INFO: Deleting Job.batch foo took: 21.491492ms
Dec 12 03:40:18.021: INFO: Terminating Job.batch foo pods took: 200.558491ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:40:52.740: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-5958" for this suite.
Dec 12 03:40:58.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:41:01.170: INFO: namespace job-5958 deletion completed in 8.382605345s


â€¢ [SLOW TEST:53.622 seconds]
[sig-apps] Job
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:41:01.174: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:41:01.348: INFO: (0) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 27.391157ms)
Dec 12 03:41:01.365: INFO: (1) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.292246ms)
Dec 12 03:41:01.382: INFO: (2) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 16.684085ms)
Dec 12 03:41:01.400: INFO: (3) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.460694ms)
Dec 12 03:41:01.419: INFO: (4) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.915653ms)
Dec 12 03:41:01.436: INFO: (5) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 16.857923ms)
Dec 12 03:41:01.453: INFO: (6) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.028471ms)
Dec 12 03:41:01.471: INFO: (7) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.945207ms)
Dec 12 03:41:01.488: INFO: (8) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 16.889288ms)
Dec 12 03:41:01.507: INFO: (9) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.049437ms)
Dec 12 03:41:01.525: INFO: (10) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.999273ms)
Dec 12 03:41:01.548: INFO: (11) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 22.919403ms)
Dec 12 03:41:01.568: INFO: (12) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.964851ms)
Dec 12 03:41:01.585: INFO: (13) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.757907ms)
Dec 12 03:41:01.604: INFO: (14) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.076112ms)
Dec 12 03:41:01.622: INFO: (15) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.122406ms)
Dec 12 03:41:01.639: INFO: (16) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.475767ms)
Dec 12 03:41:01.657: INFO: (17) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.969162ms)
Dec 12 03:41:01.675: INFO: (18) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.810797ms)
Dec 12 03:41:01.694: INFO: (19) /api/v1/nodes/ip-10-0-133-17.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.504098ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:41:01.694: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-3629" for this suite.
Dec 12 03:41:07.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:41:09.013: INFO: namespace proxy-3629 deletion completed in 7.301787399s


â€¢ [SLOW TEST:7.840 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:40:51.924: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-2824/configmap-test-368940fd-b0fe-4243-bb61-98e9fa69d7e4
STEP: Creating a pod to test consume configMaps
Dec 12 03:40:52.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76" in namespace "configmap-2824" to be "success or failure"
Dec 12 03:40:52.157: INFO: Pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76": Phase="Pending", Reason="", readiness=false. Elapsed: 19.084105ms
Dec 12 03:40:54.174: INFO: Pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036851244s
Dec 12 03:40:56.194: INFO: Pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056107237s
Dec 12 03:40:58.211: INFO: Pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073070057s
Dec 12 03:41:00.230: INFO: Pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092105742s
Dec 12 03:41:02.249: INFO: Pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111079748s
STEP: Saw pod success
Dec 12 03:41:02.249: INFO: Pod "pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76" satisfied condition "success or failure"
Dec 12 03:41:02.265: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76 container env-test: <nil>
STEP: delete the pod
Dec 12 03:41:02.320: INFO: Waiting for pod pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76 to disappear
Dec 12 03:41:02.339: INFO: Pod pod-configmaps-a1e4ef25-5c4b-422c-924d-d36f7c67ef76 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:41:02.340: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2824" for this suite.
Dec 12 03:41:08.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:41:11.013: INFO: namespace configmap-2824 deletion completed in 8.627593739s


â€¢ [SLOW TEST:19.089 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:41:09.204: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:41:09.348: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:41:09.962: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7865" for this suite.
Dec 12 03:41:16.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:41:18.555: INFO: namespace custom-resource-definition-7865 deletion completed in 8.487908485s


â€¢ [SLOW TEST:9.351 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:41:18.630: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:41:18.876: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-8475" for this suite.
Dec 12 03:41:24.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:41:27.291: INFO: namespace resourcequota-8475 deletion completed in 8.39702858s


â€¢ [SLOW TEST:8.661 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:41:11.091: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1212 03:41:41.439360    1997 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 03:41:41.439: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:41:41.439: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-877" for this suite.
Dec 12 03:41:47.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:41:49.813: INFO: namespace gc-877 deletion completed in 8.355548847s


â€¢ [SLOW TEST:38.722 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:40:36.909: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 12 03:40:37.040: INFO: PodSpec: initContainers in spec.initContainers
Dec 12 03:41:31.608: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cd32cabb-29e1-4d92-9d09-3615ef2c7f2b", GenerateName:"", Namespace:"init-container-2163", SelfLink:"/api/v1/namespaces/init-container-2163/pods/pod-init-cd32cabb-29e1-4d92-9d09-3615ef2c7f2b", UID:"e3beeed9-4034-4ea0-b41e-ff7e5c21bbe1", ResourceVersion:"42459", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711718837, loc:(*time.Location)(0x86b4b00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"40238024"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.166\"\n    ],\n    \"dns\": {},\n    \"default-route\": [\n        \"10.128.2.1\"\n    ]\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9gpzh", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0030527c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9gpzh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc008fbed20), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9gpzh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc008fbedc0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9gpzh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc008fbec80), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004e7c398), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-133-2.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00be34180), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-bnq94"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004e7c440)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004e7c460)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004e7c47c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004e7c480), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718837, loc:(*time.Location)(0x86b4b00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718837, loc:(*time.Location)(0x86b4b00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718837, loc:(*time.Location)(0x86b4b00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718837, loc:(*time.Location)(0x86b4b00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.133.2", PodIP:"10.128.2.166", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.128.2.166"}}, StartTime:(*v1.Time)(0xc00843d080), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f803f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f80460)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://942125714f911132d0dd4c6e393be5a4488f3bdab04393bd89b88f52b23f0f17", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00843d0c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00843d0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004e7c4ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:41:31.609: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-2163" for this suite.
Dec 12 03:41:59.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:02.208: INFO: namespace init-container-2163 deletion completed in 30.553626745s


â€¢ [SLOW TEST:85.299 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:41:27.298: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-6893
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6893 to expose endpoints map[]
Dec 12 03:41:27.470: INFO: successfully validated that service endpoint-test2 in namespace services-6893 exposes endpoints map[] (16.923737ms elapsed)
STEP: Creating pod pod1 in namespace services-6893
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6893 to expose endpoints map[pod1:[80]]
Dec 12 03:41:31.701: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.19804294s elapsed, will retry)
Dec 12 03:41:35.844: INFO: successfully validated that service endpoint-test2 in namespace services-6893 exposes endpoints map[pod1:[80]] (8.34073759s elapsed)
STEP: Creating pod pod2 in namespace services-6893
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6893 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 12 03:41:40.118: INFO: Unexpected endpoints: found map[9a31d43c-9182-46f2-9d37-dfa2b31e298e:[80]], expected map[pod1:[80] pod2:[80]] (4.245728441s elapsed, will retry)
Dec 12 03:41:44.328: INFO: successfully validated that service endpoint-test2 in namespace services-6893 exposes endpoints map[pod1:[80] pod2:[80]] (8.456370686s elapsed)
STEP: Deleting pod pod1 in namespace services-6893
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6893 to expose endpoints map[pod2:[80]]
Dec 12 03:41:44.383: INFO: successfully validated that service endpoint-test2 in namespace services-6893 exposes endpoints map[pod2:[80]] (34.024101ms elapsed)
STEP: Deleting pod pod2 in namespace services-6893
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6893 to expose endpoints map[]
Dec 12 03:41:44.429: INFO: successfully validated that service endpoint-test2 in namespace services-6893 exposes endpoints map[] (18.745432ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:41:44.463: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6893" for this suite.
Dec 12 03:42:02.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:05.022: INFO: namespace services-6893 deletion completed in 20.514146683s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:37.724 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:02.245: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Dec 12 03:42:02.395: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:02.675: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4866" for this suite.
Dec 12 03:42:08.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:11.175: INFO: namespace kubectl-4866 deletion completed in 8.461013803s


â€¢ [SLOW TEST:8.930 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:38:51.804: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-9008
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 12 03:38:51.994: INFO: Found 0 stateful pods, waiting for 3
Dec 12 03:39:02.014: INFO: Found 2 stateful pods, waiting for 3
Dec 12 03:39:12.016: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:39:12.016: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:39:12.016: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 03:39:22.012: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:39:22.012: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:39:22.012: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:39:22.066: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-9008 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:39:22.702: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:39:22.702: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:39:22.703: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 12 03:39:32.821: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 12 03:39:32.876: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-9008 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:39:33.434: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 03:39:33.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:39:33.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:39:43.538: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:39:43.538: INFO: Waiting for Pod statefulset-9008/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:39:43.538: INFO: Waiting for Pod statefulset-9008/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:39:53.577: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:39:53.577: INFO: Waiting for Pod statefulset-9008/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:39:53.577: INFO: Waiting for Pod statefulset-9008/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:40:03.572: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:40:03.572: INFO: Waiting for Pod statefulset-9008/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:40:13.576: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:40:13.576: INFO: Waiting for Pod statefulset-9008/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:40:23.573: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:40:33.576: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
STEP: Rolling back to a previous revision
Dec 12 03:40:43.571: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-9008 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:40:44.067: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:40:44.067: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:40:44.067: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:40:54.213: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 12 03:40:54.268: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-9008 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:40:54.805: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 03:40:54.805: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:40:54.805: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:41:04.906: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:41:04.906: INFO: Waiting for Pod statefulset-9008/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 03:41:04.906: INFO: Waiting for Pod statefulset-9008/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 03:41:14.939: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:41:14.939: INFO: Waiting for Pod statefulset-9008/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 03:41:14.940: INFO: Waiting for Pod statefulset-9008/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 03:41:24.941: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
Dec 12 03:41:24.941: INFO: Waiting for Pod statefulset-9008/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 12 03:41:34.942: INFO: Waiting for StatefulSet statefulset-9008/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 12 03:41:44.947: INFO: Deleting all statefulset in ns statefulset-9008
Dec 12 03:41:44.963: INFO: Scaling statefulset ss2 to 0
Dec 12 03:42:05.035: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:42:05.051: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:05.112: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-9008" for this suite.
Dec 12 03:42:11.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:13.612: INFO: namespace statefulset-9008 deletion completed in 8.455165031s


â€¢ [SLOW TEST:201.809 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:05.193: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Dec 12 03:42:05.474: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig api-versions'
Dec 12 03:42:06.012: INFO: stderr: ""
Dec 12 03:42:06.017: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:06.022: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6758" for this suite.
Dec 12 03:42:12.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:14.497: INFO: namespace kubectl-6758 deletion completed in 8.434597134s


â€¢ [SLOW TEST:9.304 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:41:49.871: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:41:50.050: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 12 03:41:58.797: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-2489 create -f -'
Dec 12 03:42:01.281: INFO: stderr: ""
Dec 12 03:42:01.281: INFO: stdout: "e2e-test-crd-publish-openapi-4203-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 12 03:42:01.282: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-2489 delete e2e-test-crd-publish-openapi-4203-crds test-cr'
Dec 12 03:42:01.790: INFO: stderr: ""
Dec 12 03:42:01.790: INFO: stdout: "e2e-test-crd-publish-openapi-4203-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 12 03:42:01.791: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-2489 apply -f -'
Dec 12 03:42:02.966: INFO: stderr: ""
Dec 12 03:42:02.966: INFO: stdout: "e2e-test-crd-publish-openapi-4203-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 12 03:42:02.966: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-2489 delete e2e-test-crd-publish-openapi-4203-crds test-cr'
Dec 12 03:42:03.379: INFO: stderr: ""
Dec 12 03:42:03.379: INFO: stdout: "e2e-test-crd-publish-openapi-4203-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 12 03:42:03.380: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-4203-crds'
Dec 12 03:42:04.087: INFO: stderr: ""
Dec 12 03:42:04.087: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4203-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:13.505: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2489" for this suite.
Dec 12 03:42:19.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:21.911: INFO: namespace crd-publish-openapi-2489 deletion completed in 8.362942044s


â€¢ [SLOW TEST:32.040 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:11.280: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 03:42:20.667: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:20.712: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-321" for this suite.
Dec 12 03:42:26.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:29.123: INFO: namespace container-runtime-321 deletion completed in 8.36264884s


â€¢ [SLOW TEST:17.842 seconds]
[k8s.io] Container Runtime
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:14.509: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-116f78ab-2a17-4b5f-8ebd-1c2e4d73f737
STEP: Creating a pod to test consume secrets
Dec 12 03:42:14.724: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3" in namespace "projected-6817" to be "success or failure"
Dec 12 03:42:14.741: INFO: Pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.895281ms
Dec 12 03:42:16.760: INFO: Pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03521314s
Dec 12 03:42:18.778: INFO: Pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053352133s
Dec 12 03:42:20.796: INFO: Pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07135797s
Dec 12 03:42:22.816: INFO: Pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091278302s
Dec 12 03:42:24.834: INFO: Pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109195196s
STEP: Saw pod success
Dec 12 03:42:24.834: INFO: Pod "pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3" satisfied condition "success or failure"
Dec 12 03:42:24.850: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:42:24.900: INFO: Waiting for pod pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3 to disappear
Dec 12 03:42:24.919: INFO: Pod pod-projected-secrets-fb600c2c-05fc-459d-9c34-5e0dc672b1e3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:24.919: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6817" for this suite.
Dec 12 03:42:31.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:33.381: INFO: namespace projected-6817 deletion completed in 8.410948078s


â€¢ [SLOW TEST:18.872 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:21.923: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-a67ab1dc-6a42-41eb-ae94-c159823f909e
STEP: Creating a pod to test consume secrets
Dec 12 03:42:22.118: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed" in namespace "projected-4233" to be "success or failure"
Dec 12 03:42:22.136: INFO: Pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed": Phase="Pending", Reason="", readiness=false. Elapsed: 17.885063ms
Dec 12 03:42:24.153: INFO: Pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034917125s
Dec 12 03:42:26.170: INFO: Pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052071432s
Dec 12 03:42:28.187: INFO: Pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068944689s
Dec 12 03:42:30.207: INFO: Pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089048531s
Dec 12 03:42:32.230: INFO: Pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111920324s
STEP: Saw pod success
Dec 12 03:42:32.230: INFO: Pod "pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed" satisfied condition "success or failure"
Dec 12 03:42:32.251: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:42:32.310: INFO: Waiting for pod pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed to disappear
Dec 12 03:42:32.335: INFO: Pod pod-projected-secrets-d2f6a043-3ce8-4d8f-81d7-602ff8b3deed no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:32.335: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4233" for this suite.
Dec 12 03:42:38.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:40.812: INFO: namespace projected-4233 deletion completed in 8.420168888s


â€¢ [SLOW TEST:18.890 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:13.619: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:42:13.764: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 12 03:42:13.808: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 12 03:42:18.826: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 12 03:42:22.860: INFO: Creating deployment "test-rolling-update-deployment"
Dec 12 03:42:22.880: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 12 03:42:22.912: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Dec 12 03:42:24.946: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 12 03:42:24.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:26.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:28.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:30.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718942, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:32.980: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 12 03:42:33.037: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5447 /apis/apps/v1/namespaces/deployment-5447/deployments/test-rolling-update-deployment 6ffde2bd-bfae-4dad-8caf-38b3411b8d9f 43524 1 2019-12-12 03:42:22 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002d7db58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-12 03:42:22 +0000 UTC,LastTransitionTime:2019-12-12 03:42:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-12-12 03:42:31 +0000 UTC,LastTransitionTime:2019-12-12 03:42:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 12 03:42:33.054: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-5447 /apis/apps/v1/namespaces/deployment-5447/replicasets/test-rolling-update-deployment-55d946486 3b1f20f9-cf33-4ad9-b0b7-b38f0c1c73ec 43513 1 2019-12-12 03:42:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6ffde2bd-bfae-4dad-8caf-38b3411b8d9f 0xc0033b53e0 0xc0033b53e1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0033b5448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:42:33.054: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 12 03:42:33.054: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5447 /apis/apps/v1/namespaces/deployment-5447/replicasets/test-rolling-update-controller 49ad6c67-b18b-40d5-a402-894af74d028f 43523 2 2019-12-12 03:42:13 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6ffde2bd-bfae-4dad-8caf-38b3411b8d9f 0xc0033b5317 0xc0033b5318}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0033b5378 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:42:33.070: INFO: Pod "test-rolling-update-deployment-55d946486-gzjmp" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-gzjmp test-rolling-update-deployment-55d946486- deployment-5447 /api/v1/namespaces/deployment-5447/pods/test-rolling-update-deployment-55d946486-gzjmp 657e6d85-0cce-44ef-8855-aaeb4cd6fbb3 43512 0 2019-12-12 03:42:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.176"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 3b1f20f9-cf33-4ad9-b0b7-b38f0c1c73ec 0xc002d7df90 0xc002d7df91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nmbmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nmbmt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nmbmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-z9csd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:42:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:42:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:10.128.2.176,StartTime:2019-12-12 03:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:42:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://c0be84e22c5cb62742a7a72ea922ec32319ca5d4c2d968e3fcb9b5cb00cf2b32,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:33.070: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-5447" for this suite.
Dec 12 03:42:39.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:41.548: INFO: namespace deployment-5447 deletion completed in 8.432579194s


â€¢ [SLOW TEST:27.928 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:41.689: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:42:41.977: INFO: Waiting up to 5m0s for pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31" in namespace "projected-8869" to be "success or failure"
Dec 12 03:42:42.006: INFO: Pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31": Phase="Pending", Reason="", readiness=false. Elapsed: 29.109144ms
Dec 12 03:42:44.040: INFO: Pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062981261s
Dec 12 03:42:46.057: INFO: Pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080011027s
Dec 12 03:42:48.078: INFO: Pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100986741s
Dec 12 03:42:50.103: INFO: Pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125982197s
Dec 12 03:42:52.123: INFO: Pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.146149658s
STEP: Saw pod success
Dec 12 03:42:52.123: INFO: Pod "downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31" satisfied condition "success or failure"
Dec 12 03:42:52.141: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31 container client-container: <nil>
STEP: delete the pod
Dec 12 03:42:52.192: INFO: Waiting for pod downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31 to disappear
Dec 12 03:42:52.213: INFO: Pod downwardapi-volume-740492f5-a0a4-478c-ac42-95d28a6a6a31 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:52.213: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8869" for this suite.
Dec 12 03:42:58.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:00.870: INFO: namespace projected-8869 deletion completed in 8.612608212s


â€¢ [SLOW TEST:19.181 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:33.433: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 12 03:42:34.236: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:42:34.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:36.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:38.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:40.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:42.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718954, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:42:45.431: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:45.799: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2435" for this suite.
Dec 12 03:42:51.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:42:54.365: INFO: namespace webhook-2435 deletion completed in 8.533699345s
STEP: Destroying namespace "webhook-2435-markers" for this suite.
Dec 12 03:43:00.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:03.112: INFO: namespace webhook-2435-markers deletion completed in 8.746796878s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:29.765 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:29.169: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:42:30.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:32.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:34.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:36.320: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:42:38.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718950, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:42:41.345: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:42:53.805: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-84" for this suite.
Dec 12 03:43:01.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:04.597: INFO: namespace webhook-84 deletion completed in 10.745002972s
STEP: Destroying namespace "webhook-84-markers" for this suite.
Dec 12 03:43:10.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:12.968: INFO: namespace webhook-84-markers deletion completed in 8.37155595s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:43.869 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:42:40.954: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:42:41.066: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 12 03:42:50.439: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 create -f -'
Dec 12 03:42:53.030: INFO: stderr: ""
Dec 12 03:42:53.030: INFO: stdout: "e2e-test-crd-publish-openapi-8291-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 12 03:42:53.031: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 delete e2e-test-crd-publish-openapi-8291-crds test-foo'
Dec 12 03:42:53.554: INFO: stderr: ""
Dec 12 03:42:53.554: INFO: stdout: "e2e-test-crd-publish-openapi-8291-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 12 03:42:53.555: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 apply -f -'
Dec 12 03:42:54.393: INFO: stderr: ""
Dec 12 03:42:54.394: INFO: stdout: "e2e-test-crd-publish-openapi-8291-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 12 03:42:54.394: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 delete e2e-test-crd-publish-openapi-8291-crds test-foo'
Dec 12 03:42:54.745: INFO: stderr: ""
Dec 12 03:42:54.746: INFO: stdout: "e2e-test-crd-publish-openapi-8291-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 12 03:42:54.746: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 create -f -'
Dec 12 03:42:55.546: INFO: rc: 1
Dec 12 03:42:55.547: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 apply -f -'
Dec 12 03:42:56.143: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 12 03:42:56.144: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 create -f -'
Dec 12 03:42:56.912: INFO: rc: 1
Dec 12 03:42:56.914: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=crd-publish-openapi-8742 apply -f -'
Dec 12 03:42:57.630: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 12 03:42:57.631: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-8291-crds'
Dec 12 03:42:58.493: INFO: stderr: ""
Dec 12 03:42:58.493: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 12 03:42:58.494: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-8291-crds.metadata'
Dec 12 03:42:59.276: INFO: stderr: ""
Dec 12 03:42:59.276: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 12 03:42:59.277: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-8291-crds.spec'
Dec 12 03:43:00.117: INFO: stderr: ""
Dec 12 03:43:00.117: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 12 03:43:00.118: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-8291-crds.spec.bars'
Dec 12 03:43:01.060: INFO: stderr: ""
Dec 12 03:43:01.060: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 12 03:43:01.061: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig explain e2e-test-crd-publish-openapi-8291-crds.spec.bars2'
Dec 12 03:43:02.042: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:43:11.846: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8742" for this suite.
Dec 12 03:43:17.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:20.235: INFO: namespace crd-publish-openapi-8742 deletion completed in 8.346335824s


â€¢ [SLOW TEST:39.281 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:20.283: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 12 03:43:30.563: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec pod-sharedvolume-cccd80ab-b57d-4540-9056-b1550ced50d5 -c busybox-main-container --namespace=emptydir-7348 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 12 03:43:31.430: INFO: stderr: ""
Dec 12 03:43:31.431: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:43:31.431: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-7348" for this suite.
Dec 12 03:43:37.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:39.868: INFO: namespace emptydir-7348 deletion completed in 8.389720539s


â€¢ [SLOW TEST:19.585 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:03.248: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Dec 12 03:43:11.541: INFO: Pod pod-hostip-43c15d22-d3eb-4ffc-915a-2ca395d67d8b has hostIP: 10.0.133.2
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:43:11.541: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8849" for this suite.
Dec 12 03:43:39.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:41.958: INFO: namespace pods-8849 deletion completed in 30.370093798s


â€¢ [SLOW TEST:38.710 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:13.168: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:43:14.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:43:16.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:43:18.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:43:20.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:43:22.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711718994, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:43:25.430: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:43:25.447: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:43:26.412: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1274" for this suite.
Dec 12 03:43:34.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:36.838: INFO: namespace webhook-1274 deletion completed in 10.380807585s
STEP: Destroying namespace "webhook-1274-markers" for this suite.
Dec 12 03:43:42.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:45.239: INFO: namespace webhook-1274-markers deletion completed in 8.400604728s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:32.146 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:00.881: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5610
[It] Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5610
STEP: Creating statefulset with conflicting port in namespace statefulset-5610
STEP: Waiting until pod test-pod will start running in namespace statefulset-5610
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5610
Dec 12 03:43:11.174: INFO: Observed stateful pod in namespace: statefulset-5610, name: ss-0, uid: 5870c5d9-e260-4fa1-81bf-5f1c738c5e60, status phase: Pending. Waiting for statefulset controller to delete.
Dec 12 03:43:19.125: INFO: Observed stateful pod in namespace: statefulset-5610, name: ss-0, uid: 5870c5d9-e260-4fa1-81bf-5f1c738c5e60, status phase: Failed. Waiting for statefulset controller to delete.
Dec 12 03:43:19.134: INFO: Observed stateful pod in namespace: statefulset-5610, name: ss-0, uid: 5870c5d9-e260-4fa1-81bf-5f1c738c5e60, status phase: Failed. Waiting for statefulset controller to delete.
Dec 12 03:43:19.139: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5610
STEP: Removing pod with conflicting port in namespace statefulset-5610
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5610 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 12 03:43:29.272: INFO: Deleting all statefulset in ns statefulset-5610
Dec 12 03:43:29.288: INFO: Scaling statefulset ss to 0
Dec 12 03:43:39.361: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:43:39.380: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:43:39.437: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-5610" for this suite.
Dec 12 03:43:47.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:43:49.849: INFO: namespace statefulset-5610 deletion completed in 10.367731649s


â€¢ [SLOW TEST:48.968 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:49.882: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Dec 12 03:43:50.146: INFO: Waiting up to 5m0s for pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b" in namespace "var-expansion-5961" to be "success or failure"
Dec 12 03:43:50.163: INFO: Pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.940712ms
Dec 12 03:43:52.179: INFO: Pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032943787s
Dec 12 03:43:54.196: INFO: Pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0500682s
Dec 12 03:43:56.216: INFO: Pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070105053s
Dec 12 03:43:58.232: INFO: Pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086363194s
Dec 12 03:44:00.266: INFO: Pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.119955626s
STEP: Saw pod success
Dec 12 03:44:00.266: INFO: Pod "var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b" satisfied condition "success or failure"
Dec 12 03:44:00.283: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:44:00.339: INFO: Waiting for pod var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b to disappear
Dec 12 03:44:00.356: INFO: Pod var-expansion-39ec6f14-d444-44d2-a9f9-67917076b63b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:44:00.356: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-5961" for this suite.
Dec 12 03:44:06.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:44:08.791: INFO: namespace var-expansion-5961 deletion completed in 8.390617813s


â€¢ [SLOW TEST:18.909 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:39.902: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-437
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-437
STEP: creating replication controller externalsvc in namespace services-437
I1212 03:43:40.125131    1997 runners.go:184] Created replication controller with name: externalsvc, namespace: services-437, replica count: 2
I1212 03:43:43.178946    1997 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:43:46.281980    1997 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:43:49.282209    1997 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 12 03:43:49.343: INFO: Creating new exec pod
Dec 12 03:43:59.401: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-437 execpodfwqhs -- /bin/sh -x -c nslookup clusterip-service'
Dec 12 03:44:00.030: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 12 03:44:00.030: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-437.svc.cluster.local\tcanonical name = externalsvc.services-437.svc.cluster.local.\nName:\texternalsvc.services-437.svc.cluster.local\nAddress: 172.30.58.102\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-437, will wait for the garbage collector to delete the pods
Dec 12 03:44:00.132: INFO: Deleting ReplicationController externalsvc took: 24.213397ms
Dec 12 03:44:00.333: INFO: Terminating ReplicationController externalsvc pods took: 201.788554ms
Dec 12 03:44:09.380: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:44:09.419: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-437" for this suite.
Dec 12 03:44:15.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:44:17.845: INFO: namespace services-437 deletion completed in 8.37858264s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:37.942 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:45.421: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-xsjq
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 03:43:45.614: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xsjq" in namespace "subpath-8753" to be "success or failure"
Dec 12 03:43:45.631: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Pending", Reason="", readiness=false. Elapsed: 16.978057ms
Dec 12 03:43:47.656: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041465605s
Dec 12 03:43:49.674: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059533033s
Dec 12 03:43:51.691: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076476455s
Dec 12 03:43:53.715: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100373704s
Dec 12 03:43:55.733: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 10.118816331s
Dec 12 03:43:57.749: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 12.135150179s
Dec 12 03:43:59.770: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 14.155457976s
Dec 12 03:44:01.786: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 16.17191749s
Dec 12 03:44:03.803: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 18.189175852s
Dec 12 03:44:05.822: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 20.207656104s
Dec 12 03:44:07.851: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 22.236874404s
Dec 12 03:44:09.869: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 24.254502828s
Dec 12 03:44:11.890: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 26.275324771s
Dec 12 03:44:13.907: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Running", Reason="", readiness=true. Elapsed: 28.292372763s
Dec 12 03:44:15.929: INFO: Pod "pod-subpath-test-secret-xsjq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.314441321s
STEP: Saw pod success
Dec 12 03:44:15.929: INFO: Pod "pod-subpath-test-secret-xsjq" satisfied condition "success or failure"
Dec 12 03:44:15.951: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-subpath-test-secret-xsjq container test-container-subpath-secret-xsjq: <nil>
STEP: delete the pod
Dec 12 03:44:16.013: INFO: Waiting for pod pod-subpath-test-secret-xsjq to disappear
Dec 12 03:44:16.033: INFO: Pod pod-subpath-test-secret-xsjq no longer exists
STEP: Deleting pod pod-subpath-test-secret-xsjq
Dec 12 03:44:16.033: INFO: Deleting pod "pod-subpath-test-secret-xsjq" in namespace "subpath-8753"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:44:16.050: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-8753" for this suite.
Dec 12 03:44:22.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:44:24.513: INFO: namespace subpath-8753 deletion completed in 8.416573066s


â€¢ [SLOW TEST:39.092 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:44:18.181: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-e8ca6487-bea3-429f-942b-7222507eb497
STEP: Creating a pod to test consume configMaps
Dec 12 03:44:18.438: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80" in namespace "projected-9124" to be "success or failure"
Dec 12 03:44:18.457: INFO: Pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80": Phase="Pending", Reason="", readiness=false. Elapsed: 19.151245ms
Dec 12 03:44:20.474: INFO: Pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035816592s
Dec 12 03:44:22.493: INFO: Pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054786907s
Dec 12 03:44:24.509: INFO: Pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070826128s
Dec 12 03:44:26.533: INFO: Pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094842319s
Dec 12 03:44:28.551: INFO: Pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112889363s
STEP: Saw pod success
Dec 12 03:44:28.551: INFO: Pod "pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80" satisfied condition "success or failure"
Dec 12 03:44:28.566: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:44:28.615: INFO: Waiting for pod pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80 to disappear
Dec 12 03:44:28.631: INFO: Pod pod-projected-configmaps-0b23cb90-909c-4b54-8610-483e8086cd80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:44:28.631: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9124" for this suite.
Dec 12 03:44:34.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:44:37.072: INFO: namespace projected-9124 deletion completed in 8.397585158s


â€¢ [SLOW TEST:18.891 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:44:08.865: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9019
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9019
STEP: creating replication controller externalsvc in namespace services-9019
I1212 03:44:09.109296    1994 runners.go:184] Created replication controller with name: externalsvc, namespace: services-9019, replica count: 2
I1212 03:44:12.162741    1994 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:44:15.162946    1994 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:44:18.165943    1994 runners.go:184] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:44:21.166931    1994 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 12 03:44:21.236: INFO: Creating new exec pod
Dec 12 03:44:31.300: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=services-9019 execpod8l2x6 -- /bin/sh -x -c nslookup nodeport-service'
Dec 12 03:44:31.818: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 12 03:44:31.818: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-9019.svc.cluster.local\tcanonical name = externalsvc.services-9019.svc.cluster.local.\nName:\texternalsvc.services-9019.svc.cluster.local\nAddress: 172.30.91.38\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9019, will wait for the garbage collector to delete the pods
Dec 12 03:44:31.910: INFO: Deleting ReplicationController externalsvc took: 22.923381ms
Dec 12 03:44:32.110: INFO: Terminating ReplicationController externalsvc pods took: 200.781593ms
Dec 12 03:44:42.748: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:44:42.786: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-9019" for this suite.
Dec 12 03:44:48.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:44:51.185: INFO: namespace services-9019 deletion completed in 8.354340061s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:42.320 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:44:24.551: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Dec 12 03:44:24.714: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-4121 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 12 03:44:25.055: INFO: stderr: ""
Dec 12 03:44:25.055: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Dec 12 03:44:25.055: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 12 03:44:25.055: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4121" to be "running and ready, or succeeded"
Dec 12 03:44:25.079: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 23.996737ms
Dec 12 03:44:27.096: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041027661s
Dec 12 03:44:29.113: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057827146s
Dec 12 03:44:31.131: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076018134s
Dec 12 03:44:33.151: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096511867s
Dec 12 03:44:35.168: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 10.11326186s
Dec 12 03:44:35.168: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 12 03:44:35.168: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 12 03:44:35.168: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs logs-generator logs-generator --namespace=kubectl-4121'
Dec 12 03:44:35.499: INFO: stderr: ""
Dec 12 03:44:35.499: INFO: stdout: "I1212 03:44:32.792715       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/5qws 332\nI1212 03:44:32.992881       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/t8d 577\nI1212 03:44:33.192933       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/x22v 299\nI1212 03:44:33.392990       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/975 219\nI1212 03:44:33.592939       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/d6hj 305\nI1212 03:44:33.792855       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/r9xb 250\nI1212 03:44:33.992892       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/6qr 247\nI1212 03:44:34.192919       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7md 382\nI1212 03:44:34.392886       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/rhbd 565\nI1212 03:44:34.593077       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/gpz 326\nI1212 03:44:34.792925       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/9dm 494\nI1212 03:44:34.992909       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/mf4r 558\nI1212 03:44:35.192925       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/k82b 415\nI1212 03:44:35.402378       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/nnr 431\n"
STEP: limiting log lines
Dec 12 03:44:35.499: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs logs-generator logs-generator --namespace=kubectl-4121 --tail=1'
Dec 12 03:44:35.837: INFO: stderr: ""
Dec 12 03:44:35.837: INFO: stdout: "I1212 03:44:35.792929       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4zbp 481\n"
STEP: limiting log bytes
Dec 12 03:44:35.838: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs logs-generator logs-generator --namespace=kubectl-4121 --limit-bytes=1'
Dec 12 03:44:36.135: INFO: stderr: ""
Dec 12 03:44:36.135: INFO: stdout: "I"
STEP: exposing timestamps
Dec 12 03:44:36.136: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs logs-generator logs-generator --namespace=kubectl-4121 --tail=1 --timestamps'
Dec 12 03:44:36.426: INFO: stderr: ""
Dec 12 03:44:36.426: INFO: stdout: "2019-12-12T03:44:36.393110388Z I1212 03:44:36.393000       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/fkt4 326\n"
STEP: restricting to a time range
Dec 12 03:44:38.927: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs logs-generator logs-generator --namespace=kubectl-4121 --since=1s'
Dec 12 03:44:39.235: INFO: stderr: ""
Dec 12 03:44:39.235: INFO: stdout: "I1212 03:44:38.392983       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/ffqg 478\nI1212 03:44:38.592971       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/xzbw 385\nI1212 03:44:38.792935       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/w5l 259\nI1212 03:44:38.992865       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/pkf9 554\nI1212 03:44:39.192923       1 logs_generator.go:76] 32 POST /api/v1/namespaces/kube-system/pods/nr8 315\n"
Dec 12 03:44:39.235: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs logs-generator logs-generator --namespace=kubectl-4121 --since=24h'
Dec 12 03:44:39.531: INFO: stderr: ""
Dec 12 03:44:39.532: INFO: stdout: "I1212 03:44:32.792715       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/5qws 332\nI1212 03:44:32.992881       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/t8d 577\nI1212 03:44:33.192933       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/x22v 299\nI1212 03:44:33.392990       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/975 219\nI1212 03:44:33.592939       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/d6hj 305\nI1212 03:44:33.792855       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/r9xb 250\nI1212 03:44:33.992892       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/6qr 247\nI1212 03:44:34.192919       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/7md 382\nI1212 03:44:34.392886       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/rhbd 565\nI1212 03:44:34.593077       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/gpz 326\nI1212 03:44:34.792925       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/9dm 494\nI1212 03:44:34.992909       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/mf4r 558\nI1212 03:44:35.192925       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/k82b 415\nI1212 03:44:35.402378       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/nnr 431\nI1212 03:44:35.592921       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/prdq 456\nI1212 03:44:35.792929       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4zbp 481\nI1212 03:44:35.992927       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/grls 581\nI1212 03:44:36.192920       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/5xx 280\nI1212 03:44:36.393000       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/fkt4 326\nI1212 03:44:36.592954       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/d5cw 366\nI1212 03:44:36.792926       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/rpl 471\nI1212 03:44:36.992916       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/p8gr 265\nI1212 03:44:37.192925       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/gg7j 553\nI1212 03:44:37.392918       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/g8g4 243\nI1212 03:44:37.592925       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/dxs 461\nI1212 03:44:37.792863       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/wn46 502\nI1212 03:44:37.992933       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/w8t 283\nI1212 03:44:38.192900       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/kxp 406\nI1212 03:44:38.392983       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/ffqg 478\nI1212 03:44:38.592971       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/xzbw 385\nI1212 03:44:38.792935       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/w5l 259\nI1212 03:44:38.992865       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/pkf9 554\nI1212 03:44:39.192923       1 logs_generator.go:76] 32 POST /api/v1/namespaces/kube-system/pods/nr8 315\nI1212 03:44:39.392908       1 logs_generator.go:76] 33 POST /api/v1/namespaces/default/pods/5qt 300\n"
[AfterEach] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Dec 12 03:44:39.532: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pod logs-generator --namespace=kubectl-4121'
Dec 12 03:44:52.649: INFO: stderr: ""
Dec 12 03:44:52.649: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:44:52.649: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4121" for this suite.
Dec 12 03:44:58.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:45:01.079: INFO: namespace kubectl-4121 deletion completed in 8.381632965s


â€¢ [SLOW TEST:36.529 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:44:51.322: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 03:44:51.438: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-627'
Dec 12 03:44:51.809: INFO: stderr: ""
Dec 12 03:44:51.813: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Dec 12 03:44:51.846: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-httpd-pod --namespace=kubectl-627'
Dec 12 03:45:02.668: INFO: stderr: ""
Dec 12 03:45:02.668: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:45:02.668: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-627" for this suite.
Dec 12 03:45:10.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:45:13.224: INFO: namespace kubectl-627 deletion completed in 10.494071321s


â€¢ [SLOW TEST:21.902 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:45:01.103: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:45:01.239: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:45:03.062: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5810" for this suite.
Dec 12 03:45:11.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:45:13.683: INFO: namespace custom-resource-definition-5810 deletion completed in 10.584899834s


â€¢ [SLOW TEST:12.580 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:44:37.110: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:44:37.338: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 12 03:44:47.380: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 12 03:44:49.400: INFO: Creating deployment "test-rollover-deployment"
Dec 12 03:44:49.437: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 12 03:44:51.477: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 12 03:44:51.517: INFO: Ensure that both replica sets have 1 created replica
Dec 12 03:44:51.551: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 12 03:44:51.598: INFO: Updating deployment test-rollover-deployment
Dec 12 03:44:51.598: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 12 03:44:53.635: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 12 03:44:53.668: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 12 03:44:53.699: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:44:53.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719091, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:44:55.733: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:44:55.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719091, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:44:57.736: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:44:57.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719091, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:44:59.734: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:44:59.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719091, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:45:01.747: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:45:01.747: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719100, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:45:03.743: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:45:03.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719100, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:45:05.733: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:45:05.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719100, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:45:07.733: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:45:07.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719100, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:45:09.735: INFO: all replica sets need to contain the pod-template-hash label
Dec 12 03:45:09.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719100, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719089, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:45:11.752: INFO: 
Dec 12 03:45:11.752: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 12 03:45:11.802: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5186 /apis/apps/v1/namespaces/deployment-5186/deployments/test-rollover-deployment 8e01eb96-1806-4a58-917b-5c4780d63030 46908 2 2019-12-12 03:44:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00d194228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-12 03:44:49 +0000 UTC,LastTransitionTime:2019-12-12 03:44:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-12-12 03:45:10 +0000 UTC,LastTransitionTime:2019-12-12 03:44:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 12 03:45:11.819: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-5186 /apis/apps/v1/namespaces/deployment-5186/replicasets/test-rollover-deployment-7d7dc6548c 6ecfc566-a535-4e60-b45e-084eeb80acf5 46897 2 2019-12-12 03:44:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8e01eb96-1806-4a58-917b-5c4780d63030 0xc00d1946b7 0xc00d1946b8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00d194718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:45:11.819: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 12 03:45:11.819: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5186 /apis/apps/v1/namespaces/deployment-5186/replicasets/test-rollover-controller 4818fb3c-2c2a-4494-a000-d4b706ec423e 46906 2 2019-12-12 03:44:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8e01eb96-1806-4a58-917b-5c4780d63030 0xc00d1945e7 0xc00d1945e8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00d194648 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:45:11.820: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-5186 /apis/apps/v1/namespaces/deployment-5186/replicasets/test-rollover-deployment-f6c94f66c 5a905c66-3f33-4ac5-8494-6eb9b5bfea9a 46617 2 2019-12-12 03:44:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8e01eb96-1806-4a58-917b-5c4780d63030 0xc00d194780 0xc00d194781}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00d1947f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:45:11.837: INFO: Pod "test-rollover-deployment-7d7dc6548c-vz76n" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-vz76n test-rollover-deployment-7d7dc6548c- deployment-5186 /api/v1/namespaces/deployment-5186/pods/test-rollover-deployment-7d7dc6548c-vz76n 30ab12a1-787a-432f-8d5d-88df55385c7d 46711 0 2019-12-12 03:44:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.193"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 6ecfc566-a535-4e60-b45e-084eeb80acf5 0xc00d194da7 0xc00d194da8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zdds4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zdds4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zdds4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-24z5w,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:44:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:45:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:44:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:10.128.2.193,StartTime:2019-12-12 03:44:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://50d628354999d10df93c5604f75d338d899e469f03f463ee1d2de37867dacc08,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:45:11.837: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-5186" for this suite.
Dec 12 03:45:17.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:45:20.210: INFO: namespace deployment-5186 deletion completed in 8.324107645s


â€¢ [SLOW TEST:43.104 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:45:20.265: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:45:37.586: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-5495" for this suite.
Dec 12 03:45:43.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:45:46.051: INFO: namespace resourcequota-5495 deletion completed in 8.422366805s


â€¢ [SLOW TEST:25.786 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:45:46.054: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 12 03:45:46.232: INFO: Waiting up to 5m0s for pod "pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942" in namespace "emptydir-1511" to be "success or failure"
Dec 12 03:45:46.249: INFO: Pod "pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942": Phase="Pending", Reason="", readiness=false. Elapsed: 17.157164ms
Dec 12 03:45:48.269: INFO: Pod "pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036679566s
Dec 12 03:45:50.288: INFO: Pod "pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0555058s
Dec 12 03:45:52.304: INFO: Pod "pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071704563s
Dec 12 03:45:54.321: INFO: Pod "pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.088769821s
STEP: Saw pod success
Dec 12 03:45:54.321: INFO: Pod "pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942" satisfied condition "success or failure"
Dec 12 03:45:54.336: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942 container test-container: <nil>
STEP: delete the pod
Dec 12 03:45:54.389: INFO: Waiting for pod pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942 to disappear
Dec 12 03:45:54.405: INFO: Pod pod-0d1aaede-7c7f-44cf-a1be-6cb910c6a942 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:45:54.405: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1511" for this suite.
Dec 12 03:46:00.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:46:02.808: INFO: namespace emptydir-1511 deletion completed in 8.359941115s


â€¢ [SLOW TEST:16.754 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:46:02.830: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:46:02.986: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2" in namespace "downward-api-2286" to be "success or failure"
Dec 12 03:46:03.002: INFO: Pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.5924ms
Dec 12 03:46:05.019: INFO: Pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032692297s
Dec 12 03:46:07.036: INFO: Pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050276003s
Dec 12 03:46:09.052: INFO: Pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065827274s
Dec 12 03:46:11.070: INFO: Pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084119127s
Dec 12 03:46:13.086: INFO: Pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099882132s
STEP: Saw pod success
Dec 12 03:46:13.086: INFO: Pod "downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2" satisfied condition "success or failure"
Dec 12 03:46:13.103: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2 container client-container: <nil>
STEP: delete the pod
Dec 12 03:46:13.147: INFO: Waiting for pod downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2 to disappear
Dec 12 03:46:13.170: INFO: Pod downwardapi-volume-9f0ffe4f-06d9-4812-98a1-f0e83e5c0fd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:46:13.170: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2286" for this suite.
Dec 12 03:46:19.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:46:21.537: INFO: namespace downward-api-2286 deletion completed in 8.324056338s


â€¢ [SLOW TEST:18.707 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:46:21.591: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 12 03:46:21.791: INFO: Waiting up to 5m0s for pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a" in namespace "downward-api-5014" to be "success or failure"
Dec 12 03:46:21.808: INFO: Pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.067403ms
Dec 12 03:46:23.825: INFO: Pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033989628s
Dec 12 03:46:25.843: INFO: Pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051983629s
Dec 12 03:46:27.860: INFO: Pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068803523s
Dec 12 03:46:29.876: INFO: Pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085022655s
Dec 12 03:46:31.893: INFO: Pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101899818s
STEP: Saw pod success
Dec 12 03:46:31.893: INFO: Pod "downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a" satisfied condition "success or failure"
Dec 12 03:46:31.908: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:46:31.954: INFO: Waiting for pod downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a to disappear
Dec 12 03:46:31.969: INFO: Pod downward-api-b34a82d5-40ee-4b44-805b-6c0632c7c30a no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:46:31.969: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5014" for this suite.
Dec 12 03:46:38.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:46:40.363: INFO: namespace downward-api-5014 deletion completed in 8.348328386s


â€¢ [SLOW TEST:18.773 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:46:40.380: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:46:40.552: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "tables-2084" for this suite.
Dec 12 03:46:46.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:46:48.964: INFO: namespace tables-2084 deletion completed in 8.394225929s


â€¢ [SLOW TEST:8.584 seconds]
[sig-api-machinery] Servers with support for Table transformation
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:45:13.779: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6620
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6620
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6620
Dec 12 03:45:14.022: INFO: Found 0 stateful pods, waiting for 1
Dec 12 03:45:24.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 12 03:45:24.069: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:45:24.541: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:45:24.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:45:24.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:45:24.560: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 12 03:45:34.578: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:45:34.578: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:45:34.651: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999478s
Dec 12 03:45:35.673: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.977287648s
Dec 12 03:45:36.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.955773859s
Dec 12 03:45:37.712: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.937257847s
Dec 12 03:45:38.731: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.920122204s
Dec 12 03:45:39.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.899274397s
Dec 12 03:45:40.768: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.881680408s
Dec 12 03:45:41.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.863274041s
Dec 12 03:45:42.811: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.845243265s
Dec 12 03:45:43.838: INFO: Verifying statefulset ss doesn't scale past 1 for another 817.238824ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6620
Dec 12 03:45:44.903: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:45:45.352: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 03:45:45.353: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:45:45.353: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:45:45.370: INFO: Found 1 stateful pods, waiting for 3
Dec 12 03:45:55.388: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:45:55.388: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:45:55.388: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 03:46:05.388: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:46:05.388: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:46:05.388: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 12 03:46:05.421: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:46:05.820: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:46:05.820: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:46:05.820: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:46:05.820: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:46:06.246: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:46:06.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:46:06.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:46:06.247: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:46:06.604: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:46:06.604: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:46:06.604: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:46:06.604: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:46:06.622: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 12 03:46:16.659: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:46:16.659: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:46:16.659: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:46:16.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999441s
Dec 12 03:46:17.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982523158s
Dec 12 03:46:18.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.956571091s
Dec 12 03:46:19.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.937621953s
Dec 12 03:46:20.794: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.919566152s
Dec 12 03:46:21.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.901587639s
Dec 12 03:46:22.832: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.881923883s
Dec 12 03:46:23.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.858609577s
Dec 12 03:46:24.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.837968883s
Dec 12 03:46:25.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 817.169281ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6620
Dec 12 03:46:26.915: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:27.441: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 03:46:27.441: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:46:27.441: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:46:27.442: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:28.069: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 03:46:28.069: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:46:28.069: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:46:28.070: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6620 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:28.645: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 03:46:28.645: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:46:28.645: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:46:28.645: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 12 03:46:58.718: INFO: Deleting all statefulset in ns statefulset-6620
Dec 12 03:46:58.735: INFO: Scaling statefulset ss to 0
Dec 12 03:46:58.785: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:46:58.801: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:46:58.856: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-6620" for this suite.
Dec 12 03:47:04.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:07.285: INFO: namespace statefulset-6620 deletion completed in 8.385738389s


â€¢ [SLOW TEST:113.506 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:46:49.083: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-91a27e10-b1b6-47da-a474-65111a95e8e7
STEP: Creating a pod to test consume secrets
Dec 12 03:46:49.280: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d" in namespace "projected-7754" to be "success or failure"
Dec 12 03:46:49.296: INFO: Pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.049867ms
Dec 12 03:46:51.318: INFO: Pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038010646s
Dec 12 03:46:53.336: INFO: Pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055823447s
Dec 12 03:46:55.351: INFO: Pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071617383s
Dec 12 03:46:57.369: INFO: Pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088867524s
Dec 12 03:46:59.384: INFO: Pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104606118s
STEP: Saw pod success
Dec 12 03:46:59.384: INFO: Pod "pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d" satisfied condition "success or failure"
Dec 12 03:46:59.400: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:46:59.449: INFO: Waiting for pod pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d to disappear
Dec 12 03:46:59.465: INFO: Pod pod-projected-secrets-53f55d25-6ed2-4c9b-9c19-c10161ddd99d no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:46:59.465: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7754" for this suite.
Dec 12 03:47:05.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:07.945: INFO: namespace projected-7754 deletion completed in 8.434340754s


â€¢ [SLOW TEST:18.862 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:45:13.300: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6242
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 12 03:45:13.501: INFO: Found 0 stateful pods, waiting for 3
Dec 12 03:45:23.522: INFO: Found 2 stateful pods, waiting for 3
Dec 12 03:45:33.518: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:45:33.518: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:45:33.518: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 03:45:43.523: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:45:43.523: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:45:43.523: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 12 03:45:43.638: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 12 03:45:53.768: INFO: Updating stateful set ss2
Dec 12 03:45:53.803: INFO: Waiting for Pod statefulset-6242/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 12 03:46:03.909: INFO: Found 2 stateful pods, waiting for 3
Dec 12 03:46:13.938: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:46:13.938: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:46:13.938: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 12 03:46:23.926: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:46:23.926: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:46:23.926: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 12 03:46:24.020: INFO: Updating stateful set ss2
Dec 12 03:46:24.056: INFO: Waiting for Pod statefulset-6242/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:46:34.141: INFO: Updating stateful set ss2
Dec 12 03:46:34.175: INFO: Waiting for StatefulSet statefulset-6242/ss2 to complete update
Dec 12 03:46:34.175: INFO: Waiting for Pod statefulset-6242/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 12 03:46:44.211: INFO: Waiting for StatefulSet statefulset-6242/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 12 03:46:54.208: INFO: Deleting all statefulset in ns statefulset-6242
Dec 12 03:46:54.223: INFO: Scaling statefulset ss2 to 0
Dec 12 03:47:14.293: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:47:14.309: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:14.363: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-6242" for this suite.
Dec 12 03:47:22.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:24.808: INFO: namespace statefulset-6242 deletion completed in 10.401663085s


â€¢ [SLOW TEST:131.508 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:07.348: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:47:07.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c" in namespace "downward-api-148" to be "success or failure"
Dec 12 03:47:07.540: INFO: Pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.393163ms
Dec 12 03:47:09.563: INFO: Pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041060751s
Dec 12 03:47:11.582: INFO: Pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060063723s
Dec 12 03:47:13.599: INFO: Pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076986373s
Dec 12 03:47:15.619: INFO: Pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097015625s
Dec 12 03:47:17.636: INFO: Pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.113803381s
STEP: Saw pod success
Dec 12 03:47:17.636: INFO: Pod "downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c" satisfied condition "success or failure"
Dec 12 03:47:17.654: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c container client-container: <nil>
STEP: delete the pod
Dec 12 03:47:17.709: INFO: Waiting for pod downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c to disappear
Dec 12 03:47:17.725: INFO: Pod downwardapi-volume-e6c72290-5b93-429e-bfcb-94f116a3780c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:17.725: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-148" for this suite.
Dec 12 03:47:23.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:26.184: INFO: namespace downward-api-148 deletion completed in 8.414913025s


â€¢ [SLOW TEST:18.836 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:08.028: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-6ce1c77f-ce49-4465-9696-eb16d0491e79
STEP: Creating a pod to test consume secrets
Dec 12 03:47:08.197: INFO: Waiting up to 5m0s for pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed" in namespace "secrets-2283" to be "success or failure"
Dec 12 03:47:08.212: INFO: Pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed": Phase="Pending", Reason="", readiness=false. Elapsed: 15.64817ms
Dec 12 03:47:10.230: INFO: Pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03356584s
Dec 12 03:47:12.248: INFO: Pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051555656s
Dec 12 03:47:14.266: INFO: Pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068902752s
Dec 12 03:47:16.283: INFO: Pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086188311s
Dec 12 03:47:18.301: INFO: Pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104180934s
STEP: Saw pod success
Dec 12 03:47:18.301: INFO: Pod "pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed" satisfied condition "success or failure"
Dec 12 03:47:18.316: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:47:18.362: INFO: Waiting for pod pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed to disappear
Dec 12 03:47:18.377: INFO: Pod pod-secrets-56effa45-fca5-44d4-8dab-23185e937eed no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:18.377: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2283" for this suite.
Dec 12 03:47:24.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:26.897: INFO: namespace secrets-2283 deletion completed in 8.468016446s


â€¢ [SLOW TEST:18.869 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:26.353: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-d09cbb34-ff25-4727-9dbb-424002cbe958
STEP: Creating a pod to test consume configMaps
Dec 12 03:47:26.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7" in namespace "configmap-5813" to be "success or failure"
Dec 12 03:47:26.600: INFO: Pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.554765ms
Dec 12 03:47:28.619: INFO: Pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038625932s
Dec 12 03:47:30.639: INFO: Pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05896009s
Dec 12 03:47:32.659: INFO: Pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078619371s
Dec 12 03:47:34.677: INFO: Pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096610567s
Dec 12 03:47:36.694: INFO: Pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.114383508s
STEP: Saw pod success
Dec 12 03:47:36.695: INFO: Pod "pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7" satisfied condition "success or failure"
Dec 12 03:47:36.715: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:47:36.766: INFO: Waiting for pod pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7 to disappear
Dec 12 03:47:36.783: INFO: Pod pod-configmaps-17ceb772-8917-49ea-ae3f-0714a0389fe7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:36.783: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-5813" for this suite.
Dec 12 03:47:42.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:45.243: INFO: namespace configmap-5813 deletion completed in 8.413563552s


â€¢ [SLOW TEST:18.890 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:27.246: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:47:27.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f" in namespace "downward-api-4086" to be "success or failure"
Dec 12 03:47:27.434: INFO: Pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.156427ms
Dec 12 03:47:29.450: INFO: Pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035657541s
Dec 12 03:47:31.467: INFO: Pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052186339s
Dec 12 03:47:33.484: INFO: Pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069903574s
Dec 12 03:47:35.502: INFO: Pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087267545s
Dec 12 03:47:37.519: INFO: Pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104185854s
STEP: Saw pod success
Dec 12 03:47:37.519: INFO: Pod "downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f" satisfied condition "success or failure"
Dec 12 03:47:37.534: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f container client-container: <nil>
STEP: delete the pod
Dec 12 03:47:37.581: INFO: Waiting for pod downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f to disappear
Dec 12 03:47:37.597: INFO: Pod downwardapi-volume-15d91599-7927-4137-8fee-3cb599b9270f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:37.597: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4086" for this suite.
Dec 12 03:47:43.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:46.115: INFO: namespace downward-api-4086 deletion completed in 8.474851619s


â€¢ [SLOW TEST:18.869 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:45.257: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 03:47:45.412: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-8216'
Dec 12 03:47:45.682: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 03:47:45.682: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Dec 12 03:47:47.810: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-httpd-deployment --namespace=kubectl-8216'
Dec 12 03:47:48.151: INFO: stderr: ""
Dec 12 03:47:48.151: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:48.151: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8216" for this suite.
Dec 12 03:47:54.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:47:56.627: INFO: namespace kubectl-8216 deletion completed in 8.429034829s


â€¢ [SLOW TEST:11.370 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:24.844: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-hzhf
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 03:47:25.054: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hzhf" in namespace "subpath-9465" to be "success or failure"
Dec 12 03:47:25.070: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.280026ms
Dec 12 03:47:27.087: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03309319s
Dec 12 03:47:29.104: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050008851s
Dec 12 03:47:31.123: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069015089s
Dec 12 03:47:33.139: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 8.085571786s
Dec 12 03:47:35.156: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 10.102178481s
Dec 12 03:47:37.174: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 12.120008059s
Dec 12 03:47:39.190: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 14.136125888s
Dec 12 03:47:41.208: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 16.154196906s
Dec 12 03:47:43.226: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 18.171898614s
Dec 12 03:47:45.277: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 20.222854079s
Dec 12 03:47:47.295: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 22.241067469s
Dec 12 03:47:49.311: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 24.257684928s
Dec 12 03:47:51.328: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Running", Reason="", readiness=true. Elapsed: 26.27412697s
Dec 12 03:47:53.351: INFO: Pod "pod-subpath-test-configmap-hzhf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.297013142s
STEP: Saw pod success
Dec 12 03:47:53.351: INFO: Pod "pod-subpath-test-configmap-hzhf" satisfied condition "success or failure"
Dec 12 03:47:53.368: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-subpath-test-configmap-hzhf container test-container-subpath-configmap-hzhf: <nil>
STEP: delete the pod
Dec 12 03:47:53.454: INFO: Waiting for pod pod-subpath-test-configmap-hzhf to disappear
Dec 12 03:47:53.470: INFO: Pod pod-subpath-test-configmap-hzhf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hzhf
Dec 12 03:47:53.470: INFO: Deleting pod "pod-subpath-test-configmap-hzhf" in namespace "subpath-9465"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:53.486: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-9465" for this suite.
Dec 12 03:47:59.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:48:01.934: INFO: namespace subpath-9465 deletion completed in 8.404139494s


â€¢ [SLOW TEST:37.089 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:46.180: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 12 03:47:46.413: INFO: Waiting up to 5m0s for pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f" in namespace "emptydir-4218" to be "success or failure"
Dec 12 03:47:46.431: INFO: Pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.551748ms
Dec 12 03:47:48.449: INFO: Pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035525028s
Dec 12 03:47:50.467: INFO: Pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053449957s
Dec 12 03:47:52.484: INFO: Pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070854679s
Dec 12 03:47:54.502: INFO: Pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088470108s
Dec 12 03:47:56.521: INFO: Pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107288889s
STEP: Saw pod success
Dec 12 03:47:56.521: INFO: Pod "pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f" satisfied condition "success or failure"
Dec 12 03:47:56.539: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f container test-container: <nil>
STEP: delete the pod
Dec 12 03:47:56.598: INFO: Waiting for pod pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f to disappear
Dec 12 03:47:56.616: INFO: Pod pod-9c948e6e-54f0-47c3-a7b3-b13779f7b52f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:47:56.616: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4218" for this suite.
Dec 12 03:48:02.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:48:05.030: INFO: namespace emptydir-4218 deletion completed in 8.363533865s


â€¢ [SLOW TEST:18.850 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:47:56.699: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:47:56.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04" in namespace "projected-1981" to be "success or failure"
Dec 12 03:47:56.884: INFO: Pod "downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04": Phase="Pending", Reason="", readiness=false. Elapsed: 20.367606ms
Dec 12 03:47:58.902: INFO: Pod "downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037866128s
Dec 12 03:48:00.920: INFO: Pod "downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056036868s
Dec 12 03:48:02.938: INFO: Pod "downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074162727s
Dec 12 03:48:04.955: INFO: Pod "downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.091171728s
STEP: Saw pod success
Dec 12 03:48:04.955: INFO: Pod "downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04" satisfied condition "success or failure"
Dec 12 03:48:04.972: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04 container client-container: <nil>
STEP: delete the pod
Dec 12 03:48:05.023: INFO: Waiting for pod downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04 to disappear
Dec 12 03:48:05.039: INFO: Pod downwardapi-volume-2685e445-18d0-42f1-bf45-cc2e79a5eb04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:48:05.039: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1981" for this suite.
Dec 12 03:48:11.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:48:13.701: INFO: namespace projected-1981 deletion completed in 8.613689865s


â€¢ [SLOW TEST:17.002 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:48:02.008: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:48:02.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09" in namespace "downward-api-1813" to be "success or failure"
Dec 12 03:48:02.181: INFO: Pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09": Phase="Pending", Reason="", readiness=false. Elapsed: 27.562147ms
Dec 12 03:48:04.201: INFO: Pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047456112s
Dec 12 03:48:06.218: INFO: Pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06451868s
Dec 12 03:48:08.244: INFO: Pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091042618s
Dec 12 03:48:10.263: INFO: Pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109551727s
Dec 12 03:48:12.280: INFO: Pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.12659029s
STEP: Saw pod success
Dec 12 03:48:12.280: INFO: Pod "downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09" satisfied condition "success or failure"
Dec 12 03:48:12.297: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09 container client-container: <nil>
STEP: delete the pod
Dec 12 03:48:12.350: INFO: Waiting for pod downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09 to disappear
Dec 12 03:48:12.373: INFO: Pod downwardapi-volume-01c65757-e46c-49da-9737-1f8a81d86a09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:48:12.373: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1813" for this suite.
Dec 12 03:48:18.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:48:21.009: INFO: namespace downward-api-1813 deletion completed in 8.589424519s


â€¢ [SLOW TEST:19.001 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:48:13.707: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:48:14.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:48:16.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:48:18.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:48:20.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:48:22.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719294, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:48:25.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:48:26.163: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-7125" for this suite.
Dec 12 03:48:34.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:48:36.768: INFO: namespace webhook-7125 deletion completed in 10.583276206s
STEP: Destroying namespace "webhook-7125-markers" for this suite.
Dec 12 03:48:42.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:48:45.408: INFO: namespace webhook-7125-markers deletion completed in 8.639963614s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:31.773 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:48:05.123: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 12 03:48:05.281: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:00.656: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1374" for this suite.
Dec 12 03:49:06.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:49:09.112: INFO: namespace crd-publish-openapi-1374 deletion completed in 8.411409211s


â€¢ [SLOW TEST:63.990 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:48:21.034: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 12 03:48:21.154: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-6781'
Dec 12 03:48:22.116: INFO: stderr: ""
Dec 12 03:48:22.116: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 03:48:22.117: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6781'
Dec 12 03:48:22.598: INFO: stderr: ""
Dec 12 03:48:22.598: INFO: stdout: "update-demo-nautilus-hvsdp update-demo-nautilus-tmphw "
Dec 12 03:48:22.598: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:22.998: INFO: stderr: ""
Dec 12 03:48:22.998: INFO: stdout: ""
Dec 12 03:48:22.998: INFO: update-demo-nautilus-hvsdp is created but not running
Dec 12 03:48:28.000: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6781'
Dec 12 03:48:28.372: INFO: stderr: ""
Dec 12 03:48:28.372: INFO: stdout: "update-demo-nautilus-hvsdp update-demo-nautilus-tmphw "
Dec 12 03:48:28.372: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:28.730: INFO: stderr: ""
Dec 12 03:48:28.730: INFO: stdout: ""
Dec 12 03:48:28.730: INFO: update-demo-nautilus-hvsdp is created but not running
Dec 12 03:48:33.732: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6781'
Dec 12 03:48:34.138: INFO: stderr: ""
Dec 12 03:48:34.138: INFO: stdout: "update-demo-nautilus-hvsdp update-demo-nautilus-tmphw "
Dec 12 03:48:34.139: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:34.605: INFO: stderr: ""
Dec 12 03:48:34.605: INFO: stdout: "true"
Dec 12 03:48:34.606: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:35.160: INFO: stderr: ""
Dec 12 03:48:35.160: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:48:35.160: INFO: validating pod update-demo-nautilus-hvsdp
Dec 12 03:48:35.181: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:48:35.181: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:48:35.181: INFO: update-demo-nautilus-hvsdp is verified up and running
Dec 12 03:48:35.181: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-tmphw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:35.561: INFO: stderr: ""
Dec 12 03:48:35.562: INFO: stdout: "true"
Dec 12 03:48:35.562: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-tmphw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:35.960: INFO: stderr: ""
Dec 12 03:48:35.961: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:48:35.961: INFO: validating pod update-demo-nautilus-tmphw
Dec 12 03:48:35.981: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:48:35.981: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:48:35.981: INFO: update-demo-nautilus-tmphw is verified up and running
STEP: scaling down the replication controller
Dec 12 03:48:35.986: INFO: scanned /tmp/home for discovery docs: <nil>
Dec 12 03:48:35.986: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6781'
Dec 12 03:48:37.449: INFO: stderr: ""
Dec 12 03:48:37.449: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 03:48:37.450: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6781'
Dec 12 03:48:37.788: INFO: stderr: ""
Dec 12 03:48:37.788: INFO: stdout: "update-demo-nautilus-hvsdp update-demo-nautilus-tmphw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 12 03:48:42.789: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6781'
Dec 12 03:48:43.229: INFO: stderr: ""
Dec 12 03:48:43.229: INFO: stdout: "update-demo-nautilus-hvsdp "
Dec 12 03:48:43.229: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:43.600: INFO: stderr: ""
Dec 12 03:48:43.600: INFO: stdout: "true"
Dec 12 03:48:43.600: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:43.982: INFO: stderr: ""
Dec 12 03:48:43.982: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:48:43.983: INFO: validating pod update-demo-nautilus-hvsdp
Dec 12 03:48:44.001: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:48:44.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:48:44.001: INFO: update-demo-nautilus-hvsdp is verified up and running
STEP: scaling up the replication controller
Dec 12 03:48:44.007: INFO: scanned /tmp/home for discovery docs: <nil>
Dec 12 03:48:44.007: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6781'
Dec 12 03:48:44.597: INFO: stderr: ""
Dec 12 03:48:44.597: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 12 03:48:44.598: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6781'
Dec 12 03:48:45.026: INFO: stderr: ""
Dec 12 03:48:45.026: INFO: stdout: "update-demo-nautilus-hvsdp update-demo-nautilus-sr4t6 "
Dec 12 03:48:45.026: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:45.513: INFO: stderr: ""
Dec 12 03:48:45.513: INFO: stdout: "true"
Dec 12 03:48:45.514: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:45.919: INFO: stderr: ""
Dec 12 03:48:45.919: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:48:45.919: INFO: validating pod update-demo-nautilus-hvsdp
Dec 12 03:48:45.938: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:48:45.938: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:48:45.938: INFO: update-demo-nautilus-hvsdp is verified up and running
Dec 12 03:48:45.938: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-sr4t6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:46.258: INFO: stderr: ""
Dec 12 03:48:46.258: INFO: stdout: ""
Dec 12 03:48:46.259: INFO: update-demo-nautilus-sr4t6 is created but not running
Dec 12 03:48:51.266: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6781'
Dec 12 03:48:51.730: INFO: stderr: ""
Dec 12 03:48:51.730: INFO: stdout: "update-demo-nautilus-hvsdp update-demo-nautilus-sr4t6 "
Dec 12 03:48:51.730: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:52.603: INFO: stderr: ""
Dec 12 03:48:52.603: INFO: stdout: "true"
Dec 12 03:48:52.604: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-hvsdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:53.012: INFO: stderr: ""
Dec 12 03:48:53.013: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:48:53.013: INFO: validating pod update-demo-nautilus-hvsdp
Dec 12 03:48:53.033: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:48:53.033: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:48:53.033: INFO: update-demo-nautilus-hvsdp is verified up and running
Dec 12 03:48:53.033: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-sr4t6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:53.419: INFO: stderr: ""
Dec 12 03:48:53.419: INFO: stdout: "true"
Dec 12 03:48:53.420: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-sr4t6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6781'
Dec 12 03:48:54.126: INFO: stderr: ""
Dec 12 03:48:54.126: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 12 03:48:54.126: INFO: validating pod update-demo-nautilus-sr4t6
Dec 12 03:48:54.146: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 12 03:48:54.146: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 12 03:48:54.146: INFO: update-demo-nautilus-sr4t6 is verified up and running
STEP: using delete to clean up resources
Dec 12 03:48:54.146: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-6781'
Dec 12 03:48:54.643: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:48:54.643: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 12 03:48:54.643: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6781'
Dec 12 03:48:55.188: INFO: stderr: "No resources found in kubectl-6781 namespace.\n"
Dec 12 03:48:55.188: INFO: stdout: ""
Dec 12 03:48:55.188: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=kubectl-6781 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 12 03:48:55.499: INFO: stderr: ""
Dec 12 03:48:55.500: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:48:55.500: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6781" for this suite.
Dec 12 03:49:07.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:49:09.975: INFO: namespace kubectl-6781 deletion completed in 14.422297927s


â€¢ [SLOW TEST:48.941 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:48:45.545: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-4625
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 03:48:45.711: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Dec 12 03:49:20.265: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.217:8080/dial?request=hostName&protocol=udp&host=10.128.2.215&port=8081&tries=1'] Namespace:pod-network-test-4625 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:49:20.265: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:49:20.468: INFO: Waiting for endpoints: map[]
Dec 12 03:49:20.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.217:8080/dial?request=hostName&protocol=udp&host=10.131.0.57&port=8081&tries=1'] Namespace:pod-network-test-4625 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:49:20.485: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:49:20.900: INFO: Waiting for endpoints: map[]
Dec 12 03:49:20.916: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.217:8080/dial?request=hostName&protocol=udp&host=10.129.2.56&port=8081&tries=1'] Namespace:pod-network-test-4625 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:49:20.916: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:49:21.143: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:21.143: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-4625" for this suite.
Dec 12 03:49:27.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:49:29.592: INFO: namespace pod-network-test-4625 deletion completed in 8.403293s


â€¢ [SLOW TEST:44.047 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:49:09.398: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-405d3142-fb62-4f81-a153-8b1cc2fb50dc in namespace container-probe-7140
Dec 12 03:49:17.649: INFO: Started pod liveness-405d3142-fb62-4f81-a153-8b1cc2fb50dc in namespace container-probe-7140
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 03:49:17.665: INFO: Initial restart count of pod liveness-405d3142-fb62-4f81-a153-8b1cc2fb50dc is 0
Dec 12 03:49:33.830: INFO: Restart count of pod container-probe-7140/liveness-405d3142-fb62-4f81-a153-8b1cc2fb50dc is now 1 (16.165223769s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:33.856: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7140" for this suite.
Dec 12 03:49:39.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:49:42.311: INFO: namespace container-probe-7140 deletion completed in 8.410515788s


â€¢ [SLOW TEST:32.913 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:49:29.724: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8512.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8512.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8512.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8512.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8512.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8512.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:49:40.137: INFO: DNS probes using dns-8512/dns-test-464f3553-d388-4d2c-b20c-d76f0135648b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:40.177: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-8512" for this suite.
Dec 12 03:49:46.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:49:48.569: INFO: namespace dns-8512 deletion completed in 8.345068342s


â€¢ [SLOW TEST:18.845 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:43:42.058: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2056
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-2056
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2056
Dec 12 03:43:42.266: INFO: Found 0 stateful pods, waiting for 1
Dec 12 03:43:52.283: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 12 03:43:52.300: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:43:53.032: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:43:53.032: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:43:53.033: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:43:53.050: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 12 03:44:03.068: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:44:03.068: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:44:03.288: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Dec 12 03:44:03.288: INFO: ss-0  ip-10-0-133-2.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  }]
Dec 12 03:44:03.288: INFO: ss-1                              Pending         []
Dec 12 03:44:03.288: INFO: 
Dec 12 03:44:03.288: INFO: StatefulSet ss has not reached scale 3, at 2
Dec 12 03:44:04.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980196716s
Dec 12 03:44:05.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962808078s
Dec 12 03:44:06.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9438343s
Dec 12 03:44:07.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.922854943s
Dec 12 03:44:08.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.905221416s
Dec 12 03:44:09.411: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.880380218s
Dec 12 03:44:10.432: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.855833761s
Dec 12 03:44:11.452: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.836216612s
Dec 12 03:44:12.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 814.852838ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2056
Dec 12 03:44:13.489: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:44:14.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 12 03:44:14.043: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:44:14.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:44:14.043: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:44:14.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 12 03:44:14.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:44:14.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:44:14.549: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:44:15.146: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 12 03:44:15.146: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 12 03:44:15.146: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 12 03:44:15.166: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:44:15.166: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 12 03:44:15.166: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 12 03:44:15.192: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:44:15.728: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:44:15.728: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:44:15.728: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:44:15.729: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:44:16.244: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:44:16.244: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:44:16.244: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:44:16.244: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 12 03:44:16.773: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 12 03:44:16.773: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 12 03:44:16.773: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 12 03:44:16.773: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:44:16.791: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 12 03:44:26.828: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:44:26.828: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:44:26.828: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 12 03:44:26.886: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:26.886: INFO: ss-0  ip-10-0-133-2.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  }]
Dec 12 03:44:26.886: INFO: ss-1  ip-10-0-151-48.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:26.886: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:26.886: INFO: 
Dec 12 03:44:26.886: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 12 03:44:27.905: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:27.905: INFO: ss-0  ip-10-0-133-2.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  }]
Dec 12 03:44:27.905: INFO: ss-1  ip-10-0-151-48.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:27.905: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:27.905: INFO: 
Dec 12 03:44:27.905: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 12 03:44:28.922: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:28.922: INFO: ss-0  ip-10-0-133-2.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  }]
Dec 12 03:44:28.922: INFO: ss-1  ip-10-0-151-48.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:28.922: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:28.922: INFO: 
Dec 12 03:44:28.922: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 12 03:44:29.943: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:29.943: INFO: ss-0  ip-10-0-133-2.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  }]
Dec 12 03:44:29.943: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:29.943: INFO: 
Dec 12 03:44:29.943: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 03:44:30.964: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:30.964: INFO: ss-0  ip-10-0-133-2.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  }]
Dec 12 03:44:30.964: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:30.964: INFO: 
Dec 12 03:44:30.964: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 03:44:31.988: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:31.988: INFO: ss-0  ip-10-0-133-2.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:43:42 +0000 UTC  }]
Dec 12 03:44:31.988: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:31.988: INFO: 
Dec 12 03:44:31.988: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 12 03:44:33.006: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:33.006: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:33.007: INFO: 
Dec 12 03:44:33.007: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 12 03:44:34.028: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:34.028: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:34.028: INFO: 
Dec 12 03:44:34.028: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 12 03:44:35.047: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:35.047: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:35.047: INFO: 
Dec 12 03:44:35.047: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 12 03:44:36.068: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Dec 12 03:44:36.068: INFO: ss-2  ip-10-0-133-17.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-12 03:44:03 +0000 UTC  }]
Dec 12 03:44:36.068: INFO: 
Dec 12 03:44:36.068: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2056
Dec 12 03:44:37.087: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:44:37.536: INFO: rc: 1
Dec 12 03:44:37.537: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc003837f50 exit status 1 <nil> <nil> true [0xc0000f6c78 0xc0000f6e60 0xc0000f6f48] [0xc0000f6c78 0xc0000f6e60 0xc0000f6f48] [0xc0000f6e08 0xc0000f6f30] [0x10ef5a0 0x10ef5a0] 0xc003c67da0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 12 03:44:47.538: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:44:47.826: INFO: rc: 1
Dec 12 03:44:47.827: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003514750 exit status 1 <nil> <nil> true [0xc0000f6f68 0xc0000f7070 0xc0000f7158] [0xc0000f6f68 0xc0000f7070 0xc0000f7158] [0xc0000f7000 0xc0000f7120] [0x10ef5a0 0x10ef5a0] 0xc003b00360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:44:57.827: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:44:58.143: INFO: rc: 1
Dec 12 03:44:58.143: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038a2d20 exit status 1 <nil> <nil> true [0xc0001b4918 0xc0001b4a28 0xc0001b4b60] [0xc0001b4918 0xc0001b4a28 0xc0001b4b60] [0xc0001b49e8 0xc0001b4b38] [0x10ef5a0 0x10ef5a0] 0xc003e00420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:45:08.145: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:45:08.458: INFO: rc: 1
Dec 12 03:45:08.459: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003dbcab0 exit status 1 <nil> <nil> true [0xc000011f10 0xc0025f8000 0xc0025f8018] [0xc000011f10 0xc0025f8000 0xc0025f8018] [0xc000011fc8 0xc0025f8010] [0x10ef5a0 0x10ef5a0] 0xc0032b0a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:45:18.459: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:45:18.793: INFO: rc: 1
Dec 12 03:45:18.794: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003dbd0e0 exit status 1 <nil> <nil> true [0xc0025f8020 0xc0025f8038 0xc0025f8050] [0xc0025f8020 0xc0025f8038 0xc0025f8050] [0xc0025f8030 0xc0025f8048] [0x10ef5a0 0x10ef5a0] 0xc0032b0d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:45:28.794: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:45:29.061: INFO: rc: 1
Dec 12 03:45:29.062: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003dbd6e0 exit status 1 <nil> <nil> true [0xc0025f8058 0xc0025f8070 0xc0025f8098] [0xc0025f8058 0xc0025f8070 0xc0025f8098] [0xc0025f8068 0xc0025f8088] [0x10ef5a0 0x10ef5a0] 0xc0032b1080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:45:39.063: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:45:39.459: INFO: rc: 1
Dec 12 03:45:39.460: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a84600 exit status 1 <nil> <nil> true [0xc003952000 0xc003952018 0xc003952030] [0xc003952000 0xc003952018 0xc003952030] [0xc003952010 0xc003952028] [0x10ef5a0 0x10ef5a0] 0xc003e3e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:45:49.460: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:45:49.758: INFO: rc: 1
Dec 12 03:45:49.759: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003dbdd10 exit status 1 <nil> <nil> true [0xc0025f80b0 0xc0025f80e0 0xc0025f8110] [0xc0025f80b0 0xc0025f80e0 0xc0025f8110] [0xc0025f80d0 0xc0025f8100] [0x10ef5a0 0x10ef5a0] 0xc0032b13e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:45:59.762: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:00.031: INFO: rc: 1
Dec 12 03:46:00.032: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a84c00 exit status 1 <nil> <nil> true [0xc003952038 0xc003952050 0xc003952068] [0xc003952038 0xc003952050 0xc003952068] [0xc003952048 0xc003952060] [0x10ef5a0 0x10ef5a0] 0xc003e3e540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:46:10.032: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:10.225: INFO: rc: 1
Dec 12 03:46:10.225: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a85410 exit status 1 <nil> <nil> true [0xc003952070 0xc003952088 0xc0039520a0] [0xc003952070 0xc003952088 0xc0039520a0] [0xc003952080 0xc003952098] [0x10ef5a0 0x10ef5a0] 0xc003e3e840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:46:20.226: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:20.486: INFO: rc: 1
Dec 12 03:46:20.486: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003514d80 exit status 1 <nil> <nil> true [0xc0000f71c8 0xc0000f7290 0xc0000f73c8] [0xc0000f71c8 0xc0000f7290 0xc0000f73c8] [0xc0000f7270 0xc0000f7328] [0x10ef5a0 0x10ef5a0] 0xc003b00660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:46:30.486: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:30.764: INFO: rc: 1
Dec 12 03:46:30.764: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038365a0 exit status 1 <nil> <nil> true [0xc0000108d8 0xc000010a28 0xc000010c08] [0xc0000108d8 0xc000010a28 0xc000010c08] [0xc000010a08 0xc000010b40] [0x10ef5a0 0x10ef5a0] 0xc003c660c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:46:40.765: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:41.118: INFO: rc: 1
Dec 12 03:46:41.119: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c08690 exit status 1 <nil> <nil> true [0xc0001b4000 0xc0001b41f8 0xc0001b43e0] [0xc0001b4000 0xc0001b41f8 0xc0001b43e0] [0xc0001b4110 0xc0001b4368] [0x10ef5a0 0x10ef5a0] 0xc003e00240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:46:51.120: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:46:51.511: INFO: rc: 1
Dec 12 03:46:51.511: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003836de0 exit status 1 <nil> <nil> true [0xc000010c58 0xc000010ed0 0xc000011138] [0xc000010c58 0xc000010ed0 0xc000011138] [0xc000010e28 0xc0000110f8] [0x10ef5a0 0x10ef5a0] 0xc003c663c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:47:01.513: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:47:01.857: INFO: rc: 1
Dec 12 03:47:01.857: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c08cc0 exit status 1 <nil> <nil> true [0xc0001b44a0 0xc0001b45b0 0xc0001b4848] [0xc0001b44a0 0xc0001b45b0 0xc0001b4848] [0xc0001b4598 0xc0001b4760] [0x10ef5a0 0x10ef5a0] 0xc003e00540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:47:11.858: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:47:12.159: INFO: rc: 1
Dec 12 03:47:12.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003942660 exit status 1 <nil> <nil> true [0xc003952000 0xc003952018 0xc003952030] [0xc003952000 0xc003952018 0xc003952030] [0xc003952010 0xc003952028] [0x10ef5a0 0x10ef5a0] 0xc003e3e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:47:22.161: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:47:22.521: INFO: rc: 1
Dec 12 03:47:22.522: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038373e0 exit status 1 <nil> <nil> true [0xc000011158 0xc000011440 0xc000011670] [0xc000011158 0xc000011440 0xc000011670] [0xc0000112f8 0xc0000115d8] [0x10ef5a0 0x10ef5a0] 0xc003c666c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:47:32.523: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:47:32.859: INFO: rc: 1
Dec 12 03:47:32.859: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c09350 exit status 1 <nil> <nil> true [0xc0001b4900 0xc0001b49e8 0xc0001b4b38] [0xc0001b4900 0xc0001b49e8 0xc0001b4b38] [0xc0001b4980 0xc0001b4af8] [0x10ef5a0 0x10ef5a0] 0xc003e00840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:47:42.860: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:47:43.196: INFO: rc: 1
Dec 12 03:47:43.196: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003837bf0 exit status 1 <nil> <nil> true [0xc0000116f8 0xc0000119e0 0xc000011a40] [0xc0000116f8 0xc0000119e0 0xc000011a40] [0xc000011920 0xc000011a38] [0x10ef5a0 0x10ef5a0] 0xc003c67a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:47:53.196: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:47:53.533: INFO: rc: 1
Dec 12 03:47:53.534: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038a2210 exit status 1 <nil> <nil> true [0xc000011a80 0xc000011c10 0xc000011dd0] [0xc000011a80 0xc000011c10 0xc000011dd0] [0xc000011bf8 0xc000011d78] [0x10ef5a0 0x10ef5a0] 0xc003c67da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:48:03.541: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:48:03.863: INFO: rc: 1
Dec 12 03:48:03.864: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c099e0 exit status 1 <nil> <nil> true [0xc0001b4b60 0xc0001b4cd8 0xc0001b4d78] [0xc0001b4b60 0xc0001b4cd8 0xc0001b4d78] [0xc0001b4c70 0xc0001b4cf8] [0x10ef5a0 0x10ef5a0] 0xc003e00b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:48:13.864: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:48:14.263: INFO: rc: 1
Dec 12 03:48:14.264: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a84030 exit status 1 <nil> <nil> true [0xc0001b4f08 0xc0001b4f88 0xc0001b5018] [0xc0001b4f08 0xc0001b4f88 0xc0001b5018] [0xc0001b4f68 0xc0001b4fc8] [0x10ef5a0 0x10ef5a0] 0xc003e00e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:48:24.268: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:48:24.551: INFO: rc: 1
Dec 12 03:48:24.552: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a84660 exit status 1 <nil> <nil> true [0xc0001b5048 0xc0001b50b8 0xc0001b5148] [0xc0001b5048 0xc0001b50b8 0xc0001b5148] [0xc0001b5068 0xc0001b5120] [0x10ef5a0 0x10ef5a0] 0xc003e01140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:48:34.555: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:48:35.167: INFO: rc: 1
Dec 12 03:48:35.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c086c0 exit status 1 <nil> <nil> true [0xc0000108d8 0xc000010a28 0xc000010c08] [0xc0000108d8 0xc000010a28 0xc000010c08] [0xc000010a08 0xc000010b40] [0x10ef5a0 0x10ef5a0] 0xc003c660c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:48:45.168: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:48:45.572: INFO: rc: 1
Dec 12 03:48:45.572: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038365d0 exit status 1 <nil> <nil> true [0xc0001b4000 0xc0001b41f8 0xc0001b43e0] [0xc0001b4000 0xc0001b41f8 0xc0001b43e0] [0xc0001b4110 0xc0001b4368] [0x10ef5a0 0x10ef5a0] 0xc003e00240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:48:55.574: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:48:56.079: INFO: rc: 1
Dec 12 03:48:56.080: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c08d20 exit status 1 <nil> <nil> true [0xc000010c58 0xc000010ed0 0xc000011138] [0xc000010c58 0xc000010ed0 0xc000011138] [0xc000010e28 0xc0000110f8] [0x10ef5a0 0x10ef5a0] 0xc003c663c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:49:06.082: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:49:06.659: INFO: rc: 1
Dec 12 03:49:06.659: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038a2600 exit status 1 <nil> <nil> true [0xc003952000 0xc003952018 0xc003952030] [0xc003952000 0xc003952018 0xc003952030] [0xc003952010 0xc003952028] [0x10ef5a0 0x10ef5a0] 0xc003e3e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:49:16.659: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:49:17.203: INFO: rc: 1
Dec 12 03:49:17.203: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038a2c00 exit status 1 <nil> <nil> true [0xc003952038 0xc003952050 0xc003952068] [0xc003952038 0xc003952050 0xc003952068] [0xc003952048 0xc003952060] [0x10ef5a0 0x10ef5a0] 0xc003e3e540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:49:27.204: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:49:27.652: INFO: rc: 1
Dec 12 03:49:27.652: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003836e40 exit status 1 <nil> <nil> true [0xc0001b44a0 0xc0001b45b0 0xc0001b4848] [0xc0001b44a0 0xc0001b45b0 0xc0001b4848] [0xc0001b4598 0xc0001b4760] [0x10ef5a0 0x10ef5a0] 0xc003e00540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec 12 03:49:37.653: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-2056 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 12 03:49:37.995: INFO: rc: 1
Dec 12 03:49:37.996: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Dec 12 03:49:37.996: INFO: Scaling statefulset ss to 0
Dec 12 03:49:38.046: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 12 03:49:38.062: INFO: Deleting all statefulset in ns statefulset-2056
Dec 12 03:49:38.078: INFO: Scaling statefulset ss to 0
Dec 12 03:49:38.130: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 03:49:38.155: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:38.212: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-2056" for this suite.
Dec 12 03:49:46.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:49:48.655: INFO: namespace statefulset-2056 deletion completed in 10.395717849s


â€¢ [SLOW TEST:366.669 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:49:10.345: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-j57z
STEP: Creating a pod to test atomic-volume-subpath
Dec 12 03:49:10.542: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-j57z" in namespace "subpath-5756" to be "success or failure"
Dec 12 03:49:10.561: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Pending", Reason="", readiness=false. Elapsed: 18.409462ms
Dec 12 03:49:12.583: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040466494s
Dec 12 03:49:14.600: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05743839s
Dec 12 03:49:16.617: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074231712s
Dec 12 03:49:18.634: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091429761s
Dec 12 03:49:20.651: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 10.108488915s
Dec 12 03:49:22.669: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 12.126633371s
Dec 12 03:49:24.688: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 14.145631209s
Dec 12 03:49:26.717: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 16.174434327s
Dec 12 03:49:28.733: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 18.190618795s
Dec 12 03:49:30.749: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 20.206994708s
Dec 12 03:49:32.767: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 22.224905719s
Dec 12 03:49:34.784: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 24.241338512s
Dec 12 03:49:36.808: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 26.265416259s
Dec 12 03:49:38.825: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Running", Reason="", readiness=true. Elapsed: 28.28231238s
Dec 12 03:49:40.841: INFO: Pod "pod-subpath-test-downwardapi-j57z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.298766207s
STEP: Saw pod success
Dec 12 03:49:40.917: INFO: Pod "pod-subpath-test-downwardapi-j57z" satisfied condition "success or failure"
Dec 12 03:49:40.933: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-subpath-test-downwardapi-j57z container test-container-subpath-downwardapi-j57z: <nil>
STEP: delete the pod
Dec 12 03:49:40.977: INFO: Waiting for pod pod-subpath-test-downwardapi-j57z to disappear
Dec 12 03:49:40.993: INFO: Pod pod-subpath-test-downwardapi-j57z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-j57z
Dec 12 03:49:40.993: INFO: Deleting pod "pod-subpath-test-downwardapi-j57z" in namespace "subpath-5756"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:41.009: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-5756" for this suite.
Dec 12 03:49:47.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:49:49.434: INFO: namespace subpath-5756 deletion completed in 8.38193416s


â€¢ [SLOW TEST:39.088 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:49:48.726: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 12 03:49:48.894: INFO: Waiting up to 5m0s for pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a" in namespace "downward-api-5623" to be "success or failure"
Dec 12 03:49:48.912: INFO: Pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.526883ms
Dec 12 03:49:50.929: INFO: Pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034354257s
Dec 12 03:49:52.946: INFO: Pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051384098s
Dec 12 03:49:54.964: INFO: Pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069320587s
Dec 12 03:49:56.982: INFO: Pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087201124s
Dec 12 03:49:59.001: INFO: Pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.10641673s
STEP: Saw pod success
Dec 12 03:49:59.001: INFO: Pod "downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a" satisfied condition "success or failure"
Dec 12 03:49:59.017: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:49:59.092: INFO: Waiting for pod downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a to disappear
Dec 12 03:49:59.108: INFO: Pod downward-api-57eef397-d778-4359-a4a3-6b06ce73f56a no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:59.108: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5623" for this suite.
Dec 12 03:50:05.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:07.524: INFO: namespace downward-api-5623 deletion completed in 8.371370313s


â€¢ [SLOW TEST:18.798 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:49:49.626: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 12 03:50:00.437: INFO: Successfully updated pod "labelsupdatea02b40b2-0bf1-4817-ba68-8b2b2dc77330"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:02.477: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-3759" for this suite.
Dec 12 03:50:14.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:16.856: INFO: namespace downward-api-3759 deletion completed in 14.33390456s


â€¢ [SLOW TEST:27.230 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:07.531: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-c5b817a0-3d78-4130-8aef-2c0e2dd06a82
STEP: Creating a pod to test consume configMaps
Dec 12 03:50:07.749: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795" in namespace "configmap-1075" to be "success or failure"
Dec 12 03:50:07.766: INFO: Pod "pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795": Phase="Pending", Reason="", readiness=false. Elapsed: 17.09773ms
Dec 12 03:50:09.784: INFO: Pod "pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034968836s
Dec 12 03:50:11.801: INFO: Pod "pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052201486s
Dec 12 03:50:13.819: INFO: Pod "pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069814017s
Dec 12 03:50:15.836: INFO: Pod "pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.086908818s
STEP: Saw pod success
Dec 12 03:50:15.836: INFO: Pod "pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795" satisfied condition "success or failure"
Dec 12 03:50:15.855: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:50:15.914: INFO: Waiting for pod pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795 to disappear
Dec 12 03:50:15.933: INFO: Pod pod-configmaps-9e1b802a-e9be-414a-a2c1-9d2a8eee1795 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:15.933: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-1075" for this suite.
Dec 12 03:50:22.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:24.343: INFO: namespace configmap-1075 deletion completed in 8.365769644s


â€¢ [SLOW TEST:16.813 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:16.875: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 12 03:50:17.054: INFO: Waiting up to 5m0s for pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c" in namespace "emptydir-1655" to be "success or failure"
Dec 12 03:50:17.070: INFO: Pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.666982ms
Dec 12 03:50:19.086: INFO: Pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03211448s
Dec 12 03:50:21.106: INFO: Pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051439334s
Dec 12 03:50:23.123: INFO: Pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068338171s
Dec 12 03:50:25.141: INFO: Pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087199315s
Dec 12 03:50:27.166: INFO: Pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111685762s
STEP: Saw pod success
Dec 12 03:50:27.166: INFO: Pod "pod-1c441ec5-69f7-4381-bba3-2d873137b07c" satisfied condition "success or failure"
Dec 12 03:50:27.183: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-1c441ec5-69f7-4381-bba3-2d873137b07c container test-container: <nil>
STEP: delete the pod
Dec 12 03:50:27.230: INFO: Waiting for pod pod-1c441ec5-69f7-4381-bba3-2d873137b07c to disappear
Dec 12 03:50:27.247: INFO: Pod pod-1c441ec5-69f7-4381-bba3-2d873137b07c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:27.247: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1655" for this suite.
Dec 12 03:50:33.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:35.666: INFO: namespace emptydir-1655 deletion completed in 8.375107867s


â€¢ [SLOW TEST:18.791 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:49:42.467: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-5047
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5047
STEP: Deleting pre-stop pod
Dec 12 03:50:05.800: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:05.825: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "prestop-5047" for this suite.
Dec 12 03:50:43.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:46.259: INFO: namespace prestop-5047 deletion completed in 40.390483528s


â€¢ [SLOW TEST:63.792 seconds]
[k8s.io] [sig-node] PreStop
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:24.346: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:32.599: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-2196" for this suite.
Dec 12 03:50:44.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:47.208: INFO: namespace containers-2196 deletion completed in 14.55721042s


â€¢ [SLOW TEST:22.862 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:49:48.581: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:49:48.693: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:49:56.920: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8590" for this suite.
Dec 12 03:50:45.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:47.557: INFO: namespace pods-8590 deletion completed in 50.591794861s


â€¢ [SLOW TEST:58.976 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:35.876: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:50:36.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37" in namespace "downward-api-6469" to be "success or failure"
Dec 12 03:50:36.127: INFO: Pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37": Phase="Pending", Reason="", readiness=false. Elapsed: 16.9833ms
Dec 12 03:50:38.144: INFO: Pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03402298s
Dec 12 03:50:40.161: INFO: Pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050827418s
Dec 12 03:50:42.179: INFO: Pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069025144s
Dec 12 03:50:44.195: INFO: Pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085198626s
Dec 12 03:50:46.221: INFO: Pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.110982595s
STEP: Saw pod success
Dec 12 03:50:46.221: INFO: Pod "downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37" satisfied condition "success or failure"
Dec 12 03:50:46.237: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37 container client-container: <nil>
STEP: delete the pod
Dec 12 03:50:46.281: INFO: Waiting for pod downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37 to disappear
Dec 12 03:50:46.298: INFO: Pod downwardapi-volume-274a01c7-4c66-4ae0-ad4d-9f75d74f8c37 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:46.298: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6469" for this suite.
Dec 12 03:50:52.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:50:54.876: INFO: namespace downward-api-6469 deletion completed in 8.53482968s


â€¢ [SLOW TEST:19.000 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:47.357: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:50:47.535: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f" in namespace "security-context-test-1064" to be "success or failure"
Dec 12 03:50:47.553: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.909775ms
Dec 12 03:50:49.571: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035965617s
Dec 12 03:50:51.593: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058248634s
Dec 12 03:50:53.613: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077909854s
Dec 12 03:50:55.636: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100949566s
Dec 12 03:50:57.653: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.117940016s
Dec 12 03:50:59.676: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.140925953s
Dec 12 03:50:59.676: INFO: Pod "alpine-nnp-false-eab4b21f-5e89-448e-be2f-1f02bdf0879f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:59.698: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-1064" for this suite.
Dec 12 03:51:05.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:08.371: INFO: namespace security-context-test-1064 deletion completed in 8.624390294s


â€¢ [SLOW TEST:21.013 seconds]
[k8s.io] Security Context
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:54.927: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7987.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7987.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7987.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7987.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7987.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7987.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:51:05.328: INFO: DNS probes using dns-7987/dns-test-96cc235d-f7d5-4ea6-a401-372de0e77df3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:05.387: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-7987" for this suite.
Dec 12 03:51:11.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:13.956: INFO: namespace dns-7987 deletion completed in 8.525901697s


â€¢ [SLOW TEST:19.029 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:08.427: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 12 03:51:17.830: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:17.881: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-3363" for this suite.
Dec 12 03:51:23.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:26.295: INFO: namespace container-runtime-3363 deletion completed in 8.369304766s


â€¢ [SLOW TEST:17.869 seconds]
[k8s.io] Container Runtime
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:14.107: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:19.227: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-8084" for this suite.
Dec 12 03:51:25.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:27.771: INFO: namespace watch-8084 deletion completed in 8.497869175s


â€¢ [SLOW TEST:13.664 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:26.377: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 12 03:51:26.543: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:27.671: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-7080" for this suite.
Dec 12 03:51:33.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:36.099: INFO: namespace replication-controller-7080 deletion completed in 8.374588294s


â€¢ [SLOW TEST:9.722 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:46.275: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 12 03:50:46.479: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:33.104: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6656" for this suite.
Dec 12 03:51:39.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:41.503: INFO: namespace crd-publish-openapi-6656 deletion completed in 8.351837979s


â€¢ [SLOW TEST:55.229 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:50:47.608: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:50:57.849: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-7060" for this suite.
Dec 12 03:51:43.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:46.419: INFO: namespace kubelet-test-7060 deletion completed in 48.524581424s


â€¢ [SLOW TEST:58.811 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:27.783: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 12 03:51:38.571: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-2087 pod-service-account-6a20635c-a52f-44fb-9e5e-4103186ff1fa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 12 03:51:39.118: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-2087 pod-service-account-6a20635c-a52f-44fb-9e5e-4103186ff1fa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 12 03:51:39.605: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-2087 pod-service-account-6a20635c-a52f-44fb-9e5e-4103186ff1fa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:40.098: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-2087" for this suite.
Dec 12 03:51:46.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:48.481: INFO: namespace svcaccounts-2087 deletion completed in 8.340014281s


â€¢ [SLOW TEST:20.698 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:36.130: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-515f7b61-98eb-4077-95bb-c9aee0286bd3
STEP: Creating a pod to test consume configMaps
Dec 12 03:51:36.311: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c" in namespace "projected-7620" to be "success or failure"
Dec 12 03:51:36.328: INFO: Pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.043586ms
Dec 12 03:51:38.346: INFO: Pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03438283s
Dec 12 03:51:40.364: INFO: Pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052266032s
Dec 12 03:51:42.384: INFO: Pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072291339s
Dec 12 03:51:44.405: INFO: Pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093239784s
Dec 12 03:51:46.425: INFO: Pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.113278278s
STEP: Saw pod success
Dec 12 03:51:46.425: INFO: Pod "pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c" satisfied condition "success or failure"
Dec 12 03:51:46.446: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:51:46.496: INFO: Waiting for pod pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c to disappear
Dec 12 03:51:46.513: INFO: Pod pod-projected-configmaps-a7e90623-7a30-4479-8cbf-02484eb4b84c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:46.513: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7620" for this suite.
Dec 12 03:51:54.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:51:57.063: INFO: namespace projected-7620 deletion completed in 10.501389438s


â€¢ [SLOW TEST:20.932 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:41.571: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:54.986: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-1265" for this suite.
Dec 12 03:52:01.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:03.589: INFO: namespace resourcequota-1265 deletion completed in 8.558138381s


â€¢ [SLOW TEST:22.017 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:46.593: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:51:57.914: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-6966" for this suite.
Dec 12 03:52:04.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:06.459: INFO: namespace resourcequota-6966 deletion completed in 8.501053042s


â€¢ [SLOW TEST:19.866 seconds]
[sig-api-machinery] ResourceQuota
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:03.614: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:03.765: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-5901" for this suite.
Dec 12 03:52:09.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:12.373: INFO: namespace services-5901 deletion completed in 8.590879917s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95


â€¢ [SLOW TEST:8.759 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:57.072: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 12 03:51:57.613: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9736 /api/v1/namespaces/watch-9736/configmaps/e2e-watch-test-label-changed 587e6631-447e-43e6-8f78-171b50011efb 52647 0 2019-12-12 03:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 03:51:57.614: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9736 /api/v1/namespaces/watch-9736/configmaps/e2e-watch-test-label-changed 587e6631-447e-43e6-8f78-171b50011efb 52650 0 2019-12-12 03:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 12 03:51:57.618: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9736 /api/v1/namespaces/watch-9736/configmaps/e2e-watch-test-label-changed 587e6631-447e-43e6-8f78-171b50011efb 52651 0 2019-12-12 03:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 12 03:52:07.765: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9736 /api/v1/namespaces/watch-9736/configmaps/e2e-watch-test-label-changed 587e6631-447e-43e6-8f78-171b50011efb 52829 0 2019-12-12 03:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 03:52:07.765: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9736 /api/v1/namespaces/watch-9736/configmaps/e2e-watch-test-label-changed 587e6631-447e-43e6-8f78-171b50011efb 52830 0 2019-12-12 03:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 12 03:52:07.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9736 /api/v1/namespaces/watch-9736/configmaps/e2e-watch-test-label-changed 587e6631-447e-43e6-8f78-171b50011efb 52831 0 2019-12-12 03:51:57 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:07.766: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-9736" for this suite.
Dec 12 03:52:13.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:16.375: INFO: namespace watch-9736 deletion completed in 8.562270884s


â€¢ [SLOW TEST:19.303 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:06.493: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 12 03:52:06.662: INFO: Waiting up to 5m0s for pod "pod-97a3a821-ae80-4327-954f-fec786fc6676" in namespace "emptydir-934" to be "success or failure"
Dec 12 03:52:06.679: INFO: Pod "pod-97a3a821-ae80-4327-954f-fec786fc6676": Phase="Pending", Reason="", readiness=false. Elapsed: 17.135646ms
Dec 12 03:52:08.698: INFO: Pod "pod-97a3a821-ae80-4327-954f-fec786fc6676": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035886915s
Dec 12 03:52:10.716: INFO: Pod "pod-97a3a821-ae80-4327-954f-fec786fc6676": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053541479s
Dec 12 03:52:12.748: INFO: Pod "pod-97a3a821-ae80-4327-954f-fec786fc6676": Phase="Pending", Reason="", readiness=false. Elapsed: 6.085483951s
Dec 12 03:52:14.767: INFO: Pod "pod-97a3a821-ae80-4327-954f-fec786fc6676": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10467777s
Dec 12 03:52:16.787: INFO: Pod "pod-97a3a821-ae80-4327-954f-fec786fc6676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.124483544s
STEP: Saw pod success
Dec 12 03:52:16.787: INFO: Pod "pod-97a3a821-ae80-4327-954f-fec786fc6676" satisfied condition "success or failure"
Dec 12 03:52:16.815: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-97a3a821-ae80-4327-954f-fec786fc6676 container test-container: <nil>
STEP: delete the pod
Dec 12 03:52:16.894: INFO: Waiting for pod pod-97a3a821-ae80-4327-954f-fec786fc6676 to disappear
Dec 12 03:52:16.920: INFO: Pod pod-97a3a821-ae80-4327-954f-fec786fc6676 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:16.920: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-934" for this suite.
Dec 12 03:52:23.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:25.417: INFO: namespace emptydir-934 deletion completed in 8.438292269s


â€¢ [SLOW TEST:18.924 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:12.613: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 12 03:52:12.867: INFO: Waiting up to 5m0s for pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145" in namespace "downward-api-6032" to be "success or failure"
Dec 12 03:52:12.888: INFO: Pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145": Phase="Pending", Reason="", readiness=false. Elapsed: 21.549741ms
Dec 12 03:52:14.908: INFO: Pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041064872s
Dec 12 03:52:16.926: INFO: Pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059599199s
Dec 12 03:52:18.947: INFO: Pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079855459s
Dec 12 03:52:20.971: INFO: Pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145": Phase="Pending", Reason="", readiness=false. Elapsed: 8.103988798s
Dec 12 03:52:22.988: INFO: Pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.121263927s
STEP: Saw pod success
Dec 12 03:52:22.988: INFO: Pod "downward-api-025cf1ae-5948-46a3-b995-f06096019145" satisfied condition "success or failure"
Dec 12 03:52:23.006: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downward-api-025cf1ae-5948-46a3-b995-f06096019145 container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:52:23.063: INFO: Waiting for pod downward-api-025cf1ae-5948-46a3-b995-f06096019145 to disappear
Dec 12 03:52:23.078: INFO: Pod downward-api-025cf1ae-5948-46a3-b995-f06096019145 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:23.079: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6032" for this suite.
Dec 12 03:52:29.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:31.471: INFO: namespace downward-api-6032 deletion completed in 8.347333889s


â€¢ [SLOW TEST:18.858 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:51:48.486: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 12 03:51:48.608: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:51:55.538: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:26.696: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1861" for this suite.
Dec 12 03:52:32.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:35.179: INFO: namespace crd-publish-openapi-1861 deletion completed in 8.440417771s


â€¢ [SLOW TEST:46.694 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:25.432: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:52:25.735: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9a9bc75c-8f41-47da-9df2-e853357f76a0", Controller:(*bool)(0xc008072c52), BlockOwnerDeletion:(*bool)(0xc008072c53)}}
Dec 12 03:52:25.754: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"92021ba4-c6d5-461e-a446-31d3fd3fb1c2", Controller:(*bool)(0xc002734b16), BlockOwnerDeletion:(*bool)(0xc002734b17)}}
Dec 12 03:52:25.774: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"443b5d56-72e8-4e9f-9dfd-57fc0f069fec", Controller:(*bool)(0xc002cdd486), BlockOwnerDeletion:(*bool)(0xc002cdd487)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:30.813: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-9955" for this suite.
Dec 12 03:52:36.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:39.236: INFO: namespace gc-9955 deletion completed in 8.376096285s


â€¢ [SLOW TEST:13.804 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:16.605: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:52:18.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:52:20.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:52:22.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:52:24.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:52:26.523: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719538, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:52:29.553: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:29.614: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1320" for this suite.
Dec 12 03:52:37.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:40.124: INFO: namespace webhook-1320 deletion completed in 10.478462047s
STEP: Destroying namespace "webhook-1320-markers" for this suite.
Dec 12 03:52:46.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:48.548: INFO: namespace webhook-1320-markers deletion completed in 8.423930244s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:32.012 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:31.599: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-6c127818-ba35-45fd-93bf-33e1d2ac171f
STEP: Creating a pod to test consume configMaps
Dec 12 03:52:31.810: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8" in namespace "projected-3461" to be "success or failure"
Dec 12 03:52:31.826: INFO: Pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.580215ms
Dec 12 03:52:33.846: INFO: Pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036085958s
Dec 12 03:52:35.864: INFO: Pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054123037s
Dec 12 03:52:37.882: INFO: Pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072098127s
Dec 12 03:52:39.899: INFO: Pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089341551s
Dec 12 03:52:41.917: INFO: Pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107154129s
STEP: Saw pod success
Dec 12 03:52:41.917: INFO: Pod "pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8" satisfied condition "success or failure"
Dec 12 03:52:41.933: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:52:41.980: INFO: Waiting for pod pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8 to disappear
Dec 12 03:52:41.996: INFO: Pod pod-projected-configmaps-e0454af5-9226-4d9d-9289-0f7ea0016bc8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:41.996: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3461" for this suite.
Dec 12 03:52:48.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:50.387: INFO: namespace projected-3461 deletion completed in 8.343900546s


â€¢ [SLOW TEST:18.862 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:39.315: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:52:39.488: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba" in namespace "downward-api-5800" to be "success or failure"
Dec 12 03:52:39.506: INFO: Pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 17.843019ms
Dec 12 03:52:41.523: INFO: Pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034922101s
Dec 12 03:52:43.556: INFO: Pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067954275s
Dec 12 03:52:45.589: INFO: Pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100711818s
Dec 12 03:52:47.611: INFO: Pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122995335s
Dec 12 03:52:49.628: INFO: Pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.140592979s
STEP: Saw pod success
Dec 12 03:52:49.628: INFO: Pod "downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba" satisfied condition "success or failure"
Dec 12 03:52:49.646: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba container client-container: <nil>
STEP: delete the pod
Dec 12 03:52:49.716: INFO: Waiting for pod downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba to disappear
Dec 12 03:52:49.751: INFO: Pod downwardapi-volume-2f50d284-76ea-411e-90b2-e1b377d5a1ba no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:52:50.460: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5800" for this suite.
Dec 12 03:52:56.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:52:58.872: INFO: namespace downward-api-5800 deletion completed in 8.361263049s


â€¢ [SLOW TEST:19.556 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:50.533: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-853a83dd-df03-4dd1-90d8-05c6c697722a
STEP: Creating a pod to test consume configMaps
Dec 12 03:52:50.834: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5" in namespace "projected-9539" to be "success or failure"
Dec 12 03:52:50.851: INFO: Pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.402108ms
Dec 12 03:52:52.872: INFO: Pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037804461s
Dec 12 03:52:54.891: INFO: Pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056127321s
Dec 12 03:52:56.909: INFO: Pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074272365s
Dec 12 03:52:58.926: INFO: Pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091986576s
Dec 12 03:53:00.944: INFO: Pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.109335605s
STEP: Saw pod success
Dec 12 03:53:00.944: INFO: Pod "pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5" satisfied condition "success or failure"
Dec 12 03:53:00.960: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:53:01.005: INFO: Waiting for pod pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5 to disappear
Dec 12 03:53:01.022: INFO: Pod pod-projected-configmaps-52f09515-ff1a-4049-a553-c5e11a75ada5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:01.022: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9539" for this suite.
Dec 12 03:53:07.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:09.439: INFO: namespace projected-9539 deletion completed in 8.370644433s


â€¢ [SLOW TEST:18.906 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:48.632: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9
Dec 12 03:52:48.777: INFO: Pod name my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9: Found 0 pods out of 1
Dec 12 03:52:53.795: INFO: Pod name my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9: Found 1 pods out of 1
Dec 12 03:52:53.795: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9" are running
Dec 12 03:52:59.829: INFO: Pod "my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9-qhvv5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:52:48 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:52:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:52:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-12 03:52:48 +0000 UTC Reason: Message:}])
Dec 12 03:52:59.829: INFO: Trying to dial the pod
Dec 12 03:53:04.887: INFO: Controller my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9: Got expected result from replica 1 [my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9-qhvv5]: "my-hostname-basic-62354333-9910-4402-ab6c-bb95b268fce9-qhvv5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:04.887: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-8274" for this suite.
Dec 12 03:53:10.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:13.418: INFO: namespace replication-controller-8274 deletion completed in 8.485562482s


â€¢ [SLOW TEST:24.785 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:35.201: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 12 03:52:55.586: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 03:52:55.602: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 12 03:52:57.602: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 03:52:57.620: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 12 03:52:59.602: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 03:52:59.619: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 12 03:53:01.602: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 03:53:01.622: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 12 03:53:03.602: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 12 03:53:03.619: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:03.639: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6149" for this suite.
Dec 12 03:53:15.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:18.014: INFO: namespace container-lifecycle-hook-6149 deletion completed in 14.331853191s


â€¢ [SLOW TEST:42.813 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:09.597: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 12 03:53:09.772: INFO: Waiting up to 5m0s for pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a" in namespace "emptydir-8131" to be "success or failure"
Dec 12 03:53:09.787: INFO: Pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.5511ms
Dec 12 03:53:11.806: INFO: Pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033918693s
Dec 12 03:53:13.827: INFO: Pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054901884s
Dec 12 03:53:15.847: INFO: Pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07496843s
Dec 12 03:53:17.868: INFO: Pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096168027s
Dec 12 03:53:19.884: INFO: Pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.112352229s
STEP: Saw pod success
Dec 12 03:53:19.884: INFO: Pod "pod-9e454837-fb8e-4374-81da-9cbbadf1986a" satisfied condition "success or failure"
Dec 12 03:53:19.901: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-9e454837-fb8e-4374-81da-9cbbadf1986a container test-container: <nil>
STEP: delete the pod
Dec 12 03:53:19.945: INFO: Waiting for pod pod-9e454837-fb8e-4374-81da-9cbbadf1986a to disappear
Dec 12 03:53:19.962: INFO: Pod pod-9e454837-fb8e-4374-81da-9cbbadf1986a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:19.962: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8131" for this suite.
Dec 12 03:53:26.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:28.364: INFO: namespace emptydir-8131 deletion completed in 8.3580291s


â€¢ [SLOW TEST:18.767 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:13.437: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Dec 12 03:53:13.578: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-40" to be "success or failure"
Dec 12 03:53:13.597: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 18.655417ms
Dec 12 03:53:15.615: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036490977s
Dec 12 03:53:17.631: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052863972s
Dec 12 03:53:19.648: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069439519s
Dec 12 03:53:21.669: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090503736s
Dec 12 03:53:23.686: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107756124s
STEP: Saw pod success
Dec 12 03:53:23.686: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 12 03:53:23.703: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 12 03:53:23.772: INFO: Waiting for pod pod-host-path-test to disappear
Dec 12 03:53:23.788: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:23.788: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "hostpath-40" for this suite.
Dec 12 03:53:29.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:32.223: INFO: namespace hostpath-40 deletion completed in 8.389705217s


â€¢ [SLOW TEST:18.785 seconds]
[sig-storage] HostPath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:18.078: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-776a4687-4ce8-4012-989f-e2bba3178b80
STEP: Creating a pod to test consume secrets
Dec 12 03:53:18.399: INFO: Waiting up to 5m0s for pod "pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce" in namespace "secrets-731" to be "success or failure"
Dec 12 03:53:18.418: INFO: Pod "pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 19.584601ms
Dec 12 03:53:20.436: INFO: Pod "pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037319868s
Dec 12 03:53:22.455: INFO: Pod "pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055925879s
Dec 12 03:53:24.471: INFO: Pod "pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071967176s
Dec 12 03:53:26.488: INFO: Pod "pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.088970236s
STEP: Saw pod success
Dec 12 03:53:26.488: INFO: Pod "pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce" satisfied condition "success or failure"
Dec 12 03:53:26.504: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 03:53:26.549: INFO: Waiting for pod pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce to disappear
Dec 12 03:53:26.566: INFO: Pod pod-secrets-b6bb823d-6cf7-4fc3-9320-83ac21c5a7ce no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:26.566: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-731" for this suite.
Dec 12 03:53:32.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:35.016: INFO: namespace secrets-731 deletion completed in 8.404617135s
STEP: Destroying namespace "secret-namespace-3932" for this suite.
Dec 12 03:53:41.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:43.392: INFO: namespace secret-namespace-3932 deletion completed in 8.375933437s


â€¢ [SLOW TEST:25.314 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:32.541: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Dec 12 03:53:32.704: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=kubectl-498 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 12 03:53:42.575: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 12 03:53:42.575: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:44.612: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-498" for this suite.
Dec 12 03:53:54.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:57.035: INFO: namespace kubectl-498 deletion completed in 12.378544874s


â€¢ [SLOW TEST:24.494 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:52:58.970: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:53:27.210: INFO: Container started at 2019-12-12 03:53:06 +0000 UTC, pod became ready at 2019-12-12 03:53:26 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:27.210: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7187" for this suite.
Dec 12 03:53:55.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:53:57.767: INFO: namespace container-probe-7187 deletion completed in 30.51065954s


â€¢ [SLOW TEST:58.797 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:43.467: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:53:43.633: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f" in namespace "projected-1033" to be "success or failure"
Dec 12 03:53:43.650: INFO: Pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.431868ms
Dec 12 03:53:45.670: INFO: Pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036934212s
Dec 12 03:53:47.688: INFO: Pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054876151s
Dec 12 03:53:49.705: INFO: Pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07196317s
Dec 12 03:53:51.724: INFO: Pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090899247s
Dec 12 03:53:53.740: INFO: Pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.10694694s
STEP: Saw pod success
Dec 12 03:53:53.740: INFO: Pod "downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f" satisfied condition "success or failure"
Dec 12 03:53:53.757: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f container client-container: <nil>
STEP: delete the pod
Dec 12 03:53:53.800: INFO: Waiting for pod downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f to disappear
Dec 12 03:53:53.815: INFO: Pod downwardapi-volume-fe2e11ae-b7ae-46bf-a219-9bfebfb24f7f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:53:53.815: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1033" for this suite.
Dec 12 03:53:59.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:54:02.279: INFO: namespace projected-1033 deletion completed in 8.420278139s


â€¢ [SLOW TEST:18.811 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:28.440: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7490
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 12 03:53:28.559: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Dec 12 03:54:04.958: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7490 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:04.958: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:06.153: INFO: Found all expected endpoints: [netserver-0]
Dec 12 03:54:06.170: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7490 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:06.170: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:07.369: INFO: Found all expected endpoints: [netserver-1]
Dec 12 03:54:07.386: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7490 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:07.386: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:08.580: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:08.580: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-7490" for this suite.
Dec 12 03:54:14.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:54:17.036: INFO: namespace pod-network-test-7490 deletion completed in 8.411436386s


â€¢ [SLOW TEST:48.596 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:54:02.364: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-e15c3fc9-f0e9-4179-8d7d-9283cb495bbb
STEP: Creating a pod to test consume configMaps
Dec 12 03:54:02.556: INFO: Waiting up to 5m0s for pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91" in namespace "configmap-6277" to be "success or failure"
Dec 12 03:54:02.573: INFO: Pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91": Phase="Pending", Reason="", readiness=false. Elapsed: 17.349931ms
Dec 12 03:54:04.590: INFO: Pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033677413s
Dec 12 03:54:06.609: INFO: Pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05282673s
Dec 12 03:54:08.625: INFO: Pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069599837s
Dec 12 03:54:10.642: INFO: Pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085884168s
Dec 12 03:54:12.660: INFO: Pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.103973987s
STEP: Saw pod success
Dec 12 03:54:12.660: INFO: Pod "pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91" satisfied condition "success or failure"
Dec 12 03:54:12.676: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:54:12.719: INFO: Waiting for pod pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91 to disappear
Dec 12 03:54:12.736: INFO: Pod pod-configmaps-9afd7a14-e9fa-4827-b56d-6ff131fb6c91 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:12.736: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6277" for this suite.
Dec 12 03:54:18.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:54:21.152: INFO: namespace configmap-6277 deletion completed in 8.372979056s


â€¢ [SLOW TEST:18.788 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:54:17.083: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Dec 12 03:54:17.271: INFO: Waiting up to 5m0s for pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92" in namespace "containers-2239" to be "success or failure"
Dec 12 03:54:17.291: INFO: Pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92": Phase="Pending", Reason="", readiness=false. Elapsed: 20.045734ms
Dec 12 03:54:19.307: INFO: Pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036447045s
Dec 12 03:54:21.324: INFO: Pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053015079s
Dec 12 03:54:23.340: INFO: Pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069517604s
Dec 12 03:54:25.362: INFO: Pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091111889s
Dec 12 03:54:27.380: INFO: Pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108991985s
STEP: Saw pod success
Dec 12 03:54:27.380: INFO: Pod "client-containers-7d636851-9818-495c-9f63-7b26365a7c92" satisfied condition "success or failure"
Dec 12 03:54:27.398: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod client-containers-7d636851-9818-495c-9f63-7b26365a7c92 container test-container: <nil>
STEP: delete the pod
Dec 12 03:54:27.463: INFO: Waiting for pod client-containers-7d636851-9818-495c-9f63-7b26365a7c92 to disappear
Dec 12 03:54:27.481: INFO: Pod client-containers-7d636851-9818-495c-9f63-7b26365a7c92 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:27.481: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-2239" for this suite.
Dec 12 03:54:33.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:54:35.931: INFO: namespace containers-2239 deletion completed in 8.405485243s


â€¢ [SLOW TEST:18.849 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:54:21.240: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:54:21.384: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-82653b02-7308-4e5f-9e8e-255de2882fca
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:31.522: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2298" for this suite.
Dec 12 03:54:43.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:54:45.930: INFO: namespace configmap-2298 deletion completed in 14.363778108s


â€¢ [SLOW TEST:24.690 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:54:35.950: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:54:36.100: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f" in namespace "security-context-test-9271" to be "success or failure"
Dec 12 03:54:36.118: INFO: Pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.663788ms
Dec 12 03:54:38.136: INFO: Pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0356853s
Dec 12 03:54:40.153: INFO: Pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053113246s
Dec 12 03:54:42.171: INFO: Pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070724639s
Dec 12 03:54:44.192: INFO: Pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.09149565s
Dec 12 03:54:44.192: INFO: Pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f" satisfied condition "success or failure"
Dec 12 03:54:44.211: INFO: Got logs for pod "busybox-privileged-false-fefbd050-4e9f-4fa6-9255-db84c57d923f": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:44.211: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-9271" for this suite.
Dec 12 03:54:50.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:54:52.673: INFO: namespace security-context-test-9271 deletion completed in 8.41542697s


â€¢ [SLOW TEST:16.723 seconds]
[k8s.io] Security Context
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:54:46.177: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:54:46.351: INFO: (0) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 25.594389ms)
Dec 12 03:54:46.368: INFO: (1) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 16.937928ms)
Dec 12 03:54:46.387: INFO: (2) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.416065ms)
Dec 12 03:54:46.404: INFO: (3) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.08472ms)
Dec 12 03:54:46.422: INFO: (4) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.318226ms)
Dec 12 03:54:46.439: INFO: (5) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 16.973763ms)
Dec 12 03:54:46.456: INFO: (6) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.194981ms)
Dec 12 03:54:46.474: INFO: (7) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.700112ms)
Dec 12 03:54:46.490: INFO: (8) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 15.834103ms)
Dec 12 03:54:46.506: INFO: (9) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 16.80897ms)
Dec 12 03:54:46.524: INFO: (10) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.210416ms)
Dec 12 03:54:46.541: INFO: (11) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 16.970268ms)
Dec 12 03:54:46.559: INFO: (12) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.954771ms)
Dec 12 03:54:46.577: INFO: (13) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.928682ms)
Dec 12 03:54:46.595: INFO: (14) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.988162ms)
Dec 12 03:54:46.613: INFO: (15) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.914522ms)
Dec 12 03:54:46.629: INFO: (16) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 15.92763ms)
Dec 12 03:54:46.647: INFO: (17) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.136589ms)
Dec 12 03:54:46.669: INFO: (18) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 21.754765ms)
Dec 12 03:54:46.688: INFO: (19) /api/v1/nodes/ip-10-0-133-17.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.395404ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:46.688: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-9740" for this suite.
Dec 12 03:54:52.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:54:53.951: INFO: namespace proxy-9740 deletion completed in 7.245992694s


â€¢ [SLOW TEST:7.775 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:57.775: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 12 03:54:10.240: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:10.240: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:10.426: INFO: Exec stderr: ""
Dec 12 03:54:10.426: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:10.426: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:10.630: INFO: Exec stderr: ""
Dec 12 03:54:10.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:10.630: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:10.819: INFO: Exec stderr: ""
Dec 12 03:54:10.819: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:10.819: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:11.012: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 12 03:54:11.012: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:11.012: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:11.194: INFO: Exec stderr: ""
Dec 12 03:54:11.194: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:11.194: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:11.378: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 12 03:54:11.378: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:11.378: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:11.583: INFO: Exec stderr: ""
Dec 12 03:54:11.583: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:11.583: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:11.810: INFO: Exec stderr: ""
Dec 12 03:54:11.810: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:11.810: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:11.997: INFO: Exec stderr: ""
Dec 12 03:54:11.997: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-425 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 12 03:54:11.997: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 03:54:12.260: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:12.261: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-425" for this suite.
Dec 12 03:54:58.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:55:00.698: INFO: namespace e2e-kubelet-etc-hosts-425 deletion completed in 48.393405678s


â€¢ [SLOW TEST:62.924 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:53:57.045: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:53:57.223: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Creating first CR 
Dec 12 03:53:57.459: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T03:53:57Z generation:1 name:name1 resourceVersion:54689 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f7c2032b-c415-4d9f-8d6d-d24b81395fcc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 12 03:54:07.478: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T03:54:07Z generation:1 name:name2 resourceVersion:54855 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ff5fb3b5-7ee3-4dac-a29e-07b3f64baa3e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 12 03:54:17.497: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T03:53:57Z generation:2 name:name1 resourceVersion:55028 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f7c2032b-c415-4d9f-8d6d-d24b81395fcc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 12 03:54:27.517: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T03:54:07Z generation:2 name:name2 resourceVersion:55175 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ff5fb3b5-7ee3-4dac-a29e-07b3f64baa3e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 12 03:54:37.541: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T03:53:57Z generation:2 name:name1 resourceVersion:55329 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f7c2032b-c415-4d9f-8d6d-d24b81395fcc] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 12 03:54:47.565: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-12T03:54:07Z generation:2 name:name2 resourceVersion:55426 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ff5fb3b5-7ee3-4dac-a29e-07b3f64baa3e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:54:58.116: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-watch-7332" for this suite.
Dec 12 03:55:04.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:55:06.554: INFO: namespace crd-watch-7332 deletion completed in 8.388016849s


â€¢ [SLOW TEST:69.509 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:55:00.730: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:55:00.917: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2a575a71-7618-471d-80e6-7177341e461e" in namespace "security-context-test-2455" to be "success or failure"
Dec 12 03:55:00.941: INFO: Pod "busybox-user-65534-2a575a71-7618-471d-80e6-7177341e461e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.612294ms
Dec 12 03:55:02.957: INFO: Pod "busybox-user-65534-2a575a71-7618-471d-80e6-7177341e461e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039924086s
Dec 12 03:55:04.973: INFO: Pod "busybox-user-65534-2a575a71-7618-471d-80e6-7177341e461e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056096005s
Dec 12 03:55:06.993: INFO: Pod "busybox-user-65534-2a575a71-7618-471d-80e6-7177341e461e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075890161s
Dec 12 03:55:09.011: INFO: Pod "busybox-user-65534-2a575a71-7618-471d-80e6-7177341e461e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.093836339s
Dec 12 03:55:09.011: INFO: Pod "busybox-user-65534-2a575a71-7618-471d-80e6-7177341e461e" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:55:09.011: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-2455" for this suite.
Dec 12 03:55:15.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:55:17.413: INFO: namespace security-context-test-2455 deletion completed in 8.355585981s


â€¢ [SLOW TEST:16.684 seconds]
[k8s.io] Security Context
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:55:06.606: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:55:06.821: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98" in namespace "security-context-test-6800" to be "success or failure"
Dec 12 03:55:06.840: INFO: Pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98": Phase="Pending", Reason="", readiness=false. Elapsed: 18.371795ms
Dec 12 03:55:08.856: INFO: Pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034714523s
Dec 12 03:55:10.872: INFO: Pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05113363s
Dec 12 03:55:12.889: INFO: Pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067812105s
Dec 12 03:55:14.911: INFO: Pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089709655s
Dec 12 03:55:16.930: INFO: Pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.108338953s
Dec 12 03:55:16.930: INFO: Pod "busybox-readonly-false-1fb16a6a-520b-406e-9d3b-59856584ab98" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:55:16.930: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-6800" for this suite.
Dec 12 03:55:23.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:55:25.307: INFO: namespace security-context-test-6800 deletion completed in 8.333063672s


â€¢ [SLOW TEST:18.701 seconds]
[k8s.io] Security Context
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:54:53.962: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 12 03:55:04.764: INFO: Successfully updated pod "annotationupdate4f57ffd1-3bc5-47b6-a899-97362084bac1"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:55:06.808: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2205" for this suite.
Dec 12 03:55:24.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:55:27.170: INFO: namespace downward-api-2205 deletion completed in 20.317679541s


â€¢ [SLOW TEST:33.208 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:55:27.285: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 12 03:55:27.420: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8427'
Dec 12 03:55:27.629: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 12 03:55:27.629: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec 12 03:55:27.674: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-f5pg8]
Dec 12 03:55:27.674: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-f5pg8" in namespace "kubectl-8427" to be "running and ready"
Dec 12 03:55:27.690: INFO: Pod "e2e-test-httpd-rc-f5pg8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.521718ms
Dec 12 03:55:29.706: INFO: Pod "e2e-test-httpd-rc-f5pg8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03152471s
Dec 12 03:55:31.723: INFO: Pod "e2e-test-httpd-rc-f5pg8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048418475s
Dec 12 03:55:33.742: INFO: Pod "e2e-test-httpd-rc-f5pg8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067298988s
Dec 12 03:55:35.758: INFO: Pod "e2e-test-httpd-rc-f5pg8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083530916s
Dec 12 03:55:37.776: INFO: Pod "e2e-test-httpd-rc-f5pg8": Phase="Running", Reason="", readiness=true. Elapsed: 10.101708982s
Dec 12 03:55:37.776: INFO: Pod "e2e-test-httpd-rc-f5pg8" satisfied condition "running and ready"
Dec 12 03:55:37.776: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-f5pg8]
Dec 12 03:55:37.776: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs rc/e2e-test-httpd-rc --namespace=kubectl-8427'
Dec 12 03:55:37.990: INFO: stderr: ""
Dec 12 03:55:37.990: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.128.3.8. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.128.3.8. Set the 'ServerName' directive globally to suppress this message\n[Thu Dec 12 03:55:35.474073 2019] [mpm_event:notice] [pid 1:tid 140283018103656] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Thu Dec 12 03:55:35.474141 2019] [core:notice] [pid 1:tid 140283018103656] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Dec 12 03:55:37.990: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-httpd-rc --namespace=kubectl-8427'
Dec 12 03:55:38.185: INFO: stderr: ""
Dec 12 03:55:38.186: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:55:38.186: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8427" for this suite.
Dec 12 03:55:44.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:55:46.536: INFO: namespace kubectl-8427 deletion completed in 8.308904006s


â€¢ [SLOW TEST:19.251 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:54:52.708: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 12 03:55:02.930: INFO: &Pod{ObjectMeta:{send-events-cfe2f171-40c5-4f3a-a355-a82b79e58681  events-4570 /api/v1/namespaces/events-4570/pods/send-events-cfe2f171-40c5-4f3a-a355-a82b79e58681 a5e55f41-fee7-41fb-8f2a-fdacbf6e0718 55674 0 2019-12-12 03:54:52 +0000 UTC <nil> <nil> map[name:foo time:834591740] map[k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.2"
    ],
    "dns": {},
    "default-route": [
        "10.128.2.1"
    ]
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6z65l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6z65l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6z65l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:54:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:54:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:10.128.3.2,StartTime:2019-12-12 03:54:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-12 03:55:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://8957280af9aaeb76f31adff293f823dc3b9de88064811245e881988b5b2c1819,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 12 03:55:04.948: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 12 03:55:06.966: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:55:06.989: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-4570" for this suite.
Dec 12 03:55:53.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:55:55.380: INFO: namespace events-4570 deletion completed in 48.344363222s


â€¢ [SLOW TEST:62.672 seconds]
[k8s.io] [sig-node] Events
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:55:46.554: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:55:46.715: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2" in namespace "projected-4952" to be "success or failure"
Dec 12 03:55:46.731: INFO: Pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.959797ms
Dec 12 03:55:48.748: INFO: Pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032760054s
Dec 12 03:55:50.764: INFO: Pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049276753s
Dec 12 03:55:52.781: INFO: Pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066315615s
Dec 12 03:55:54.798: INFO: Pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082934015s
Dec 12 03:55:56.814: INFO: Pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099474073s
STEP: Saw pod success
Dec 12 03:55:56.814: INFO: Pod "downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2" satisfied condition "success or failure"
Dec 12 03:55:56.830: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2 container client-container: <nil>
STEP: delete the pod
Dec 12 03:55:56.875: INFO: Waiting for pod downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2 to disappear
Dec 12 03:55:56.891: INFO: Pod downwardapi-volume-9d1cceee-9001-4a3c-8c26-0353b47ef8d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:55:56.891: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4952" for this suite.
Dec 12 03:56:02.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:05.310: INFO: namespace projected-4952 deletion completed in 8.375582189s


â€¢ [SLOW TEST:18.756 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:55:25.359: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 12 03:55:36.131: INFO: Successfully updated pod "pod-update-597622cd-0959-45a6-9e61-f24616219f5e"
STEP: verifying the updated pod is in kubernetes
Dec 12 03:55:36.163: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:55:36.163: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8601" for this suite.
Dec 12 03:56:04.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:06.553: INFO: namespace pods-8601 deletion completed in 30.345917976s


â€¢ [SLOW TEST:41.194 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:55:55.402: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-685f447d-c24f-4005-bc08-585e45b9f641
STEP: Creating a pod to test consume configMaps
Dec 12 03:55:55.588: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29" in namespace "projected-3039" to be "success or failure"
Dec 12 03:55:55.606: INFO: Pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29": Phase="Pending", Reason="", readiness=false. Elapsed: 17.789524ms
Dec 12 03:55:57.623: INFO: Pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035554224s
Dec 12 03:55:59.641: INFO: Pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052805209s
Dec 12 03:56:01.657: INFO: Pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069534211s
Dec 12 03:56:03.676: INFO: Pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08782309s
Dec 12 03:56:05.693: INFO: Pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104816453s
STEP: Saw pod success
Dec 12 03:56:05.693: INFO: Pod "pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29" satisfied condition "success or failure"
Dec 12 03:56:05.709: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:56:05.756: INFO: Waiting for pod pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29 to disappear
Dec 12 03:56:05.771: INFO: Pod pod-projected-configmaps-95a860c5-7b8f-49f9-b606-f98b3a5fad29 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:05.771: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3039" for this suite.
Dec 12 03:56:11.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:14.173: INFO: namespace projected-3039 deletion completed in 8.356375544s


â€¢ [SLOW TEST:18.770 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:05.346: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:05.492: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-6467" for this suite.
Dec 12 03:56:17.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:19.817: INFO: namespace pods-6467 deletion completed in 14.305867812s


â€¢ [SLOW TEST:14.472 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:06.560: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 12 03:56:17.394: INFO: Successfully updated pod "labelsupdateb4b9d5f6-6bb3-4667-bef1-2ba0d27d905f"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:23.482: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3147" for this suite.
Dec 12 03:56:35.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:37.866: INFO: namespace projected-3147 deletion completed in 14.340353462s


â€¢ [SLOW TEST:31.306 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:14.197: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:56:15.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:56:17.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:56:19.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:56:21.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:56:23.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719775, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:56:26.464: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 12 03:56:26.525: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:26.568: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1846" for this suite.
Dec 12 03:56:32.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:35.005: INFO: namespace webhook-1846 deletion completed in 8.406132202s
STEP: Destroying namespace "webhook-1846-markers" for this suite.
Dec 12 03:56:41.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:43.323: INFO: namespace webhook-1846-markers deletion completed in 8.317331572s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:29.194 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:19.853: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 12 03:56:20.006: INFO: namespace kubectl-448
Dec 12 03:56:20.006: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-448'
Dec 12 03:56:20.624: INFO: stderr: ""
Dec 12 03:56:20.624: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 12 03:56:21.642: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:21.642: INFO: Found 0 / 1
Dec 12 03:56:22.645: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:22.645: INFO: Found 0 / 1
Dec 12 03:56:23.642: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:23.642: INFO: Found 0 / 1
Dec 12 03:56:24.641: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:24.641: INFO: Found 0 / 1
Dec 12 03:56:25.641: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:25.641: INFO: Found 0 / 1
Dec 12 03:56:26.645: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:26.645: INFO: Found 0 / 1
Dec 12 03:56:27.644: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:27.644: INFO: Found 0 / 1
Dec 12 03:56:28.641: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:28.641: INFO: Found 1 / 1
Dec 12 03:56:28.641: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 12 03:56:28.657: INFO: Selector matched 1 pods for map[app:redis]
Dec 12 03:56:28.657: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 12 03:56:28.657: INFO: wait on redis-master startup in kubectl-448 
Dec 12 03:56:28.657: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-dwhjg redis-master --namespace=kubectl-448'
Dec 12 03:56:28.883: INFO: stderr: ""
Dec 12 03:56:28.883: INFO: stdout: "1:C 12 Dec 2019 03:56:28.136 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 12 Dec 2019 03:56:28.136 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 12 Dec 2019 03:56:28.136 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 12 Dec 2019 03:56:28.138 * Running mode=standalone, port=6379.\n1:M 12 Dec 2019 03:56:28.138 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 12 Dec 2019 03:56:28.138 # Server initialized\n1:M 12 Dec 2019 03:56:28.138 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 12 Dec 2019 03:56:28.138 * Ready to accept connections\n"
STEP: exposing RC
Dec 12 03:56:28.883: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-448'
Dec 12 03:56:29.106: INFO: stderr: ""
Dec 12 03:56:29.106: INFO: stdout: "service/rm2 exposed\n"
Dec 12 03:56:29.123: INFO: Service rm2 in namespace kubectl-448 found.
STEP: exposing service
Dec 12 03:56:31.156: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-448'
Dec 12 03:56:31.383: INFO: stderr: ""
Dec 12 03:56:31.383: INFO: stdout: "service/rm3 exposed\n"
Dec 12 03:56:31.399: INFO: Service rm3 in namespace kubectl-448 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:33.430: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-448" for this suite.
Dec 12 03:56:45.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:47.770: INFO: namespace kubectl-448 deletion completed in 14.298435224s


â€¢ [SLOW TEST:27.918 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:55:17.416: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:17.595: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7821" for this suite.
Dec 12 03:56:45.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:47.996: INFO: namespace container-probe-7821 deletion completed in 30.351528579s


â€¢ [SLOW TEST:90.580 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:37.887: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 12 03:56:38.078: INFO: Waiting up to 5m0s for pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb" in namespace "emptydir-5049" to be "success or failure"
Dec 12 03:56:38.095: INFO: Pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.80428ms
Dec 12 03:56:40.111: INFO: Pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033189698s
Dec 12 03:56:42.128: INFO: Pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049974916s
Dec 12 03:56:44.146: INFO: Pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067908435s
Dec 12 03:56:46.163: INFO: Pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084898825s
Dec 12 03:56:48.182: INFO: Pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.104650806s
STEP: Saw pod success
Dec 12 03:56:48.183: INFO: Pod "pod-661857ab-ed08-4832-bc75-0335ee5040bb" satisfied condition "success or failure"
Dec 12 03:56:48.201: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-661857ab-ed08-4832-bc75-0335ee5040bb container test-container: <nil>
STEP: delete the pod
Dec 12 03:56:48.246: INFO: Waiting for pod pod-661857ab-ed08-4832-bc75-0335ee5040bb to disappear
Dec 12 03:56:48.262: INFO: Pod pod-661857ab-ed08-4832-bc75-0335ee5040bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:48.262: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5049" for this suite.
Dec 12 03:56:54.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:56:56.659: INFO: namespace emptydir-5049 deletion completed in 8.353162149s


â€¢ [SLOW TEST:18.771 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:56.668: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:56:56.848: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 12 03:56:57.974: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:57.990: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-2538" for this suite.
Dec 12 03:57:04.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:06.366: INFO: namespace replication-controller-2538 deletion completed in 8.330750382s


â€¢ [SLOW TEST:9.698 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:47.777: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 12 03:56:47.933: INFO: Waiting up to 5m0s for pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623" in namespace "emptydir-2922" to be "success or failure"
Dec 12 03:56:47.948: INFO: Pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623": Phase="Pending", Reason="", readiness=false. Elapsed: 14.954322ms
Dec 12 03:56:49.965: INFO: Pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031474041s
Dec 12 03:56:51.981: INFO: Pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047813082s
Dec 12 03:56:53.998: INFO: Pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064727518s
Dec 12 03:56:56.014: INFO: Pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080574541s
Dec 12 03:56:58.035: INFO: Pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101913467s
STEP: Saw pod success
Dec 12 03:56:58.035: INFO: Pod "pod-e5b9e4c1-ea84-4877-80ac-b416d2665623" satisfied condition "success or failure"
Dec 12 03:56:58.051: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-e5b9e4c1-ea84-4877-80ac-b416d2665623 container test-container: <nil>
STEP: delete the pod
Dec 12 03:56:58.093: INFO: Waiting for pod pod-e5b9e4c1-ea84-4877-80ac-b416d2665623 to disappear
Dec 12 03:56:58.108: INFO: Pod pod-e5b9e4c1-ea84-4877-80ac-b416d2665623 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:58.108: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2922" for this suite.
Dec 12 03:57:04.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:06.456: INFO: namespace emptydir-2922 deletion completed in 8.306661201s


â€¢ [SLOW TEST:18.678 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:43.419: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:56:44.918: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 03:56:46.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:56:48.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:56:50.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:56:52.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719804, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:56:56.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:56:56.153: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-6627" for this suite.
Dec 12 03:57:02.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:04.565: INFO: namespace webhook-6627 deletion completed in 8.381303914s
STEP: Destroying namespace "webhook-6627-markers" for this suite.
Dec 12 03:57:10.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:12.893: INFO: namespace webhook-6627-markers deletion completed in 8.327188098s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:29.541 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:06.382: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-599/configmap-test-d8df22dd-8ed0-4616-988c-c25ba408ae4f
STEP: Creating a pod to test consume configMaps
Dec 12 03:57:06.559: INFO: Waiting up to 5m0s for pod "pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959" in namespace "configmap-599" to be "success or failure"
Dec 12 03:57:06.575: INFO: Pod "pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959": Phase="Pending", Reason="", readiness=false. Elapsed: 15.948191ms
Dec 12 03:57:08.592: INFO: Pod "pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033650526s
Dec 12 03:57:10.608: INFO: Pod "pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049682934s
Dec 12 03:57:12.625: INFO: Pod "pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065880452s
Dec 12 03:57:14.641: INFO: Pod "pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.082508342s
STEP: Saw pod success
Dec 12 03:57:14.641: INFO: Pod "pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959" satisfied condition "success or failure"
Dec 12 03:57:14.658: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959 container env-test: <nil>
STEP: delete the pod
Dec 12 03:57:14.707: INFO: Waiting for pod pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959 to disappear
Dec 12 03:57:14.723: INFO: Pod pod-configmaps-889b7404-b79a-4a5b-8bfb-47817320f959 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:14.724: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-599" for this suite.
Dec 12 03:57:20.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:23.109: INFO: namespace configmap-599 deletion completed in 8.341389939s


â€¢ [SLOW TEST:16.727 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:06.542: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 12 03:57:06.725: INFO: Waiting up to 5m0s for pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7" in namespace "downward-api-3161" to be "success or failure"
Dec 12 03:57:06.743: INFO: Pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.697908ms
Dec 12 03:57:08.759: INFO: Pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034780565s
Dec 12 03:57:10.776: INFO: Pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051120333s
Dec 12 03:57:12.791: INFO: Pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066615177s
Dec 12 03:57:14.810: INFO: Pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085132704s
Dec 12 03:57:16.826: INFO: Pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101884063s
STEP: Saw pod success
Dec 12 03:57:16.826: INFO: Pod "downward-api-765da195-6c7f-416d-8674-66fadbe4aae7" satisfied condition "success or failure"
Dec 12 03:57:16.842: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downward-api-765da195-6c7f-416d-8674-66fadbe4aae7 container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:57:16.887: INFO: Waiting for pod downward-api-765da195-6c7f-416d-8674-66fadbe4aae7 to disappear
Dec 12 03:57:16.902: INFO: Pod downward-api-765da195-6c7f-416d-8674-66fadbe4aae7 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:16.903: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-3161" for this suite.
Dec 12 03:57:22.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:25.288: INFO: namespace downward-api-3161 deletion completed in 8.34349988s


â€¢ [SLOW TEST:18.746 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:56:48.067: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:56:58.327: INFO: Waiting up to 5m0s for pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb" in namespace "pods-1251" to be "success or failure"
Dec 12 03:56:58.342: INFO: Pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.509688ms
Dec 12 03:57:00.363: INFO: Pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035806058s
Dec 12 03:57:02.381: INFO: Pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054503288s
Dec 12 03:57:04.398: INFO: Pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071068801s
Dec 12 03:57:06.414: INFO: Pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08715552s
Dec 12 03:57:08.431: INFO: Pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.103991904s
STEP: Saw pod success
Dec 12 03:57:08.431: INFO: Pod "client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb" satisfied condition "success or failure"
Dec 12 03:57:08.447: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb container env3cont: <nil>
STEP: delete the pod
Dec 12 03:57:08.495: INFO: Waiting for pod client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb to disappear
Dec 12 03:57:08.511: INFO: Pod client-envvars-c9b92a7a-1cfe-4470-9d04-9793531eeeeb no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:08.511: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-1251" for this suite.
Dec 12 03:57:36.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:39.099: INFO: namespace pods-1251 deletion completed in 30.544914498s


â€¢ [SLOW TEST:51.032 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:23.123: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9082.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9082.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 12 03:57:33.493: INFO: DNS probes using dns-9082/dns-test-b39a36e5-3f35-463f-b50a-7b46285c40f8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:33.521: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-9082" for this suite.
Dec 12 03:57:41.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:44.081: INFO: namespace dns-9082 deletion completed in 10.529977337s


â€¢ [SLOW TEST:20.959 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:12.979: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:57:14.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:16.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:18.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:20.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:22.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719834, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:57:25.190: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:25.405: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-7736" for this suite.
Dec 12 03:57:31.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:33.756: INFO: namespace webhook-7736 deletion completed in 8.316589311s
STEP: Destroying namespace "webhook-7736-markers" for this suite.
Dec 12 03:57:41.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:44.123: INFO: namespace webhook-7736-markers deletion completed in 10.367152988s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:31.250 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:25.308: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 12 03:57:26.164: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:57:26.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:28.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:30.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:32.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:34.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719846, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:57:37.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:57:37.315: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6416-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:38.272: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2418" for this suite.
Dec 12 03:57:46.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:48.613: INFO: namespace webhook-2418 deletion completed in 10.29264369s
STEP: Destroying namespace "webhook-2418-markers" for this suite.
Dec 12 03:57:54.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:57:57.227: INFO: namespace webhook-2418-markers deletion completed in 8.613472093s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:32.036 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:39.142: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:51.419: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-6848" for this suite.
Dec 12 03:57:59.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:01.772: INFO: namespace kubelet-test-6848 deletion completed in 10.30909061s


â€¢ [SLOW TEST:22.631 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:44.232: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 12 03:57:44.919: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:57:45.018: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 03:57:47.068: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:49.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:51.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:53.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:57:56.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:56.792: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1922" for this suite.
Dec 12 03:58:04.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:07.164: INFO: namespace webhook-1922 deletion completed in 10.35231131s
STEP: Destroying namespace "webhook-1922-markers" for this suite.
Dec 12 03:58:13.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:15.459: INFO: namespace webhook-1922-markers deletion completed in 8.295108361s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:31.291 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:44.128: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:57:44.961: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 12 03:57:47.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:49.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:51.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:57:53.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719865, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719864, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:57:56.064: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:57:56.081: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4642-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:57:56.877: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-8972" for this suite.
Dec 12 03:58:05.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:07.294: INFO: namespace webhook-8972 deletion completed in 10.370051336s
STEP: Destroying namespace "webhook-8972-markers" for this suite.
Dec 12 03:58:13.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:15.708: INFO: namespace webhook-8972-markers deletion completed in 8.413544284s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:31.766 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:57:57.546: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 12 03:57:57.751: INFO: Waiting up to 5m0s for pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1" in namespace "downward-api-2376" to be "success or failure"
Dec 12 03:57:57.772: INFO: Pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.071317ms
Dec 12 03:57:59.790: INFO: Pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039728623s
Dec 12 03:58:01.808: INFO: Pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057272457s
Dec 12 03:58:03.826: INFO: Pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075463563s
Dec 12 03:58:05.842: INFO: Pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091506329s
Dec 12 03:58:07.859: INFO: Pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107914903s
STEP: Saw pod success
Dec 12 03:58:07.859: INFO: Pod "downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1" satisfied condition "success or failure"
Dec 12 03:58:07.874: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1 container dapi-container: <nil>
STEP: delete the pod
Dec 12 03:58:07.919: INFO: Waiting for pod downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1 to disappear
Dec 12 03:58:07.934: INFO: Pod downward-api-e18fd455-6dad-4161-aff2-8c5397d503f1 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:58:07.934: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2376" for this suite.
Dec 12 03:58:14.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:16.424: INFO: namespace downward-api-2376 deletion completed in 8.447806833s


â€¢ [SLOW TEST:18.879 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:01.826: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Dec 12 03:58:02.618: INFO: created pod pod-service-account-defaultsa
Dec 12 03:58:02.618: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 12 03:58:02.648: INFO: created pod pod-service-account-mountsa
Dec 12 03:58:02.648: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 12 03:58:02.705: INFO: created pod pod-service-account-nomountsa
Dec 12 03:58:02.705: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 12 03:58:02.749: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 12 03:58:02.749: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 12 03:58:02.832: INFO: created pod pod-service-account-mountsa-mountspec
Dec 12 03:58:02.832: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 12 03:58:02.867: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 12 03:58:02.867: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 12 03:58:02.898: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 12 03:58:02.898: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 12 03:58:02.934: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 12 03:58:02.934: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 12 03:58:02.966: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 12 03:58:02.966: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:58:02.966: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-3411" for this suite.
Dec 12 03:58:15.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:17.472: INFO: namespace svcaccounts-3411 deletion completed in 14.473529488s


â€¢ [SLOW TEST:15.646 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:15.935: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:58:16.236: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30" in namespace "downward-api-4582" to be "success or failure"
Dec 12 03:58:16.261: INFO: Pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30": Phase="Pending", Reason="", readiness=false. Elapsed: 24.943048ms
Dec 12 03:58:18.278: INFO: Pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041437715s
Dec 12 03:58:20.294: INFO: Pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057885192s
Dec 12 03:58:22.311: INFO: Pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074500169s
Dec 12 03:58:24.327: INFO: Pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090860284s
Dec 12 03:58:26.344: INFO: Pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.107446299s
STEP: Saw pod success
Dec 12 03:58:26.344: INFO: Pod "downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30" satisfied condition "success or failure"
Dec 12 03:58:26.360: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30 container client-container: <nil>
STEP: delete the pod
Dec 12 03:58:26.408: INFO: Waiting for pod downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30 to disappear
Dec 12 03:58:26.423: INFO: Pod downwardapi-volume-b7911ae0-067f-48d1-9b83-519b3612dc30 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:58:26.423: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4582" for this suite.
Dec 12 03:58:32.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:34.796: INFO: namespace downward-api-4582 deletion completed in 8.328393081s


â€¢ [SLOW TEST:18.861 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:34.826: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 12 03:58:35.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c" in namespace "projected-6494" to be "success or failure"
Dec 12 03:58:35.038: INFO: Pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.43393ms
Dec 12 03:58:37.056: INFO: Pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033722659s
Dec 12 03:58:39.072: INFO: Pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050568078s
Dec 12 03:58:41.089: INFO: Pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067030185s
Dec 12 03:58:43.107: INFO: Pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085157065s
Dec 12 03:58:45.123: INFO: Pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.101246874s
STEP: Saw pod success
Dec 12 03:58:45.123: INFO: Pod "downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c" satisfied condition "success or failure"
Dec 12 03:58:45.139: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c container client-container: <nil>
STEP: delete the pod
Dec 12 03:58:45.186: INFO: Waiting for pod downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c to disappear
Dec 12 03:58:45.202: INFO: Pod downwardapi-volume-a99ac211-c147-453e-bc24-07db78b4f78c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:58:45.202: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6494" for this suite.
Dec 12 03:58:51.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:53.599: INFO: namespace projected-6494 deletion completed in 8.353533578s


â€¢ [SLOW TEST:18.772 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:16.429: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:58:17.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-86d95b659d\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:58:19.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:58:21.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:58:23.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:58:25.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719897, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:58:28.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 12 03:58:38.716: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig attach --namespace=webhook-5035 to-be-attached-pod -i -c=container1'
Dec 12 03:58:38.991: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:58:39.012: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-5035" for this suite.
Dec 12 03:58:45.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:47.358: INFO: namespace webhook-5035 deletion completed in 8.30307167s
STEP: Destroying namespace "webhook-5035-markers" for this suite.
Dec 12 03:58:53.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:58:55.716: INFO: namespace webhook-5035-markers deletion completed in 8.358460924s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:39.356 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:15.537: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 12 03:58:25.771: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:58:25.826: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-5239" for this suite.
Dec 12 03:58:59.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:02.176: INFO: namespace replicaset-5239 deletion completed in 36.307250279s


â€¢ [SLOW TEST:46.639 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:55.792: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-9f0db8da-e0bb-494a-88b9-7e45ee5f715d
STEP: Creating a pod to test consume configMaps
Dec 12 03:58:55.973: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c" in namespace "projected-8725" to be "success or failure"
Dec 12 03:58:55.990: INFO: Pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.973302ms
Dec 12 03:58:58.007: INFO: Pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033297702s
Dec 12 03:59:00.023: INFO: Pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049231783s
Dec 12 03:59:02.039: INFO: Pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065469685s
Dec 12 03:59:04.056: INFO: Pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082323183s
Dec 12 03:59:06.072: INFO: Pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098569035s
STEP: Saw pod success
Dec 12 03:59:06.072: INFO: Pod "pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c" satisfied condition "success or failure"
Dec 12 03:59:06.087: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 12 03:59:06.421: INFO: Waiting for pod pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c to disappear
Dec 12 03:59:06.437: INFO: Pod pod-projected-configmaps-1c002ca1-50a8-43a0-b516-0d7c1581809c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:06.437: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8725" for this suite.
Dec 12 03:59:12.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:14.773: INFO: namespace projected-8725 deletion completed in 8.295017353s


â€¢ [SLOW TEST:18.981 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:14.778: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1212 03:59:25.058760    1994 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 03:59:25.058: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:25.058: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-3514" for this suite.
Dec 12 03:59:31.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:33.394: INFO: namespace gc-3514 deletion completed in 8.318661434s


â€¢ [SLOW TEST:18.615 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:02.215: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 12 03:59:18.447: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 12 03:59:33.658: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:33.675: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-641" for this suite.
Dec 12 03:59:41.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:44.011: INFO: namespace pods-641 deletion completed in 10.319512432s


â€¢ [SLOW TEST:41.796 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:17.522: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:58:17.645: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-e11af981-2515-49a3-9fd6-bbc6fb884769
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-e11af981-2515-49a3-9fd6-bbc6fb884769
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:33.074: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9501" for this suite.
Dec 12 03:59:45.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:47.434: INFO: namespace configmap-9501 deletion completed in 14.311768149s


â€¢ [SLOW TEST:89.913 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:33.397: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 12 03:59:33.526: INFO: Creating deployment "test-recreate-deployment"
Dec 12 03:59:33.546: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 12 03:59:33.582: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 12 03:59:35.617: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 12 03:59:35.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:37.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:39.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:41.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719973, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:43.648: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 12 03:59:43.682: INFO: Updating deployment test-recreate-deployment
Dec 12 03:59:43.682: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 12 03:59:43.797: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8298 /apis/apps/v1/namespaces/deployment-8298/deployments/test-recreate-deployment bd07b016-298b-43cc-8a7c-59887d21bbb7 60509 2 2019-12-12 03:59:33 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0000f3918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-12 03:59:43 +0000 UTC,LastTransitionTime:2019-12-12 03:59:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-12 03:59:43 +0000 UTC,LastTransitionTime:2019-12-12 03:59:33 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 12 03:59:43.816: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-8298 /apis/apps/v1/namespaces/deployment-8298/replicasets/test-recreate-deployment-5f94c574ff 1bf6d357-6d59-48ba-b044-d2d56552e8fb 60505 1 2019-12-12 03:59:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bd07b016-298b-43cc-8a7c-59887d21bbb7 0xc003745c57 0xc003745c58}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003745cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:59:43.816: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 12 03:59:43.816: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-8298 /apis/apps/v1/namespaces/deployment-8298/replicasets/test-recreate-deployment-68fc85c7bb 5bbf1fae-74c5-4b09-9f13-8bbc0f5d8e0e 60499 2 2019-12-12 03:59:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bd07b016-298b-43cc-8a7c-59887d21bbb7 0xc003745d27 0xc003745d28}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003745d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 12 03:59:43.831: INFO: Pod "test-recreate-deployment-5f94c574ff-kvjq9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-kvjq9 test-recreate-deployment-5f94c574ff- deployment-8298 /api/v1/namespaces/deployment-8298/pods/test-recreate-deployment-5f94c574ff-kvjq9 6d7cc600-6120-4c33-b54f-cda8bf7549a7 60510 0 2019-12-12 03:59:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 1bf6d357-6d59-48ba-b044-d2d56552e8fb 0xc00034aa77 0xc00034aa78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h8cdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h8cdk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h8cdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-133-2.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-86t6g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:59:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:59:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:59:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-12 03:59:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.133.2,PodIP:,StartTime:2019-12-12 03:59:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:43.831: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-8298" for this suite.
Dec 12 03:59:49.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:52.173: INFO: namespace deployment-8298 deletion completed in 8.297804411s


â€¢ [SLOW TEST:18.776 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:44.013: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1212 03:59:44.844763    1997 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 12 03:59:44.844: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:44.844: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-3436" for this suite.
Dec 12 03:59:50.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:53.181: INFO: namespace gc-3436 deletion completed in 8.320584161s


â€¢ [SLOW TEST:9.169 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:58:53.657: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Dec 12 03:58:53.800: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 12 03:58:53.800: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9553'
Dec 12 03:58:54.379: INFO: stderr: ""
Dec 12 03:58:54.379: INFO: stdout: "service/redis-slave created\n"
Dec 12 03:58:54.379: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 12 03:58:54.379: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9553'
Dec 12 03:58:54.975: INFO: stderr: ""
Dec 12 03:58:54.975: INFO: stdout: "service/redis-master created\n"
Dec 12 03:58:54.975: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 12 03:58:54.975: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9553'
Dec 12 03:58:55.617: INFO: stderr: ""
Dec 12 03:58:55.617: INFO: stdout: "service/frontend created\n"
Dec 12 03:58:55.618: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 12 03:58:55.618: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9553'
Dec 12 03:58:56.002: INFO: stderr: ""
Dec 12 03:58:56.002: INFO: stdout: "deployment.apps/frontend created\n"
Dec 12 03:58:56.003: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 12 03:58:56.003: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9553'
Dec 12 03:58:56.394: INFO: stderr: ""
Dec 12 03:58:56.394: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 12 03:58:56.395: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 12 03:58:56.395: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9553'
Dec 12 03:58:56.957: INFO: stderr: ""
Dec 12 03:58:56.957: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 12 03:58:56.957: INFO: Waiting for all frontend pods to be Running.
Dec 12 03:59:22.008: INFO: Waiting for frontend to serve content.
Dec 12 03:59:27.047: INFO: Trying to add a new entry to the guestbook.
Dec 12 03:59:32.083: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 12 03:59:32.110: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9553'
Dec 12 03:59:32.316: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:59:32.316: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 03:59:32.317: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9553'
Dec 12 03:59:32.534: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:59:32.534: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 03:59:32.535: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9553'
Dec 12 03:59:32.721: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:59:32.722: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 03:59:32.722: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9553'
Dec 12 03:59:32.890: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:59:32.890: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 03:59:32.890: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9553'
Dec 12 03:59:33.057: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:59:33.057: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 12 03:59:33.057: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9553'
Dec 12 03:59:33.242: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 12 03:59:33.242: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:33.242: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9553" for this suite.
Dec 12 03:59:51.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 03:59:53.630: INFO: namespace kubectl-9553 deletion completed in 20.343193033s


â€¢ [SLOW TEST:59.973 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:53.639: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 12 03:59:53.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4227 /api/v1/namespaces/watch-4227/configmaps/e2e-watch-test-watch-closed 832f7aa5-e250-4f70-abdd-ff85be685d6c 60868 0 2019-12-12 03:59:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 12 03:59:53.857: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4227 /api/v1/namespaces/watch-4227/configmaps/e2e-watch-test-watch-closed 832f7aa5-e250-4f70-abdd-ff85be685d6c 60870 0 2019-12-12 03:59:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 12 03:59:53.927: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4227 /api/v1/namespaces/watch-4227/configmaps/e2e-watch-test-watch-closed 832f7aa5-e250-4f70-abdd-ff85be685d6c 60872 0 2019-12-12 03:59:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 12 03:59:53.927: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4227 /api/v1/namespaces/watch-4227/configmaps/e2e-watch-test-watch-closed 832f7aa5-e250-4f70-abdd-ff85be685d6c 60874 0 2019-12-12 03:59:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 03:59:53.927: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-4227" for this suite.
Dec 12 04:00:00.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:00:02.336: INFO: namespace watch-4227 deletion completed in 8.391158575s


â€¢ [SLOW TEST:8.697 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:52.227: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 12 03:59:52.378: INFO: Waiting up to 5m0s for pod "pod-6e628fd7-977f-4252-b882-96e616de7589" in namespace "emptydir-4244" to be "success or failure"
Dec 12 03:59:52.395: INFO: Pod "pod-6e628fd7-977f-4252-b882-96e616de7589": Phase="Pending", Reason="", readiness=false. Elapsed: 17.244358ms
Dec 12 03:59:54.411: INFO: Pod "pod-6e628fd7-977f-4252-b882-96e616de7589": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033150239s
Dec 12 03:59:56.427: INFO: Pod "pod-6e628fd7-977f-4252-b882-96e616de7589": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04904987s
Dec 12 03:59:58.443: INFO: Pod "pod-6e628fd7-977f-4252-b882-96e616de7589": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065199606s
Dec 12 04:00:00.460: INFO: Pod "pod-6e628fd7-977f-4252-b882-96e616de7589": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082091653s
Dec 12 04:00:02.489: INFO: Pod "pod-6e628fd7-977f-4252-b882-96e616de7589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.111036513s
STEP: Saw pod success
Dec 12 04:00:02.489: INFO: Pod "pod-6e628fd7-977f-4252-b882-96e616de7589" satisfied condition "success or failure"
Dec 12 04:00:02.509: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-6e628fd7-977f-4252-b882-96e616de7589 container test-container: <nil>
STEP: delete the pod
Dec 12 04:00:02.592: INFO: Waiting for pod pod-6e628fd7-977f-4252-b882-96e616de7589 to disappear
Dec 12 04:00:02.610: INFO: Pod pod-6e628fd7-977f-4252-b882-96e616de7589 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:00:02.610: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4244" for this suite.
Dec 12 04:00:08.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:00:11.145: INFO: namespace emptydir-4244 deletion completed in 8.488936182s


â€¢ [SLOW TEST:18.918 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:53.192: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1847
I1212 03:59:53.336725    1997 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1847, replica count: 1
I1212 03:59:54.387264    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:59:55.387478    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:59:56.387687    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:59:57.387906    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:59:58.388323    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 03:59:59.388560    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 04:00:00.388862    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 04:00:01.389911    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1212 04:00:02.390961    1997 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 12 04:00:02.520: INFO: Created: latency-svc-75rss
Dec 12 04:00:02.536: INFO: Got endpoints: latency-svc-75rss [44.28341ms]
Dec 12 04:00:02.585: INFO: Created: latency-svc-rwqc8
Dec 12 04:00:02.593: INFO: Got endpoints: latency-svc-rwqc8 [56.773995ms]
Dec 12 04:00:02.595: INFO: Created: latency-svc-769xs
Dec 12 04:00:02.606: INFO: Got endpoints: latency-svc-769xs [69.216021ms]
Dec 12 04:00:02.612: INFO: Created: latency-svc-4pm6x
Dec 12 04:00:02.619: INFO: Got endpoints: latency-svc-4pm6x [82.239986ms]
Dec 12 04:00:02.628: INFO: Created: latency-svc-fsjcq
Dec 12 04:00:02.630: INFO: Got endpoints: latency-svc-fsjcq [93.975057ms]
Dec 12 04:00:02.644: INFO: Created: latency-svc-4g87x
Dec 12 04:00:02.650: INFO: Got endpoints: latency-svc-4g87x [112.958223ms]
Dec 12 04:00:02.653: INFO: Created: latency-svc-wslsb
Dec 12 04:00:02.660: INFO: Got endpoints: latency-svc-wslsb [40.902939ms]
Dec 12 04:00:02.668: INFO: Created: latency-svc-djhxl
Dec 12 04:00:02.672: INFO: Got endpoints: latency-svc-djhxl [134.82416ms]
Dec 12 04:00:02.676: INFO: Created: latency-svc-cvnrz
Dec 12 04:00:02.679: INFO: Got endpoints: latency-svc-cvnrz [142.380642ms]
Dec 12 04:00:02.682: INFO: Created: latency-svc-kjkll
Dec 12 04:00:02.699: INFO: Created: latency-svc-9wjjf
Dec 12 04:00:02.699: INFO: Got endpoints: latency-svc-kjkll [162.555978ms]
Dec 12 04:00:02.707: INFO: Got endpoints: latency-svc-9wjjf [169.735321ms]
Dec 12 04:00:02.710: INFO: Created: latency-svc-6rth5
Dec 12 04:00:02.716: INFO: Got endpoints: latency-svc-6rth5 [178.978546ms]
Dec 12 04:00:02.729: INFO: Created: latency-svc-ft8r5
Dec 12 04:00:02.740: INFO: Got endpoints: latency-svc-ft8r5 [202.85199ms]
Dec 12 04:00:02.740: INFO: Created: latency-svc-df4m9
Dec 12 04:00:02.742: INFO: Got endpoints: latency-svc-df4m9 [205.104823ms]
Dec 12 04:00:02.747: INFO: Created: latency-svc-2cjlb
Dec 12 04:00:02.752: INFO: Got endpoints: latency-svc-2cjlb [214.799378ms]
Dec 12 04:00:02.754: INFO: Created: latency-svc-wzvfl
Dec 12 04:00:02.765: INFO: Got endpoints: latency-svc-wzvfl [228.222962ms]
Dec 12 04:00:02.774: INFO: Created: latency-svc-88v2s
Dec 12 04:00:02.774: INFO: Got endpoints: latency-svc-88v2s [237.360649ms]
Dec 12 04:00:02.774: INFO: Created: latency-svc-d8qnr
Dec 12 04:00:02.781: INFO: Got endpoints: latency-svc-d8qnr [187.914535ms]
Dec 12 04:00:02.784: INFO: Created: latency-svc-nkbcp
Dec 12 04:00:02.793: INFO: Got endpoints: latency-svc-nkbcp [187.223268ms]
Dec 12 04:00:02.883: INFO: Created: latency-svc-6clbt
Dec 12 04:00:02.883: INFO: Got endpoints: latency-svc-6clbt [252.130694ms]
Dec 12 04:00:02.891: INFO: Created: latency-svc-txp4q
Dec 12 04:00:02.895: INFO: Created: latency-svc-68qzj
Dec 12 04:00:02.900: INFO: Got endpoints: latency-svc-txp4q [249.966798ms]
Dec 12 04:00:02.903: INFO: Got endpoints: latency-svc-68qzj [243.935216ms]
Dec 12 04:00:02.917: INFO: Created: latency-svc-pqkmd
Dec 12 04:00:02.917: INFO: Got endpoints: latency-svc-pqkmd [245.406198ms]
Dec 12 04:00:02.930: INFO: Created: latency-svc-vqxrz
Dec 12 04:00:02.935: INFO: Created: latency-svc-cv2p2
Dec 12 04:00:02.936: INFO: Got endpoints: latency-svc-vqxrz [257.397869ms]
Dec 12 04:00:02.944: INFO: Got endpoints: latency-svc-cv2p2 [244.344314ms]
Dec 12 04:00:02.947: INFO: Created: latency-svc-bmtbf
Dec 12 04:00:02.951: INFO: Got endpoints: latency-svc-bmtbf [244.601546ms]
Dec 12 04:00:02.960: INFO: Created: latency-svc-cr24z
Dec 12 04:00:02.963: INFO: Got endpoints: latency-svc-cr24z [246.925121ms]
Dec 12 04:00:02.967: INFO: Created: latency-svc-smsx6
Dec 12 04:00:02.974: INFO: Got endpoints: latency-svc-smsx6 [233.890283ms]
Dec 12 04:00:02.983: INFO: Created: latency-svc-27bwh
Dec 12 04:00:02.985: INFO: Got endpoints: latency-svc-27bwh [242.968251ms]
Dec 12 04:00:02.990: INFO: Created: latency-svc-xlqm7
Dec 12 04:00:02.995: INFO: Created: latency-svc-4qxrr
Dec 12 04:00:03.071: INFO: Got endpoints: latency-svc-4qxrr [305.393388ms]
Dec 12 04:00:03.076: INFO: Got endpoints: latency-svc-xlqm7 [324.287563ms]
Dec 12 04:00:03.087: INFO: Created: latency-svc-n2n8r
Dec 12 04:00:03.092: INFO: Got endpoints: latency-svc-n2n8r [317.222129ms]
Dec 12 04:00:03.096: INFO: Created: latency-svc-k5p24
Dec 12 04:00:03.103: INFO: Got endpoints: latency-svc-k5p24 [321.862012ms]
Dec 12 04:00:03.105: INFO: Created: latency-svc-hx2nz
Dec 12 04:00:03.114: INFO: Got endpoints: latency-svc-hx2nz [320.664148ms]
Dec 12 04:00:03.120: INFO: Created: latency-svc-f26vk
Dec 12 04:00:03.126: INFO: Created: latency-svc-c9qkg
Dec 12 04:00:03.126: INFO: Got endpoints: latency-svc-f26vk [243.671408ms]
Dec 12 04:00:03.133: INFO: Created: latency-svc-fddqt
Dec 12 04:00:03.133: INFO: Got endpoints: latency-svc-c9qkg [233.836849ms]
Dec 12 04:00:03.140: INFO: Got endpoints: latency-svc-fddqt [236.886099ms]
Dec 12 04:00:03.147: INFO: Created: latency-svc-795jz
Dec 12 04:00:03.150: INFO: Got endpoints: latency-svc-795jz [233.457244ms]
Dec 12 04:00:03.152: INFO: Created: latency-svc-t8p4c
Dec 12 04:00:03.162: INFO: Created: latency-svc-xxhb9
Dec 12 04:00:03.162: INFO: Got endpoints: latency-svc-t8p4c [225.189718ms]
Dec 12 04:00:03.178: INFO: Got endpoints: latency-svc-xxhb9 [233.881012ms]
Dec 12 04:00:03.183: INFO: Created: latency-svc-2fp5k
Dec 12 04:00:03.193: INFO: Created: latency-svc-cplg8
Dec 12 04:00:03.194: INFO: Got endpoints: latency-svc-2fp5k [243.082103ms]
Dec 12 04:00:03.201: INFO: Created: latency-svc-9hqp6
Dec 12 04:00:03.203: INFO: Got endpoints: latency-svc-cplg8 [240.679097ms]
Dec 12 04:00:03.210: INFO: Got endpoints: latency-svc-9hqp6 [235.931595ms]
Dec 12 04:00:03.217: INFO: Created: latency-svc-grwgt
Dec 12 04:00:03.222: INFO: Created: latency-svc-nhtpn
Dec 12 04:00:03.222: INFO: Got endpoints: latency-svc-grwgt [236.71843ms]
Dec 12 04:00:03.231: INFO: Created: latency-svc-qw7z7
Dec 12 04:00:03.231: INFO: Got endpoints: latency-svc-nhtpn [160.28593ms]
Dec 12 04:00:03.238: INFO: Got endpoints: latency-svc-qw7z7 [162.109264ms]
Dec 12 04:00:03.245: INFO: Created: latency-svc-pqqp9
Dec 12 04:00:03.254: INFO: Got endpoints: latency-svc-pqqp9 [161.954377ms]
Dec 12 04:00:03.257: INFO: Created: latency-svc-4l5ks
Dec 12 04:00:03.265: INFO: Created: latency-svc-twqx7
Dec 12 04:00:03.277: INFO: Got endpoints: latency-svc-4l5ks [174.358657ms]
Dec 12 04:00:03.282: INFO: Created: latency-svc-k9gmb
Dec 12 04:00:03.289: INFO: Got endpoints: latency-svc-twqx7 [175.144036ms]
Dec 12 04:00:03.293: INFO: Got endpoints: latency-svc-k9gmb [166.364405ms]
Dec 12 04:00:03.294: INFO: Created: latency-svc-bgjg5
Dec 12 04:00:03.300: INFO: Created: latency-svc-5t7dl
Dec 12 04:00:03.304: INFO: Got endpoints: latency-svc-bgjg5 [170.801658ms]
Dec 12 04:00:03.310: INFO: Got endpoints: latency-svc-5t7dl [169.021026ms]
Dec 12 04:00:03.316: INFO: Created: latency-svc-wprzt
Dec 12 04:00:03.326: INFO: Created: latency-svc-7bj49
Dec 12 04:00:03.335: INFO: Got endpoints: latency-svc-wprzt [184.87103ms]
Dec 12 04:00:03.338: INFO: Got endpoints: latency-svc-7bj49 [176.176054ms]
Dec 12 04:00:03.340: INFO: Created: latency-svc-knpvd
Dec 12 04:00:03.355: INFO: Created: latency-svc-6wcxp
Dec 12 04:00:03.362: INFO: Got endpoints: latency-svc-6wcxp [167.624063ms]
Dec 12 04:00:03.362: INFO: Got endpoints: latency-svc-knpvd [184.685839ms]
Dec 12 04:00:03.368: INFO: Created: latency-svc-nqrb4
Dec 12 04:00:03.374: INFO: Created: latency-svc-kmmgp
Dec 12 04:00:03.388: INFO: Created: latency-svc-tssgk
Dec 12 04:00:03.388: INFO: Got endpoints: latency-svc-kmmgp [178.830958ms]
Dec 12 04:00:03.388: INFO: Got endpoints: latency-svc-nqrb4 [184.903722ms]
Dec 12 04:00:03.399: INFO: Got endpoints: latency-svc-tssgk [177.393154ms]
Dec 12 04:00:03.405: INFO: Created: latency-svc-kfjvj
Dec 12 04:00:03.415: INFO: Created: latency-svc-xqkzs
Dec 12 04:00:03.419: INFO: Got endpoints: latency-svc-kfjvj [188.474968ms]
Dec 12 04:00:03.431: INFO: Got endpoints: latency-svc-xqkzs [193.271196ms]
Dec 12 04:00:03.434: INFO: Created: latency-svc-wvswc
Dec 12 04:00:03.436: INFO: Got endpoints: latency-svc-wvswc [182.188471ms]
Dec 12 04:00:03.440: INFO: Created: latency-svc-mh4xh
Dec 12 04:00:03.446: INFO: Created: latency-svc-sdc9g
Dec 12 04:00:03.449: INFO: Got endpoints: latency-svc-mh4xh [172.003367ms]
Dec 12 04:00:03.461: INFO: Created: latency-svc-h4fbz
Dec 12 04:00:03.461: INFO: Got endpoints: latency-svc-sdc9g [171.841557ms]
Dec 12 04:00:03.467: INFO: Created: latency-svc-94drg
Dec 12 04:00:03.472: INFO: Created: latency-svc-5cnln
Dec 12 04:00:03.475: INFO: Got endpoints: latency-svc-h4fbz [182.642844ms]
Dec 12 04:00:03.478: INFO: Got endpoints: latency-svc-94drg [174.088976ms]
Dec 12 04:00:03.487: INFO: Got endpoints: latency-svc-5cnln [177.856678ms]
Dec 12 04:00:03.494: INFO: Created: latency-svc-pt6q5
Dec 12 04:00:03.497: INFO: Created: latency-svc-cv5lx
Dec 12 04:00:03.503: INFO: Created: latency-svc-ljzpc
Dec 12 04:00:03.507: INFO: Got endpoints: latency-svc-cv5lx [169.522647ms]
Dec 12 04:00:03.512: INFO: Got endpoints: latency-svc-ljzpc [149.212191ms]
Dec 12 04:00:03.512: INFO: Got endpoints: latency-svc-pt6q5 [176.471572ms]
Dec 12 04:00:03.519: INFO: Created: latency-svc-rq2c8
Dec 12 04:00:03.523: INFO: Got endpoints: latency-svc-rq2c8 [161.36851ms]
Dec 12 04:00:03.525: INFO: Created: latency-svc-mzthd
Dec 12 04:00:03.529: INFO: Created: latency-svc-djx6w
Dec 12 04:00:03.536: INFO: Got endpoints: latency-svc-mzthd [147.47797ms]
Dec 12 04:00:03.542: INFO: Got endpoints: latency-svc-djx6w [153.578167ms]
Dec 12 04:00:03.543: INFO: Created: latency-svc-nsrmc
Dec 12 04:00:03.550: INFO: Got endpoints: latency-svc-nsrmc [150.593467ms]
Dec 12 04:00:03.550: INFO: Created: latency-svc-kllqj
Dec 12 04:00:03.557: INFO: Got endpoints: latency-svc-kllqj [137.375575ms]
Dec 12 04:00:03.560: INFO: Created: latency-svc-qvbbh
Dec 12 04:00:03.568: INFO: Got endpoints: latency-svc-qvbbh [136.834005ms]
Dec 12 04:00:03.575: INFO: Created: latency-svc-6tp2h
Dec 12 04:00:03.579: INFO: Created: latency-svc-24k25
Dec 12 04:00:03.579: INFO: Got endpoints: latency-svc-6tp2h [143.506384ms]
Dec 12 04:00:03.586: INFO: Created: latency-svc-sn4lt
Dec 12 04:00:03.591: INFO: Got endpoints: latency-svc-24k25 [141.541293ms]
Dec 12 04:00:03.593: INFO: Got endpoints: latency-svc-sn4lt [132.545411ms]
Dec 12 04:00:03.598: INFO: Created: latency-svc-p2rsb
Dec 12 04:00:03.603: INFO: Got endpoints: latency-svc-p2rsb [127.700162ms]
Dec 12 04:00:03.606: INFO: Created: latency-svc-7b8rf
Dec 12 04:00:03.615: INFO: Got endpoints: latency-svc-7b8rf [136.915489ms]
Dec 12 04:00:03.618: INFO: Created: latency-svc-kbwb5
Dec 12 04:00:03.625: INFO: Got endpoints: latency-svc-kbwb5 [137.759612ms]
Dec 12 04:00:03.628: INFO: Created: latency-svc-t2t2j
Dec 12 04:00:03.638: INFO: Got endpoints: latency-svc-t2t2j [130.686338ms]
Dec 12 04:00:03.642: INFO: Created: latency-svc-vjkf9
Dec 12 04:00:03.648: INFO: Created: latency-svc-t8cgd
Dec 12 04:00:03.651: INFO: Got endpoints: latency-svc-vjkf9 [139.49737ms]
Dec 12 04:00:03.660: INFO: Got endpoints: latency-svc-t8cgd [148.055983ms]
Dec 12 04:00:03.664: INFO: Created: latency-svc-dr8s4
Dec 12 04:00:03.677: INFO: Got endpoints: latency-svc-dr8s4 [153.972957ms]
Dec 12 04:00:03.677: INFO: Created: latency-svc-fv87v
Dec 12 04:00:03.681: INFO: Got endpoints: latency-svc-fv87v [145.055998ms]
Dec 12 04:00:03.685: INFO: Created: latency-svc-j2t76
Dec 12 04:00:03.694: INFO: Got endpoints: latency-svc-j2t76 [152.31169ms]
Dec 12 04:00:03.701: INFO: Created: latency-svc-wvkrm
Dec 12 04:00:03.706: INFO: Created: latency-svc-brc5w
Dec 12 04:00:03.710: INFO: Got endpoints: latency-svc-wvkrm [159.931405ms]
Dec 12 04:00:03.719: INFO: Got endpoints: latency-svc-brc5w [162.492015ms]
Dec 12 04:00:03.727: INFO: Created: latency-svc-sx7hv
Dec 12 04:00:03.732: INFO: Got endpoints: latency-svc-sx7hv [163.690492ms]
Dec 12 04:00:03.745: INFO: Created: latency-svc-stsqx
Dec 12 04:00:03.753: INFO: Got endpoints: latency-svc-stsqx [173.797137ms]
Dec 12 04:00:03.758: INFO: Created: latency-svc-zm2kb
Dec 12 04:00:03.769: INFO: Created: latency-svc-9h8ch
Dec 12 04:00:03.779: INFO: Got endpoints: latency-svc-9h8ch [185.315864ms]
Dec 12 04:00:03.785: INFO: Got endpoints: latency-svc-zm2kb [193.989718ms]
Dec 12 04:00:03.795: INFO: Created: latency-svc-4p4gj
Dec 12 04:00:03.803: INFO: Created: latency-svc-kdn8x
Dec 12 04:00:03.805: INFO: Got endpoints: latency-svc-4p4gj [201.801411ms]
Dec 12 04:00:03.810: INFO: Created: latency-svc-582gr
Dec 12 04:00:03.819: INFO: Got endpoints: latency-svc-582gr [194.024074ms]
Dec 12 04:00:03.825: INFO: Got endpoints: latency-svc-kdn8x [210.006927ms]
Dec 12 04:00:03.833: INFO: Created: latency-svc-kqkqw
Dec 12 04:00:03.839: INFO: Created: latency-svc-kb6pw
Dec 12 04:00:03.839: INFO: Got endpoints: latency-svc-kqkqw [200.614128ms]
Dec 12 04:00:03.845: INFO: Created: latency-svc-24zlz
Dec 12 04:00:03.849: INFO: Got endpoints: latency-svc-kb6pw [197.991795ms]
Dec 12 04:00:03.853: INFO: Got endpoints: latency-svc-24zlz [192.764181ms]
Dec 12 04:00:03.863: INFO: Created: latency-svc-7wc72
Dec 12 04:00:03.868: INFO: Got endpoints: latency-svc-7wc72 [190.219272ms]
Dec 12 04:00:03.869: INFO: Created: latency-svc-pjjbt
Dec 12 04:00:03.875: INFO: Created: latency-svc-xdlnq
Dec 12 04:00:03.880: INFO: Got endpoints: latency-svc-pjjbt [199.392212ms]
Dec 12 04:00:03.885: INFO: Got endpoints: latency-svc-xdlnq [190.230956ms]
Dec 12 04:00:03.890: INFO: Created: latency-svc-mqphw
Dec 12 04:00:03.892: INFO: Got endpoints: latency-svc-mqphw [182.68454ms]
Dec 12 04:00:03.896: INFO: Created: latency-svc-fmtq2
Dec 12 04:00:03.902: INFO: Got endpoints: latency-svc-fmtq2 [182.825328ms]
Dec 12 04:00:03.904: INFO: Created: latency-svc-89q8r
Dec 12 04:00:03.914: INFO: Got endpoints: latency-svc-89q8r [182.146857ms]
Dec 12 04:00:03.920: INFO: Created: latency-svc-4dz55
Dec 12 04:00:03.924: INFO: Got endpoints: latency-svc-4dz55 [170.737803ms]
Dec 12 04:00:03.929: INFO: Created: latency-svc-vnp5v
Dec 12 04:00:03.937: INFO: Got endpoints: latency-svc-vnp5v [158.702893ms]
Dec 12 04:00:03.937: INFO: Created: latency-svc-5n4bt
Dec 12 04:00:03.945: INFO: Got endpoints: latency-svc-5n4bt [160.073195ms]
Dec 12 04:00:03.948: INFO: Created: latency-svc-nwqsq
Dec 12 04:00:03.955: INFO: Got endpoints: latency-svc-nwqsq [149.526105ms]
Dec 12 04:00:03.959: INFO: Created: latency-svc-j6fjf
Dec 12 04:00:03.965: INFO: Got endpoints: latency-svc-j6fjf [145.920719ms]
Dec 12 04:00:03.965: INFO: Created: latency-svc-wf6lh
Dec 12 04:00:03.980: INFO: Got endpoints: latency-svc-wf6lh [154.115347ms]
Dec 12 04:00:03.983: INFO: Created: latency-svc-28t6z
Dec 12 04:00:03.991: INFO: Got endpoints: latency-svc-28t6z [152.555886ms]
Dec 12 04:00:03.997: INFO: Created: latency-svc-sx4tl
Dec 12 04:00:04.004: INFO: Got endpoints: latency-svc-sx4tl [154.599362ms]
Dec 12 04:00:04.007: INFO: Created: latency-svc-tm2fq
Dec 12 04:00:04.017: INFO: Got endpoints: latency-svc-tm2fq [163.926526ms]
Dec 12 04:00:04.023: INFO: Created: latency-svc-95fws
Dec 12 04:00:04.027: INFO: Created: latency-svc-pcxvm
Dec 12 04:00:04.033: INFO: Got endpoints: latency-svc-95fws [164.819639ms]
Dec 12 04:00:04.033: INFO: Created: latency-svc-cr59n
Dec 12 04:00:04.037: INFO: Got endpoints: latency-svc-pcxvm [156.956587ms]
Dec 12 04:00:04.043: INFO: Got endpoints: latency-svc-cr59n [158.370195ms]
Dec 12 04:00:04.046: INFO: Created: latency-svc-j7r24
Dec 12 04:00:04.052: INFO: Got endpoints: latency-svc-j7r24 [159.28877ms]
Dec 12 04:00:04.055: INFO: Created: latency-svc-fnjvx
Dec 12 04:00:04.060: INFO: Got endpoints: latency-svc-fnjvx [158.256505ms]
Dec 12 04:00:04.061: INFO: Created: latency-svc-6kr9b
Dec 12 04:00:04.068: INFO: Got endpoints: latency-svc-6kr9b [153.997089ms]
Dec 12 04:00:04.072: INFO: Created: latency-svc-znflg
Dec 12 04:00:04.087: INFO: Got endpoints: latency-svc-znflg [162.839025ms]
Dec 12 04:00:04.099: INFO: Created: latency-svc-v4p6b
Dec 12 04:00:04.100: INFO: Got endpoints: latency-svc-v4p6b [162.197201ms]
Dec 12 04:00:04.105: INFO: Created: latency-svc-fdwzb
Dec 12 04:00:04.112: INFO: Got endpoints: latency-svc-fdwzb [167.696618ms]
Dec 12 04:00:04.118: INFO: Created: latency-svc-w4l57
Dec 12 04:00:04.123: INFO: Created: latency-svc-5gfrk
Dec 12 04:00:04.128: INFO: Created: latency-svc-99p8b
Dec 12 04:00:04.131: INFO: Got endpoints: latency-svc-w4l57 [176.356237ms]
Dec 12 04:00:04.136: INFO: Got endpoints: latency-svc-99p8b [155.974147ms]
Dec 12 04:00:04.155: INFO: Got endpoints: latency-svc-5gfrk [189.177637ms]
Dec 12 04:00:04.155: INFO: Created: latency-svc-xs7v5
Dec 12 04:00:04.160: INFO: Got endpoints: latency-svc-xs7v5 [168.225189ms]
Dec 12 04:00:04.164: INFO: Created: latency-svc-jxpfv
Dec 12 04:00:04.169: INFO: Created: latency-svc-xfcc5
Dec 12 04:00:04.171: INFO: Got endpoints: latency-svc-jxpfv [166.710909ms]
Dec 12 04:00:04.177: INFO: Got endpoints: latency-svc-xfcc5 [160.320888ms]
Dec 12 04:00:04.181: INFO: Created: latency-svc-786q9
Dec 12 04:00:04.187: INFO: Got endpoints: latency-svc-786q9 [154.603541ms]
Dec 12 04:00:04.193: INFO: Created: latency-svc-sg8x4
Dec 12 04:00:04.200: INFO: Created: latency-svc-dj8k8
Dec 12 04:00:04.214: INFO: Got endpoints: latency-svc-dj8k8 [170.773513ms]
Dec 12 04:00:04.218: INFO: Got endpoints: latency-svc-sg8x4 [180.910343ms]
Dec 12 04:00:04.229: INFO: Created: latency-svc-2xb82
Dec 12 04:00:04.235: INFO: Got endpoints: latency-svc-2xb82 [182.905708ms]
Dec 12 04:00:04.236: INFO: Created: latency-svc-ctrbt
Dec 12 04:00:04.241: INFO: Created: latency-svc-z4t7k
Dec 12 04:00:04.251: INFO: Got endpoints: latency-svc-ctrbt [189.987996ms]
Dec 12 04:00:04.253: INFO: Got endpoints: latency-svc-z4t7k [184.166772ms]
Dec 12 04:00:04.257: INFO: Created: latency-svc-f2knv
Dec 12 04:00:04.263: INFO: Got endpoints: latency-svc-f2knv [176.427274ms]
Dec 12 04:00:04.265: INFO: Created: latency-svc-gv8d7
Dec 12 04:00:04.271: INFO: Created: latency-svc-jxqvp
Dec 12 04:00:04.271: INFO: Got endpoints: latency-svc-gv8d7 [171.509171ms]
Dec 12 04:00:04.281: INFO: Created: latency-svc-ggkch
Dec 12 04:00:04.282: INFO: Got endpoints: latency-svc-jxqvp [169.372991ms]
Dec 12 04:00:04.289: INFO: Created: latency-svc-24wtq
Dec 12 04:00:04.289: INFO: Got endpoints: latency-svc-ggkch [157.834363ms]
Dec 12 04:00:04.295: INFO: Created: latency-svc-7g6n2
Dec 12 04:00:04.299: INFO: Got endpoints: latency-svc-24wtq [163.298823ms]
Dec 12 04:00:04.307: INFO: Got endpoints: latency-svc-7g6n2 [152.521867ms]
Dec 12 04:00:04.316: INFO: Created: latency-svc-zh9jv
Dec 12 04:00:04.325: INFO: Created: latency-svc-f82zw
Dec 12 04:00:04.330: INFO: Got endpoints: latency-svc-zh9jv [169.94454ms]
Dec 12 04:00:04.331: INFO: Created: latency-svc-nxr84
Dec 12 04:00:04.341: INFO: Got endpoints: latency-svc-f82zw [170.167174ms]
Dec 12 04:00:04.341: INFO: Got endpoints: latency-svc-nxr84 [163.736642ms]
Dec 12 04:00:04.348: INFO: Created: latency-svc-tmzsb
Dec 12 04:00:04.356: INFO: Got endpoints: latency-svc-tmzsb [169.033718ms]
Dec 12 04:00:04.360: INFO: Created: latency-svc-m592d
Dec 12 04:00:04.366: INFO: Created: latency-svc-82v4c
Dec 12 04:00:04.374: INFO: Got endpoints: latency-svc-m592d [160.260324ms]
Dec 12 04:00:04.379: INFO: Got endpoints: latency-svc-82v4c [160.472766ms]
Dec 12 04:00:04.394: INFO: Created: latency-svc-v752n
Dec 12 04:00:04.394: INFO: Got endpoints: latency-svc-v752n [158.779887ms]
Dec 12 04:00:04.403: INFO: Created: latency-svc-t4rmd
Dec 12 04:00:04.410: INFO: Created: latency-svc-r7b2t
Dec 12 04:00:04.415: INFO: Got endpoints: latency-svc-t4rmd [163.985667ms]
Dec 12 04:00:04.419: INFO: Got endpoints: latency-svc-r7b2t [166.616005ms]
Dec 12 04:00:04.427: INFO: Created: latency-svc-mrlbz
Dec 12 04:00:04.436: INFO: Created: latency-svc-5bjdw
Dec 12 04:00:04.439: INFO: Got endpoints: latency-svc-mrlbz [176.007211ms]
Dec 12 04:00:04.443: INFO: Created: latency-svc-2n4hh
Dec 12 04:00:04.443: INFO: Got endpoints: latency-svc-5bjdw [172.314867ms]
Dec 12 04:00:04.459: INFO: Got endpoints: latency-svc-2n4hh [177.50913ms]
Dec 12 04:00:04.464: INFO: Created: latency-svc-8w6qp
Dec 12 04:00:04.469: INFO: Created: latency-svc-bk9s5
Dec 12 04:00:04.476: INFO: Created: latency-svc-ghzlc
Dec 12 04:00:04.478: INFO: Got endpoints: latency-svc-8w6qp [188.771052ms]
Dec 12 04:00:04.492: INFO: Got endpoints: latency-svc-bk9s5 [192.877277ms]
Dec 12 04:00:04.492: INFO: Got endpoints: latency-svc-ghzlc [184.786482ms]
Dec 12 04:00:04.511: INFO: Created: latency-svc-7tmg7
Dec 12 04:00:04.511: INFO: Got endpoints: latency-svc-7tmg7 [180.957883ms]
Dec 12 04:00:04.512: INFO: Created: latency-svc-lz26d
Dec 12 04:00:04.519: INFO: Created: latency-svc-wqvgr
Dec 12 04:00:04.526: INFO: Got endpoints: latency-svc-lz26d [185.274878ms]
Dec 12 04:00:04.529: INFO: Got endpoints: latency-svc-wqvgr [188.552924ms]
Dec 12 04:00:04.539: INFO: Created: latency-svc-h8wpl
Dec 12 04:00:04.548: INFO: Got endpoints: latency-svc-h8wpl [191.443585ms]
Dec 12 04:00:04.555: INFO: Created: latency-svc-b2rhx
Dec 12 04:00:04.557: INFO: Created: latency-svc-jfftq
Dec 12 04:00:04.565: INFO: Got endpoints: latency-svc-b2rhx [190.547556ms]
Dec 12 04:00:04.567: INFO: Got endpoints: latency-svc-jfftq [188.078211ms]
Dec 12 04:00:04.586: INFO: Created: latency-svc-lcjtx
Dec 12 04:00:04.586: INFO: Got endpoints: latency-svc-lcjtx [192.803491ms]
Dec 12 04:00:04.590: INFO: Created: latency-svc-gtgdg
Dec 12 04:00:04.596: INFO: Created: latency-svc-2brfc
Dec 12 04:00:04.604: INFO: Got endpoints: latency-svc-gtgdg [189.609671ms]
Dec 12 04:00:04.605: INFO: Got endpoints: latency-svc-2brfc [185.806372ms]
Dec 12 04:00:04.616: INFO: Created: latency-svc-z4v4l
Dec 12 04:00:04.625: INFO: Got endpoints: latency-svc-z4v4l [185.76507ms]
Dec 12 04:00:04.626: INFO: Created: latency-svc-87h7w
Dec 12 04:00:04.634: INFO: Created: latency-svc-nrkbg
Dec 12 04:00:04.636: INFO: Got endpoints: latency-svc-87h7w [192.572354ms]
Dec 12 04:00:04.644: INFO: Got endpoints: latency-svc-nrkbg [184.445606ms]
Dec 12 04:00:04.648: INFO: Created: latency-svc-lc6f5
Dec 12 04:00:04.656: INFO: Got endpoints: latency-svc-lc6f5 [178.009223ms]
Dec 12 04:00:04.659: INFO: Created: latency-svc-cn55f
Dec 12 04:00:04.670: INFO: Created: latency-svc-q9m9x
Dec 12 04:00:04.672: INFO: Got endpoints: latency-svc-cn55f [180.303259ms]
Dec 12 04:00:04.679: INFO: Got endpoints: latency-svc-q9m9x [187.202147ms]
Dec 12 04:00:04.695: INFO: Created: latency-svc-6cq5j
Dec 12 04:00:04.695: INFO: Got endpoints: latency-svc-6cq5j [183.909935ms]
Dec 12 04:00:04.698: INFO: Created: latency-svc-79vgx
Dec 12 04:00:04.705: INFO: Created: latency-svc-9ftql
Dec 12 04:00:04.712: INFO: Got endpoints: latency-svc-79vgx [185.602634ms]
Dec 12 04:00:04.715: INFO: Got endpoints: latency-svc-9ftql [185.092438ms]
Dec 12 04:00:04.731: INFO: Created: latency-svc-w9c5g
Dec 12 04:00:04.738: INFO: Got endpoints: latency-svc-w9c5g [189.976248ms]
Dec 12 04:00:04.762: INFO: Created: latency-svc-2qtp4
Dec 12 04:00:04.762: INFO: Got endpoints: latency-svc-2qtp4 [196.840295ms]
Dec 12 04:00:04.776: INFO: Created: latency-svc-psdb6
Dec 12 04:00:04.785: INFO: Got endpoints: latency-svc-psdb6 [217.450773ms]
Dec 12 04:00:04.793: INFO: Created: latency-svc-df8vs
Dec 12 04:00:04.800: INFO: Created: latency-svc-zcm6j
Dec 12 04:00:04.803: INFO: Got endpoints: latency-svc-df8vs [216.290042ms]
Dec 12 04:00:04.804: INFO: Created: latency-svc-tt5gx
Dec 12 04:00:04.813: INFO: Got endpoints: latency-svc-zcm6j [208.44772ms]
Dec 12 04:00:04.815: INFO: Got endpoints: latency-svc-tt5gx [210.30261ms]
Dec 12 04:00:04.830: INFO: Created: latency-svc-hm6cl
Dec 12 04:00:04.839: INFO: Got endpoints: latency-svc-hm6cl [214.023607ms]
Dec 12 04:00:04.848: INFO: Created: latency-svc-zfn68
Dec 12 04:00:04.852: INFO: Got endpoints: latency-svc-zfn68 [215.654124ms]
Dec 12 04:00:04.856: INFO: Created: latency-svc-l56sg
Dec 12 04:00:04.864: INFO: Got endpoints: latency-svc-l56sg [219.898955ms]
Dec 12 04:00:04.868: INFO: Created: latency-svc-h8nr2
Dec 12 04:00:04.875: INFO: Got endpoints: latency-svc-h8nr2 [219.424828ms]
Dec 12 04:00:04.881: INFO: Created: latency-svc-dcj2j
Dec 12 04:00:04.882: INFO: Created: latency-svc-ch5gz
Dec 12 04:00:04.891: INFO: Got endpoints: latency-svc-dcj2j [218.72503ms]
Dec 12 04:00:04.894: INFO: Got endpoints: latency-svc-ch5gz [215.296983ms]
Dec 12 04:00:04.902: INFO: Created: latency-svc-6hq9t
Dec 12 04:00:04.916: INFO: Got endpoints: latency-svc-6hq9t [221.452094ms]
Dec 12 04:00:04.921: INFO: Created: latency-svc-p9jqx
Dec 12 04:00:04.928: INFO: Created: latency-svc-fxhns
Dec 12 04:00:04.933: INFO: Got endpoints: latency-svc-p9jqx [221.00245ms]
Dec 12 04:00:04.944: INFO: Got endpoints: latency-svc-fxhns [229.879106ms]
Dec 12 04:00:04.945: INFO: Created: latency-svc-zrn78
Dec 12 04:00:04.950: INFO: Got endpoints: latency-svc-zrn78 [212.745804ms]
Dec 12 04:00:04.961: INFO: Created: latency-svc-mkmtx
Dec 12 04:00:04.968: INFO: Got endpoints: latency-svc-mkmtx [206.200684ms]
Dec 12 04:00:04.969: INFO: Created: latency-svc-52xnf
Dec 12 04:00:04.983: INFO: Got endpoints: latency-svc-52xnf [198.944437ms]
Dec 12 04:00:04.984: INFO: Created: latency-svc-v8455
Dec 12 04:00:04.993: INFO: Created: latency-svc-rct5n
Dec 12 04:00:04.996: INFO: Got endpoints: latency-svc-v8455 [193.735517ms]
Dec 12 04:00:04.997: INFO: Created: latency-svc-nkjg2
Dec 12 04:00:05.010: INFO: Got endpoints: latency-svc-rct5n [197.479671ms]
Dec 12 04:00:05.013: INFO: Got endpoints: latency-svc-nkjg2 [197.71784ms]
Dec 12 04:00:05.026: INFO: Created: latency-svc-tf99m
Dec 12 04:00:05.032: INFO: Created: latency-svc-dn7bs
Dec 12 04:00:05.038: INFO: Created: latency-svc-r5jms
Dec 12 04:00:05.041: INFO: Got endpoints: latency-svc-tf99m [201.350392ms]
Dec 12 04:00:05.046: INFO: Got endpoints: latency-svc-dn7bs [194.490533ms]
Dec 12 04:00:05.047: INFO: Got endpoints: latency-svc-r5jms [182.920155ms]
Dec 12 04:00:05.058: INFO: Created: latency-svc-8fgr5
Dec 12 04:00:05.061: INFO: Got endpoints: latency-svc-8fgr5 [186.199265ms]
Dec 12 04:00:05.066: INFO: Created: latency-svc-bj7jk
Dec 12 04:00:05.083: INFO: Got endpoints: latency-svc-bj7jk [191.867805ms]
Dec 12 04:00:05.083: INFO: Latencies: [40.902939ms 56.773995ms 69.216021ms 82.239986ms 93.975057ms 112.958223ms 127.700162ms 130.686338ms 132.545411ms 134.82416ms 136.834005ms 136.915489ms 137.375575ms 137.759612ms 139.49737ms 141.541293ms 142.380642ms 143.506384ms 145.055998ms 145.920719ms 147.47797ms 148.055983ms 149.212191ms 149.526105ms 150.593467ms 152.31169ms 152.521867ms 152.555886ms 153.578167ms 153.972957ms 153.997089ms 154.115347ms 154.599362ms 154.603541ms 155.974147ms 156.956587ms 157.834363ms 158.256505ms 158.370195ms 158.702893ms 158.779887ms 159.28877ms 159.931405ms 160.073195ms 160.260324ms 160.28593ms 160.320888ms 160.472766ms 161.36851ms 161.954377ms 162.109264ms 162.197201ms 162.492015ms 162.555978ms 162.839025ms 163.298823ms 163.690492ms 163.736642ms 163.926526ms 163.985667ms 164.819639ms 166.364405ms 166.616005ms 166.710909ms 167.624063ms 167.696618ms 168.225189ms 169.021026ms 169.033718ms 169.372991ms 169.522647ms 169.735321ms 169.94454ms 170.167174ms 170.737803ms 170.773513ms 170.801658ms 171.509171ms 171.841557ms 172.003367ms 172.314867ms 173.797137ms 174.088976ms 174.358657ms 175.144036ms 176.007211ms 176.176054ms 176.356237ms 176.427274ms 176.471572ms 177.393154ms 177.50913ms 177.856678ms 178.009223ms 178.830958ms 178.978546ms 180.303259ms 180.910343ms 180.957883ms 182.146857ms 182.188471ms 182.642844ms 182.68454ms 182.825328ms 182.905708ms 182.920155ms 183.909935ms 184.166772ms 184.445606ms 184.685839ms 184.786482ms 184.87103ms 184.903722ms 185.092438ms 185.274878ms 185.315864ms 185.602634ms 185.76507ms 185.806372ms 186.199265ms 187.202147ms 187.223268ms 187.914535ms 188.078211ms 188.474968ms 188.552924ms 188.771052ms 189.177637ms 189.609671ms 189.976248ms 189.987996ms 190.219272ms 190.230956ms 190.547556ms 191.443585ms 191.867805ms 192.572354ms 192.764181ms 192.803491ms 192.877277ms 193.271196ms 193.735517ms 193.989718ms 194.024074ms 194.490533ms 196.840295ms 197.479671ms 197.71784ms 197.991795ms 198.944437ms 199.392212ms 200.614128ms 201.350392ms 201.801411ms 202.85199ms 205.104823ms 206.200684ms 208.44772ms 210.006927ms 210.30261ms 212.745804ms 214.023607ms 214.799378ms 215.296983ms 215.654124ms 216.290042ms 217.450773ms 218.72503ms 219.424828ms 219.898955ms 221.00245ms 221.452094ms 225.189718ms 228.222962ms 229.879106ms 233.457244ms 233.836849ms 233.881012ms 233.890283ms 235.931595ms 236.71843ms 236.886099ms 237.360649ms 240.679097ms 242.968251ms 243.082103ms 243.671408ms 243.935216ms 244.344314ms 244.601546ms 245.406198ms 246.925121ms 249.966798ms 252.130694ms 257.397869ms 305.393388ms 317.222129ms 320.664148ms 321.862012ms 324.287563ms]
Dec 12 04:00:05.083: INFO: 50 %ile: 182.188471ms
Dec 12 04:00:05.083: INFO: 90 %ile: 236.71843ms
Dec 12 04:00:05.083: INFO: 99 %ile: 321.862012ms
Dec 12 04:00:05.083: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:00:05.083: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svc-latency-1847" for this suite.
Dec 12 04:00:25.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:00:27.432: INFO: namespace svc-latency-1847 deletion completed in 22.317465715s


â€¢ [SLOW TEST:34.240 seconds]
[sig-network] Service endpoints latency
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 03:59:47.440: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 12 03:59:48.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:50.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:52.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:54.673: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 12 03:59:56.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711719988, loc:(*time.Location)(0x86b4b00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 12 03:59:59.696: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:00:10.068: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-593" for this suite.
Dec 12 04:00:18.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:00:20.438: INFO: namespace webhook-593 deletion completed in 10.325470744s
STEP: Destroying namespace "webhook-593-markers" for this suite.
Dec 12 04:00:26.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:00:28.745: INFO: namespace webhook-593-markers deletion completed in 8.306905439s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103


â€¢ [SLOW TEST:41.377 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:00:27.438: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-860e0593-ab12-46dd-8e2d-20e3838d4921
STEP: Creating a pod to test consume secrets
Dec 12 04:00:27.627: INFO: Waiting up to 5m0s for pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538" in namespace "secrets-3848" to be "success or failure"
Dec 12 04:00:27.644: INFO: Pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538": Phase="Pending", Reason="", readiness=false. Elapsed: 17.768951ms
Dec 12 04:00:29.661: INFO: Pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033936652s
Dec 12 04:00:31.677: INFO: Pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050762719s
Dec 12 04:00:33.694: INFO: Pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067031449s
Dec 12 04:00:35.710: INFO: Pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083885201s
Dec 12 04:00:37.727: INFO: Pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.100038824s
STEP: Saw pod success
Dec 12 04:00:37.727: INFO: Pod "pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538" satisfied condition "success or failure"
Dec 12 04:00:37.743: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538 container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 04:00:37.786: INFO: Waiting for pod pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538 to disappear
Dec 12 04:00:37.802: INFO: Pod pod-secrets-a459110c-d8db-4aff-ae2e-e3a227d68538 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:00:37.802: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3848" for this suite.
Dec 12 04:00:43.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:00:46.160: INFO: namespace secrets-3848 deletion completed in 8.314346161s


â€¢ [SLOW TEST:18.721 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:00:28.856: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-d66cf3df-fae1-4c28-a51a-a875894eb12d
STEP: Creating a pod to test consume secrets
Dec 12 04:00:29.012: INFO: Waiting up to 5m0s for pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc" in namespace "secrets-2350" to be "success or failure"
Dec 12 04:00:29.029: INFO: Pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.164486ms
Dec 12 04:00:31.045: INFO: Pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03266775s
Dec 12 04:00:33.061: INFO: Pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048665666s
Dec 12 04:00:35.077: INFO: Pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064866719s
Dec 12 04:00:37.098: INFO: Pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085144164s
Dec 12 04:00:39.114: INFO: Pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.102034907s
STEP: Saw pod success
Dec 12 04:00:39.115: INFO: Pod "pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc" satisfied condition "success or failure"
Dec 12 04:00:39.130: INFO: Trying to get logs from node ip-10-0-133-2.ec2.internal pod pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc container secret-volume-test: <nil>
STEP: delete the pod
Dec 12 04:00:39.176: INFO: Waiting for pod pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc to disappear
Dec 12 04:00:39.192: INFO: Pod pod-secrets-0774fa34-dbff-4532-a6d6-e061cc4917bc no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:00:39.192: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2350" for this suite.
Dec 12 04:00:45.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:00:47.557: INFO: namespace secrets-2350 deletion completed in 8.32245939s


â€¢ [SLOW TEST:18.701 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:00:47.577: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Dec 12 04:00:47.704: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig cluster-info'
Dec 12 04:00:47.878: INFO: stderr: ""
Dec 12 04:00:47.878: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.ci-op-8rty34r5-fb0a9.origin-ci-int-aws.dev.rhcloud.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:00:47.878: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-7564" for this suite.
Dec 12 04:01:01.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:01:04.189: INFO: namespace kubectl-7564 deletion completed in 16.293398238s


â€¢ [SLOW TEST:16.612 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:00:02.344: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-cc026048-e34a-4bfc-9217-5a3d4274940a in namespace container-probe-2722
Dec 12 04:00:12.545: INFO: Started pod busybox-cc026048-e34a-4bfc-9217-5a3d4274940a in namespace container-probe-2722
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 04:00:12.562: INFO: Initial restart count of pod busybox-cc026048-e34a-4bfc-9217-5a3d4274940a is 0
Dec 12 04:01:03.010: INFO: Restart count of pod container-probe-2722/busybox-cc026048-e34a-4bfc-9217-5a3d4274940a is now 1 (50.447738614s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:01:03.037: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-2722" for this suite.
Dec 12 04:01:09.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:01:11.414: INFO: namespace container-probe-2722 deletion completed in 8.334030473s


â€¢ [SLOW TEST:69.070 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:00:11.152: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 12 04:00:31.605: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 04:00:31.620: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 04:00:33.620: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 04:00:33.637: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 04:00:35.620: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 04:00:35.636: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 04:00:37.620: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 04:00:37.637: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 04:00:39.620: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 04:00:39.637: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 04:00:41.620: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 04:00:41.636: INFO: Pod pod-with-poststart-http-hook still exists
Dec 12 04:00:43.621: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 12 04:00:43.638: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:00:43.638: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8442" for this suite.
Dec 12 04:01:11.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:01:14.225: INFO: namespace container-lifecycle-hook-8442 deletion completed in 30.545336772s


â€¢ [SLOW TEST:63.073 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:00:46.161: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3715
[It] should have a working scale subresource [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-3715
Dec 12 04:00:46.354: INFO: Found 0 stateful pods, waiting for 1
Dec 12 04:00:56.372: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 12 04:00:56.456: INFO: Deleting all statefulset in ns statefulset-3715
Dec 12 04:00:56.473: INFO: Scaling statefulset ss to 0
Dec 12 04:01:16.542: INFO: Waiting for statefulset status.replicas updated to 0
Dec 12 04:01:16.559: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:01:16.611: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-3715" for this suite.
Dec 12 04:01:22.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:01:25.081: INFO: namespace statefulset-3715 deletion completed in 8.427716705s


â€¢ [SLOW TEST:38.919 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
Dec 12 04:01:25.093: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:01:14.248: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:01:24.600: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-603" for this suite.
Dec 12 04:01:30.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:01:32.963: INFO: namespace emptydir-wrapper-603 deletion completed in 8.321012601s


â€¢ [SLOW TEST:18.715 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
Dec 12 04:01:32.965: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:01:11.419: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 12 04:01:11.541: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 12 04:01:38.462: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Dec 12 04:01:46.106: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:02:15.046: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6910" for this suite.
Dec 12 04:02:21.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:02:23.426: INFO: namespace crd-publish-openapi-6910 deletion completed in 8.335114575s


â€¢ [SLOW TEST:72.008 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
Dec 12 04:02:23.428: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 12 04:01:04.228: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-9e15c509-cdf5-451b-a66c-c6a34089c0e2 in namespace container-probe-7123
Dec 12 04:01:14.424: INFO: Started pod test-webserver-9e15c509-cdf5-451b-a66c-c6a34089c0e2 in namespace container-probe-7123
STEP: checking the pod's current state and verifying that restartCount is present
Dec 12 04:01:14.443: INFO: Initial restart count of pod test-webserver-9e15c509-cdf5-451b-a66c-c6a34089c0e2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 12 04:05:14.537: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7123" for this suite.
Dec 12 04:05:20.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 12 04:05:23.000: INFO: namespace container-probe-7123 deletion completed in 8.419151973s


â€¢ [SLOW TEST:258.772 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
Dec 12 04:05:23.007: INFO: Running AfterSuite actions on all nodes
Dec 12 04:05:23.013: INFO: Running AfterSuite actions on node 1
Dec 12 04:05:23.013: INFO: Dumping logs locally to: /tmp/artifacts
Dec 12 04:05:23.014: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory


Ran 261 of 4731 Specs in 2403.297 seconds
SUCCESS! -- 261 Passed | 0 Failed | 0 Pending | 4470 Skipped


Ginkgo ran 1 suite in 40m5.153677138s
Test Suite Passed
