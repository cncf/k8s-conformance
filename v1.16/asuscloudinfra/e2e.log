I0519 08:19:03.843700      25 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-934715428
I0519 08:19:03.843904      25 e2e.go:92] Starting e2e run "94558208-8ded-493d-85b3-0a7253b84911" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1589876342 - Will randomize all specs
Will run 276 of 4732 specs

May 19 08:19:03.864: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 08:19:03.868: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 19 08:19:03.888: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 19 08:19:03.932: INFO: 31 / 31 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 19 08:19:03.932: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 19 08:19:03.932: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 19 08:19:03.942: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May 19 08:19:03.942: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 19 08:19:03.942: INFO: e2e test version: v1.16.9
May 19 08:19:03.943: INFO: kube-apiserver version: v1.16.9
May 19 08:19:03.943: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 08:19:03.948: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:19:03.949: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename events
May 19 08:19:03.997: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 19 08:19:06.024: INFO: &Pod{ObjectMeta:{send-events-2758a1d9-4cfb-4d1a-a75b-c89c4c902a94  events-5163 /api/v1/namespaces/events-5163/pods/send-events-2758a1d9-4cfb-4d1a-a75b-c89c4c902a94 f1bf7c7f-4cfe-49af-80de-c53c3a666b3b 1097998 0 2020-05-19 08:19:04 +0000 UTC <nil> <nil> map[name:foo time:999519758] map[cni.projectcalico.org/podIP:10.244.161.106/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kvngs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kvngs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kvngs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:19:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:19:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:10.244.161.106,StartTime:2020-05-19 08:19:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:19:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://ef7924420d9eb954d9e1924c8a5ba79591ba02a2c66106ac3dfcb967db220a31,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.161.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 19 08:19:08.029: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 19 08:19:10.039: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:19:10.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5163" for this suite.
May 19 08:19:54.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:19:54.122: INFO: namespace events-5163 deletion completed in 44.071494321s

• [SLOW TEST:50.173 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:19:54.122: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc
May 19 08:19:54.154: INFO: Pod name my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc: Found 0 pods out of 1
May 19 08:19:59.158: INFO: Pod name my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc: Found 1 pods out of 1
May 19 08:19:59.158: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc" are running
May 19 08:19:59.161: INFO: Pod "my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc-sbhrp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 08:19:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 08:19:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 08:19:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 08:19:54 +0000 UTC Reason: Message:}])
May 19 08:19:59.161: INFO: Trying to dial the pod
May 19 08:20:04.170: INFO: Controller my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc: Got expected result from replica 1 [my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc-sbhrp]: "my-hostname-basic-c65d2f59-4e5a-46ce-aad2-89b83fef8cdc-sbhrp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:20:04.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2696" for this suite.
May 19 08:20:10.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:20:10.258: INFO: namespace replication-controller-2696 deletion completed in 6.083853524s

• [SLOW TEST:16.136 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:20:10.258: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 08:20:10.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e3e93fd-b82a-43f0-bf5f-d883423d28db" in namespace "projected-7813" to be "success or failure"
May 19 08:20:10.295: INFO: Pod "downwardapi-volume-4e3e93fd-b82a-43f0-bf5f-d883423d28db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022354ms
May 19 08:20:12.299: INFO: Pod "downwardapi-volume-4e3e93fd-b82a-43f0-bf5f-d883423d28db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005361081s
STEP: Saw pod success
May 19 08:20:12.299: INFO: Pod "downwardapi-volume-4e3e93fd-b82a-43f0-bf5f-d883423d28db" satisfied condition "success or failure"
May 19 08:20:12.301: INFO: Trying to get logs from node nchc-kubemaster01 pod downwardapi-volume-4e3e93fd-b82a-43f0-bf5f-d883423d28db container client-container: <nil>
STEP: delete the pod
May 19 08:20:12.333: INFO: Waiting for pod downwardapi-volume-4e3e93fd-b82a-43f0-bf5f-d883423d28db to disappear
May 19 08:20:12.335: INFO: Pod downwardapi-volume-4e3e93fd-b82a-43f0-bf5f-d883423d28db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:20:12.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7813" for this suite.
May 19 08:20:18.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:20:18.410: INFO: namespace projected-7813 deletion completed in 6.072328218s

• [SLOW TEST:8.152 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:20:18.410: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:20:18.440: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:20:18.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1554" for this suite.
May 19 08:20:24.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:20:24.558: INFO: namespace custom-resource-definition-1554 deletion completed in 6.07837691s

• [SLOW TEST:6.147 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:20:24.558: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-211534b0-73f1-40cb-a70f-0598799b81b7
STEP: Creating a pod to test consume configMaps
May 19 08:20:24.593: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd" in namespace "configmap-5353" to be "success or failure"
May 19 08:20:24.595: INFO: Pod "pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.990505ms
May 19 08:20:26.598: INFO: Pod "pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005124665s
May 19 08:20:28.602: INFO: Pod "pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00860608s
STEP: Saw pod success
May 19 08:20:28.602: INFO: Pod "pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd" satisfied condition "success or failure"
May 19 08:20:28.604: INFO: Trying to get logs from node nchc-worker03 pod pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd container configmap-volume-test: <nil>
STEP: delete the pod
May 19 08:20:28.634: INFO: Waiting for pod pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd to disappear
May 19 08:20:28.636: INFO: Pod pod-configmaps-3ececf7a-c896-444d-8dd2-f4fbe357f0dd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:20:28.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5353" for this suite.
May 19 08:20:34.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:20:34.711: INFO: namespace configmap-5353 deletion completed in 6.071327868s

• [SLOW TEST:10.153 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:20:34.712: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 19 08:20:34.738: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:20:38.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-997" for this suite.
May 19 08:20:50.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:20:50.808: INFO: namespace init-container-997 deletion completed in 12.072493005s

• [SLOW TEST:16.097 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:20:50.808: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-09b9186d-e9dd-4bb8-bde6-b5f8818f99ed
STEP: Creating a pod to test consume secrets
May 19 08:20:50.925: INFO: Waiting up to 5m0s for pod "pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4" in namespace "secrets-2937" to be "success or failure"
May 19 08:20:50.927: INFO: Pod "pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864091ms
May 19 08:20:52.931: INFO: Pod "pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005610627s
May 19 08:20:54.934: INFO: Pod "pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009295563s
STEP: Saw pod success
May 19 08:20:54.934: INFO: Pod "pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4" satisfied condition "success or failure"
May 19 08:20:54.937: INFO: Trying to get logs from node nchc-worker03 pod pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4 container secret-volume-test: <nil>
STEP: delete the pod
May 19 08:20:54.954: INFO: Waiting for pod pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4 to disappear
May 19 08:20:54.956: INFO: Pod pod-secrets-29aab48a-15e0-4421-83f6-d3bdda269db4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:20:54.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2937" for this suite.
May 19 08:21:00.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:21:01.032: INFO: namespace secrets-2937 deletion completed in 6.072937522s

• [SLOW TEST:10.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:21:01.033: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 19 08:21:01.059: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
May 19 08:21:01.472: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 19 08:21:12.532: INFO: Waited 9.018627716s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:21:12.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9511" for this suite.
May 19 08:21:19.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:21:19.181: INFO: namespace aggregator-9511 deletion completed in 6.164120197s

• [SLOW TEST:18.149 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:21:19.181: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7453
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7453
I0519 08:21:19.229192      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-7453, replica count: 2
I0519 08:21:22.279625      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 19 08:21:22.279: INFO: Creating new exec pod
May 19 08:21:25.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-7453 execpodgjfxh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 19 08:21:25.720: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 19 08:21:25.720: INFO: stdout: ""
May 19 08:21:25.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-7453 execpodgjfxh -- /bin/sh -x -c nc -zv -t -w 2 10.106.100.77 80'
May 19 08:21:25.999: INFO: stderr: "+ nc -zv -t -w 2 10.106.100.77 80\nConnection to 10.106.100.77 80 port [tcp/http] succeeded!\n"
May 19 08:21:25.999: INFO: stdout: ""
May 19 08:21:25.999: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:21:26.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7453" for this suite.
May 19 08:21:32.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:21:32.098: INFO: namespace services-7453 deletion completed in 6.076022596s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.917 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:21:32.098: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
May 19 08:21:32.126: INFO: Waiting up to 1m0s for all nodes to be ready
May 19 08:22:32.155: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:22:32.158: INFO: Starting informer...
STEP: Starting pod...
May 19 08:22:32.368: INFO: Pod is running on nchc-kubemaster01. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 19 08:22:32.379: INFO: Pod wasn't evicted. Proceeding
May 19 08:22:32.379: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 19 08:23:47.389: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:23:47.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9539" for this suite.
May 19 08:24:15.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:24:15.471: INFO: namespace taint-single-pod-9539 deletion completed in 28.076519304s

• [SLOW TEST:163.372 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:24:15.472: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 08:24:15.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635" in namespace "projected-7489" to be "success or failure"
May 19 08:24:15.506: INFO: Pod "downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635": Phase="Pending", Reason="", readiness=false. Elapsed: 1.979051ms
May 19 08:24:17.509: INFO: Pod "downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005021997s
May 19 08:24:19.513: INFO: Pod "downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008998105s
STEP: Saw pod success
May 19 08:24:19.513: INFO: Pod "downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635" satisfied condition "success or failure"
May 19 08:24:19.516: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635 container client-container: <nil>
STEP: delete the pod
May 19 08:24:19.543: INFO: Waiting for pod downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635 to disappear
May 19 08:24:19.545: INFO: Pod downwardapi-volume-61f11b61-8896-4016-8908-729da76ae635 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:24:19.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7489" for this suite.
May 19 08:24:25.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:24:25.630: INFO: namespace projected-7489 deletion completed in 6.080907786s

• [SLOW TEST:10.158 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:24:25.630: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-4c508566-587b-433e-81aa-f7caec00b7ea
STEP: Creating secret with name s-test-opt-upd-a802e431-e450-4ee3-8bca-e14256ed8853
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4c508566-587b-433e-81aa-f7caec00b7ea
STEP: Updating secret s-test-opt-upd-a802e431-e450-4ee3-8bca-e14256ed8853
STEP: Creating secret with name s-test-opt-create-b3b630aa-fbe6-4f0a-8d1c-0164607f5442
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:26:00.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-692" for this suite.
May 19 08:26:12.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:26:12.280: INFO: namespace projected-692 deletion completed in 12.074671048s

• [SLOW TEST:106.650 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:26:12.281: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-d776e0d8-c6fe-4f0b-980b-91685d53dc35
STEP: Creating secret with name s-test-opt-upd-fc8e48ec-9a98-40ef-94cc-93c103ce0429
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d776e0d8-c6fe-4f0b-980b-91685d53dc35
STEP: Updating secret s-test-opt-upd-fc8e48ec-9a98-40ef-94cc-93c103ce0429
STEP: Creating secret with name s-test-opt-create-b1f30677-2e92-4f5a-aad1-ccca8810a322
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:26:20.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9551" for this suite.
May 19 08:26:48.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:26:48.495: INFO: namespace secrets-9551 deletion completed in 28.074555279s

• [SLOW TEST:36.215 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:26:48.496: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:26:48.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3083" for this suite.
May 19 08:26:54.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:26:54.607: INFO: namespace tables-3083 deletion completed in 6.078285919s

• [SLOW TEST:6.111 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:26:54.608: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 19 08:26:54.655: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:54.655: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:54.655: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:54.657: INFO: Number of nodes with available pods: 0
May 19 08:26:54.657: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:26:55.661: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:55.662: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:55.662: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:55.664: INFO: Number of nodes with available pods: 0
May 19 08:26:55.664: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:26:56.661: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:56.661: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:56.661: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:56.664: INFO: Number of nodes with available pods: 1
May 19 08:26:56.664: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 08:26:57.662: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:57.663: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:57.663: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:57.666: INFO: Number of nodes with available pods: 2
May 19 08:26:57.666: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 19 08:26:57.679: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:57.679: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:57.679: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:57.682: INFO: Number of nodes with available pods: 1
May 19 08:26:57.682: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:26:58.687: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:58.687: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:58.687: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:58.690: INFO: Number of nodes with available pods: 1
May 19 08:26:58.690: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:26:59.686: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:59.686: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:59.686: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:26:59.689: INFO: Number of nodes with available pods: 1
May 19 08:26:59.689: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:27:00.687: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:00.687: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:00.687: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:00.690: INFO: Number of nodes with available pods: 1
May 19 08:27:00.690: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:27:01.686: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:01.686: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:01.686: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:01.688: INFO: Number of nodes with available pods: 1
May 19 08:27:01.688: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:27:02.687: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:02.687: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:02.687: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:02.690: INFO: Number of nodes with available pods: 1
May 19 08:27:02.691: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:27:03.687: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:03.687: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:03.687: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:03.690: INFO: Number of nodes with available pods: 1
May 19 08:27:03.690: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:27:04.687: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:04.687: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:04.687: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:04.690: INFO: Number of nodes with available pods: 1
May 19 08:27:04.690: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:27:05.688: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:05.688: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:05.688: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:05.691: INFO: Number of nodes with available pods: 1
May 19 08:27:05.691: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:27:06.688: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:06.688: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:06.688: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:27:06.691: INFO: Number of nodes with available pods: 2
May 19 08:27:06.691: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2834, will wait for the garbage collector to delete the pods
May 19 08:27:06.753: INFO: Deleting DaemonSet.extensions daemon-set took: 6.079004ms
May 19 08:27:07.053: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.271486ms
May 19 08:27:15.255: INFO: Number of nodes with available pods: 0
May 19 08:27:15.255: INFO: Number of running nodes: 0, number of available pods: 0
May 19 08:27:15.260: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2834/daemonsets","resourceVersion":"1099629"},"items":null}

May 19 08:27:15.262: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2834/pods","resourceVersion":"1099629"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:27:15.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2834" for this suite.
May 19 08:27:21.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:27:21.354: INFO: namespace daemonsets-2834 deletion completed in 6.080762038s

• [SLOW TEST:26.747 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:27:21.355: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 19 08:27:21.387: INFO: Waiting up to 5m0s for pod "pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65" in namespace "emptydir-8800" to be "success or failure"
May 19 08:27:21.389: INFO: Pod "pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030107ms
May 19 08:27:23.393: INFO: Pod "pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00630535s
May 19 08:27:25.397: INFO: Pod "pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010455086s
STEP: Saw pod success
May 19 08:27:25.397: INFO: Pod "pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65" satisfied condition "success or failure"
May 19 08:27:25.399: INFO: Trying to get logs from node nchc-worker03 pod pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65 container test-container: <nil>
STEP: delete the pod
May 19 08:27:25.415: INFO: Waiting for pod pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65 to disappear
May 19 08:27:25.417: INFO: Pod pod-1de8c05a-85c3-42fe-b1fb-bdc784f76f65 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:27:25.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8800" for this suite.
May 19 08:27:31.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:27:31.500: INFO: namespace emptydir-8800 deletion completed in 6.079667609s

• [SLOW TEST:10.145 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:27:31.501: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 19 08:27:31.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6758'
May 19 08:27:31.645: INFO: stderr: ""
May 19 08:27:31.645: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
May 19 08:27:31.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete pods e2e-test-httpd-pod --namespace=kubectl-6758'
May 19 08:27:45.254: INFO: stderr: ""
May 19 08:27:45.254: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:27:45.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6758" for this suite.
May 19 08:27:51.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:27:51.332: INFO: namespace kubectl-6758 deletion completed in 6.073793941s

• [SLOW TEST:19.832 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:27:51.333: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:28:04.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2514" for this suite.
May 19 08:28:10.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:28:10.485: INFO: namespace resourcequota-2514 deletion completed in 6.073290308s

• [SLOW TEST:19.151 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:28:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 19 08:28:10.533: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:10.533: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:10.533: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:10.535: INFO: Number of nodes with available pods: 0
May 19 08:28:10.535: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:28:11.540: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:11.540: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:11.540: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:11.543: INFO: Number of nodes with available pods: 0
May 19 08:28:11.543: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 08:28:12.540: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:12.540: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:12.540: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:12.543: INFO: Number of nodes with available pods: 1
May 19 08:28:12.543: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 08:28:13.540: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:13.540: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:13.540: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:13.543: INFO: Number of nodes with available pods: 2
May 19 08:28:13.543: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 19 08:28:13.556: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:13.556: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:13.556: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:13.559: INFO: Number of nodes with available pods: 1
May 19 08:28:13.559: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 08:28:14.565: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:14.565: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:14.566: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:14.569: INFO: Number of nodes with available pods: 1
May 19 08:28:14.569: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 08:28:15.565: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:15.565: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:15.565: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:15.568: INFO: Number of nodes with available pods: 1
May 19 08:28:15.568: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 08:28:16.564: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:16.565: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:16.565: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 08:28:16.567: INFO: Number of nodes with available pods: 2
May 19 08:28:16.567: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9755, will wait for the garbage collector to delete the pods
May 19 08:28:16.631: INFO: Deleting DaemonSet.extensions daemon-set took: 6.815981ms
May 19 08:28:16.931: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.184811ms
May 19 08:28:23.934: INFO: Number of nodes with available pods: 0
May 19 08:28:23.934: INFO: Number of running nodes: 0, number of available pods: 0
May 19 08:28:23.936: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9755/daemonsets","resourceVersion":"1099949"},"items":null}

May 19 08:28:23.939: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9755/pods","resourceVersion":"1099949"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:28:23.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9755" for this suite.
May 19 08:28:29.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:28:30.025: INFO: namespace daemonsets-9755 deletion completed in 6.074760488s

• [SLOW TEST:19.540 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:28:30.025: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:28:46.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8193" for this suite.
May 19 08:28:52.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:28:52.199: INFO: namespace resourcequota-8193 deletion completed in 6.07483075s

• [SLOW TEST:22.174 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:28:52.199: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
May 19 08:28:52.232: INFO: Waiting up to 5m0s for pod "client-containers-604fadf0-d6a8-4f3f-848e-a7bc102f46d4" in namespace "containers-6375" to be "success or failure"
May 19 08:28:52.234: INFO: Pod "client-containers-604fadf0-d6a8-4f3f-848e-a7bc102f46d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057624ms
May 19 08:28:54.237: INFO: Pod "client-containers-604fadf0-d6a8-4f3f-848e-a7bc102f46d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004882172s
STEP: Saw pod success
May 19 08:28:54.237: INFO: Pod "client-containers-604fadf0-d6a8-4f3f-848e-a7bc102f46d4" satisfied condition "success or failure"
May 19 08:28:54.239: INFO: Trying to get logs from node nchc-kubemaster01 pod client-containers-604fadf0-d6a8-4f3f-848e-a7bc102f46d4 container test-container: <nil>
STEP: delete the pod
May 19 08:28:54.263: INFO: Waiting for pod client-containers-604fadf0-d6a8-4f3f-848e-a7bc102f46d4 to disappear
May 19 08:28:54.265: INFO: Pod client-containers-604fadf0-d6a8-4f3f-848e-a7bc102f46d4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:28:54.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6375" for this suite.
May 19 08:29:00.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:29:00.345: INFO: namespace containers-6375 deletion completed in 6.075916309s

• [SLOW TEST:8.145 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:29:00.345: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:29:00.371: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Creating first CR 
May 19 08:29:00.901: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-19T08:29:00Z generation:1 name:name1 resourceVersion:1100151 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d125ed52-3d26-4a4e-9e7d-3cdae9a07657] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 19 08:29:10.907: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-19T08:29:10Z generation:1 name:name2 resourceVersion:1100171 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:77ee7388-5d93-4097-b2d2-f8b12cad2849] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 19 08:29:20.914: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-19T08:29:00Z generation:2 name:name1 resourceVersion:1100191 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d125ed52-3d26-4a4e-9e7d-3cdae9a07657] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 19 08:29:30.921: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-19T08:29:10Z generation:2 name:name2 resourceVersion:1100212 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:77ee7388-5d93-4097-b2d2-f8b12cad2849] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 19 08:29:40.928: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-19T08:29:00Z generation:2 name:name1 resourceVersion:1100231 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:d125ed52-3d26-4a4e-9e7d-3cdae9a07657] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 19 08:29:50.938: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-05-19T08:29:10Z generation:2 name:name2 resourceVersion:1100251 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:77ee7388-5d93-4097-b2d2-f8b12cad2849] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:30:01.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2165" for this suite.
May 19 08:30:07.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:30:07.528: INFO: namespace crd-watch-2165 deletion completed in 6.077040281s

• [SLOW TEST:67.184 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:30:07.529: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 19 08:30:07.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-369'
May 19 08:30:07.667: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 19 08:30:07.667: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
May 19 08:30:07.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete jobs e2e-test-httpd-job --namespace=kubectl-369'
May 19 08:30:07.774: INFO: stderr: ""
May 19 08:30:07.774: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:30:07.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-369" for this suite.
May 19 08:30:13.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:30:13.853: INFO: namespace kubectl-369 deletion completed in 6.074805816s

• [SLOW TEST:6.325 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:30:13.853: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 08:30:13.886: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c" in namespace "downward-api-4798" to be "success or failure"
May 19 08:30:13.888: INFO: Pod "downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.874358ms
May 19 08:30:15.891: INFO: Pod "downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005692762s
May 19 08:30:17.895: INFO: Pod "downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009042525s
STEP: Saw pod success
May 19 08:30:17.895: INFO: Pod "downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c" satisfied condition "success or failure"
May 19 08:30:17.897: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c container client-container: <nil>
STEP: delete the pod
May 19 08:30:17.924: INFO: Waiting for pod downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c to disappear
May 19 08:30:17.927: INFO: Pod downwardapi-volume-fb063ea0-52fc-4ed9-a5a2-f42371649b6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:30:17.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4798" for this suite.
May 19 08:30:23.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:30:24.016: INFO: namespace downward-api-4798 deletion completed in 6.086143038s

• [SLOW TEST:10.163 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:30:24.017: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-e74b0e7d-f573-4e0a-9a9c-2a8a96a2b90a
STEP: Creating a pod to test consume configMaps
May 19 08:30:24.054: INFO: Waiting up to 5m0s for pod "pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920" in namespace "configmap-6140" to be "success or failure"
May 19 08:30:24.056: INFO: Pod "pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920": Phase="Pending", Reason="", readiness=false. Elapsed: 1.98841ms
May 19 08:30:26.060: INFO: Pod "pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006197094s
May 19 08:30:28.063: INFO: Pod "pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009367013s
STEP: Saw pod success
May 19 08:30:28.063: INFO: Pod "pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920" satisfied condition "success or failure"
May 19 08:30:28.066: INFO: Trying to get logs from node nchc-worker03 pod pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920 container configmap-volume-test: <nil>
STEP: delete the pod
May 19 08:30:28.082: INFO: Waiting for pod pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920 to disappear
May 19 08:30:28.083: INFO: Pod pod-configmaps-54bdc0be-e176-445d-8ae5-4c7b0ec02920 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:30:28.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6140" for this suite.
May 19 08:30:34.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:30:34.165: INFO: namespace configmap-6140 deletion completed in 6.077990905s

• [SLOW TEST:10.149 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:30:34.166: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
May 19 08:30:34.198: INFO: Waiting up to 5m0s for pod "pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3" in namespace "emptydir-6157" to be "success or failure"
May 19 08:30:34.200: INFO: Pod "pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.934213ms
May 19 08:30:36.204: INFO: Pod "pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005697651s
May 19 08:30:38.207: INFO: Pod "pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008829227s
STEP: Saw pod success
May 19 08:30:38.207: INFO: Pod "pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3" satisfied condition "success or failure"
May 19 08:30:38.209: INFO: Trying to get logs from node nchc-worker03 pod pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3 container test-container: <nil>
STEP: delete the pod
May 19 08:30:38.226: INFO: Waiting for pod pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3 to disappear
May 19 08:30:38.228: INFO: Pod pod-1ee808cb-2c3a-40c0-9bc9-b267256309c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:30:38.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6157" for this suite.
May 19 08:30:44.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:30:44.303: INFO: namespace emptydir-6157 deletion completed in 6.07112792s

• [SLOW TEST:10.137 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:30:44.304: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
May 19 08:30:44.849: INFO: created pod pod-service-account-defaultsa
May 19 08:30:44.849: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 19 08:30:44.853: INFO: created pod pod-service-account-mountsa
May 19 08:30:44.853: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 19 08:30:44.856: INFO: created pod pod-service-account-nomountsa
May 19 08:30:44.856: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 19 08:30:44.860: INFO: created pod pod-service-account-defaultsa-mountspec
May 19 08:30:44.860: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 19 08:30:44.864: INFO: created pod pod-service-account-mountsa-mountspec
May 19 08:30:44.864: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 19 08:30:44.869: INFO: created pod pod-service-account-nomountsa-mountspec
May 19 08:30:44.869: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 19 08:30:44.873: INFO: created pod pod-service-account-defaultsa-nomountspec
May 19 08:30:44.873: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 19 08:30:44.878: INFO: created pod pod-service-account-mountsa-nomountspec
May 19 08:30:44.878: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 19 08:30:44.881: INFO: created pod pod-service-account-nomountsa-nomountspec
May 19 08:30:44.881: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:30:44.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6479" for this suite.
May 19 08:30:50.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:30:50.959: INFO: namespace svcaccounts-6479 deletion completed in 6.074076173s

• [SLOW TEST:6.656 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:30:50.960: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 19 08:30:50.991: INFO: Waiting up to 5m0s for pod "downward-api-338771ea-0b31-4eb6-839e-623f51e7f434" in namespace "downward-api-9167" to be "success or failure"
May 19 08:30:50.993: INFO: Pod "downward-api-338771ea-0b31-4eb6-839e-623f51e7f434": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098202ms
May 19 08:30:52.997: INFO: Pod "downward-api-338771ea-0b31-4eb6-839e-623f51e7f434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005573611s
STEP: Saw pod success
May 19 08:30:52.997: INFO: Pod "downward-api-338771ea-0b31-4eb6-839e-623f51e7f434" satisfied condition "success or failure"
May 19 08:30:52.999: INFO: Trying to get logs from node nchc-kubemaster01 pod downward-api-338771ea-0b31-4eb6-839e-623f51e7f434 container dapi-container: <nil>
STEP: delete the pod
May 19 08:30:53.028: INFO: Waiting for pod downward-api-338771ea-0b31-4eb6-839e-623f51e7f434 to disappear
May 19 08:30:53.030: INFO: Pod downward-api-338771ea-0b31-4eb6-839e-623f51e7f434 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:30:53.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9167" for this suite.
May 19 08:30:59.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:30:59.104: INFO: namespace downward-api-9167 deletion completed in 6.07060138s

• [SLOW TEST:8.144 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:30:59.104: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 19 08:30:59.130: INFO: PodSpec: initContainers in spec.initContainers
May 19 08:31:52.506: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8e1a76b8-3e3d-4200-a13e-6bb0c4fad0c7", GenerateName:"", Namespace:"init-container-4542", SelfLink:"/api/v1/namespaces/init-container-4542/pods/pod-init-8e1a76b8-3e3d-4200-a13e-6bb0c4fad0c7", UID:"f4404e7b-20a0-43c9-9219-83c663061036", ResourceVersion:"1100837", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63725473859, loc:(*time.Location)(0x78a2900)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"130936326"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.148.95/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-vmtp8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00187dc40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vmtp8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vmtp8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vmtp8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00398b928), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"nchc-worker03", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0038841e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00398b9c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00398b9e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00398b9e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00398b9ec), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725473846, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725473846, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725473846, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725473859, loc:(*time.Location)(0x78a2900)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.78.26.181", PodIP:"10.244.148.95", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.148.95"}}, StartTime:(*v1.Time)(0xc00394d920), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022e8850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022e88c0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://cb781f70fa58b232e9a214a4d32a25ebfc7a2046e2894bbf2ec0f82d41c0c183", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00394d960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00394d940), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00398ba6f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:31:52.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4542" for this suite.
May 19 08:32:10.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:32:10.588: INFO: namespace init-container-4542 deletion completed in 18.076349318s

• [SLOW TEST:71.483 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:32:10.588: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May 19 08:32:10.614: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:32:36.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1507" for this suite.
May 19 08:32:42.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:32:42.107: INFO: namespace crd-publish-openapi-1507 deletion completed in 6.080004668s

• [SLOW TEST:31.519 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:32:42.108: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-58a61b2e-8072-4fa2-8c29-f88d3f68027f
STEP: Creating a pod to test consume secrets
May 19 08:32:42.143: INFO: Waiting up to 5m0s for pod "pod-secrets-e816e88b-b63f-4e32-a97e-27d94cf1fff7" in namespace "secrets-7100" to be "success or failure"
May 19 08:32:42.145: INFO: Pod "pod-secrets-e816e88b-b63f-4e32-a97e-27d94cf1fff7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.974023ms
May 19 08:32:44.149: INFO: Pod "pod-secrets-e816e88b-b63f-4e32-a97e-27d94cf1fff7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005452715s
STEP: Saw pod success
May 19 08:32:44.149: INFO: Pod "pod-secrets-e816e88b-b63f-4e32-a97e-27d94cf1fff7" satisfied condition "success or failure"
May 19 08:32:44.151: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-secrets-e816e88b-b63f-4e32-a97e-27d94cf1fff7 container secret-volume-test: <nil>
STEP: delete the pod
May 19 08:32:44.178: INFO: Waiting for pod pod-secrets-e816e88b-b63f-4e32-a97e-27d94cf1fff7 to disappear
May 19 08:32:44.180: INFO: Pod pod-secrets-e816e88b-b63f-4e32-a97e-27d94cf1fff7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:32:44.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7100" for this suite.
May 19 08:32:50.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:32:50.253: INFO: namespace secrets-7100 deletion completed in 6.069568342s

• [SLOW TEST:8.145 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:32:50.254: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 19 08:32:52.296: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:32:52.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1583" for this suite.
May 19 08:32:58.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:32:58.382: INFO: namespace container-runtime-1583 deletion completed in 6.072956811s

• [SLOW TEST:8.128 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:32:58.382: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 19 08:32:58.410: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 19 08:32:58.421: INFO: Waiting for terminating namespaces to be deleted...
May 19 08:32:58.423: INFO: 
Logging pods the kubelet thinks is on node nchc-kubemaster01 before test
May 19 08:32:58.428: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-7hsd4 from sonobuoy started at 2020-05-19 08:18:56 +0000 UTC (2 container statuses recorded)
May 19 08:32:58.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 19 08:32:58.428: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 08:32:58.428: INFO: kube-proxy-qg52l from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 08:32:58.428: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 08:32:58.428: INFO: calico-node-7jgfk from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 08:32:58.428: INFO: 	Container calico-node ready: true, restart count 0
May 19 08:32:58.428: INFO: 
Logging pods the kubelet thinks is on node nchc-worker03 before test
May 19 08:32:58.447: INFO: sonobuoy from sonobuoy started at 2020-05-19 08:18:41 +0000 UTC (1 container statuses recorded)
May 19 08:32:58.447: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 19 08:32:58.447: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-hw9m8 from sonobuoy started at 2020-05-19 08:18:43 +0000 UTC (2 container statuses recorded)
May 19 08:32:58.447: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 19 08:32:58.447: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 08:32:58.447: INFO: calico-node-724vj from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 08:32:58.447: INFO: 	Container calico-node ready: true, restart count 0
May 19 08:32:58.447: INFO: kube-proxy-5l969 from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 08:32:58.447: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-331d0978-56cf-446f-809c-4f60e579a47e 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-331d0978-56cf-446f-809c-4f60e579a47e off the node nchc-kubemaster01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-331d0978-56cf-446f-809c-4f60e579a47e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:33:06.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2139" for this suite.
May 19 08:33:34.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:33:34.592: INFO: namespace sched-pred-2139 deletion completed in 28.074750698s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:36.210 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:33:34.592: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2174
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 19 08:33:34.620: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 19 08:33:58.677: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.148.97 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2174 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 08:33:58.677: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 08:33:59.863: INFO: Found all expected endpoints: [netserver-0]
May 19 08:33:59.866: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.161.83 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2174 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 08:33:59.866: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 08:34:01.052: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:34:01.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2174" for this suite.
May 19 08:34:13.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:34:13.135: INFO: namespace pod-network-test-2174 deletion completed in 12.079579985s

• [SLOW TEST:38.543 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:34:13.136: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7112
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May 19 08:34:13.171: INFO: Found 0 stateful pods, waiting for 3
May 19 08:34:23.176: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 19 08:34:23.176: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 19 08:34:23.176: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 19 08:34:23.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7112 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:34:23.581: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:34:23.582: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:34:23.582: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 19 08:34:33.611: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 19 08:34:43.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7112 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:34:43.908: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 19 08:34:43.908: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:34:43.908: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:34:53.927: INFO: Waiting for StatefulSet statefulset-7112/ss2 to complete update
May 19 08:34:53.927: INFO: Waiting for Pod statefulset-7112/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 19 08:34:53.927: INFO: Waiting for Pod statefulset-7112/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 19 08:34:53.927: INFO: Waiting for Pod statefulset-7112/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 19 08:35:03.935: INFO: Waiting for StatefulSet statefulset-7112/ss2 to complete update
May 19 08:35:03.935: INFO: Waiting for Pod statefulset-7112/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 19 08:35:03.935: INFO: Waiting for Pod statefulset-7112/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 19 08:35:13.933: INFO: Waiting for StatefulSet statefulset-7112/ss2 to complete update
STEP: Rolling back to a previous revision
May 19 08:35:23.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7112 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:35:24.265: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:35:24.265: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:35:24.265: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:35:24.289: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 19 08:35:34.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7112 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:35:34.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 19 08:35:34.595: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:35:34.595: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:35:44.616: INFO: Waiting for StatefulSet statefulset-7112/ss2 to complete update
May 19 08:35:44.616: INFO: Waiting for Pod statefulset-7112/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 19 08:35:54.624: INFO: Waiting for StatefulSet statefulset-7112/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 19 08:36:04.625: INFO: Deleting all statefulset in ns statefulset-7112
May 19 08:36:04.628: INFO: Scaling statefulset ss2 to 0
May 19 08:36:24.643: INFO: Waiting for statefulset status.replicas updated to 0
May 19 08:36:24.646: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:36:24.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7112" for this suite.
May 19 08:36:30.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:36:30.760: INFO: namespace statefulset-7112 deletion completed in 6.09525725s

• [SLOW TEST:137.625 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:36:30.761: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:36:37.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1373" for this suite.
May 19 08:36:43.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:36:43.911: INFO: namespace resourcequota-1373 deletion completed in 6.100285966s

• [SLOW TEST:13.150 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:36:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:36:59.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4432" for this suite.
May 19 08:37:06.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:37:06.080: INFO: namespace resourcequota-4432 deletion completed in 6.088313877s

• [SLOW TEST:22.169 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:37:06.080: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:37:06.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-2683'
May 19 08:37:06.387: INFO: stderr: ""
May 19 08:37:06.387: INFO: stdout: "replicationcontroller/redis-master created\n"
May 19 08:37:06.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-2683'
May 19 08:37:06.688: INFO: stderr: ""
May 19 08:37:06.688: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 19 08:37:07.693: INFO: Selector matched 1 pods for map[app:redis]
May 19 08:37:07.693: INFO: Found 0 / 1
May 19 08:37:08.693: INFO: Selector matched 1 pods for map[app:redis]
May 19 08:37:08.693: INFO: Found 1 / 1
May 19 08:37:08.693: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 19 08:37:08.696: INFO: Selector matched 1 pods for map[app:redis]
May 19 08:37:08.696: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 19 08:37:08.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 describe pod redis-master-l2mb2 --namespace=kubectl-2683'
May 19 08:37:08.831: INFO: stderr: ""
May 19 08:37:08.831: INFO: stdout: "Name:         redis-master-l2mb2\nNamespace:    kubectl-2683\nPriority:     0\nNode:         nchc-kubemaster01/10.78.26.171\nStart Time:   Tue, 19 May 2020 08:37:06 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.244.161.118/32\nStatus:       Running\nIP:           10.244.161.118\nIPs:\n  IP:           10.244.161.118\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://940f1f402e399a5ac5649e4f0f64b5a91f365deeee2e5d640bab3485c9a6efc4\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 19 May 2020 08:37:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vz9zq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-vz9zq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vz9zq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                        Message\n  ----    ------     ----       ----                        -------\n  Normal  Scheduled  <unknown>  default-scheduler           Successfully assigned kubectl-2683/redis-master-l2mb2 to nchc-kubemaster01\n  Normal  Pulled     1s         kubelet, nchc-kubemaster01  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, nchc-kubemaster01  Created container redis-master\n  Normal  Started    1s         kubelet, nchc-kubemaster01  Started container redis-master\n"
May 19 08:37:08.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 describe rc redis-master --namespace=kubectl-2683'
May 19 08:37:08.957: INFO: stderr: ""
May 19 08:37:08.957: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2683\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-l2mb2\n"
May 19 08:37:08.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 describe service redis-master --namespace=kubectl-2683'
May 19 08:37:09.080: INFO: stderr: ""
May 19 08:37:09.080: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2683\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.111.197.6\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.161.118:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 19 08:37:09.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 describe node mgmt01'
May 19 08:37:09.219: INFO: stderr: ""
May 19 08:37:09.219: INFO: stdout: "Name:               mgmt01\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=mgmt01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.78.26.187/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.102.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 13 May 2020 03:05:01 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 13 May 2020 03:06:18 +0000   Wed, 13 May 2020 03:06:18 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 19 May 2020 08:36:22 +0000   Wed, 13 May 2020 03:05:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 19 May 2020 08:36:22 +0000   Wed, 13 May 2020 03:05:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 19 May 2020 08:36:22 +0000   Wed, 13 May 2020 03:05:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 19 May 2020 08:36:22 +0000   Wed, 13 May 2020 03:06:22 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.78.26.187\n  Hostname:    mgmt01\nCapacity:\n cpu:                24\n ephemeral-storage:  51175Mi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65751828Ki\n pods:               110\nAllocatable:\n cpu:                24\n ephemeral-storage:  48294789041\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65649428Ki\n pods:               110\nSystem Info:\n Machine ID:                 f1ec15c8143b40c5bdc5617c8cc81d14\n System UUID:                3B81EEC0-5BE4-11D9-BA69-08606E1E4236\n Boot ID:                    2c6c35c2-0e92-4054-97fb-e3a0c3ae27d2\n Kernel Version:             3.10.0-957.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.0\n Kubelet Version:            v1.16.9\n Kube-Proxy Version:         v1.16.9\nPodCIDR:                     10.244.0.0/24\nPodCIDRs:                    10.244.0.0/24\nNon-terminated Pods:         (13 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-kube-controllers-bc44d789c-d7hhd                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                calico-node-j4xt7                                          250m (1%)     0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                coredns-5644d7b6d9-6mxkc                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d5h\n  kube-system                coredns-5644d7b6d9-9jh6r                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d5h\n  kube-system                etcd-mgmt01                                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                kube-apiserver-mgmt01                                      250m (1%)     0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                kube-controller-manager-mgmt01                             200m (0%)     0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                kube-haproxy-mgmt01                                        100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                kube-keepalived-mgmt01                                     100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                kube-proxy-nrvbd                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d5h\n  kube-system                kube-scheduler-mgmt01                                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d5h\n  kubernetes-dashboard       dashboard-metrics-scraper-c79c65bb7-lg96d                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d4h\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-9bjqv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1200m (5%)  0 (0%)\n  memory             140Mi (0%)  340Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
May 19 08:37:09.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 describe namespace kubectl-2683'
May 19 08:37:09.331: INFO: stderr: ""
May 19 08:37:09.331: INFO: stdout: "Name:         kubectl-2683\nLabels:       e2e-framework=kubectl\n              e2e-run=94558208-8ded-493d-85b3-0a7253b84911\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:37:09.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2683" for this suite.
May 19 08:37:37.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:37:37.428: INFO: namespace kubectl-2683 deletion completed in 28.085634434s

• [SLOW TEST:31.348 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:37:37.428: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-a9beb2bb-12cc-48cd-a14d-1cddd1801834
STEP: Creating a pod to test consume secrets
May 19 08:37:37.467: INFO: Waiting up to 5m0s for pod "pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62" in namespace "secrets-8809" to be "success or failure"
May 19 08:37:37.469: INFO: Pod "pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274345ms
May 19 08:37:39.475: INFO: Pod "pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007582765s
May 19 08:37:41.479: INFO: Pod "pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012216439s
STEP: Saw pod success
May 19 08:37:41.479: INFO: Pod "pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62" satisfied condition "success or failure"
May 19 08:37:41.482: INFO: Trying to get logs from node nchc-worker03 pod pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62 container secret-volume-test: <nil>
STEP: delete the pod
May 19 08:37:41.514: INFO: Waiting for pod pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62 to disappear
May 19 08:37:41.516: INFO: Pod pod-secrets-98fe1f73-32fa-42d3-accc-fab09435cf62 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:37:41.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8809" for this suite.
May 19 08:37:47.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:37:47.610: INFO: namespace secrets-8809 deletion completed in 6.088716238s

• [SLOW TEST:10.182 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:37:47.610: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-00175592-ec54-4558-bf02-e941dd0c101e
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:37:47.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2620" for this suite.
May 19 08:37:53.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:37:53.733: INFO: namespace secrets-2620 deletion completed in 6.09047794s

• [SLOW TEST:6.123 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:37:53.734: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4221
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-4221
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4221
May 19 08:37:53.774: INFO: Found 0 stateful pods, waiting for 1
May 19 08:38:03.781: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 19 08:38:03.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-4221 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:38:04.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:38:04.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:38:04.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:38:04.363: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:38:04.363: INFO: Waiting for statefulset status.replicas updated to 0
May 19 08:38:04.376: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May 19 08:38:04.376: INFO: ss-0  nchc-worker03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:04.376: INFO: 
May 19 08:38:04.376: INFO: StatefulSet ss has not reached scale 3, at 1
May 19 08:38:05.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997088805s
May 19 08:38:06.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992953275s
May 19 08:38:07.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987913915s
May 19 08:38:08.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983384264s
May 19 08:38:09.399: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978754809s
May 19 08:38:10.403: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974186815s
May 19 08:38:11.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969654929s
May 19 08:38:12.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964439427s
May 19 08:38:13.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.97382ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4221
May 19 08:38:14.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-4221 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:38:14.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 19 08:38:14.846: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:38:14.846: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:38:14.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-4221 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:38:15.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 19 08:38:15.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:38:15.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:38:15.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-4221 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:38:15.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 19 08:38:15.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:38:15.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:38:15.554: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 19 08:38:15.554: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 19 08:38:15.554: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 19 08:38:15.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-4221 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:38:15.979: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:38:15.979: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:38:15.979: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:38:15.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-4221 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:38:16.293: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:38:16.293: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:38:16.293: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:38:16.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-4221 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:38:16.782: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:38:16.782: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:38:16.782: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:38:16.782: INFO: Waiting for statefulset status.replicas updated to 0
May 19 08:38:16.786: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 19 08:38:26.795: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:38:26.795: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:38:26.795: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:38:26.805: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
May 19 08:38:26.806: INFO: ss-0  nchc-worker03      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:26.806: INFO: ss-1  nchc-kubemaster01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  }]
May 19 08:38:26.806: INFO: ss-2  nchc-worker03      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  }]
May 19 08:38:26.806: INFO: 
May 19 08:38:26.806: INFO: StatefulSet ss has not reached scale 0, at 3
May 19 08:38:27.813: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
May 19 08:38:27.813: INFO: ss-0  nchc-worker03      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:27.813: INFO: ss-1  nchc-kubemaster01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  }]
May 19 08:38:27.813: INFO: ss-2  nchc-worker03      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  }]
May 19 08:38:27.813: INFO: 
May 19 08:38:27.813: INFO: StatefulSet ss has not reached scale 0, at 3
May 19 08:38:28.819: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
May 19 08:38:28.819: INFO: ss-0  nchc-worker03      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:28.819: INFO: ss-1  nchc-kubemaster01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  }]
May 19 08:38:28.819: INFO: ss-2  nchc-worker03      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  }]
May 19 08:38:28.819: INFO: 
May 19 08:38:28.819: INFO: StatefulSet ss has not reached scale 0, at 3
May 19 08:38:29.825: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May 19 08:38:29.825: INFO: ss-0  nchc-worker03  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:29.825: INFO: ss-2  nchc-worker03  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:04 +0000 UTC  }]
May 19 08:38:29.825: INFO: 
May 19 08:38:29.825: INFO: StatefulSet ss has not reached scale 0, at 2
May 19 08:38:30.831: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May 19 08:38:30.831: INFO: ss-0  nchc-worker03  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:30.831: INFO: 
May 19 08:38:30.831: INFO: StatefulSet ss has not reached scale 0, at 1
May 19 08:38:31.837: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May 19 08:38:31.837: INFO: ss-0  nchc-worker03  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:31.837: INFO: 
May 19 08:38:31.837: INFO: StatefulSet ss has not reached scale 0, at 1
May 19 08:38:32.842: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May 19 08:38:32.842: INFO: ss-0  nchc-worker03  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:32.842: INFO: 
May 19 08:38:32.842: INFO: StatefulSet ss has not reached scale 0, at 1
May 19 08:38:33.847: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May 19 08:38:33.847: INFO: ss-0  nchc-worker03  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:38:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-19 08:37:53 +0000 UTC  }]
May 19 08:38:33.847: INFO: 
May 19 08:38:33.847: INFO: StatefulSet ss has not reached scale 0, at 1
May 19 08:38:34.852: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.954463921s
May 19 08:38:35.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.698343ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4221
May 19 08:38:36.862: INFO: Scaling statefulset ss to 0
May 19 08:38:36.872: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 19 08:38:36.875: INFO: Deleting all statefulset in ns statefulset-4221
May 19 08:38:36.878: INFO: Scaling statefulset ss to 0
May 19 08:38:36.886: INFO: Waiting for statefulset status.replicas updated to 0
May 19 08:38:36.888: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:38:36.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4221" for this suite.
May 19 08:38:42.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:38:42.994: INFO: namespace statefulset-4221 deletion completed in 6.090701785s

• [SLOW TEST:49.260 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:38:42.994: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-4335
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4335 to expose endpoints map[]
May 19 08:38:43.041: INFO: Get endpoints failed (2.314294ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 19 08:38:44.043: INFO: successfully validated that service endpoint-test2 in namespace services-4335 exposes endpoints map[] (1.005140841s elapsed)
STEP: Creating pod pod1 in namespace services-4335
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4335 to expose endpoints map[pod1:[80]]
May 19 08:38:46.068: INFO: successfully validated that service endpoint-test2 in namespace services-4335 exposes endpoints map[pod1:[80]] (2.017796714s elapsed)
STEP: Creating pod pod2 in namespace services-4335
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4335 to expose endpoints map[pod1:[80] pod2:[80]]
May 19 08:38:49.109: INFO: successfully validated that service endpoint-test2 in namespace services-4335 exposes endpoints map[pod1:[80] pod2:[80]] (3.036361105s elapsed)
STEP: Deleting pod pod1 in namespace services-4335
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4335 to expose endpoints map[pod2:[80]]
May 19 08:38:50.126: INFO: successfully validated that service endpoint-test2 in namespace services-4335 exposes endpoints map[pod2:[80]] (1.010330682s elapsed)
STEP: Deleting pod pod2 in namespace services-4335
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4335 to expose endpoints map[]
May 19 08:38:51.138: INFO: successfully validated that service endpoint-test2 in namespace services-4335 exposes endpoints map[] (1.006621846s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:38:51.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4335" for this suite.
May 19 08:39:19.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:39:19.265: INFO: namespace services-4335 deletion completed in 28.094416126s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:36.271 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:39:19.265: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:39:19.294: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 19 08:39:19.302: INFO: Pod name sample-pod: Found 0 pods out of 1
May 19 08:39:24.307: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 19 08:39:24.307: INFO: Creating deployment "test-rolling-update-deployment"
May 19 08:39:24.312: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 19 08:39:24.318: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 19 08:39:26.324: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 19 08:39:26.327: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 19 08:39:26.335: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4415 /apis/apps/v1/namespaces/deployment-4415/deployments/test-rolling-update-deployment 14b26047-af51-45e5-b0b6-c137941f14a2 1102872 1 2020-05-19 08:39:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a33818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-19 08:39:24 +0000 UTC,LastTransitionTime:2020-05-19 08:39:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-05-19 08:39:26 +0000 UTC,LastTransitionTime:2020-05-19 08:39:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 19 08:39:26.339: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-4415 /apis/apps/v1/namespaces/deployment-4415/replicasets/test-rolling-update-deployment-55d946486 6ad9d125-4b4c-4c71-abd2-61db4953d5f1 1102861 1 2020-05-19 08:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 14b26047-af51-45e5-b0b6-c137941f14a2 0xc003a33cf0 0xc003a33cf1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a33d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 19 08:39:26.339: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 19 08:39:26.339: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4415 /apis/apps/v1/namespaces/deployment-4415/replicasets/test-rolling-update-controller 1ed91952-cd9f-4d47-adc0-7de6082eb4b7 1102870 2 2020-05-19 08:39:19 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 14b26047-af51-45e5-b0b6-c137941f14a2 0xc003a33c27 0xc003a33c28}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a33c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 19 08:39:26.342: INFO: Pod "test-rolling-update-deployment-55d946486-2n77l" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-2n77l test-rolling-update-deployment-55d946486- deployment-4415 /api/v1/namespaces/deployment-4415/pods/test-rolling-update-deployment-55d946486-2n77l 0ffd51f1-4775-4c7f-8a79-d811ae5a249b 1102860 0 2020-05-19 08:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:10.244.161.119/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 6ad9d125-4b4c-4c71-abd2-61db4953d5f1 0xc001ae6460 0xc001ae6461}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ktfnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ktfnn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ktfnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:39:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:39:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:39:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:10.244.161.119,StartTime:2020-05-19 08:39:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:39:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://72570cdbb05e1f4c3335f2de44df3d5b54fd953f20a56347a40b6f5e1e37f9af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.161.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:39:26.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4415" for this suite.
May 19 08:39:32.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:39:32.435: INFO: namespace deployment-4415 deletion completed in 6.088974515s

• [SLOW TEST:13.170 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:39:32.435: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 19 08:39:32.472: INFO: Waiting up to 5m0s for pod "downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789" in namespace "downward-api-1158" to be "success or failure"
May 19 08:39:32.474: INFO: Pod "downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262192ms
May 19 08:39:34.478: INFO: Pod "downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00671225s
May 19 08:39:36.483: INFO: Pod "downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011595261s
STEP: Saw pod success
May 19 08:39:36.483: INFO: Pod "downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789" satisfied condition "success or failure"
May 19 08:39:36.486: INFO: Trying to get logs from node nchc-worker03 pod downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789 container dapi-container: <nil>
STEP: delete the pod
May 19 08:39:36.519: INFO: Waiting for pod downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789 to disappear
May 19 08:39:36.522: INFO: Pod downward-api-3dd0dd14-ca56-46c0-84e3-bd14e6263789 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:39:36.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1158" for this suite.
May 19 08:39:42.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:39:42.612: INFO: namespace downward-api-1158 deletion completed in 6.086066771s

• [SLOW TEST:10.177 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:39:42.612: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
May 19 08:39:43.681: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0519 08:39:43.681308      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:39:43.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8295" for this suite.
May 19 08:39:49.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:39:49.772: INFO: namespace gc-8295 deletion completed in 6.087671257s

• [SLOW TEST:7.161 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:39:49.773: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1167.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1167.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1167.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1167.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1167.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1167.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 08:39:53.839: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-1167/dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d: the server could not find the requested resource (get pods dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d)
May 19 08:39:53.841: INFO: Unable to read jessie_udp@PodARecord from pod dns-1167/dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d: the server could not find the requested resource (get pods dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d)
May 19 08:39:53.844: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1167/dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d: the server could not find the requested resource (get pods dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d)
May 19 08:39:53.844: INFO: Lookups using dns-1167/dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

May 19 08:39:58.873: INFO: DNS probes using dns-1167/dns-test-2bae07b6-4aee-44a6-abb9-5bd63e21269d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:39:58.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1167" for this suite.
May 19 08:40:04.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:40:04.977: INFO: namespace dns-1167 deletion completed in 6.089465227s

• [SLOW TEST:15.205 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:40:04.978: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
May 19 08:40:05.004: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-934715428 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:40:05.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9397" for this suite.
May 19 08:40:11.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:40:11.192: INFO: namespace kubectl-9397 deletion completed in 6.095630976s

• [SLOW TEST:6.214 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:40:11.192: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-6c8985ea-2da7-4742-b0ca-9aef1b1dabf0
STEP: Creating a pod to test consume configMaps
May 19 08:40:11.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-734cabb6-2e2a-4378-9990-53719602431f" in namespace "configmap-1548" to be "success or failure"
May 19 08:40:11.234: INFO: Pod "pod-configmaps-734cabb6-2e2a-4378-9990-53719602431f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.367863ms
May 19 08:40:13.240: INFO: Pod "pod-configmaps-734cabb6-2e2a-4378-9990-53719602431f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007768475s
STEP: Saw pod success
May 19 08:40:13.240: INFO: Pod "pod-configmaps-734cabb6-2e2a-4378-9990-53719602431f" satisfied condition "success or failure"
May 19 08:40:13.243: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-configmaps-734cabb6-2e2a-4378-9990-53719602431f container configmap-volume-test: <nil>
STEP: delete the pod
May 19 08:40:13.278: INFO: Waiting for pod pod-configmaps-734cabb6-2e2a-4378-9990-53719602431f to disappear
May 19 08:40:13.281: INFO: Pod pod-configmaps-734cabb6-2e2a-4378-9990-53719602431f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:40:13.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1548" for this suite.
May 19 08:40:19.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:40:19.372: INFO: namespace configmap-1548 deletion completed in 6.087354522s

• [SLOW TEST:8.180 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:40:19.373: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-16da6d92-787f-4252-a1f2-d1d5f9d1ee2d
STEP: Creating a pod to test consume secrets
May 19 08:40:19.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b772fe9-0fdf-47be-b349-f9e723dace26" in namespace "projected-4923" to be "success or failure"
May 19 08:40:19.414: INFO: Pod "pod-projected-secrets-0b772fe9-0fdf-47be-b349-f9e723dace26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640668ms
May 19 08:40:21.419: INFO: Pod "pod-projected-secrets-0b772fe9-0fdf-47be-b349-f9e723dace26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006825186s
STEP: Saw pod success
May 19 08:40:21.419: INFO: Pod "pod-projected-secrets-0b772fe9-0fdf-47be-b349-f9e723dace26" satisfied condition "success or failure"
May 19 08:40:21.421: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-secrets-0b772fe9-0fdf-47be-b349-f9e723dace26 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 19 08:40:21.439: INFO: Waiting for pod pod-projected-secrets-0b772fe9-0fdf-47be-b349-f9e723dace26 to disappear
May 19 08:40:21.441: INFO: Pod pod-projected-secrets-0b772fe9-0fdf-47be-b349-f9e723dace26 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:40:21.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4923" for this suite.
May 19 08:40:27.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:40:27.537: INFO: namespace projected-4923 deletion completed in 6.092150035s

• [SLOW TEST:8.164 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:40:27.538: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
May 19 08:40:27.574: INFO: Waiting up to 5m0s for pod "client-containers-e84f2087-da45-446f-82d6-f17b353252d6" in namespace "containers-6226" to be "success or failure"
May 19 08:40:27.576: INFO: Pod "client-containers-e84f2087-da45-446f-82d6-f17b353252d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133124ms
May 19 08:40:29.580: INFO: Pod "client-containers-e84f2087-da45-446f-82d6-f17b353252d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005636958s
May 19 08:40:31.584: INFO: Pod "client-containers-e84f2087-da45-446f-82d6-f17b353252d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009902144s
STEP: Saw pod success
May 19 08:40:31.584: INFO: Pod "client-containers-e84f2087-da45-446f-82d6-f17b353252d6" satisfied condition "success or failure"
May 19 08:40:31.586: INFO: Trying to get logs from node nchc-worker03 pod client-containers-e84f2087-da45-446f-82d6-f17b353252d6 container test-container: <nil>
STEP: delete the pod
May 19 08:40:31.605: INFO: Waiting for pod client-containers-e84f2087-da45-446f-82d6-f17b353252d6 to disappear
May 19 08:40:31.607: INFO: Pod client-containers-e84f2087-da45-446f-82d6-f17b353252d6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:40:31.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6226" for this suite.
May 19 08:40:37.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:40:37.697: INFO: namespace containers-6226 deletion completed in 6.085201843s

• [SLOW TEST:10.159 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:40:37.697: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4017.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4017.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4017.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 08:40:39.757: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.761: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.764: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.767: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.776: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.779: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.782: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.785: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:39.792: INFO: Lookups using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local]

May 19 08:40:44.798: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.802: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.805: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.808: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.817: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.820: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.823: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.826: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:44.832: INFO: Lookups using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local]

May 19 08:40:49.797: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.800: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.804: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.807: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.816: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.819: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.822: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.825: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:49.831: INFO: Lookups using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local]

May 19 08:40:54.797: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.802: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.805: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.808: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.818: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.821: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.824: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.827: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:54.833: INFO: Lookups using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local]

May 19 08:40:59.797: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.800: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.803: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.806: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.816: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.818: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.821: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.824: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:40:59.830: INFO: Lookups using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local]

May 19 08:41:04.797: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.801: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.805: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.808: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.817: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.820: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.823: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.826: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:04.832: INFO: Lookups using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4017.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local]

May 19 08:41:09.807: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:09.816: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:09.819: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:09.822: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:09.824: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local from pod dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374: the server could not find the requested resource (get pods dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374)
May 19 08:41:09.830: INFO: Lookups using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 failed for: [wheezy_tcp@dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4017.svc.cluster.local jessie_udp@dns-test-service-2.dns-4017.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4017.svc.cluster.local]

May 19 08:41:14.832: INFO: DNS probes using dns-4017/dns-test-0efbee67-5cc7-46e9-b590-b80abfe80374 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:41:14.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4017" for this suite.
May 19 08:41:20.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:41:20.948: INFO: namespace dns-4017 deletion completed in 6.090104843s

• [SLOW TEST:43.250 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:41:20.948: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 19 08:41:25.013: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 19 08:41:25.016: INFO: Pod pod-with-prestop-exec-hook still exists
May 19 08:41:27.016: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 19 08:41:27.020: INFO: Pod pod-with-prestop-exec-hook still exists
May 19 08:41:29.016: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 19 08:41:29.020: INFO: Pod pod-with-prestop-exec-hook still exists
May 19 08:41:31.016: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 19 08:41:31.020: INFO: Pod pod-with-prestop-exec-hook still exists
May 19 08:41:33.016: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 19 08:41:33.021: INFO: Pod pod-with-prestop-exec-hook still exists
May 19 08:41:35.016: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 19 08:41:35.022: INFO: Pod pod-with-prestop-exec-hook still exists
May 19 08:41:37.016: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 19 08:41:37.021: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:41:37.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2242" for this suite.
May 19 08:41:49.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:41:49.124: INFO: namespace container-lifecycle-hook-2242 deletion completed in 12.089089693s

• [SLOW TEST:28.175 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:41:49.124: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 08:41:49.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff" in namespace "downward-api-360" to be "success or failure"
May 19 08:41:49.165: INFO: Pod "downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.308288ms
May 19 08:41:51.169: INFO: Pod "downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006264304s
May 19 08:41:53.173: INFO: Pod "downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010509398s
STEP: Saw pod success
May 19 08:41:53.173: INFO: Pod "downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff" satisfied condition "success or failure"
May 19 08:41:53.177: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff container client-container: <nil>
STEP: delete the pod
May 19 08:41:53.197: INFO: Waiting for pod downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff to disappear
May 19 08:41:53.199: INFO: Pod downwardapi-volume-e8f99a32-384f-41d9-898d-b942100110ff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:41:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-360" for this suite.
May 19 08:41:59.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:41:59.290: INFO: namespace downward-api-360 deletion completed in 6.086586028s

• [SLOW TEST:10.166 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:41:59.290: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
May 19 08:41:59.320: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-934715428 proxy --unix-socket=/tmp/kubectl-proxy-unix271056919/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:41:59.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2643" for this suite.
May 19 08:42:05.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:42:05.492: INFO: namespace kubectl-2643 deletion completed in 6.0917487s

• [SLOW TEST:6.202 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:42:05.492: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:42:05.531: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3305a157-9f4f-4ed2-b823-88dd9fefefb8" in namespace "security-context-test-2667" to be "success or failure"
May 19 08:42:05.533: INFO: Pod "busybox-readonly-false-3305a157-9f4f-4ed2-b823-88dd9fefefb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256953ms
May 19 08:42:07.536: INFO: Pod "busybox-readonly-false-3305a157-9f4f-4ed2-b823-88dd9fefefb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005336659s
May 19 08:42:09.541: INFO: Pod "busybox-readonly-false-3305a157-9f4f-4ed2-b823-88dd9fefefb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01008322s
May 19 08:42:09.541: INFO: Pod "busybox-readonly-false-3305a157-9f4f-4ed2-b823-88dd9fefefb8" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:42:09.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2667" for this suite.
May 19 08:42:15.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:42:15.642: INFO: namespace security-context-test-2667 deletion completed in 6.095835941s

• [SLOW TEST:10.149 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:42:15.642: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:42:15.670: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:42:21.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8443" for this suite.
May 19 08:42:27.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:42:28.007: INFO: namespace custom-resource-definition-8443 deletion completed in 6.087246945s

• [SLOW TEST:12.365 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:42:28.007: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 08:42:28.045: INFO: Waiting up to 5m0s for pod "downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5" in namespace "downward-api-1741" to be "success or failure"
May 19 08:42:28.047: INFO: Pod "downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.667836ms
May 19 08:42:30.051: INFO: Pod "downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006368369s
May 19 08:42:32.056: INFO: Pod "downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010998154s
STEP: Saw pod success
May 19 08:42:32.056: INFO: Pod "downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5" satisfied condition "success or failure"
May 19 08:42:32.059: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5 container client-container: <nil>
STEP: delete the pod
May 19 08:42:32.079: INFO: Waiting for pod downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5 to disappear
May 19 08:42:32.082: INFO: Pod downwardapi-volume-260100bf-fad7-4df4-9c23-ed2589a600d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:42:32.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1741" for this suite.
May 19 08:42:38.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:42:38.175: INFO: namespace downward-api-1741 deletion completed in 6.08828585s

• [SLOW TEST:10.168 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:42:38.175: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 19 08:42:40.231: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:42:40.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-805" for this suite.
May 19 08:42:46.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:42:46.337: INFO: namespace container-runtime-805 deletion completed in 6.089259032s

• [SLOW TEST:8.162 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:42:46.337: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-131f1f9c-ce13-4fd4-8223-3b29c466f9d1
STEP: Creating a pod to test consume secrets
May 19 08:42:46.378: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe" in namespace "projected-2401" to be "success or failure"
May 19 08:42:46.380: INFO: Pod "pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390143ms
May 19 08:42:48.384: INFO: Pod "pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005698372s
May 19 08:42:50.390: INFO: Pod "pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012275849s
STEP: Saw pod success
May 19 08:42:50.390: INFO: Pod "pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe" satisfied condition "success or failure"
May 19 08:42:50.395: INFO: Trying to get logs from node nchc-worker03 pod pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe container secret-volume-test: <nil>
STEP: delete the pod
May 19 08:42:50.424: INFO: Waiting for pod pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe to disappear
May 19 08:42:50.427: INFO: Pod pod-projected-secrets-c7095e20-d1c5-4b09-bcd4-88cf572ddefe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:42:50.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2401" for this suite.
May 19 08:42:56.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:42:56.528: INFO: namespace projected-2401 deletion completed in 6.097491622s

• [SLOW TEST:10.191 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:42:56.529: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7466
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7466
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7466
May 19 08:42:56.572: INFO: Found 0 stateful pods, waiting for 1
May 19 08:43:06.579: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 19 08:43:06.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:43:06.885: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:43:06.885: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:43:06.885: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:43:06.890: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 19 08:43:16.896: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:43:16.896: INFO: Waiting for statefulset status.replicas updated to 0
May 19 08:43:16.911: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998114s
May 19 08:43:17.915: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996849689s
May 19 08:43:18.920: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99209538s
May 19 08:43:19.925: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986918036s
May 19 08:43:20.930: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981919698s
May 19 08:43:21.934: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977342369s
May 19 08:43:22.940: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972872318s
May 19 08:43:23.945: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96716997s
May 19 08:43:24.949: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.962745526s
May 19 08:43:25.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.900632ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7466
May 19 08:43:26.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:43:27.237: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 19 08:43:27.237: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:43:27.237: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:43:27.241: INFO: Found 1 stateful pods, waiting for 3
May 19 08:43:37.247: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 19 08:43:37.247: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 19 08:43:37.247: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 19 08:43:37.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:43:37.536: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:43:37.536: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:43:37.536: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:43:37.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:43:38.034: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:43:38.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:43:38.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:43:38.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 19 08:43:38.536: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 19 08:43:38.536: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 19 08:43:38.536: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 19 08:43:38.536: INFO: Waiting for statefulset status.replicas updated to 0
May 19 08:43:38.540: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 19 08:43:48.549: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:43:48.549: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:43:48.549: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 19 08:43:48.560: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998464s
May 19 08:43:49.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99663674s
May 19 08:43:50.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992296666s
May 19 08:43:51.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987366494s
May 19 08:43:52.579: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982430595s
May 19 08:43:53.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977916823s
May 19 08:43:54.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973425959s
May 19 08:43:55.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969084138s
May 19 08:43:56.597: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965002969s
May 19 08:43:57.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.603248ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7466
May 19 08:43:58.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:43:58.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 19 08:43:58.884: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:43:58.884: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:43:58.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:43:59.310: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 19 08:43:59.310: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 19 08:43:59.310: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 19 08:43:59.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:44:00.022: INFO: rc: 126
May 19 08:44:00.022: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:
OCI runtime exec failed: exec failed: cannot exec a container that has stopped: unknown

stderr:
command terminated with exit code 126

error:
exit status 126
May 19 08:44:10.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:44:10.179: INFO: rc: 1
May 19 08:44:10.179: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 19 08:44:20.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:44:20.290: INFO: rc: 1
May 19 08:44:20.290: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:44:30.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:44:30.483: INFO: rc: 1
May 19 08:44:30.483: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:44:40.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:44:40.594: INFO: rc: 1
May 19 08:44:40.594: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:44:50.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:44:50.711: INFO: rc: 1
May 19 08:44:50.711: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:45:00.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:45:00.819: INFO: rc: 1
May 19 08:45:00.819: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:45:10.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:45:10.943: INFO: rc: 1
May 19 08:45:10.944: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:45:20.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:45:21.056: INFO: rc: 1
May 19 08:45:21.056: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:45:31.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:45:31.167: INFO: rc: 1
May 19 08:45:31.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:45:41.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:45:41.275: INFO: rc: 1
May 19 08:45:41.275: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:45:51.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:45:51.385: INFO: rc: 1
May 19 08:45:51.385: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:46:01.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:46:01.490: INFO: rc: 1
May 19 08:46:01.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:46:11.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:46:11.597: INFO: rc: 1
May 19 08:46:11.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:46:21.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:46:21.702: INFO: rc: 1
May 19 08:46:21.702: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:46:31.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:46:31.804: INFO: rc: 1
May 19 08:46:31.804: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:46:41.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:46:41.916: INFO: rc: 1
May 19 08:46:41.916: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:46:51.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:46:52.021: INFO: rc: 1
May 19 08:46:52.021: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:47:02.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:47:02.139: INFO: rc: 1
May 19 08:47:02.139: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:47:12.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:47:12.262: INFO: rc: 1
May 19 08:47:12.262: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:47:22.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:47:22.367: INFO: rc: 1
May 19 08:47:22.367: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:47:32.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:47:32.490: INFO: rc: 1
May 19 08:47:32.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:47:42.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:47:42.599: INFO: rc: 1
May 19 08:47:42.600: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:47:52.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:47:52.719: INFO: rc: 1
May 19 08:47:52.719: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:48:02.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:48:02.825: INFO: rc: 1
May 19 08:48:02.825: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:48:12.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:48:12.937: INFO: rc: 1
May 19 08:48:12.937: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:48:22.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:48:23.041: INFO: rc: 1
May 19 08:48:23.041: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:48:33.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:48:33.153: INFO: rc: 1
May 19 08:48:33.153: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:48:43.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:48:43.266: INFO: rc: 1
May 19 08:48:43.266: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:48:53.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:48:53.374: INFO: rc: 1
May 19 08:48:53.374: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May 19 08:49:03.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=statefulset-7466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 19 08:49:03.476: INFO: rc: 1
May 19 08:49:03.476: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
May 19 08:49:03.476: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 19 08:49:03.487: INFO: Deleting all statefulset in ns statefulset-7466
May 19 08:49:03.489: INFO: Scaling statefulset ss to 0
May 19 08:49:03.498: INFO: Waiting for statefulset status.replicas updated to 0
May 19 08:49:03.500: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:49:03.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7466" for this suite.
May 19 08:49:09.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:49:09.603: INFO: namespace statefulset-7466 deletion completed in 6.08847149s

• [SLOW TEST:373.075 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:49:09.604: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:49:09.633: INFO: Creating deployment "webserver-deployment"
May 19 08:49:09.639: INFO: Waiting for observed generation 1
May 19 08:49:11.646: INFO: Waiting for all required pods to come up
May 19 08:49:11.650: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 19 08:49:17.659: INFO: Waiting for deployment "webserver-deployment" to complete
May 19 08:49:17.666: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 19 08:49:17.674: INFO: Updating deployment webserver-deployment
May 19 08:49:17.674: INFO: Waiting for observed generation 2
May 19 08:49:19.681: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 19 08:49:19.686: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 19 08:49:19.689: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 19 08:49:19.698: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 19 08:49:19.698: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 19 08:49:19.700: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 19 08:49:19.705: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 19 08:49:19.705: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 19 08:49:19.712: INFO: Updating deployment webserver-deployment
May 19 08:49:19.712: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 19 08:49:19.717: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 19 08:49:19.720: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 19 08:49:19.727: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7190 /apis/apps/v1/namespaces/deployment-7190/deployments/webserver-deployment 39374feb-7831-4a5a-8770-eb30a52199e6 1105097 3 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001ae70d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-05-19 08:49:17 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-19 08:49:19 +0000 UTC,LastTransitionTime:2020-05-19 08:49:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 19 08:49:19.734: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-7190 /apis/apps/v1/namespaces/deployment-7190/replicasets/webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 1105093 3 2020-05-19 08:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 39374feb-7831-4a5a-8770-eb30a52199e6 0xc003a32217 0xc003a32218}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a32288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 19 08:49:19.734: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 19 08:49:19.734: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-7190 /apis/apps/v1/namespaces/deployment-7190/replicasets/webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 1105091 3 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 39374feb-7831-4a5a-8770-eb30a52199e6 0xc003a32157 0xc003a32158}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a321b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 19 08:49:19.742: INFO: Pod "webserver-deployment-595b5b9587-5gj6l" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5gj6l webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-5gj6l e37dfcd7-df84-4bc7-94a5-1629d7230647 1105005 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.148.117/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc001ae77c7 0xc001ae77c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:10.244.148.117,StartTime:2020-05-19 08:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5713a42a95011f53094c9c96fc2be572915e3fa9c1be0386942ddda9e5178bfb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.148.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.742: INFO: Pod "webserver-deployment-595b5b9587-5xfb4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5xfb4 webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-5xfb4 a7c1c742-2e6a-43f1-8081-d0bc40362e71 1105008 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.148.116/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc001ae7947 0xc001ae7948}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:10.244.148.116,StartTime:2020-05-19 08:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a107cef8515faa4baa8344755940ad2da8c0b12a2a7d7a2235fa2002397ebc29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.148.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.743: INFO: Pod "webserver-deployment-595b5b9587-6fjpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6fjpn webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-6fjpn 598800f2-3896-497c-8c3c-40400aef4f9c 1105100 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc001ae7ac7 0xc001ae7ac8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.743: INFO: Pod "webserver-deployment-595b5b9587-7ksx9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7ksx9 webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-7ksx9 a6e2d9ea-f39f-441f-ade0-2f61c381a272 1105002 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.148.118/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc001ae7bd7 0xc001ae7bd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:10.244.148.118,StartTime:2020-05-19 08:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0a9a00037e34b816156d2e898be59372d3b4659584ca5ffe4f3ccb1dce4ec9ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.148.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.743: INFO: Pod "webserver-deployment-595b5b9587-bvz8h" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bvz8h webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-bvz8h 5a6f867f-b6c5-4dee-8dad-a94f30ec2b7c 1104949 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.161.67/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc001ae7d67 0xc001ae7d68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:10.244.161.67,StartTime:2020-05-19 08:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ff2193e4fc71b70617dcc5f0a801418be68541deee55ee5dbddf4095003433a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.161.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.743: INFO: Pod "webserver-deployment-595b5b9587-dff7f" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dff7f webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-dff7f 88216285-bfea-40c1-afda-81fb87d47da7 1104996 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.148.120/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e6057 0xc0022e6058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:10.244.148.120,StartTime:2020-05-19 08:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://bdf8aab8a47855de23f9bb3e142f6e5e075c21bd69610ae72071163271861a83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.148.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.744: INFO: Pod "webserver-deployment-595b5b9587-dlk24" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dlk24 webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-dlk24 9d8b1f9a-ea73-46c0-a023-edff192c37a8 1105113 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e61e7 0xc0022e61e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.744: INFO: Pod "webserver-deployment-595b5b9587-fxpb4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fxpb4 webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-fxpb4 d55d2aa0-5188-4c89-9cdb-8379187075d3 1104999 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.148.119/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e62f0 0xc0022e62f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:10.244.148.119,StartTime:2020-05-19 08:48:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8a49d0b6f9956d75a1173825420790fb6f5ee721b834212072446d83e20214ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.148.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.744: INFO: Pod "webserver-deployment-595b5b9587-mnxqs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mnxqs webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-mnxqs 2ab49df1-54f5-4854-8193-b554975830e6 1104944 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.161.121/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e6657 0xc0022e6658}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:10.244.161.121,StartTime:2020-05-19 08:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://32ec432c78ccba2ab5dca74630b718e0e66abe9b12a2ab049816ff3b14724143,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.161.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.744: INFO: Pod "webserver-deployment-595b5b9587-pp86k" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pp86k webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-pp86k f45aa28d-aebd-4e8c-af43-fa34f0f43d67 1105112 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e68a7 0xc0022e68a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.744: INFO: Pod "webserver-deployment-595b5b9587-rz99g" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rz99g webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-rz99g ecc47572-99e3-43fc-8d57-76ddfb484dfc 1105109 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e6a30 0xc0022e6a31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.744: INFO: Pod "webserver-deployment-595b5b9587-sd7b9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sd7b9 webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-sd7b9 01087327-b5fd-4fbc-90c7-a2a74ebd8bcb 1105111 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e6b50 0xc0022e6b51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.745: INFO: Pod "webserver-deployment-595b5b9587-vbqk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vbqk2 webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-vbqk2 2dfea611-4e1d-4f63-a5b8-19541ae91d4e 1105105 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e6c57 0xc0022e6c58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.745: INFO: Pod "webserver-deployment-595b5b9587-wmnv2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wmnv2 webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-wmnv2 51a053a5-ef13-4818-aa1e-74cdc21c8d77 1104935 0 2020-05-19 08:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.244.161.76/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e6d77 0xc0022e6d78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:10.244.161.76,StartTime:2020-05-19 08:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 08:49:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://467c0052223c13aafc67832f18559b784e49eed25aac79ce880dcbe989c8b306,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.161.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.745: INFO: Pod "webserver-deployment-595b5b9587-xhgxk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xhgxk webserver-deployment-595b5b9587- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-595b5b9587-xhgxk d24ea9cb-7f03-4b46-8852-643612a7165c 1105108 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 8e966995-2ee6-4e3c-a695-ce805be21861 0xc0022e6ee7 0xc0022e6ee8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.745: INFO: Pod "webserver-deployment-c7997dcc8-2tp6q" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2tp6q webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-2tp6q 1123f126-cc51-43f9-a1ce-1403434dbea1 1105116 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e6ff0 0xc0022e6ff1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.745: INFO: Pod "webserver-deployment-c7997dcc8-2vrv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2vrv6 webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-2vrv6 ad445e51-1e9d-4c16-a426-14456a7f7496 1105115 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e70f0 0xc0022e70f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.746: INFO: Pod "webserver-deployment-c7997dcc8-7b286" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7b286 webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-7b286 01c88236-9411-41bd-bffc-400483bf9a10 1105085 0 2020-05-19 08:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.244.161.72/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7200 0xc0022e7201}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:,StartTime:2020-05-19 08:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.746: INFO: Pod "webserver-deployment-c7997dcc8-9h8p9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9h8p9 webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-9h8p9 9b282d10-7e18-4aa3-b5dd-9ed0db79561d 1105064 0 2020-05-19 08:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7367 0xc0022e7368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:,StartTime:2020-05-19 08:49:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.746: INFO: Pod "webserver-deployment-c7997dcc8-9mlg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9mlg7 webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-9mlg7 59cb4bf2-9859-4221-ba93-ff1ee7ea5d09 1105102 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e74e7 0xc0022e74e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.746: INFO: Pod "webserver-deployment-c7997dcc8-hvpr8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hvpr8 webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-hvpr8 2fee9ea9-f64a-4b77-b339-e5569a71c0f8 1105061 0 2020-05-19 08:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7607 0xc0022e7608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:,StartTime:2020-05-19 08:49:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.746: INFO: Pod "webserver-deployment-c7997dcc8-jxlhm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jxlhm webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-jxlhm b5599c20-8c8f-4c3d-a794-6b4407660ee2 1105118 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7787 0xc0022e7788}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.746: INFO: Pod "webserver-deployment-c7997dcc8-n5bvl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-n5bvl webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-n5bvl 74df89c0-f9fd-4eea-80e3-53395d4d5a06 1105051 0 2020-05-19 08:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7890 0xc0022e7891}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:,StartTime:2020-05-19 08:49:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.747: INFO: Pod "webserver-deployment-c7997dcc8-p5gd5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p5gd5 webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-p5gd5 441d6eea-96fa-4669-925d-435274724ed2 1105119 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7a07 0xc0022e7a08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.747: INFO: Pod "webserver-deployment-c7997dcc8-t68jn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-t68jn webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-t68jn 8cb3c301-68e4-4aa5-a835-dc5360f88761 1105104 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7b10 0xc0022e7b11}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.747: INFO: Pod "webserver-deployment-c7997dcc8-vbf5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vbf5k webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-vbf5k c1f2ea5f-46a9-4bc1-8126-24623596e622 1105084 0 2020-05-19 08:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.244.161.71/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7c20 0xc0022e7c21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:,StartTime:2020-05-19 08:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 08:49:19.747: INFO: Pod "webserver-deployment-c7997dcc8-wc5n8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wc5n8 webserver-deployment-c7997dcc8- deployment-7190 /api/v1/namespaces/deployment-7190/pods/webserver-deployment-c7997dcc8-wc5n8 c8b73fff-1748-41a6-841a-bceacfd00d94 1105117 0 2020-05-19 08:49:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 64622d13-d6ae-46a0-bf99-14906fa3fd44 0xc0022e7d87 0xc0022e7d88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kghp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kghp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kghp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 08:49:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:49:19.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7190" for this suite.
May 19 08:49:25.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:49:25.843: INFO: namespace deployment-7190 deletion completed in 6.089531557s

• [SLOW TEST:16.240 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:49:25.843: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May 19 08:49:55.909: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0519 08:49:55.909078      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:49:55.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2454" for this suite.
May 19 08:50:01.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:50:02.005: INFO: namespace gc-2454 deletion completed in 6.092209799s

• [SLOW TEST:36.161 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:50:02.005: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 08:50:03.146: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 08:50:05.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475003, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475003, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475003, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475003, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 08:50:08.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:50:08.186: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3607-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:50:09.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7544" for this suite.
May 19 08:50:15.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:50:15.428: INFO: namespace webhook-7544 deletion completed in 6.094408923s
STEP: Destroying namespace "webhook-7544-markers" for this suite.
May 19 08:50:21.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:50:21.514: INFO: namespace webhook-7544-markers deletion completed in 6.085305749s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.523 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:50:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7785.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.164.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.164.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.164.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.164.145_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7785.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.164.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.164.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.164.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.164.145_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 08:50:25.600: INFO: Unable to read wheezy_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.604: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.607: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.610: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.631: INFO: Unable to read jessie_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.634: INFO: Unable to read jessie_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.636: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.639: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:25.656: INFO: Lookups using dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b failed for: [wheezy_udp@dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_udp@dns-test-service.dns-7785.svc.cluster.local jessie_tcp@dns-test-service.dns-7785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local]

May 19 08:50:30.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.669: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.672: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.693: INFO: Unable to read jessie_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.696: INFO: Unable to read jessie_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.699: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.702: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:30.720: INFO: Lookups using dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b failed for: [wheezy_udp@dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_udp@dns-test-service.dns-7785.svc.cluster.local jessie_tcp@dns-test-service.dns-7785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local]

May 19 08:50:35.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.665: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.668: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.671: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.694: INFO: Unable to read jessie_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.697: INFO: Unable to read jessie_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.700: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.703: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:35.720: INFO: Lookups using dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b failed for: [wheezy_udp@dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_udp@dns-test-service.dns-7785.svc.cluster.local jessie_tcp@dns-test-service.dns-7785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local]

May 19 08:50:40.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.669: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.672: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.693: INFO: Unable to read jessie_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.696: INFO: Unable to read jessie_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.699: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.702: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:40.720: INFO: Lookups using dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b failed for: [wheezy_udp@dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_udp@dns-test-service.dns-7785.svc.cluster.local jessie_tcp@dns-test-service.dns-7785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local]

May 19 08:50:45.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.668: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.671: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.693: INFO: Unable to read jessie_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.696: INFO: Unable to read jessie_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.699: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.702: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:45.719: INFO: Lookups using dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b failed for: [wheezy_udp@dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_udp@dns-test-service.dns-7785.svc.cluster.local jessie_tcp@dns-test-service.dns-7785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local]

May 19 08:50:50.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.665: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.669: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.672: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.695: INFO: Unable to read jessie_udp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.699: INFO: Unable to read jessie_tcp@dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.702: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.705: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local from pod dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b: the server could not find the requested resource (get pods dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b)
May 19 08:50:50.724: INFO: Lookups using dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b failed for: [wheezy_udp@dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@dns-test-service.dns-7785.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_udp@dns-test-service.dns-7785.svc.cluster.local jessie_tcp@dns-test-service.dns-7785.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7785.svc.cluster.local]

May 19 08:50:55.719: INFO: DNS probes using dns-7785/dns-test-64d32fd6-b55c-42ba-83c2-69f05749b79b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:50:55.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7785" for this suite.
May 19 08:51:01.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:51:01.856: INFO: namespace dns-7785 deletion completed in 6.088451585s

• [SLOW TEST:40.329 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:51:01.857: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 08:51:01.894: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31" in namespace "downward-api-3468" to be "success or failure"
May 19 08:51:01.896: INFO: Pod "downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307589ms
May 19 08:51:03.900: INFO: Pod "downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005904546s
May 19 08:51:05.903: INFO: Pod "downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009810905s
STEP: Saw pod success
May 19 08:51:05.904: INFO: Pod "downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31" satisfied condition "success or failure"
May 19 08:51:05.906: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31 container client-container: <nil>
STEP: delete the pod
May 19 08:51:05.939: INFO: Waiting for pod downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31 to disappear
May 19 08:51:05.941: INFO: Pod downwardapi-volume-aa888336-1427-4154-a545-5c3446c67a31 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:51:05.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3468" for this suite.
May 19 08:51:11.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:51:12.040: INFO: namespace downward-api-3468 deletion completed in 6.094863736s

• [SLOW TEST:10.183 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:51:12.040: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:51:12.069: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 19 08:51:14.098: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:51:15.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8155" for this suite.
May 19 08:51:21.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:51:21.197: INFO: namespace replication-controller-8155 deletion completed in 6.088667956s

• [SLOW TEST:9.157 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:51:21.198: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May 19 08:51:21.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-277'
May 19 08:51:21.496: INFO: stderr: ""
May 19 08:51:21.496: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 19 08:51:21.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-277'
May 19 08:51:21.606: INFO: stderr: ""
May 19 08:51:21.606: INFO: stdout: "update-demo-nautilus-ph449 update-demo-nautilus-tnfzz "
May 19 08:51:21.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-ph449 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-277'
May 19 08:51:21.708: INFO: stderr: ""
May 19 08:51:21.708: INFO: stdout: ""
May 19 08:51:21.708: INFO: update-demo-nautilus-ph449 is created but not running
May 19 08:51:26.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-277'
May 19 08:51:26.827: INFO: stderr: ""
May 19 08:51:26.827: INFO: stdout: "update-demo-nautilus-ph449 update-demo-nautilus-tnfzz "
May 19 08:51:26.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-ph449 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-277'
May 19 08:51:26.933: INFO: stderr: ""
May 19 08:51:26.933: INFO: stdout: "true"
May 19 08:51:26.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-ph449 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-277'
May 19 08:51:27.033: INFO: stderr: ""
May 19 08:51:27.033: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 08:51:27.033: INFO: validating pod update-demo-nautilus-ph449
May 19 08:51:27.039: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 08:51:27.039: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 08:51:27.039: INFO: update-demo-nautilus-ph449 is verified up and running
May 19 08:51:27.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-tnfzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-277'
May 19 08:51:27.140: INFO: stderr: ""
May 19 08:51:27.141: INFO: stdout: "true"
May 19 08:51:27.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-tnfzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-277'
May 19 08:51:27.240: INFO: stderr: ""
May 19 08:51:27.240: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 08:51:27.240: INFO: validating pod update-demo-nautilus-tnfzz
May 19 08:51:27.246: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 08:51:27.246: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 08:51:27.246: INFO: update-demo-nautilus-tnfzz is verified up and running
STEP: using delete to clean up resources
May 19 08:51:27.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-277'
May 19 08:51:27.349: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 08:51:27.349: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 19 08:51:27.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-277'
May 19 08:51:27.456: INFO: stderr: "No resources found in kubectl-277 namespace.\n"
May 19 08:51:27.456: INFO: stdout: ""
May 19 08:51:27.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -l name=update-demo --namespace=kubectl-277 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 19 08:51:27.557: INFO: stderr: ""
May 19 08:51:27.557: INFO: stdout: "update-demo-nautilus-ph449\nupdate-demo-nautilus-tnfzz\n"
May 19 08:51:28.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-277'
May 19 08:51:28.175: INFO: stderr: "No resources found in kubectl-277 namespace.\n"
May 19 08:51:28.175: INFO: stdout: ""
May 19 08:51:28.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -l name=update-demo --namespace=kubectl-277 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 19 08:51:28.284: INFO: stderr: ""
May 19 08:51:28.284: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:51:28.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-277" for this suite.
May 19 08:51:40.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:51:40.385: INFO: namespace kubectl-277 deletion completed in 12.093061554s

• [SLOW TEST:19.187 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:51:40.385: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 19 08:51:43.439: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:51:43.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7376" for this suite.
May 19 08:51:49.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:51:49.544: INFO: namespace container-runtime-7376 deletion completed in 6.089176408s

• [SLOW TEST:9.159 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:51:49.544: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-eda47559-d621-4015-8868-d7254922e1c4
STEP: Creating configMap with name cm-test-opt-upd-6350f9ff-b8ad-4d8f-9c9e-9950b7c7a350
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-eda47559-d621-4015-8868-d7254922e1c4
STEP: Updating configmap cm-test-opt-upd-6350f9ff-b8ad-4d8f-9c9e-9950b7c7a350
STEP: Creating configMap with name cm-test-opt-create-1c6b339c-f5cd-4187-9784-27bd6d4673bb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:53:12.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3459" for this suite.
May 19 08:53:24.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:53:24.205: INFO: namespace configmap-3459 deletion completed in 12.088211427s

• [SLOW TEST:94.660 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:53:24.205: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 in namespace container-probe-3100
May 19 08:53:28.251: INFO: Started pod liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 in namespace container-probe-3100
STEP: checking the pod's current state and verifying that restartCount is present
May 19 08:53:28.253: INFO: Initial restart count of pod liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 is 0
May 19 08:53:40.279: INFO: Restart count of pod container-probe-3100/liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 is now 1 (12.026336955s elapsed)
May 19 08:54:00.319: INFO: Restart count of pod container-probe-3100/liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 is now 2 (32.066038689s elapsed)
May 19 08:54:20.366: INFO: Restart count of pod container-probe-3100/liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 is now 3 (52.112540548s elapsed)
May 19 08:54:40.409: INFO: Restart count of pod container-probe-3100/liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 is now 4 (1m12.155641925s elapsed)
May 19 08:55:50.566: INFO: Restart count of pod container-probe-3100/liveness-142eeaad-fee1-4c97-9b7b-5ebb060dc808 is now 5 (2m22.312868143s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:55:50.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3100" for this suite.
May 19 08:55:56.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:55:56.673: INFO: namespace container-probe-3100 deletion completed in 6.092840196s

• [SLOW TEST:152.468 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:55:56.674: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-8588
STEP: creating replication controller nodeport-test in namespace services-8588
I0519 08:55:56.724665      25 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-8588, replica count: 2
May 19 08:55:59.775: INFO: Creating new exec pod
I0519 08:55:59.775127      25 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 19 08:56:04.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-8588 execpod48pnx -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 19 08:56:05.209: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 19 08:56:05.209: INFO: stdout: ""
May 19 08:56:05.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-8588 execpod48pnx -- /bin/sh -x -c nc -zv -t -w 2 10.103.235.32 80'
May 19 08:56:05.525: INFO: stderr: "+ nc -zv -t -w 2 10.103.235.32 80\nConnection to 10.103.235.32 80 port [tcp/http] succeeded!\n"
May 19 08:56:05.525: INFO: stdout: ""
May 19 08:56:05.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-8588 execpod48pnx -- /bin/sh -x -c nc -zv -t -w 2 10.78.26.171 31212'
May 19 08:56:05.844: INFO: stderr: "+ nc -zv -t -w 2 10.78.26.171 31212\nConnection to 10.78.26.171 31212 port [tcp/31212] succeeded!\n"
May 19 08:56:05.844: INFO: stdout: ""
May 19 08:56:05.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-8588 execpod48pnx -- /bin/sh -x -c nc -zv -t -w 2 10.78.26.181 31212'
May 19 08:56:06.140: INFO: stderr: "+ nc -zv -t -w 2 10.78.26.181 31212\nConnection to 10.78.26.181 31212 port [tcp/31212] succeeded!\n"
May 19 08:56:06.140: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:56:06.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8588" for this suite.
May 19 08:56:12.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:56:12.239: INFO: namespace services-8588 deletion completed in 6.094369322s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.566 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:56:12.240: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:56:15.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8130" for this suite.
May 19 08:56:27.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:56:27.387: INFO: namespace replication-controller-8130 deletion completed in 12.086287201s

• [SLOW TEST:15.148 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:56:27.388: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-544748e6-2509-4040-bee5-7c11be3de3c5
STEP: Creating a pod to test consume configMaps
May 19 08:56:27.427: INFO: Waiting up to 5m0s for pod "pod-configmaps-fafc9750-d4b1-47b9-9327-0abc7e34ef15" in namespace "configmap-3653" to be "success or failure"
May 19 08:56:27.430: INFO: Pod "pod-configmaps-fafc9750-d4b1-47b9-9327-0abc7e34ef15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222102ms
May 19 08:56:29.433: INFO: Pod "pod-configmaps-fafc9750-d4b1-47b9-9327-0abc7e34ef15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005771916s
STEP: Saw pod success
May 19 08:56:29.433: INFO: Pod "pod-configmaps-fafc9750-d4b1-47b9-9327-0abc7e34ef15" satisfied condition "success or failure"
May 19 08:56:29.436: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-configmaps-fafc9750-d4b1-47b9-9327-0abc7e34ef15 container configmap-volume-test: <nil>
STEP: delete the pod
May 19 08:56:29.465: INFO: Waiting for pod pod-configmaps-fafc9750-d4b1-47b9-9327-0abc7e34ef15 to disappear
May 19 08:56:29.467: INFO: Pod pod-configmaps-fafc9750-d4b1-47b9-9327-0abc7e34ef15 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:56:29.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3653" for this suite.
May 19 08:56:35.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:56:35.561: INFO: namespace configmap-3653 deletion completed in 6.09039369s

• [SLOW TEST:8.173 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:56:35.562: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:56:37.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6531" for this suite.
May 19 08:57:05.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:57:05.716: INFO: namespace containers-6531 deletion completed in 28.091697191s

• [SLOW TEST:30.154 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:57:05.717: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 08:57:06.959: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 08:57:08.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475426, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475426, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475426, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475426, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 08:57:11.989: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:57:12.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6876" for this suite.
May 19 08:57:18.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:57:18.256: INFO: namespace webhook-6876 deletion completed in 6.090430916s
STEP: Destroying namespace "webhook-6876-markers" for this suite.
May 19 08:57:24.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:57:24.346: INFO: namespace webhook-6876-markers deletion completed in 6.090244646s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.643 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:57:24.359: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:57:39.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9183" for this suite.
May 19 08:57:45.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:57:45.559: INFO: namespace namespaces-9183 deletion completed in 6.08999077s
STEP: Destroying namespace "nsdeletetest-2210" for this suite.
May 19 08:57:45.561: INFO: Namespace nsdeletetest-2210 was already deleted
STEP: Destroying namespace "nsdeletetest-9878" for this suite.
May 19 08:57:51.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:57:51.647: INFO: namespace nsdeletetest-9878 deletion completed in 6.085890678s

• [SLOW TEST:27.288 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:57:51.647: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 19 08:57:51.675: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 19 08:57:51.685: INFO: Waiting for terminating namespaces to be deleted...
May 19 08:57:51.688: INFO: 
Logging pods the kubelet thinks is on node nchc-kubemaster01 before test
May 19 08:57:51.695: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-7hsd4 from sonobuoy started at 2020-05-19 08:18:56 +0000 UTC (2 container statuses recorded)
May 19 08:57:51.695: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 19 08:57:51.695: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 08:57:51.695: INFO: kube-proxy-qg52l from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 08:57:51.695: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 08:57:51.695: INFO: calico-node-7jgfk from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 08:57:51.695: INFO: 	Container calico-node ready: true, restart count 0
May 19 08:57:51.695: INFO: 
Logging pods the kubelet thinks is on node nchc-worker03 before test
May 19 08:57:51.714: INFO: sonobuoy from sonobuoy started at 2020-05-19 08:18:41 +0000 UTC (1 container statuses recorded)
May 19 08:57:51.714: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 19 08:57:51.714: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-hw9m8 from sonobuoy started at 2020-05-19 08:18:43 +0000 UTC (2 container statuses recorded)
May 19 08:57:51.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 19 08:57:51.714: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 08:57:51.714: INFO: calico-node-724vj from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 08:57:51.714: INFO: 	Container calico-node ready: true, restart count 0
May 19 08:57:51.714: INFO: kube-proxy-5l969 from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 08:57:51.714: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node nchc-kubemaster01
STEP: verifying the node has the label node nchc-worker03
May 19 08:57:51.756: INFO: Pod calico-node-724vj requesting resource cpu=250m on Node nchc-worker03
May 19 08:57:51.756: INFO: Pod calico-node-7jgfk requesting resource cpu=250m on Node nchc-kubemaster01
May 19 08:57:51.756: INFO: Pod kube-proxy-5l969 requesting resource cpu=0m on Node nchc-worker03
May 19 08:57:51.756: INFO: Pod kube-proxy-qg52l requesting resource cpu=0m on Node nchc-kubemaster01
May 19 08:57:51.756: INFO: Pod sonobuoy requesting resource cpu=0m on Node nchc-worker03
May 19 08:57:51.756: INFO: Pod sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-7hsd4 requesting resource cpu=0m on Node nchc-kubemaster01
May 19 08:57:51.756: INFO: Pod sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-hw9m8 requesting resource cpu=0m on Node nchc-worker03
STEP: Starting Pods to consume most of the cluster CPU.
May 19 08:57:51.756: INFO: Creating a pod which consumes cpu=22225m on Node nchc-kubemaster01
May 19 08:57:51.765: INFO: Creating a pod which consumes cpu=61425m on Node nchc-worker03
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e11cae4-4c1a-46f4-8362-2cbf72534a43.16106237e25f444c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e11cae4-4c1a-46f4-8362-2cbf72534a43.16106237e51c5254], Reason = [Created], Message = [Created container filler-pod-4e11cae4-4c1a-46f4-8362-2cbf72534a43]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e11cae4-4c1a-46f4-8362-2cbf72534a43.1610623800ceb860], Reason = [Started], Message = [Started container filler-pod-4e11cae4-4c1a-46f4-8362-2cbf72534a43]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e11cae4-4c1a-46f4-8362-2cbf72534a43.1610623a7aed921f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8539/filler-pod-4e11cae4-4c1a-46f4-8362-2cbf72534a43 to nchc-worker03]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4dfb37a-2c22-4bc6-ab37-7aff58edbbe8.1610623a7acc02eb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8539/filler-pod-d4dfb37a-2c22-4bc6-ab37-7aff58edbbe8 to nchc-kubemaster01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4dfb37a-2c22-4bc6-ab37-7aff58edbbe8.1610623aafa0e933], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4dfb37a-2c22-4bc6-ab37-7aff58edbbe8.1610623ab167a832], Reason = [Created], Message = [Created container filler-pod-d4dfb37a-2c22-4bc6-ab37-7aff58edbbe8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d4dfb37a-2c22-4bc6-ab37-7aff58edbbe8.1610623abe7ab84d], Reason = [Started], Message = [Started container filler-pod-d4dfb37a-2c22-4bc6-ab37-7aff58edbbe8]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1610623b6a81f284], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node nchc-kubemaster01
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node nchc-worker03
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:57:56.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8539" for this suite.
May 19 08:58:02.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:58:02.947: INFO: namespace sched-pred-8539 deletion completed in 6.123070522s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.300 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:58:02.948: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May 19 08:58:02.987: INFO: Waiting up to 5m0s for pod "pod-fe1b399d-f496-4a69-9a4b-775a2110519e" in namespace "emptydir-1607" to be "success or failure"
May 19 08:58:02.989: INFO: Pod "pod-fe1b399d-f496-4a69-9a4b-775a2110519e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.381622ms
May 19 08:58:04.993: INFO: Pod "pod-fe1b399d-f496-4a69-9a4b-775a2110519e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006778693s
May 19 08:58:06.998: INFO: Pod "pod-fe1b399d-f496-4a69-9a4b-775a2110519e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011338985s
STEP: Saw pod success
May 19 08:58:06.998: INFO: Pod "pod-fe1b399d-f496-4a69-9a4b-775a2110519e" satisfied condition "success or failure"
May 19 08:58:07.001: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-fe1b399d-f496-4a69-9a4b-775a2110519e container test-container: <nil>
STEP: delete the pod
May 19 08:58:07.021: INFO: Waiting for pod pod-fe1b399d-f496-4a69-9a4b-775a2110519e to disappear
May 19 08:58:07.023: INFO: Pod pod-fe1b399d-f496-4a69-9a4b-775a2110519e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:58:07.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1607" for this suite.
May 19 08:58:13.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:58:13.117: INFO: namespace emptydir-1607 deletion completed in 6.089828735s

• [SLOW TEST:10.169 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:58:13.117: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May 19 08:58:17.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec pod-sharedvolume-d764908a-e17b-4247-b139-1c65ae2e576f -c busybox-main-container --namespace=emptydir-2842 -- cat /usr/share/volumeshare/shareddata.txt'
May 19 08:58:17.455: INFO: stderr: ""
May 19 08:58:17.455: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:58:17.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2842" for this suite.
May 19 08:58:23.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:58:23.549: INFO: namespace emptydir-2842 deletion completed in 6.089260917s

• [SLOW TEST:10.432 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:58:23.549: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 08:58:23.578: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 19 08:58:27.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 create -f -'
May 19 08:58:27.849: INFO: stderr: ""
May 19 08:58:27.849: INFO: stdout: "e2e-test-crd-publish-openapi-9095-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 19 08:58:27.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 delete e2e-test-crd-publish-openapi-9095-crds test-foo'
May 19 08:58:27.949: INFO: stderr: ""
May 19 08:58:27.949: INFO: stdout: "e2e-test-crd-publish-openapi-9095-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 19 08:58:27.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 apply -f -'
May 19 08:58:28.238: INFO: stderr: ""
May 19 08:58:28.239: INFO: stdout: "e2e-test-crd-publish-openapi-9095-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 19 08:58:28.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 delete e2e-test-crd-publish-openapi-9095-crds test-foo'
May 19 08:58:28.341: INFO: stderr: ""
May 19 08:58:28.341: INFO: stdout: "e2e-test-crd-publish-openapi-9095-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 19 08:58:28.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 create -f -'
May 19 08:58:28.600: INFO: rc: 1
May 19 08:58:28.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 apply -f -'
May 19 08:58:28.848: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 19 08:58:28.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 create -f -'
May 19 08:58:29.220: INFO: rc: 1
May 19 08:58:29.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-1605 apply -f -'
May 19 08:58:29.523: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 19 08:58:29.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-9095-crds'
May 19 08:58:29.866: INFO: stderr: ""
May 19 08:58:29.866: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9095-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 19 08:58:29.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-9095-crds.metadata'
May 19 08:58:30.149: INFO: stderr: ""
May 19 08:58:30.150: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9095-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 19 08:58:30.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-9095-crds.spec'
May 19 08:58:30.435: INFO: stderr: ""
May 19 08:58:30.435: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9095-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 19 08:58:30.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-9095-crds.spec.bars'
May 19 08:58:30.725: INFO: stderr: ""
May 19 08:58:30.725: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9095-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 19 08:58:30.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-9095-crds.spec.bars2'
May 19 08:58:31.054: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:58:35.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1605" for this suite.
May 19 08:58:41.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:58:41.663: INFO: namespace crd-publish-openapi-1605 deletion completed in 6.092518712s

• [SLOW TEST:18.114 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:58:41.663: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9432
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 19 08:58:41.695: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 19 08:59:03.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.161.112:8080/dial?request=hostName&protocol=udp&host=10.244.161.82&port=8081&tries=1'] Namespace:pod-network-test-9432 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 08:59:03.771: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 08:59:04.526: INFO: Waiting for endpoints: map[]
May 19 08:59:04.530: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.161.112:8080/dial?request=hostName&protocol=udp&host=10.244.148.72&port=8081&tries=1'] Namespace:pod-network-test-9432 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 08:59:04.530: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 08:59:05.088: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:59:05.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9432" for this suite.
May 19 08:59:17.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:59:17.199: INFO: namespace pod-network-test-9432 deletion completed in 12.104422816s

• [SLOW TEST:35.536 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:59:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 08:59:17.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4" in namespace "downward-api-7363" to be "success or failure"
May 19 08:59:17.240: INFO: Pod "downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494138ms
May 19 08:59:19.244: INFO: Pod "downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006283385s
May 19 08:59:21.248: INFO: Pod "downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010961864s
STEP: Saw pod success
May 19 08:59:21.248: INFO: Pod "downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4" satisfied condition "success or failure"
May 19 08:59:21.251: INFO: Trying to get logs from node nchc-kubemaster01 pod downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4 container client-container: <nil>
STEP: delete the pod
May 19 08:59:21.270: INFO: Waiting for pod downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4 to disappear
May 19 08:59:21.272: INFO: Pod downwardapi-volume-bf9ffd98-1c90-4103-9e06-337397dbf3a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:59:21.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7363" for this suite.
May 19 08:59:27.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:59:27.360: INFO: namespace downward-api-7363 deletion completed in 6.084095008s

• [SLOW TEST:10.160 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:59:27.360: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 08:59:27.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5334" for this suite.
May 19 08:59:33.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 08:59:33.479: INFO: namespace custom-resource-definition-5334 deletion completed in 6.085319413s

• [SLOW TEST:6.119 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 08:59:33.479: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 19 08:59:33.515: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107939 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 19 08:59:33.515: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107939 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 19 08:59:43.524: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107958 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 19 08:59:43.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107958 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 19 08:59:53.534: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107978 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 19 08:59:53.534: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107978 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 19 09:00:03.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107998 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 19 09:00:03.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-a 76a399b7-913b-43c7-82b0-71431020dcbb 1107998 0 2020-05-19 08:59:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 19 09:00:13.552: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-b 4d3b0ff2-6284-49d3-ae65-c6c2a649cdbf 1108018 0 2020-05-19 09:00:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 19 09:00:13.552: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-b 4d3b0ff2-6284-49d3-ae65-c6c2a649cdbf 1108018 0 2020-05-19 09:00:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 19 09:00:23.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-b 4d3b0ff2-6284-49d3-ae65-c6c2a649cdbf 1108040 0 2020-05-19 09:00:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 19 09:00:23.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1160 /api/v1/namespaces/watch-1160/configmaps/e2e-watch-test-configmap-b 4d3b0ff2-6284-49d3-ae65-c6c2a649cdbf 1108040 0 2020-05-19 09:00:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:00:33.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1160" for this suite.
May 19 09:00:39.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:00:39.655: INFO: namespace watch-1160 deletion completed in 6.086475024s

• [SLOW TEST:66.176 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:00:39.656: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e8977e46-8c83-46ac-919b-59c3e79c116f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-e8977e46-8c83-46ac-919b-59c3e79c116f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:02:10.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5559" for this suite.
May 19 09:02:22.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:02:22.319: INFO: namespace projected-5559 deletion completed in 12.087237352s

• [SLOW TEST:102.664 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:02:22.320: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 19 09:02:23.169: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 19 09:02:25.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475743, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475743, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475743, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475743, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:02:28.202: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:02:28.206: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:02:29.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-287" for this suite.
May 19 09:02:35.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:02:35.557: INFO: namespace crd-webhook-287 deletion completed in 6.093628411s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.250 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:02:35.571: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1448/secret-test-f35d9d9e-3b23-429e-aa6c-096a2308df46
STEP: Creating a pod to test consume secrets
May 19 09:02:35.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2" in namespace "secrets-1448" to be "success or failure"
May 19 09:02:35.616: INFO: Pod "pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.290199ms
May 19 09:02:37.620: INFO: Pod "pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006274864s
May 19 09:02:39.624: INFO: Pod "pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01068885s
STEP: Saw pod success
May 19 09:02:39.625: INFO: Pod "pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2" satisfied condition "success or failure"
May 19 09:02:39.627: INFO: Trying to get logs from node nchc-worker03 pod pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2 container env-test: <nil>
STEP: delete the pod
May 19 09:02:39.647: INFO: Waiting for pod pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2 to disappear
May 19 09:02:39.650: INFO: Pod pod-configmaps-c6df7a02-6180-4f1c-9657-6cc4653ef5f2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:02:39.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1448" for this suite.
May 19 09:02:45.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:02:45.742: INFO: namespace secrets-1448 deletion completed in 6.088890729s

• [SLOW TEST:10.172 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:02:45.743: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:02:45.779: INFO: Waiting up to 5m0s for pod "downwardapi-volume-784a4af3-361b-46d9-b5d3-0ab001126159" in namespace "projected-7262" to be "success or failure"
May 19 09:02:45.782: INFO: Pod "downwardapi-volume-784a4af3-361b-46d9-b5d3-0ab001126159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.304446ms
May 19 09:02:47.785: INFO: Pod "downwardapi-volume-784a4af3-361b-46d9-b5d3-0ab001126159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005522104s
STEP: Saw pod success
May 19 09:02:47.785: INFO: Pod "downwardapi-volume-784a4af3-361b-46d9-b5d3-0ab001126159" satisfied condition "success or failure"
May 19 09:02:47.788: INFO: Trying to get logs from node nchc-kubemaster01 pod downwardapi-volume-784a4af3-361b-46d9-b5d3-0ab001126159 container client-container: <nil>
STEP: delete the pod
May 19 09:02:47.817: INFO: Waiting for pod downwardapi-volume-784a4af3-361b-46d9-b5d3-0ab001126159 to disappear
May 19 09:02:47.819: INFO: Pod downwardapi-volume-784a4af3-361b-46d9-b5d3-0ab001126159 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:02:47.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7262" for this suite.
May 19 09:02:53.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:02:53.915: INFO: namespace projected-7262 deletion completed in 6.091599562s

• [SLOW TEST:8.172 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:02:53.915: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:02:57.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7781" for this suite.
May 19 09:03:04.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:03:04.081: INFO: namespace emptydir-wrapper-7781 deletion completed in 6.086511063s

• [SLOW TEST:10.166 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:03:04.081: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:03:04.122: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 19 09:03:04.133: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:04.133: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:04.133: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:04.135: INFO: Number of nodes with available pods: 0
May 19 09:03:04.135: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 09:03:05.140: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:05.140: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:05.140: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:05.143: INFO: Number of nodes with available pods: 0
May 19 09:03:05.143: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 09:03:06.140: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:06.140: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:06.140: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:06.143: INFO: Number of nodes with available pods: 1
May 19 09:03:06.143: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 09:03:07.140: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:07.141: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:07.141: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:07.144: INFO: Number of nodes with available pods: 2
May 19 09:03:07.144: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 19 09:03:07.166: INFO: Wrong image for pod: daemon-set-bwzq2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:07.166: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:07.171: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:07.171: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:07.171: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:08.176: INFO: Wrong image for pod: daemon-set-bwzq2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:08.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:08.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:08.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:08.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:09.176: INFO: Wrong image for pod: daemon-set-bwzq2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:09.176: INFO: Pod daemon-set-bwzq2 is not available
May 19 09:03:09.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:09.181: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:09.181: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:09.181: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:10.176: INFO: Pod daemon-set-hf2pt is not available
May 19 09:03:10.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:10.181: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:10.181: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:10.181: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:11.176: INFO: Pod daemon-set-hf2pt is not available
May 19 09:03:11.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:11.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:11.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:11.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:12.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:12.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:12.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:12.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:13.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:13.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:13.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:13.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:14.175: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:14.179: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:14.179: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:14.179: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:15.175: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:15.175: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:15.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:15.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:15.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:16.175: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:16.176: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:16.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:16.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:16.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:17.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:17.176: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:17.181: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:17.181: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:17.181: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:18.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:18.176: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:18.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:18.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:18.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:19.176: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:19.176: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:19.181: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:19.181: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:19.181: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:20.175: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:20.175: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:20.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:20.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:20.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:21.175: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:21.175: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:21.180: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:21.180: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:21.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:22.177: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:22.177: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:22.181: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:22.181: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:22.181: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:23.175: INFO: Wrong image for pod: daemon-set-m5ztm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
May 19 09:03:23.175: INFO: Pod daemon-set-m5ztm is not available
May 19 09:03:23.179: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:23.179: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:23.180: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:24.175: INFO: Pod daemon-set-lb8gh is not available
May 19 09:03:24.179: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:24.179: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:24.179: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 19 09:03:24.183: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:24.183: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:24.184: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:24.186: INFO: Number of nodes with available pods: 1
May 19 09:03:24.186: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 09:03:25.192: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:25.192: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:25.192: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:25.195: INFO: Number of nodes with available pods: 1
May 19 09:03:25.195: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 09:03:26.193: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:26.193: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:26.193: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:26.197: INFO: Number of nodes with available pods: 1
May 19 09:03:26.197: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 09:03:27.193: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:27.193: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:27.193: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:03:27.197: INFO: Number of nodes with available pods: 2
May 19 09:03:27.197: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8801, will wait for the garbage collector to delete the pods
May 19 09:03:27.273: INFO: Deleting DaemonSet.extensions daemon-set took: 8.562805ms
May 19 09:03:27.573: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.246284ms
May 19 09:03:35.276: INFO: Number of nodes with available pods: 0
May 19 09:03:35.276: INFO: Number of running nodes: 0, number of available pods: 0
May 19 09:03:35.279: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8801/daemonsets","resourceVersion":"1108742"},"items":null}

May 19 09:03:35.281: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8801/pods","resourceVersion":"1108742"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:03:35.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8801" for this suite.
May 19 09:03:41.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:03:41.390: INFO: namespace daemonsets-8801 deletion completed in 6.095518692s

• [SLOW TEST:37.308 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:03:41.390: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1475
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1475
STEP: creating replication controller externalsvc in namespace services-1475
I0519 09:03:41.453349      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1475, replica count: 2
I0519 09:03:44.503852      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 19 09:03:44.526: INFO: Creating new exec pod
May 19 09:03:48.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-1475 execpodcj4k4 -- /bin/sh -x -c nslookup clusterip-service'
May 19 09:03:49.018: INFO: stderr: "+ nslookup clusterip-service\n"
May 19 09:03:49.018: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1475.svc.cluster.local\tcanonical name = externalsvc.services-1475.svc.cluster.local.\nName:\texternalsvc.services-1475.svc.cluster.local\nAddress: 10.101.80.195\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1475, will wait for the garbage collector to delete the pods
May 19 09:03:49.078: INFO: Deleting ReplicationController externalsvc took: 6.486253ms
May 19 09:03:49.179: INFO: Terminating ReplicationController externalsvc pods took: 100.275861ms
May 19 09:03:54.202: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:03:54.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1475" for this suite.
May 19 09:04:00.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:04:00.307: INFO: namespace services-1475 deletion completed in 6.089429283s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.918 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:04:00.308: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-1d0aa4c6-1e40-4908-ace1-d8021ca9ed64
STEP: Creating a pod to test consume secrets
May 19 09:04:00.348: INFO: Waiting up to 5m0s for pod "pod-secrets-f5d095ae-0455-4421-bdc1-569736a00306" in namespace "secrets-3615" to be "success or failure"
May 19 09:04:00.350: INFO: Pod "pod-secrets-f5d095ae-0455-4421-bdc1-569736a00306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.208833ms
May 19 09:04:02.354: INFO: Pod "pod-secrets-f5d095ae-0455-4421-bdc1-569736a00306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005680157s
STEP: Saw pod success
May 19 09:04:02.354: INFO: Pod "pod-secrets-f5d095ae-0455-4421-bdc1-569736a00306" satisfied condition "success or failure"
May 19 09:04:02.356: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-secrets-f5d095ae-0455-4421-bdc1-569736a00306 container secret-env-test: <nil>
STEP: delete the pod
May 19 09:04:02.375: INFO: Waiting for pod pod-secrets-f5d095ae-0455-4421-bdc1-569736a00306 to disappear
May 19 09:04:02.378: INFO: Pod pod-secrets-f5d095ae-0455-4421-bdc1-569736a00306 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:04:02.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3615" for this suite.
May 19 09:04:08.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:04:08.470: INFO: namespace secrets-3615 deletion completed in 6.089039982s

• [SLOW TEST:8.163 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:04:08.470: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 19 09:04:08.499: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:04:13.070: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:04:30.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5586" for this suite.
May 19 09:04:36.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:04:36.307: INFO: namespace crd-publish-openapi-5586 deletion completed in 6.09914693s

• [SLOW TEST:27.837 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:04:36.308: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:04:37.893: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:04:39.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475877, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475877, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475877, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475877, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:04:42.926: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:04:42.930: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:04:44.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6521" for this suite.
May 19 09:04:50.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:04:50.182: INFO: namespace webhook-6521 deletion completed in 6.09302169s
STEP: Destroying namespace "webhook-6521-markers" for this suite.
May 19 09:04:56.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:04:56.270: INFO: namespace webhook-6521-markers deletion completed in 6.087857133s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.975 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:04:56.283: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-3cf7658f-967f-493a-b9cb-2d4366249f9c
STEP: Creating a pod to test consume secrets
May 19 09:04:56.323: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3" in namespace "projected-2041" to be "success or failure"
May 19 09:04:56.325: INFO: Pod "pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211417ms
May 19 09:04:58.330: INFO: Pod "pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006412524s
May 19 09:05:00.334: INFO: Pod "pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011086114s
STEP: Saw pod success
May 19 09:05:00.334: INFO: Pod "pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3" satisfied condition "success or failure"
May 19 09:05:00.337: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 19 09:05:00.358: INFO: Waiting for pod pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3 to disappear
May 19 09:05:00.360: INFO: Pod pod-projected-secrets-f309ab0e-6ce3-4a04-ae8d-169edb8b27a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:05:00.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2041" for this suite.
May 19 09:05:06.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:05:06.461: INFO: namespace projected-2041 deletion completed in 6.096320552s

• [SLOW TEST:10.178 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:05:06.461: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:05:06.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa" in namespace "projected-5040" to be "success or failure"
May 19 09:05:06.506: INFO: Pod "downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.28042ms
May 19 09:05:08.511: INFO: Pod "downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006662s
May 19 09:05:10.515: INFO: Pod "downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010558105s
STEP: Saw pod success
May 19 09:05:10.515: INFO: Pod "downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa" satisfied condition "success or failure"
May 19 09:05:10.518: INFO: Trying to get logs from node nchc-kubemaster01 pod downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa container client-container: <nil>
STEP: delete the pod
May 19 09:05:10.537: INFO: Waiting for pod downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa to disappear
May 19 09:05:10.539: INFO: Pod downwardapi-volume-7f40b6a4-5cd1-4651-86b9-8671342ec8fa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:05:10.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5040" for this suite.
May 19 09:05:16.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:05:16.635: INFO: namespace projected-5040 deletion completed in 6.091825711s

• [SLOW TEST:10.174 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:05:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:05:18.084: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:05:20.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475918, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475918, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475918, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725475918, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:05:23.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:05:23.121: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3163-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:05:24.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7218" for this suite.
May 19 09:05:30.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:05:30.449: INFO: namespace webhook-7218 deletion completed in 6.095230872s
STEP: Destroying namespace "webhook-7218-markers" for this suite.
May 19 09:05:36.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:05:36.538: INFO: namespace webhook-7218-markers deletion completed in 6.089161298s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.915 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:05:36.551: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May 19 09:05:36.587: INFO: Waiting up to 5m0s for pod "pod-3eb3e6ac-84fc-496f-8a04-500a7fe1628b" in namespace "emptydir-5285" to be "success or failure"
May 19 09:05:36.590: INFO: Pod "pod-3eb3e6ac-84fc-496f-8a04-500a7fe1628b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.344047ms
May 19 09:05:38.594: INFO: Pod "pod-3eb3e6ac-84fc-496f-8a04-500a7fe1628b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006240921s
STEP: Saw pod success
May 19 09:05:38.594: INFO: Pod "pod-3eb3e6ac-84fc-496f-8a04-500a7fe1628b" satisfied condition "success or failure"
May 19 09:05:38.596: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-3eb3e6ac-84fc-496f-8a04-500a7fe1628b container test-container: <nil>
STEP: delete the pod
May 19 09:05:38.615: INFO: Waiting for pod pod-3eb3e6ac-84fc-496f-8a04-500a7fe1628b to disappear
May 19 09:05:38.617: INFO: Pod pod-3eb3e6ac-84fc-496f-8a04-500a7fe1628b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:05:38.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5285" for this suite.
May 19 09:05:44.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:05:44.710: INFO: namespace emptydir-5285 deletion completed in 6.088765641s

• [SLOW TEST:8.159 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:05:44.710: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-8vzk
STEP: Creating a pod to test atomic-volume-subpath
May 19 09:05:44.754: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8vzk" in namespace "subpath-3433" to be "success or failure"
May 19 09:05:44.757: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.244661ms
May 19 09:05:46.762: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007062266s
May 19 09:05:48.767: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 4.012189552s
May 19 09:05:50.772: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 6.017187419s
May 19 09:05:52.777: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 8.022617401s
May 19 09:05:54.781: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 10.026838204s
May 19 09:05:56.787: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 12.032579473s
May 19 09:05:58.792: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 14.037925574s
May 19 09:06:00.797: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 16.042657482s
May 19 09:06:02.802: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 18.047188384s
May 19 09:06:04.807: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 20.052068009s
May 19 09:06:06.812: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Running", Reason="", readiness=true. Elapsed: 22.057952595s
May 19 09:06:08.817: INFO: Pod "pod-subpath-test-configmap-8vzk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.062587003s
STEP: Saw pod success
May 19 09:06:08.817: INFO: Pod "pod-subpath-test-configmap-8vzk" satisfied condition "success or failure"
May 19 09:06:08.820: INFO: Trying to get logs from node nchc-worker03 pod pod-subpath-test-configmap-8vzk container test-container-subpath-configmap-8vzk: <nil>
STEP: delete the pod
May 19 09:06:08.852: INFO: Waiting for pod pod-subpath-test-configmap-8vzk to disappear
May 19 09:06:08.854: INFO: Pod pod-subpath-test-configmap-8vzk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8vzk
May 19 09:06:08.854: INFO: Deleting pod "pod-subpath-test-configmap-8vzk" in namespace "subpath-3433"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:06:08.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3433" for this suite.
May 19 09:06:14.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:06:14.950: INFO: namespace subpath-3433 deletion completed in 6.089405676s

• [SLOW TEST:30.239 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:06:14.950: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 19 09:06:14.978: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:06:19.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8330" for this suite.
May 19 09:06:25.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:06:25.537: INFO: namespace init-container-8330 deletion completed in 6.095157468s

• [SLOW TEST:10.587 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:06:25.537: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
May 19 09:06:29.592: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-934715428 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May 19 09:06:34.698: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:06:34.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2065" for this suite.
May 19 09:06:40.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:06:40.802: INFO: namespace pods-2065 deletion completed in 6.096109977s

• [SLOW TEST:15.265 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:06:40.802: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0519 09:06:50.864230      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 19 09:06:50.864: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:06:50.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6827" for this suite.
May 19 09:06:56.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:06:56.961: INFO: namespace gc-6827 deletion completed in 6.092396393s

• [SLOW TEST:16.159 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:06:56.961: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
May 19 09:06:56.997: INFO: Waiting up to 5m0s for pod "pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa" in namespace "emptydir-1673" to be "success or failure"
May 19 09:06:56.999: INFO: Pod "pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314434ms
May 19 09:06:59.003: INFO: Pod "pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.006168495s
May 19 09:07:01.007: INFO: Pod "pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010455089s
STEP: Saw pod success
May 19 09:07:01.007: INFO: Pod "pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa" satisfied condition "success or failure"
May 19 09:07:01.010: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa container test-container: <nil>
STEP: delete the pod
May 19 09:07:01.028: INFO: Waiting for pod pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa to disappear
May 19 09:07:01.030: INFO: Pod pod-45c81085-951f-470c-9ba7-7d6f7b7b85aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:07:01.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1673" for this suite.
May 19 09:07:07.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:07:07.125: INFO: namespace emptydir-1673 deletion completed in 6.090021056s

• [SLOW TEST:10.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:07:07.125: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:07:07.155: INFO: Creating deployment "test-recreate-deployment"
May 19 09:07:07.161: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 19 09:07:07.166: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 19 09:07:09.173: INFO: Waiting deployment "test-recreate-deployment" to complete
May 19 09:07:09.176: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725476027, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725476027, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725476027, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725476027, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 19 09:07:11.179: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 19 09:07:11.186: INFO: Updating deployment test-recreate-deployment
May 19 09:07:11.186: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 19 09:07:11.242: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8856 /apis/apps/v1/namespaces/deployment-8856/deployments/test-recreate-deployment adabf7fa-12b0-443d-a892-d58626fd0f5f 1109876 2 2020-05-19 09:07:07 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003925f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-05-19 09:07:11 +0000 UTC,LastTransitionTime:2020-05-19 09:07:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-05-19 09:07:11 +0000 UTC,LastTransitionTime:2020-05-19 09:07:07 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 19 09:07:11.245: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-8856 /apis/apps/v1/namespaces/deployment-8856/replicasets/test-recreate-deployment-5f94c574ff 92606ad4-b827-48e0-8b27-8a5c79024c29 1109874 1 2020-05-19 09:07:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment adabf7fa-12b0-443d-a892-d58626fd0f5f 0xc003b582e7 0xc003b582e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003b58348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 19 09:07:11.245: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 19 09:07:11.245: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-8856 /apis/apps/v1/namespaces/deployment-8856/replicasets/test-recreate-deployment-68fc85c7bb 3b9ed0ec-bac8-4644-b974-08c8cd5ab4b8 1109865 2 2020-05-19 09:07:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment adabf7fa-12b0-443d-a892-d58626fd0f5f 0xc003b583b7 0xc003b583b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003b58418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 19 09:07:11.248: INFO: Pod "test-recreate-deployment-5f94c574ff-xwjtg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-xwjtg test-recreate-deployment-5f94c574ff- deployment-8856 /api/v1/namespaces/deployment-8856/pods/test-recreate-deployment-5f94c574ff-xwjtg 95ac02fd-0aeb-46d0-b001-03d40a51137b 1109877 0 2020-05-19 09:07:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 92606ad4-b827-48e0-8b27-8a5c79024c29 0xc003b588b7 0xc003b588b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b9ltk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b9ltk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b9ltk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:07:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:07:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:,StartTime:2020-05-19 09:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:07:11.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8856" for this suite.
May 19 09:07:17.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:07:17.349: INFO: namespace deployment-8856 deletion completed in 6.097171511s

• [SLOW TEST:10.224 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:07:17.349: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6401
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6401
STEP: creating replication controller externalsvc in namespace services-6401
I0519 09:07:17.414674      25 runners.go:184] Created replication controller with name: externalsvc, namespace: services-6401, replica count: 2
I0519 09:07:20.465205      25 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 19 09:07:20.492: INFO: Creating new exec pod
May 19 09:07:24.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-6401 execpodhpr2s -- /bin/sh -x -c nslookup nodeport-service'
May 19 09:07:25.038: INFO: stderr: "+ nslookup nodeport-service\n"
May 19 09:07:25.038: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-6401.svc.cluster.local\tcanonical name = externalsvc.services-6401.svc.cluster.local.\nName:\texternalsvc.services-6401.svc.cluster.local\nAddress: 10.96.144.53\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6401, will wait for the garbage collector to delete the pods
May 19 09:07:25.100: INFO: Deleting ReplicationController externalsvc took: 7.26255ms
May 19 09:07:25.400: INFO: Terminating ReplicationController externalsvc pods took: 300.305372ms
May 19 09:07:30.024: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:07:30.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6401" for this suite.
May 19 09:07:36.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:07:36.123: INFO: namespace services-6401 deletion completed in 6.084376961s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.774 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:07:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
May 19 09:07:36.161: INFO: Waiting up to 5m0s for pod "var-expansion-660637f6-fbd7-4bc8-9580-557e0eb31a26" in namespace "var-expansion-5694" to be "success or failure"
May 19 09:07:36.163: INFO: Pod "var-expansion-660637f6-fbd7-4bc8-9580-557e0eb31a26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319253ms
May 19 09:07:38.169: INFO: Pod "var-expansion-660637f6-fbd7-4bc8-9580-557e0eb31a26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007925447s
STEP: Saw pod success
May 19 09:07:38.169: INFO: Pod "var-expansion-660637f6-fbd7-4bc8-9580-557e0eb31a26" satisfied condition "success or failure"
May 19 09:07:38.172: INFO: Trying to get logs from node nchc-kubemaster01 pod var-expansion-660637f6-fbd7-4bc8-9580-557e0eb31a26 container dapi-container: <nil>
STEP: delete the pod
May 19 09:07:38.190: INFO: Waiting for pod var-expansion-660637f6-fbd7-4bc8-9580-557e0eb31a26 to disappear
May 19 09:07:38.193: INFO: Pod var-expansion-660637f6-fbd7-4bc8-9580-557e0eb31a26 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:07:38.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5694" for this suite.
May 19 09:07:44.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:07:44.280: INFO: namespace var-expansion-5694 deletion completed in 6.083440586s

• [SLOW TEST:8.156 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:07:44.281: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 19 09:07:48.874: INFO: Successfully updated pod "labelsupdate1e677cac-8f21-4dba-bccf-a8336a6c45d7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:07:50.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1856" for this suite.
May 19 09:08:18.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:08:18.984: INFO: namespace downward-api-1856 deletion completed in 28.088635849s

• [SLOW TEST:34.704 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:08:18.985: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 19 09:08:19.024: INFO: Waiting up to 5m0s for pod "pod-bfdbf7db-0375-43e7-a50d-e00afab25d67" in namespace "emptydir-2150" to be "success or failure"
May 19 09:08:19.027: INFO: Pod "pod-bfdbf7db-0375-43e7-a50d-e00afab25d67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.764288ms
May 19 09:08:21.031: INFO: Pod "pod-bfdbf7db-0375-43e7-a50d-e00afab25d67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00654676s
May 19 09:08:23.035: INFO: Pod "pod-bfdbf7db-0375-43e7-a50d-e00afab25d67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010992385s
STEP: Saw pod success
May 19 09:08:23.035: INFO: Pod "pod-bfdbf7db-0375-43e7-a50d-e00afab25d67" satisfied condition "success or failure"
May 19 09:08:23.038: INFO: Trying to get logs from node nchc-worker03 pod pod-bfdbf7db-0375-43e7-a50d-e00afab25d67 container test-container: <nil>
STEP: delete the pod
May 19 09:08:23.059: INFO: Waiting for pod pod-bfdbf7db-0375-43e7-a50d-e00afab25d67 to disappear
May 19 09:08:23.061: INFO: Pod pod-bfdbf7db-0375-43e7-a50d-e00afab25d67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:08:23.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2150" for this suite.
May 19 09:08:29.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:08:29.156: INFO: namespace emptydir-2150 deletion completed in 6.09015662s

• [SLOW TEST:10.170 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:08:29.156: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:08:29.184: INFO: Creating ReplicaSet my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42
May 19 09:08:29.193: INFO: Pod name my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42: Found 0 pods out of 1
May 19 09:08:34.197: INFO: Pod name my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42: Found 1 pods out of 1
May 19 09:08:34.197: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42" is running
May 19 09:08:34.200: INFO: Pod "my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42-9hggx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 09:08:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 09:08:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 09:08:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-19 09:08:29 +0000 UTC Reason: Message:}])
May 19 09:08:34.200: INFO: Trying to dial the pod
May 19 09:08:39.212: INFO: Controller my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42: Got expected result from replica 1 [my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42-9hggx]: "my-hostname-basic-f0a76c2d-5023-4167-9c1e-ded233ed5a42-9hggx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:08:39.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-963" for this suite.
May 19 09:08:45.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:08:45.307: INFO: namespace replicaset-963 deletion completed in 6.090366567s

• [SLOW TEST:16.151 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:08:45.307: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-3fafa560-1c23-48e1-aec0-9397d19ec4cb
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:08:49.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4465" for this suite.
May 19 09:09:01.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:09:01.474: INFO: namespace configmap-4465 deletion completed in 12.086278696s

• [SLOW TEST:16.167 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:09:01.474: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 19 09:09:01.517: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9757 /api/v1/namespaces/watch-9757/configmaps/e2e-watch-test-watch-closed 4e2ae11f-d604-4003-a227-2018af2e9eb7 1110419 0 2020-05-19 09:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 19 09:09:01.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9757 /api/v1/namespaces/watch-9757/configmaps/e2e-watch-test-watch-closed 4e2ae11f-d604-4003-a227-2018af2e9eb7 1110420 0 2020-05-19 09:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 19 09:09:01.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9757 /api/v1/namespaces/watch-9757/configmaps/e2e-watch-test-watch-closed 4e2ae11f-d604-4003-a227-2018af2e9eb7 1110421 0 2020-05-19 09:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 19 09:09:01.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9757 /api/v1/namespaces/watch-9757/configmaps/e2e-watch-test-watch-closed 4e2ae11f-d604-4003-a227-2018af2e9eb7 1110422 0 2020-05-19 09:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:09:01.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9757" for this suite.
May 19 09:09:07.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:09:07.625: INFO: namespace watch-9757 deletion completed in 6.090855113s

• [SLOW TEST:6.150 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:09:07.625: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 19 09:09:07.660: INFO: Waiting up to 5m0s for pod "pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f" in namespace "emptydir-8280" to be "success or failure"
May 19 09:09:07.662: INFO: Pod "pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.234395ms
May 19 09:09:09.666: INFO: Pod "pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006131687s
May 19 09:09:11.670: INFO: Pod "pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010736622s
STEP: Saw pod success
May 19 09:09:11.670: INFO: Pod "pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f" satisfied condition "success or failure"
May 19 09:09:11.673: INFO: Trying to get logs from node nchc-worker03 pod pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f container test-container: <nil>
STEP: delete the pod
May 19 09:09:11.691: INFO: Waiting for pod pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f to disappear
May 19 09:09:11.693: INFO: Pod pod-21427d02-eff0-4f0a-8814-3ebfe7a9112f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:09:11.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8280" for this suite.
May 19 09:09:17.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:09:17.784: INFO: namespace emptydir-8280 deletion completed in 6.087417755s

• [SLOW TEST:10.159 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:09:17.784: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-f221c927-87e8-4fc3-8814-d82f3bfdd64c
STEP: Creating a pod to test consume configMaps
May 19 09:09:17.824: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41" in namespace "projected-8071" to be "success or failure"
May 19 09:09:17.826: INFO: Pod "pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24976ms
May 19 09:09:19.832: INFO: Pod "pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00777368s
May 19 09:09:21.836: INFO: Pod "pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012489036s
STEP: Saw pod success
May 19 09:09:21.836: INFO: Pod "pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41" satisfied condition "success or failure"
May 19 09:09:21.839: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 19 09:09:21.871: INFO: Waiting for pod pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41 to disappear
May 19 09:09:21.873: INFO: Pod pod-projected-configmaps-29065944-7875-4e94-9b4b-c80d2e1d7a41 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:09:21.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8071" for this suite.
May 19 09:09:27.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:09:27.967: INFO: namespace projected-8071 deletion completed in 6.090655713s

• [SLOW TEST:10.183 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:09:27.968: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-ca2fb648-e8ca-4e34-848e-e632151d505d in namespace container-probe-6391
May 19 09:09:32.012: INFO: Started pod test-webserver-ca2fb648-e8ca-4e34-848e-e632151d505d in namespace container-probe-6391
STEP: checking the pod's current state and verifying that restartCount is present
May 19 09:09:32.015: INFO: Initial restart count of pod test-webserver-ca2fb648-e8ca-4e34-848e-e632151d505d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:13:32.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6391" for this suite.
May 19 09:13:38.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:13:38.661: INFO: namespace container-probe-6391 deletion completed in 6.089275979s

• [SLOW TEST:250.694 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:13:38.662: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6747
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-6747
May 19 09:13:38.705: INFO: Found 0 stateful pods, waiting for 1
May 19 09:13:48.711: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 19 09:13:48.729: INFO: Deleting all statefulset in ns statefulset-6747
May 19 09:13:48.732: INFO: Scaling statefulset ss to 0
May 19 09:13:58.758: INFO: Waiting for statefulset status.replicas updated to 0
May 19 09:13:58.762: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:13:58.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6747" for this suite.
May 19 09:14:04.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:14:04.868: INFO: namespace statefulset-6747 deletion completed in 6.089644606s

• [SLOW TEST:26.206 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:14:04.869: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-zw8h
STEP: Creating a pod to test atomic-volume-subpath
May 19 09:14:04.912: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zw8h" in namespace "subpath-471" to be "success or failure"
May 19 09:14:04.914: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.320929ms
May 19 09:14:06.919: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006974335s
May 19 09:14:08.924: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 4.012875054s
May 19 09:14:10.929: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 6.01708943s
May 19 09:14:12.933: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 8.021676276s
May 19 09:14:14.938: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 10.026223871s
May 19 09:14:16.942: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 12.030581704s
May 19 09:14:18.947: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 14.035907411s
May 19 09:14:20.952: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 16.040092943s
May 19 09:14:22.956: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 18.044549603s
May 19 09:14:24.960: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Running", Reason="", readiness=true. Elapsed: 20.048709712s
May 19 09:14:26.964: INFO: Pod "pod-subpath-test-projected-zw8h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0527733s
STEP: Saw pod success
May 19 09:14:26.964: INFO: Pod "pod-subpath-test-projected-zw8h" satisfied condition "success or failure"
May 19 09:14:26.967: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-subpath-test-projected-zw8h container test-container-subpath-projected-zw8h: <nil>
STEP: delete the pod
May 19 09:14:26.997: INFO: Waiting for pod pod-subpath-test-projected-zw8h to disappear
May 19 09:14:26.999: INFO: Pod pod-subpath-test-projected-zw8h no longer exists
STEP: Deleting pod pod-subpath-test-projected-zw8h
May 19 09:14:26.999: INFO: Deleting pod "pod-subpath-test-projected-zw8h" in namespace "subpath-471"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:14:27.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-471" for this suite.
May 19 09:14:33.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:14:33.097: INFO: namespace subpath-471 deletion completed in 6.09154718s

• [SLOW TEST:28.229 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:14:33.098: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:14:33.124: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:14:35.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7703" for this suite.
May 19 09:15:19.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:15:19.290: INFO: namespace pods-7703 deletion completed in 44.091010463s

• [SLOW TEST:46.193 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:15:19.291: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:15:23.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3343" for this suite.
May 19 09:16:07.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:16:07.452: INFO: namespace kubelet-test-3343 deletion completed in 44.087905345s

• [SLOW TEST:48.161 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:16:07.452: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:16:11.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8489" for this suite.
May 19 09:16:17.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:16:17.590: INFO: namespace kubelet-test-8489 deletion completed in 6.090957059s

• [SLOW TEST:10.138 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:16:17.591: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May 19 09:16:17.626: INFO: Waiting up to 5m0s for pod "pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf" in namespace "emptydir-3307" to be "success or failure"
May 19 09:16:17.628: INFO: Pod "pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182366ms
May 19 09:16:19.632: INFO: Pod "pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00571798s
May 19 09:16:21.636: INFO: Pod "pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010378685s
STEP: Saw pod success
May 19 09:16:21.637: INFO: Pod "pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf" satisfied condition "success or failure"
May 19 09:16:21.639: INFO: Trying to get logs from node nchc-worker03 pod pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf container test-container: <nil>
STEP: delete the pod
May 19 09:16:21.656: INFO: Waiting for pod pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf to disappear
May 19 09:16:21.658: INFO: Pod pod-6f984bd5-aa39-4462-b809-0ecb5ee262cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:16:21.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3307" for this suite.
May 19 09:16:27.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:16:27.748: INFO: namespace emptydir-3307 deletion completed in 6.08574947s

• [SLOW TEST:10.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:16:27.749: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:16:53.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3540" for this suite.
May 19 09:16:59.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:17:00.072: INFO: namespace container-runtime-3540 deletion completed in 6.091154017s

• [SLOW TEST:32.323 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:17:00.072: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5677, will wait for the garbage collector to delete the pods
May 19 09:17:02.173: INFO: Deleting Job.batch foo took: 7.789106ms
May 19 09:17:02.473: INFO: Terminating Job.batch foo pods took: 300.266911ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:17:45.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5677" for this suite.
May 19 09:17:51.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:17:51.369: INFO: namespace job-5677 deletion completed in 6.088512367s

• [SLOW TEST:51.296 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:17:51.369: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 19 09:17:51.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7934'
May 19 09:17:51.592: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 19 09:17:51.592: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
May 19 09:17:51.598: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 19 09:17:51.601: INFO: scanned /root for discovery docs: <nil>
May 19 09:17:51.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7934'
May 19 09:18:07.403: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 19 09:18:07.403: INFO: stdout: "Created e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1\nScaling up e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
May 19 09:18:07.403: INFO: stdout: "Created e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1\nScaling up e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
May 19 09:18:07.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7934'
May 19 09:18:07.514: INFO: stderr: ""
May 19 09:18:07.514: INFO: stdout: "e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1-d9vxv "
May 19 09:18:07.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1-d9vxv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7934'
May 19 09:18:07.612: INFO: stderr: ""
May 19 09:18:07.612: INFO: stdout: "true"
May 19 09:18:07.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1-d9vxv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7934'
May 19 09:18:07.713: INFO: stderr: ""
May 19 09:18:07.713: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
May 19 09:18:07.713: INFO: e2e-test-httpd-rc-4df88e3361f1c5d5a78affeba0e197c1-d9vxv is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
May 19 09:18:07.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete rc e2e-test-httpd-rc --namespace=kubectl-7934'
May 19 09:18:07.815: INFO: stderr: ""
May 19 09:18:07.815: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:18:07.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7934" for this suite.
May 19 09:18:19.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:18:19.908: INFO: namespace kubectl-7934 deletion completed in 12.089319826s

• [SLOW TEST:28.539 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:18:19.908: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 19 09:18:19.950: INFO: Waiting up to 5m0s for pod "pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7" in namespace "emptydir-5896" to be "success or failure"
May 19 09:18:19.953: INFO: Pod "pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992609ms
May 19 09:18:21.958: INFO: Pod "pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007839584s
May 19 09:18:23.962: INFO: Pod "pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011380366s
STEP: Saw pod success
May 19 09:18:23.962: INFO: Pod "pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7" satisfied condition "success or failure"
May 19 09:18:23.964: INFO: Trying to get logs from node nchc-worker03 pod pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7 container test-container: <nil>
STEP: delete the pod
May 19 09:18:23.995: INFO: Waiting for pod pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7 to disappear
May 19 09:18:23.998: INFO: Pod pod-cb0829ed-1f2b-4ab5-aadf-879b4a247fd7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:18:23.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5896" for this suite.
May 19 09:18:30.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:18:30.091: INFO: namespace emptydir-5896 deletion completed in 6.088918012s

• [SLOW TEST:10.182 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:18:30.091: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:18:47.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3574" for this suite.
May 19 09:18:53.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:18:53.258: INFO: namespace resourcequota-3574 deletion completed in 6.085447331s

• [SLOW TEST:23.166 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:18:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:18:53.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6667" for this suite.
May 19 09:19:21.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:19:21.407: INFO: namespace kubelet-test-6667 deletion completed in 28.102482803s

• [SLOW TEST:28.149 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:19:21.407: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-c5ca466f-e503-4970-bb72-8b4fab286c3c
STEP: Creating a pod to test consume configMaps
May 19 09:19:21.448: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede" in namespace "configmap-6052" to be "success or failure"
May 19 09:19:21.451: INFO: Pod "pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede": Phase="Pending", Reason="", readiness=false. Elapsed: 2.500499ms
May 19 09:19:23.455: INFO: Pod "pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006682434s
May 19 09:19:25.459: INFO: Pod "pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011119225s
STEP: Saw pod success
May 19 09:19:25.459: INFO: Pod "pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede" satisfied condition "success or failure"
May 19 09:19:25.463: INFO: Trying to get logs from node nchc-worker03 pod pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede container configmap-volume-test: <nil>
STEP: delete the pod
May 19 09:19:25.483: INFO: Waiting for pod pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede to disappear
May 19 09:19:25.485: INFO: Pod pod-configmaps-ab94590b-0c16-42d9-b935-cf0900645ede no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:19:25.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6052" for this suite.
May 19 09:19:31.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:19:31.574: INFO: namespace configmap-6052 deletion completed in 6.084523176s

• [SLOW TEST:10.167 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:19:31.574: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May 19 09:19:31.603: INFO: namespace kubectl-1425
May 19 09:19:31.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-1425'
May 19 09:19:31.896: INFO: stderr: ""
May 19 09:19:31.896: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 19 09:19:32.901: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:19:32.901: INFO: Found 0 / 1
May 19 09:19:33.901: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:19:33.901: INFO: Found 0 / 1
May 19 09:19:34.901: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:19:34.901: INFO: Found 1 / 1
May 19 09:19:34.901: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 19 09:19:34.905: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:19:34.905: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 19 09:19:34.905: INFO: wait on redis-master startup in kubectl-1425 
May 19 09:19:34.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs redis-master-sbg5v redis-master --namespace=kubectl-1425'
May 19 09:19:35.034: INFO: stderr: ""
May 19 09:19:35.034: INFO: stdout: "1:C 19 May 2020 09:19:21.141 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 19 May 2020 09:19:21.141 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 19 May 2020 09:19:21.141 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 19 May 2020 09:19:21.144 * Running mode=standalone, port=6379.\n1:M 19 May 2020 09:19:21.144 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 May 2020 09:19:21.144 # Server initialized\n1:M 19 May 2020 09:19:21.144 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 May 2020 09:19:21.144 * Ready to accept connections\n"
STEP: exposing RC
May 19 09:19:35.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1425'
May 19 09:19:35.161: INFO: stderr: ""
May 19 09:19:35.161: INFO: stdout: "service/rm2 exposed\n"
May 19 09:19:35.164: INFO: Service rm2 in namespace kubectl-1425 found.
STEP: exposing service
May 19 09:19:37.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1425'
May 19 09:19:37.298: INFO: stderr: ""
May 19 09:19:37.298: INFO: stdout: "service/rm3 exposed\n"
May 19 09:19:37.301: INFO: Service rm3 in namespace kubectl-1425 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:19:39.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1425" for this suite.
May 19 09:20:07.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:20:07.398: INFO: namespace kubectl-1425 deletion completed in 28.084741849s

• [SLOW TEST:35.823 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:20:07.398: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 19 09:20:15.466: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 19 09:20:15.469: INFO: Pod pod-with-prestop-http-hook still exists
May 19 09:20:17.469: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 19 09:20:17.473: INFO: Pod pod-with-prestop-http-hook still exists
May 19 09:20:19.469: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 19 09:20:19.474: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:20:19.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1349" for this suite.
May 19 09:20:31.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:20:31.577: INFO: namespace container-lifecycle-hook-1349 deletion completed in 12.08962392s

• [SLOW TEST:24.179 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:20:31.577: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 19 09:20:31.613: INFO: Waiting up to 5m0s for pod "downward-api-dc570345-f802-408d-af48-c7dea262f9fc" in namespace "downward-api-5629" to be "success or failure"
May 19 09:20:31.615: INFO: Pod "downward-api-dc570345-f802-408d-af48-c7dea262f9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403977ms
May 19 09:20:33.619: INFO: Pod "downward-api-dc570345-f802-408d-af48-c7dea262f9fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0058599s
STEP: Saw pod success
May 19 09:20:33.619: INFO: Pod "downward-api-dc570345-f802-408d-af48-c7dea262f9fc" satisfied condition "success or failure"
May 19 09:20:33.621: INFO: Trying to get logs from node nchc-kubemaster01 pod downward-api-dc570345-f802-408d-af48-c7dea262f9fc container dapi-container: <nil>
STEP: delete the pod
May 19 09:20:33.652: INFO: Waiting for pod downward-api-dc570345-f802-408d-af48-c7dea262f9fc to disappear
May 19 09:20:33.655: INFO: Pod downward-api-dc570345-f802-408d-af48-c7dea262f9fc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:20:33.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5629" for this suite.
May 19 09:20:39.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:20:39.743: INFO: namespace downward-api-5629 deletion completed in 6.084987559s

• [SLOW TEST:8.166 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:20:39.744: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
May 19 09:20:39.771: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:21:01.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7851" for this suite.
May 19 09:21:07.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:21:07.327: INFO: namespace crd-publish-openapi-7851 deletion completed in 6.100832292s

• [SLOW TEST:27.584 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:21:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:21:07.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c" in namespace "projected-4221" to be "success or failure"
May 19 09:21:07.368: INFO: Pod "downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161902ms
May 19 09:21:09.371: INFO: Pod "downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00601509s
May 19 09:21:11.376: INFO: Pod "downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010453696s
STEP: Saw pod success
May 19 09:21:11.376: INFO: Pod "downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c" satisfied condition "success or failure"
May 19 09:21:11.378: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c container client-container: <nil>
STEP: delete the pod
May 19 09:21:11.399: INFO: Waiting for pod downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c to disappear
May 19 09:21:11.401: INFO: Pod downwardapi-volume-da9e4b7e-65d7-401c-88c0-e01520f1874c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:21:11.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4221" for this suite.
May 19 09:21:17.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:21:17.493: INFO: namespace projected-4221 deletion completed in 6.088596596s

• [SLOW TEST:10.166 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:21:17.494: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:21:17.552: INFO: Create a RollingUpdate DaemonSet
May 19 09:21:17.558: INFO: Check that daemon pods launch on every node of the cluster
May 19 09:21:17.562: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:17.562: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:17.562: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:17.564: INFO: Number of nodes with available pods: 0
May 19 09:21:17.564: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 09:21:18.570: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:18.570: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:18.570: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:18.573: INFO: Number of nodes with available pods: 0
May 19 09:21:18.573: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 09:21:19.569: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:19.569: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:19.569: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:19.571: INFO: Number of nodes with available pods: 1
May 19 09:21:19.571: INFO: Node nchc-worker03 is running more than one daemon pod
May 19 09:21:20.569: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:20.570: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:20.570: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:20.573: INFO: Number of nodes with available pods: 2
May 19 09:21:20.573: INFO: Number of running nodes: 2, number of available pods: 2
May 19 09:21:20.573: INFO: Update the DaemonSet to trigger a rollout
May 19 09:21:20.581: INFO: Updating DaemonSet daemon-set
May 19 09:21:23.596: INFO: Roll back the DaemonSet before rollout is complete
May 19 09:21:23.604: INFO: Updating DaemonSet daemon-set
May 19 09:21:23.604: INFO: Make sure DaemonSet rollback is complete
May 19 09:21:23.606: INFO: Wrong image for pod: daemon-set-przsn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 19 09:21:23.606: INFO: Pod daemon-set-przsn is not available
May 19 09:21:23.611: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:23.611: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:23.611: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:24.614: INFO: Wrong image for pod: daemon-set-przsn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 19 09:21:24.615: INFO: Pod daemon-set-przsn is not available
May 19 09:21:24.619: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:24.619: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:24.619: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:25.616: INFO: Wrong image for pod: daemon-set-przsn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 19 09:21:25.616: INFO: Pod daemon-set-przsn is not available
May 19 09:21:25.620: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:25.620: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:25.620: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:26.615: INFO: Pod daemon-set-84bmr is not available
May 19 09:21:26.620: INFO: DaemonSet pods can't tolerate node mgmt01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:26.620: INFO: DaemonSet pods can't tolerate node mgmt02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 19 09:21:26.620: INFO: DaemonSet pods can't tolerate node mgmt03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9853, will wait for the garbage collector to delete the pods
May 19 09:21:26.686: INFO: Deleting DaemonSet.extensions daemon-set took: 7.335201ms
May 19 09:21:26.986: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.235272ms
May 19 09:21:30.090: INFO: Number of nodes with available pods: 0
May 19 09:21:30.090: INFO: Number of running nodes: 0, number of available pods: 0
May 19 09:21:30.092: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9853/daemonsets","resourceVersion":"1112904"},"items":null}

May 19 09:21:30.095: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9853/pods","resourceVersion":"1112904"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:21:30.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9853" for this suite.
May 19 09:21:36.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:21:36.193: INFO: namespace daemonsets-9853 deletion completed in 6.084710145s

• [SLOW TEST:18.699 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:21:36.193: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:21:36.223: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:21:37.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4509" for this suite.
May 19 09:21:43.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:21:43.335: INFO: namespace custom-resource-definition-4509 deletion completed in 6.084678017s

• [SLOW TEST:7.142 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:21:43.336: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:21:43.363: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 19 09:21:47.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-2731 create -f -'
May 19 09:21:47.838: INFO: stderr: ""
May 19 09:21:47.838: INFO: stdout: "e2e-test-crd-publish-openapi-7105-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 19 09:21:47.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-2731 delete e2e-test-crd-publish-openapi-7105-crds test-cr'
May 19 09:21:47.996: INFO: stderr: ""
May 19 09:21:47.996: INFO: stdout: "e2e-test-crd-publish-openapi-7105-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 19 09:21:47.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-2731 apply -f -'
May 19 09:21:48.207: INFO: stderr: ""
May 19 09:21:48.207: INFO: stdout: "e2e-test-crd-publish-openapi-7105-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 19 09:21:48.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-2731 delete e2e-test-crd-publish-openapi-7105-crds test-cr'
May 19 09:21:48.314: INFO: stderr: ""
May 19 09:21:48.314: INFO: stdout: "e2e-test-crd-publish-openapi-7105-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 19 09:21:48.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-7105-crds'
May 19 09:21:48.518: INFO: stderr: ""
May 19 09:21:48.518: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7105-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:21:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2731" for this suite.
May 19 09:21:59.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:21:59.131: INFO: namespace crd-publish-openapi-2731 deletion completed in 6.089745574s

• [SLOW TEST:15.796 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:21:59.132: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 19 09:22:03.213: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:03.216: INFO: Pod pod-with-poststart-http-hook still exists
May 19 09:22:05.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:05.221: INFO: Pod pod-with-poststart-http-hook still exists
May 19 09:22:07.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:07.221: INFO: Pod pod-with-poststart-http-hook still exists
May 19 09:22:09.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:09.221: INFO: Pod pod-with-poststart-http-hook still exists
May 19 09:22:11.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:11.221: INFO: Pod pod-with-poststart-http-hook still exists
May 19 09:22:13.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:13.222: INFO: Pod pod-with-poststart-http-hook still exists
May 19 09:22:15.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:15.221: INFO: Pod pod-with-poststart-http-hook still exists
May 19 09:22:17.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 19 09:22:17.221: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:22:17.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3234" for this suite.
May 19 09:22:45.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:22:45.318: INFO: namespace container-lifecycle-hook-3234 deletion completed in 28.092309066s

• [SLOW TEST:46.187 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:22:45.319: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:22:45.359: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b83f37bd-b1d1-408c-bae2-2403992f204b" in namespace "security-context-test-6424" to be "success or failure"
May 19 09:22:45.362: INFO: Pod "alpine-nnp-false-b83f37bd-b1d1-408c-bae2-2403992f204b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.356064ms
May 19 09:22:47.365: INFO: Pod "alpine-nnp-false-b83f37bd-b1d1-408c-bae2-2403992f204b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005940498s
May 19 09:22:49.369: INFO: Pod "alpine-nnp-false-b83f37bd-b1d1-408c-bae2-2403992f204b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010184733s
May 19 09:22:49.369: INFO: Pod "alpine-nnp-false-b83f37bd-b1d1-408c-bae2-2403992f204b" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:22:49.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6424" for this suite.
May 19 09:22:55.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:22:55.485: INFO: namespace security-context-test-6424 deletion completed in 6.090389731s

• [SLOW TEST:10.166 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:22:55.485: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4193/configmap-test-809c81c2-dd15-4622-95cd-074320de3187
STEP: Creating a pod to test consume configMaps
May 19 09:22:55.527: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6cbf4cc-c15f-45ec-9dd1-1a9d16d788db" in namespace "configmap-4193" to be "success or failure"
May 19 09:22:55.529: INFO: Pod "pod-configmaps-e6cbf4cc-c15f-45ec-9dd1-1a9d16d788db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246831ms
May 19 09:22:57.532: INFO: Pod "pod-configmaps-e6cbf4cc-c15f-45ec-9dd1-1a9d16d788db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005269592s
STEP: Saw pod success
May 19 09:22:57.532: INFO: Pod "pod-configmaps-e6cbf4cc-c15f-45ec-9dd1-1a9d16d788db" satisfied condition "success or failure"
May 19 09:22:57.535: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-configmaps-e6cbf4cc-c15f-45ec-9dd1-1a9d16d788db container env-test: <nil>
STEP: delete the pod
May 19 09:22:57.575: INFO: Waiting for pod pod-configmaps-e6cbf4cc-c15f-45ec-9dd1-1a9d16d788db to disappear
May 19 09:22:57.577: INFO: Pod pod-configmaps-e6cbf4cc-c15f-45ec-9dd1-1a9d16d788db no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:22:57.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4193" for this suite.
May 19 09:23:03.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:23:03.672: INFO: namespace configmap-4193 deletion completed in 6.090080888s

• [SLOW TEST:8.187 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:23:03.672: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:23:05.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8830" for this suite.
May 19 09:23:49.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:23:49.817: INFO: namespace kubelet-test-8830 deletion completed in 44.087210833s

• [SLOW TEST:46.145 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:23:49.818: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3540
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3540
I0519 09:23:49.878858      25 runners.go:184] Created replication controller with name: externalname-service, namespace: services-3540, replica count: 2
May 19 09:23:52.929: INFO: Creating new exec pod
I0519 09:23:52.929369      25 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 19 09:23:57.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-3540 execpod7v5g5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 19 09:23:58.258: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 19 09:23:58.258: INFO: stdout: ""
May 19 09:23:58.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-3540 execpod7v5g5 -- /bin/sh -x -c nc -zv -t -w 2 10.104.115.25 80'
May 19 09:23:58.564: INFO: stderr: "+ nc -zv -t -w 2 10.104.115.25 80\nConnection to 10.104.115.25 80 port [tcp/http] succeeded!\n"
May 19 09:23:58.564: INFO: stdout: ""
May 19 09:23:58.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-3540 execpod7v5g5 -- /bin/sh -x -c nc -zv -t -w 2 10.78.26.171 32515'
May 19 09:23:58.885: INFO: stderr: "+ nc -zv -t -w 2 10.78.26.171 32515\nConnection to 10.78.26.171 32515 port [tcp/32515] succeeded!\n"
May 19 09:23:58.885: INFO: stdout: ""
May 19 09:23:58.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 exec --namespace=services-3540 execpod7v5g5 -- /bin/sh -x -c nc -zv -t -w 2 10.78.26.181 32515'
May 19 09:23:59.175: INFO: stderr: "+ nc -zv -t -w 2 10.78.26.181 32515\nConnection to 10.78.26.181 32515 port [tcp/32515] succeeded!\n"
May 19 09:23:59.175: INFO: stdout: ""
May 19 09:23:59.175: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:23:59.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3540" for this suite.
May 19 09:24:05.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:24:05.289: INFO: namespace services-3540 deletion completed in 6.080944381s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.472 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:24:05.290: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 19 09:24:05.327: INFO: Waiting up to 5m0s for pod "downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7" in namespace "downward-api-201" to be "success or failure"
May 19 09:24:05.329: INFO: Pod "downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.219662ms
May 19 09:24:07.332: INFO: Pod "downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005112935s
May 19 09:24:09.342: INFO: Pod "downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01497278s
STEP: Saw pod success
May 19 09:24:09.342: INFO: Pod "downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7" satisfied condition "success or failure"
May 19 09:24:09.344: INFO: Trying to get logs from node nchc-kubemaster01 pod downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7 container dapi-container: <nil>
STEP: delete the pod
May 19 09:24:09.363: INFO: Waiting for pod downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7 to disappear
May 19 09:24:09.366: INFO: Pod downward-api-6a0b9ea7-a120-42d9-97a8-e345de1fb0d7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:24:09.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-201" for this suite.
May 19 09:24:15.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:24:15.459: INFO: namespace downward-api-201 deletion completed in 6.0895966s

• [SLOW TEST:10.169 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:24:15.459: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
May 19 09:24:15.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 cluster-info'
May 19 09:24:15.593: INFO: stderr: ""
May 19 09:24:15.593: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:24:15.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6968" for this suite.
May 19 09:24:21.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:24:21.687: INFO: namespace kubectl-6968 deletion completed in 6.088388885s

• [SLOW TEST:6.228 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:24:21.687: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
May 19 09:24:21.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-8210'
May 19 09:24:21.997: INFO: stderr: ""
May 19 09:24:21.997: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 19 09:24:21.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8210'
May 19 09:24:22.102: INFO: stderr: ""
May 19 09:24:22.102: INFO: stdout: "update-demo-nautilus-dsf6w update-demo-nautilus-v426p "
May 19 09:24:22.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-dsf6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:22.201: INFO: stderr: ""
May 19 09:24:22.201: INFO: stdout: ""
May 19 09:24:22.201: INFO: update-demo-nautilus-dsf6w is created but not running
May 19 09:24:27.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8210'
May 19 09:24:27.316: INFO: stderr: ""
May 19 09:24:27.316: INFO: stdout: "update-demo-nautilus-dsf6w update-demo-nautilus-v426p "
May 19 09:24:27.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-dsf6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:27.423: INFO: stderr: ""
May 19 09:24:27.424: INFO: stdout: "true"
May 19 09:24:27.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-dsf6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:27.521: INFO: stderr: ""
May 19 09:24:27.521: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:24:27.521: INFO: validating pod update-demo-nautilus-dsf6w
May 19 09:24:27.527: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:24:27.527: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:24:27.527: INFO: update-demo-nautilus-dsf6w is verified up and running
May 19 09:24:27.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-v426p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:27.625: INFO: stderr: ""
May 19 09:24:27.625: INFO: stdout: "true"
May 19 09:24:27.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-v426p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:27.723: INFO: stderr: ""
May 19 09:24:27.723: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:24:27.723: INFO: validating pod update-demo-nautilus-v426p
May 19 09:24:27.729: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:24:27.729: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:24:27.729: INFO: update-demo-nautilus-v426p is verified up and running
STEP: rolling-update to new replication controller
May 19 09:24:27.732: INFO: scanned /root for discovery docs: <nil>
May 19 09:24:27.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8210'
May 19 09:24:50.195: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 19 09:24:50.195: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 19 09:24:50.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8210'
May 19 09:24:50.304: INFO: stderr: ""
May 19 09:24:50.304: INFO: stdout: "update-demo-kitten-5rbbj update-demo-kitten-7z946 "
May 19 09:24:50.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-kitten-5rbbj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:50.408: INFO: stderr: ""
May 19 09:24:50.408: INFO: stdout: "true"
May 19 09:24:50.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-kitten-5rbbj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:50.504: INFO: stderr: ""
May 19 09:24:50.504: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 19 09:24:50.504: INFO: validating pod update-demo-kitten-5rbbj
May 19 09:24:50.510: INFO: got data: {
  "image": "kitten.jpg"
}

May 19 09:24:50.510: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 19 09:24:50.510: INFO: update-demo-kitten-5rbbj is verified up and running
May 19 09:24:50.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-kitten-7z946 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:50.606: INFO: stderr: ""
May 19 09:24:50.606: INFO: stdout: "true"
May 19 09:24:50.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-kitten-7z946 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8210'
May 19 09:24:50.704: INFO: stderr: ""
May 19 09:24:50.704: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 19 09:24:50.704: INFO: validating pod update-demo-kitten-7z946
May 19 09:24:50.710: INFO: got data: {
  "image": "kitten.jpg"
}

May 19 09:24:50.710: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 19 09:24:50.710: INFO: update-demo-kitten-7z946 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:24:50.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8210" for this suite.
May 19 09:25:18.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:25:18.802: INFO: namespace kubectl-8210 deletion completed in 28.0873967s

• [SLOW TEST:57.115 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:25:18.802: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
May 19 09:25:18.837: INFO: Waiting up to 5m0s for pod "pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e" in namespace "emptydir-6179" to be "success or failure"
May 19 09:25:18.840: INFO: Pod "pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781125ms
May 19 09:25:20.844: INFO: Pod "pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006322466s
May 19 09:25:22.848: INFO: Pod "pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010788381s
STEP: Saw pod success
May 19 09:25:22.848: INFO: Pod "pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e" satisfied condition "success or failure"
May 19 09:25:22.851: INFO: Trying to get logs from node nchc-worker03 pod pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e container test-container: <nil>
STEP: delete the pod
May 19 09:25:22.882: INFO: Waiting for pod pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e to disappear
May 19 09:25:22.884: INFO: Pod pod-10748c30-5d65-4a5c-8af9-5ad9170dd76e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:25:22.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6179" for this suite.
May 19 09:25:28.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:25:28.974: INFO: namespace emptydir-6179 deletion completed in 6.085807469s

• [SLOW TEST:10.171 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:25:28.974: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
May 19 09:25:33.023: INFO: Pod pod-hostip-340d4e5f-aac1-4141-9b17-99320b67a504 has hostIP: 10.78.26.181
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:25:33.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5869" for this suite.
May 19 09:26:01.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:26:01.111: INFO: namespace pods-5869 deletion completed in 28.082935992s

• [SLOW TEST:32.137 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:26:01.111: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:26:09.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5903" for this suite.
May 19 09:26:15.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:26:15.248: INFO: namespace job-5903 deletion completed in 6.094393612s

• [SLOW TEST:14.136 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:26:15.248: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1438.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1438.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1438.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1438.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1438.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1438.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 09:26:17.320: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc: the server could not find the requested resource (get pods dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc)
May 19 09:26:17.322: INFO: Unable to read jessie_udp@PodARecord from pod dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc: the server could not find the requested resource (get pods dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc)
May 19 09:26:17.326: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc: the server could not find the requested resource (get pods dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc)
May 19 09:26:17.326: INFO: Lookups using dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc failed for: [jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

May 19 09:26:22.347: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc: the server could not find the requested resource (get pods dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc)
May 19 09:26:22.350: INFO: Unable to read jessie_udp@PodARecord from pod dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc: the server could not find the requested resource (get pods dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc)
May 19 09:26:22.353: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc: the server could not find the requested resource (get pods dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc)
May 19 09:26:22.353: INFO: Lookups using dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc failed for: [jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

May 19 09:26:27.354: INFO: DNS probes using dns-1438/dns-test-b197baa9-ed5b-4aed-bba3-cef3dc1147dc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:26:27.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1438" for this suite.
May 19 09:26:33.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:26:33.464: INFO: namespace dns-1438 deletion completed in 6.085053743s

• [SLOW TEST:18.216 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:26:33.464: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-6c7ad5d5-0584-4546-b09c-716effc59e55
STEP: Creating a pod to test consume configMaps
May 19 09:26:33.504: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177" in namespace "configmap-6287" to be "success or failure"
May 19 09:26:33.507: INFO: Pod "pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275881ms
May 19 09:26:35.511: INFO: Pod "pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006620961s
May 19 09:26:37.515: INFO: Pod "pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010882439s
STEP: Saw pod success
May 19 09:26:37.515: INFO: Pod "pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177" satisfied condition "success or failure"
May 19 09:26:37.518: INFO: Trying to get logs from node nchc-worker03 pod pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177 container configmap-volume-test: <nil>
STEP: delete the pod
May 19 09:26:37.538: INFO: Waiting for pod pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177 to disappear
May 19 09:26:37.541: INFO: Pod pod-configmaps-ba3dd2ad-b42c-47b9-8897-0ad07ed4c177 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:26:37.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6287" for this suite.
May 19 09:26:43.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:26:43.632: INFO: namespace configmap-6287 deletion completed in 6.085900859s

• [SLOW TEST:10.168 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:26:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:26:44.687: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:26:47.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:26:57.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8462" for this suite.
May 19 09:27:03.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:27:03.921: INFO: namespace webhook-8462 deletion completed in 6.092239327s
STEP: Destroying namespace "webhook-8462-markers" for this suite.
May 19 09:27:09.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:27:10.009: INFO: namespace webhook-8462-markers deletion completed in 6.08839388s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.389 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:27:10.022: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:27:26.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7637" for this suite.
May 19 09:27:32.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:27:32.225: INFO: namespace resourcequota-7637 deletion completed in 6.084438362s

• [SLOW TEST:22.203 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:27:32.225: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
May 19 09:27:32.261: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8806" to be "success or failure"
May 19 09:27:32.264: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281258ms
May 19 09:27:34.268: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007007258s
STEP: Saw pod success
May 19 09:27:34.268: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 19 09:27:34.272: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 19 09:27:34.305: INFO: Waiting for pod pod-host-path-test to disappear
May 19 09:27:34.308: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:27:34.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8806" for this suite.
May 19 09:27:40.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:27:40.407: INFO: namespace hostpath-8806 deletion completed in 6.095857088s

• [SLOW TEST:8.182 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:27:40.408: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 19 09:27:44.958: INFO: Successfully updated pod "adopt-release-fw6dz"
STEP: Checking that the Job readopts the Pod
May 19 09:27:44.958: INFO: Waiting up to 15m0s for pod "adopt-release-fw6dz" in namespace "job-6825" to be "adopted"
May 19 09:27:44.961: INFO: Pod "adopt-release-fw6dz": Phase="Running", Reason="", readiness=true. Elapsed: 2.771971ms
May 19 09:27:46.965: INFO: Pod "adopt-release-fw6dz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007030516s
May 19 09:27:46.965: INFO: Pod "adopt-release-fw6dz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 19 09:27:47.474: INFO: Successfully updated pod "adopt-release-fw6dz"
STEP: Checking that the Job releases the Pod
May 19 09:27:47.474: INFO: Waiting up to 15m0s for pod "adopt-release-fw6dz" in namespace "job-6825" to be "released"
May 19 09:27:47.476: INFO: Pod "adopt-release-fw6dz": Phase="Running", Reason="", readiness=true. Elapsed: 2.430163ms
May 19 09:27:49.480: INFO: Pod "adopt-release-fw6dz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006125779s
May 19 09:27:49.480: INFO: Pod "adopt-release-fw6dz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:27:49.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6825" for this suite.
May 19 09:28:35.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:28:35.574: INFO: namespace job-6825 deletion completed in 46.089083971s

• [SLOW TEST:55.166 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:28:35.574: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-7cf0e5f3-5885-48e2-868c-0c2168118ebe in namespace container-probe-4983
May 19 09:28:39.617: INFO: Started pod liveness-7cf0e5f3-5885-48e2-868c-0c2168118ebe in namespace container-probe-4983
STEP: checking the pod's current state and verifying that restartCount is present
May 19 09:28:39.621: INFO: Initial restart count of pod liveness-7cf0e5f3-5885-48e2-868c-0c2168118ebe is 0
May 19 09:28:59.668: INFO: Restart count of pod container-probe-4983/liveness-7cf0e5f3-5885-48e2-868c-0c2168118ebe is now 1 (20.046807138s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:28:59.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4983" for this suite.
May 19 09:29:05.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:29:05.770: INFO: namespace container-probe-4983 deletion completed in 6.087567436s

• [SLOW TEST:30.196 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:29:05.770: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 19 09:29:08.329: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1c618004-7074-4de7-8298-53ec2e49e4bd"
May 19 09:29:08.329: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1c618004-7074-4de7-8298-53ec2e49e4bd" in namespace "pods-6981" to be "terminated due to deadline exceeded"
May 19 09:29:08.332: INFO: Pod "pod-update-activedeadlineseconds-1c618004-7074-4de7-8298-53ec2e49e4bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.241239ms
May 19 09:29:10.337: INFO: Pod "pod-update-activedeadlineseconds-1c618004-7074-4de7-8298-53ec2e49e4bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007204004s
May 19 09:29:12.342: INFO: Pod "pod-update-activedeadlineseconds-1c618004-7074-4de7-8298-53ec2e49e4bd": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012455846s
May 19 09:29:12.342: INFO: Pod "pod-update-activedeadlineseconds-1c618004-7074-4de7-8298-53ec2e49e4bd" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:29:12.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6981" for this suite.
May 19 09:29:18.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:29:18.445: INFO: namespace pods-6981 deletion completed in 6.097335649s

• [SLOW TEST:12.675 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:29:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:29:19.663: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:29:21.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477359, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477359, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477359, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477359, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:29:24.696: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:29:36.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-910" for this suite.
May 19 09:29:42.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:29:42.922: INFO: namespace webhook-910 deletion completed in 6.092067026s
STEP: Destroying namespace "webhook-910-markers" for this suite.
May 19 09:29:48.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:29:49.015: INFO: namespace webhook-910-markers deletion completed in 6.093310079s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.583 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:29:49.029: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:29:49.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1" in namespace "downward-api-2622" to be "success or failure"
May 19 09:29:49.069: INFO: Pod "downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281189ms
May 19 09:29:51.075: INFO: Pod "downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007598403s
May 19 09:29:53.080: INFO: Pod "downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012861839s
STEP: Saw pod success
May 19 09:29:53.080: INFO: Pod "downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1" satisfied condition "success or failure"
May 19 09:29:53.083: INFO: Trying to get logs from node nchc-kubemaster01 pod downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1 container client-container: <nil>
STEP: delete the pod
May 19 09:29:53.115: INFO: Waiting for pod downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1 to disappear
May 19 09:29:53.117: INFO: Pod downwardapi-volume-0df766a0-44a2-40e8-bf4e-a0169b7a1dd1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:29:53.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2622" for this suite.
May 19 09:29:59.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:29:59.210: INFO: namespace downward-api-2622 deletion completed in 6.089265442s

• [SLOW TEST:10.182 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:29:59.211: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-5d8860e6-d8c3-44a5-9fb9-b08201925798
STEP: Creating a pod to test consume secrets
May 19 09:29:59.255: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc" in namespace "projected-8661" to be "success or failure"
May 19 09:29:59.259: INFO: Pod "pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572783ms
May 19 09:30:01.262: INFO: Pod "pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007283624s
May 19 09:30:03.266: INFO: Pod "pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011314554s
STEP: Saw pod success
May 19 09:30:03.266: INFO: Pod "pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc" satisfied condition "success or failure"
May 19 09:30:03.269: INFO: Trying to get logs from node nchc-worker03 pod pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc container projected-secret-volume-test: <nil>
STEP: delete the pod
May 19 09:30:03.300: INFO: Waiting for pod pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc to disappear
May 19 09:30:03.303: INFO: Pod pod-projected-secrets-4aade6c9-f754-4e61-88e7-c048623e6ccc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:30:03.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8661" for this suite.
May 19 09:30:09.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:30:09.404: INFO: namespace projected-8661 deletion completed in 6.096895154s

• [SLOW TEST:10.192 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:30:09.404: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:30:09.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5143" for this suite.
May 19 09:30:37.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:30:37.537: INFO: namespace pods-5143 deletion completed in 28.089508848s

• [SLOW TEST:28.133 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:30:37.538: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:31:03.585: INFO: Container started at 2020-05-19 09:30:38 +0000 UTC, pod became ready at 2020-05-19 09:31:02 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:31:03.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5323" for this suite.
May 19 09:31:31.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:31:31.677: INFO: namespace container-probe-5323 deletion completed in 28.086569815s

• [SLOW TEST:54.139 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:31:31.677: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 19 09:31:31.713: INFO: Waiting up to 5m0s for pod "pod-e57e5997-29ee-47eb-84f8-ddd8d659883b" in namespace "emptydir-4860" to be "success or failure"
May 19 09:31:31.716: INFO: Pod "pod-e57e5997-29ee-47eb-84f8-ddd8d659883b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.332732ms
May 19 09:31:33.719: INFO: Pod "pod-e57e5997-29ee-47eb-84f8-ddd8d659883b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005444048s
STEP: Saw pod success
May 19 09:31:33.719: INFO: Pod "pod-e57e5997-29ee-47eb-84f8-ddd8d659883b" satisfied condition "success or failure"
May 19 09:31:33.722: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-e57e5997-29ee-47eb-84f8-ddd8d659883b container test-container: <nil>
STEP: delete the pod
May 19 09:31:33.751: INFO: Waiting for pod pod-e57e5997-29ee-47eb-84f8-ddd8d659883b to disappear
May 19 09:31:33.753: INFO: Pod pod-e57e5997-29ee-47eb-84f8-ddd8d659883b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:31:33.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4860" for this suite.
May 19 09:31:39.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:31:39.843: INFO: namespace emptydir-4860 deletion completed in 6.086010093s

• [SLOW TEST:8.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:31:39.843: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:31:39.881: INFO: (0) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.533188ms)
May 19 09:31:39.885: INFO: (1) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.045615ms)
May 19 09:31:39.889: INFO: (2) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.030808ms)
May 19 09:31:39.894: INFO: (3) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.027525ms)
May 19 09:31:39.897: INFO: (4) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.900482ms)
May 19 09:31:39.901: INFO: (5) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.829522ms)
May 19 09:31:39.905: INFO: (6) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.825891ms)
May 19 09:31:39.910: INFO: (7) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.305707ms)
May 19 09:31:39.914: INFO: (8) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.154778ms)
May 19 09:31:39.918: INFO: (9) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.15366ms)
May 19 09:31:39.922: INFO: (10) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.978985ms)
May 19 09:31:39.926: INFO: (11) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.174334ms)
May 19 09:31:39.930: INFO: (12) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.979544ms)
May 19 09:31:39.934: INFO: (13) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.012859ms)
May 19 09:31:39.938: INFO: (14) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.003429ms)
May 19 09:31:39.942: INFO: (15) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.050713ms)
May 19 09:31:39.946: INFO: (16) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.872615ms)
May 19 09:31:39.950: INFO: (17) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.946997ms)
May 19 09:31:39.954: INFO: (18) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.930724ms)
May 19 09:31:39.958: INFO: (19) /api/v1/nodes/nchc-kubemaster01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.115457ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:31:39.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9996" for this suite.
May 19 09:31:45.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:31:46.052: INFO: namespace proxy-9996 deletion completed in 6.088254405s

• [SLOW TEST:6.209 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:31:46.053: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-29293b91-cbc4-4f2b-9d3a-dab9f762e94b
STEP: Creating a pod to test consume configMaps
May 19 09:31:46.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-254f5535-e235-4774-bdfc-b0f034f06d97" in namespace "projected-2351" to be "success or failure"
May 19 09:31:46.096: INFO: Pod "pod-projected-configmaps-254f5535-e235-4774-bdfc-b0f034f06d97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306471ms
May 19 09:31:48.100: INFO: Pod "pod-projected-configmaps-254f5535-e235-4774-bdfc-b0f034f06d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005802548s
STEP: Saw pod success
May 19 09:31:48.100: INFO: Pod "pod-projected-configmaps-254f5535-e235-4774-bdfc-b0f034f06d97" satisfied condition "success or failure"
May 19 09:31:48.102: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-configmaps-254f5535-e235-4774-bdfc-b0f034f06d97 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 19 09:31:48.120: INFO: Waiting for pod pod-projected-configmaps-254f5535-e235-4774-bdfc-b0f034f06d97 to disappear
May 19 09:31:48.123: INFO: Pod pod-projected-configmaps-254f5535-e235-4774-bdfc-b0f034f06d97 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:31:48.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2351" for this suite.
May 19 09:31:54.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:31:54.211: INFO: namespace projected-2351 deletion completed in 6.084266481s

• [SLOW TEST:8.158 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:31:54.211: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-fe20173e-0623-4775-9fd4-4b76d4ef1737
STEP: Creating a pod to test consume configMaps
May 19 09:31:54.252: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3fdaf73b-1e9a-4ea2-926f-becc5b6882b4" in namespace "projected-2867" to be "success or failure"
May 19 09:31:54.255: INFO: Pod "pod-projected-configmaps-3fdaf73b-1e9a-4ea2-926f-becc5b6882b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.839857ms
May 19 09:31:56.259: INFO: Pod "pod-projected-configmaps-3fdaf73b-1e9a-4ea2-926f-becc5b6882b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007032541s
STEP: Saw pod success
May 19 09:31:56.259: INFO: Pod "pod-projected-configmaps-3fdaf73b-1e9a-4ea2-926f-becc5b6882b4" satisfied condition "success or failure"
May 19 09:31:56.261: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-configmaps-3fdaf73b-1e9a-4ea2-926f-becc5b6882b4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 19 09:31:56.280: INFO: Waiting for pod pod-projected-configmaps-3fdaf73b-1e9a-4ea2-926f-becc5b6882b4 to disappear
May 19 09:31:56.283: INFO: Pod pod-projected-configmaps-3fdaf73b-1e9a-4ea2-926f-becc5b6882b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:31:56.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2867" for this suite.
May 19 09:32:02.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:32:02.372: INFO: namespace projected-2867 deletion completed in 6.085713193s

• [SLOW TEST:8.161 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:32:02.372: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-3bc8f6de-8197-420e-838c-78150d6ff84a
STEP: Creating a pod to test consume configMaps
May 19 09:32:02.421: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd" in namespace "projected-6403" to be "success or failure"
May 19 09:32:02.424: INFO: Pod "pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.38714ms
May 19 09:32:04.428: INFO: Pod "pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006383427s
May 19 09:32:06.432: INFO: Pod "pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010637642s
STEP: Saw pod success
May 19 09:32:06.432: INFO: Pod "pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd" satisfied condition "success or failure"
May 19 09:32:06.435: INFO: Trying to get logs from node nchc-worker03 pod pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 19 09:32:06.468: INFO: Waiting for pod pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd to disappear
May 19 09:32:06.470: INFO: Pod pod-projected-configmaps-f9dede31-41f9-4bd2-a91f-b33b2f2f5afd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:32:06.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6403" for this suite.
May 19 09:32:12.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:32:12.563: INFO: namespace projected-6403 deletion completed in 6.089833819s

• [SLOW TEST:10.191 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:32:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:32:23.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6284" for this suite.
May 19 09:32:29.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:32:29.725: INFO: namespace resourcequota-6284 deletion completed in 6.088222068s

• [SLOW TEST:17.162 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:32:29.726: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 19 09:32:29.753: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 19 09:32:47.059: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:32:51.125: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:33:08.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8503" for this suite.
May 19 09:33:14.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:33:14.942: INFO: namespace crd-publish-openapi-8503 deletion completed in 6.087681977s

• [SLOW TEST:45.216 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:33:14.942: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
May 19 09:33:14.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-5156 -- logs-generator --log-lines-total 100 --run-duration 20s'
May 19 09:33:15.179: INFO: stderr: ""
May 19 09:33:15.179: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
May 19 09:33:15.179: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 19 09:33:15.179: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5156" to be "running and ready, or succeeded"
May 19 09:33:15.183: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08207ms
May 19 09:33:17.187: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00739181s
May 19 09:33:17.187: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 19 09:33:17.187: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 19 09:33:17.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs logs-generator logs-generator --namespace=kubectl-5156'
May 19 09:33:17.325: INFO: stderr: ""
May 19 09:33:17.325: INFO: stdout: "I0519 09:33:16.333791       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5z6 466\nI0519 09:33:16.533953       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/gclp 580\nI0519 09:33:16.734016       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/wv2 525\nI0519 09:33:16.934031       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/9nm 299\nI0519 09:33:17.134012       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/cjjr 212\n"
STEP: limiting log lines
May 19 09:33:17.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs logs-generator logs-generator --namespace=kubectl-5156 --tail=1'
May 19 09:33:17.449: INFO: stderr: ""
May 19 09:33:17.449: INFO: stdout: "I0519 09:33:17.333963       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/49nq 449\n"
STEP: limiting log bytes
May 19 09:33:17.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs logs-generator logs-generator --namespace=kubectl-5156 --limit-bytes=1'
May 19 09:33:17.565: INFO: stderr: ""
May 19 09:33:17.565: INFO: stdout: "I"
STEP: exposing timestamps
May 19 09:33:17.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs logs-generator logs-generator --namespace=kubectl-5156 --tail=1 --timestamps'
May 19 09:33:17.676: INFO: stderr: ""
May 19 09:33:17.676: INFO: stdout: "2020-05-19T09:33:17.534199786Z I0519 09:33:17.534020       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/5tv 424\n"
STEP: restricting to a time range
May 19 09:33:20.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs logs-generator logs-generator --namespace=kubectl-5156 --since=1s'
May 19 09:33:20.296: INFO: stderr: ""
May 19 09:33:20.296: INFO: stdout: "I0519 09:33:19.333989       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/8hqt 245\nI0519 09:33:19.533993       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/tb8 578\nI0519 09:33:19.734001       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/whd 282\nI0519 09:33:19.933998       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/gcs 232\nI0519 09:33:20.134001       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/pmzs 294\n"
May 19 09:33:20.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs logs-generator logs-generator --namespace=kubectl-5156 --since=24h'
May 19 09:33:20.415: INFO: stderr: ""
May 19 09:33:20.415: INFO: stdout: "I0519 09:33:16.333791       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5z6 466\nI0519 09:33:16.533953       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/gclp 580\nI0519 09:33:16.734016       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/wv2 525\nI0519 09:33:16.934031       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/9nm 299\nI0519 09:33:17.134012       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/cjjr 212\nI0519 09:33:17.333963       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/49nq 449\nI0519 09:33:17.534020       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/5tv 424\nI0519 09:33:17.734001       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/86qf 335\nI0519 09:33:17.934022       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/5gjk 200\nI0519 09:33:18.134006       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/b6j 534\nI0519 09:33:18.333951       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/t6k9 452\nI0519 09:33:18.533986       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/c9x 578\nI0519 09:33:18.733980       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/65k 353\nI0519 09:33:18.933996       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/qgp 451\nI0519 09:33:19.133945       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/cbrm 276\nI0519 09:33:19.333989       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/8hqt 245\nI0519 09:33:19.533993       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/tb8 578\nI0519 09:33:19.734001       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/whd 282\nI0519 09:33:19.933998       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/gcs 232\nI0519 09:33:20.134001       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/pmzs 294\nI0519 09:33:20.334004       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/jzkd 577\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
May 19 09:33:20.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete pod logs-generator --namespace=kubectl-5156'
May 19 09:33:25.255: INFO: stderr: ""
May 19 09:33:25.255: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:33:25.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5156" for this suite.
May 19 09:33:31.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:33:31.347: INFO: namespace kubectl-5156 deletion completed in 6.084325637s

• [SLOW TEST:16.404 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:33:31.347: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5424
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 19 09:33:31.375: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 19 09:33:55.440: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.148.118:8080/dial?request=hostName&protocol=http&host=10.244.161.102&port=8080&tries=1'] Namespace:pod-network-test-5424 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:33:55.440: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:33:55.649: INFO: Waiting for endpoints: map[]
May 19 09:33:55.653: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.148.118:8080/dial?request=hostName&protocol=http&host=10.244.148.119&port=8080&tries=1'] Namespace:pod-network-test-5424 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:33:55.653: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:33:55.868: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:33:55.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5424" for this suite.
May 19 09:34:07.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:34:07.968: INFO: namespace pod-network-test-5424 deletion completed in 12.094648636s

• [SLOW TEST:36.621 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:34:07.968: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 19 09:34:07.997: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:34:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:34:29.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3695" for this suite.
May 19 09:34:35.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:34:35.743: INFO: namespace crd-publish-openapi-3695 deletion completed in 6.090995163s

• [SLOW TEST:27.775 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:34:35.743: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
May 19 09:34:35.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-8773'
May 19 09:34:36.051: INFO: stderr: ""
May 19 09:34:36.051: INFO: stdout: "pod/pause created\n"
May 19 09:34:36.051: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 19 09:34:36.051: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8773" to be "running and ready"
May 19 09:34:36.055: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468787ms
May 19 09:34:38.059: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.0080704s
May 19 09:34:38.060: INFO: Pod "pause" satisfied condition "running and ready"
May 19 09:34:38.060: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
May 19 09:34:38.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 label pods pause testing-label=testing-label-value --namespace=kubectl-8773'
May 19 09:34:38.178: INFO: stderr: ""
May 19 09:34:38.178: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 19 09:34:38.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pod pause -L testing-label --namespace=kubectl-8773'
May 19 09:34:38.281: INFO: stderr: ""
May 19 09:34:38.281: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 19 09:34:38.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 label pods pause testing-label- --namespace=kubectl-8773'
May 19 09:34:38.394: INFO: stderr: ""
May 19 09:34:38.394: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 19 09:34:38.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pod pause -L testing-label --namespace=kubectl-8773'
May 19 09:34:38.492: INFO: stderr: ""
May 19 09:34:38.492: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
May 19 09:34:38.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-8773'
May 19 09:34:38.594: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 09:34:38.594: INFO: stdout: "pod \"pause\" force deleted\n"
May 19 09:34:38.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get rc,svc -l name=pause --no-headers --namespace=kubectl-8773'
May 19 09:34:38.699: INFO: stderr: "No resources found in kubectl-8773 namespace.\n"
May 19 09:34:38.699: INFO: stdout: ""
May 19 09:34:38.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -l name=pause --namespace=kubectl-8773 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 19 09:34:38.796: INFO: stderr: ""
May 19 09:34:38.796: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:34:38.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8773" for this suite.
May 19 09:34:44.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:34:44.888: INFO: namespace kubectl-8773 deletion completed in 6.08788306s

• [SLOW TEST:9.145 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:34:44.889: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 19 09:34:44.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4864'
May 19 09:34:45.033: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 19 09:34:45.033: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
May 19 09:34:45.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete deployment e2e-test-httpd-deployment --namespace=kubectl-4864'
May 19 09:34:45.147: INFO: stderr: ""
May 19 09:34:45.147: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:34:45.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4864" for this suite.
May 19 09:34:51.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:34:51.245: INFO: namespace kubectl-4864 deletion completed in 6.092487133s

• [SLOW TEST:6.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:34:51.245: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0519 09:35:01.343012      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 19 09:35:01.343: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:35:01.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7315" for this suite.
May 19 09:35:07.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:35:07.436: INFO: namespace gc-7315 deletion completed in 6.089645041s

• [SLOW TEST:16.192 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:35:07.436: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:35:08.086: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:35:11.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:35:11.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3176" for this suite.
May 19 09:35:17.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:35:17.358: INFO: namespace webhook-3176 deletion completed in 6.09015496s
STEP: Destroying namespace "webhook-3176-markers" for this suite.
May 19 09:35:23.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:35:23.449: INFO: namespace webhook-3176-markers deletion completed in 6.091062071s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.026 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:35:23.463: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:35:24.456: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:35:26.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477724, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477724, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477724, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477724, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:35:29.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:35:29.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9604" for this suite.
May 19 09:35:35.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:35:35.652: INFO: namespace webhook-9604 deletion completed in 6.088561927s
STEP: Destroying namespace "webhook-9604-markers" for this suite.
May 19 09:35:41.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:35:41.745: INFO: namespace webhook-9604-markers deletion completed in 6.092377342s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.295 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:35:41.758: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:35:41.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281" in namespace "projected-3345" to be "success or failure"
May 19 09:35:41.797: INFO: Pod "downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136894ms
May 19 09:35:43.801: INFO: Pod "downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006943355s
May 19 09:35:45.806: INFO: Pod "downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011081774s
STEP: Saw pod success
May 19 09:35:45.806: INFO: Pod "downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281" satisfied condition "success or failure"
May 19 09:35:45.809: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281 container client-container: <nil>
STEP: delete the pod
May 19 09:35:45.838: INFO: Waiting for pod downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281 to disappear
May 19 09:35:45.841: INFO: Pod downwardapi-volume-78861cc9-ed50-4211-ad2e-d98e5ee34281 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:35:45.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3345" for this suite.
May 19 09:35:51.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:35:51.934: INFO: namespace projected-3345 deletion completed in 6.089529593s

• [SLOW TEST:10.176 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:35:51.935: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-mbrw
STEP: Creating a pod to test atomic-volume-subpath
May 19 09:35:51.977: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mbrw" in namespace "subpath-6356" to be "success or failure"
May 19 09:35:51.979: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330218ms
May 19 09:35:53.985: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007980792s
May 19 09:35:55.994: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 4.017101481s
May 19 09:35:57.998: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 6.020989306s
May 19 09:36:00.003: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 8.025837672s
May 19 09:36:02.008: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 10.030908486s
May 19 09:36:04.013: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 12.036078546s
May 19 09:36:06.018: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 14.040830739s
May 19 09:36:08.023: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 16.045596063s
May 19 09:36:10.027: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 18.049788401s
May 19 09:36:12.032: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 20.05535719s
May 19 09:36:14.038: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Running", Reason="", readiness=true. Elapsed: 22.060966069s
May 19 09:36:16.042: INFO: Pod "pod-subpath-test-configmap-mbrw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064589402s
STEP: Saw pod success
May 19 09:36:16.042: INFO: Pod "pod-subpath-test-configmap-mbrw" satisfied condition "success or failure"
May 19 09:36:16.044: INFO: Trying to get logs from node nchc-worker03 pod pod-subpath-test-configmap-mbrw container test-container-subpath-configmap-mbrw: <nil>
STEP: delete the pod
May 19 09:36:16.063: INFO: Waiting for pod pod-subpath-test-configmap-mbrw to disappear
May 19 09:36:16.066: INFO: Pod pod-subpath-test-configmap-mbrw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mbrw
May 19 09:36:16.066: INFO: Deleting pod "pod-subpath-test-configmap-mbrw" in namespace "subpath-6356"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:36:16.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6356" for this suite.
May 19 09:36:22.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:36:22.162: INFO: namespace subpath-6356 deletion completed in 6.090261469s

• [SLOW TEST:30.227 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:36:22.162: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-1558c64d-767a-4d67-9e57-bc1724b4298a
STEP: Creating configMap with name cm-test-opt-upd-2afe97e1-126d-4674-bf53-657ddd49dea9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1558c64d-767a-4d67-9e57-bc1724b4298a
STEP: Updating configmap cm-test-opt-upd-2afe97e1-126d-4674-bf53-657ddd49dea9
STEP: Creating configMap with name cm-test-opt-create-a5c37c8d-21b8-4d4f-8769-b5a586935a4f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:36:30.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-869" for this suite.
May 19 09:36:58.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:36:58.402: INFO: namespace projected-869 deletion completed in 28.08666533s

• [SLOW TEST:36.239 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:36:58.402: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:36:58.439: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 19 09:37:03.444: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 19 09:37:03.444: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 19 09:37:03.460: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6190 /apis/apps/v1/namespaces/deployment-6190/deployments/test-cleanup-deployment c4551574-7e19-43ac-b42d-a52eb5f8d8ba 1117108 1 2020-05-19 09:37:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001cef3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 19 09:37:03.463: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-6190 /apis/apps/v1/namespaces/deployment-6190/replicasets/test-cleanup-deployment-65db99849b 10d9c037-b066-40f7-b147-9397e5e51ff9 1117110 1 2020-05-19 09:37:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment c4551574-7e19-43ac-b42d-a52eb5f8d8ba 0xc001cef817 0xc001cef818}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001cef878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 19 09:37:03.463: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 19 09:37:03.463: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6190 /apis/apps/v1/namespaces/deployment-6190/replicasets/test-cleanup-controller 7c096c8f-daa0-4197-8fe0-2ec1fae277d9 1117109 1 2020-05-19 09:36:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c4551574-7e19-43ac-b42d-a52eb5f8d8ba 0xc001cef72f 0xc001cef740}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001cef7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 19 09:37:03.467: INFO: Pod "test-cleanup-controller-bdxvf" is available:
&Pod{ObjectMeta:{test-cleanup-controller-bdxvf test-cleanup-controller- deployment-6190 /api/v1/namespaces/deployment-6190/pods/test-cleanup-controller-bdxvf 37fa52dc-d7e3-48c5-ad00-dab447c25ab0 1117099 0 2020-05-19 09:36:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.161.79/32] [{apps/v1 ReplicaSet test-cleanup-controller 7c096c8f-daa0-4197-8fe0-2ec1fae277d9 0xc001cefde7 0xc001cefde8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4wcnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4wcnw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4wcnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-kubemaster01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:36:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.171,PodIP:10.244.161.79,StartTime:2020-05-19 09:36:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 09:36:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f829eff1b486987f4d7f0a9efd7a28be475217eb9c1c1cb21554e6c7bb3dc23c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.161.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 19 09:37:03.467: INFO: Pod "test-cleanup-deployment-65db99849b-l2cvk" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-l2cvk test-cleanup-deployment-65db99849b- deployment-6190 /api/v1/namespaces/deployment-6190/pods/test-cleanup-deployment-65db99849b-l2cvk 7472362b-1473-4bc7-a1ef-d2ff7c96386d 1117111 0 2020-05-19 09:37:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 10d9c037-b066-40f7-b147-9397e5e51ff9 0xc001ceff67 0xc001ceff68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4wcnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4wcnw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4wcnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:37:03.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6190" for this suite.
May 19 09:37:09.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:37:09.575: INFO: namespace deployment-6190 deletion completed in 6.103209322s

• [SLOW TEST:11.173 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:37:09.576: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2147
I0519 09:37:09.608597      25 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2147, replica count: 1
I0519 09:37:10.659024      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0519 09:37:11.659247      25 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 19 09:37:11.774: INFO: Created: latency-svc-vx4wz
May 19 09:37:11.778: INFO: Got endpoints: latency-svc-vx4wz [18.834858ms]
May 19 09:37:11.793: INFO: Created: latency-svc-vbjdn
May 19 09:37:11.797: INFO: Got endpoints: latency-svc-vbjdn [19.05996ms]
May 19 09:37:11.805: INFO: Created: latency-svc-wnnpl
May 19 09:37:11.808: INFO: Got endpoints: latency-svc-wnnpl [29.92679ms]
May 19 09:37:11.816: INFO: Created: latency-svc-h6sv5
May 19 09:37:11.819: INFO: Got endpoints: latency-svc-h6sv5 [41.399783ms]
May 19 09:37:11.828: INFO: Created: latency-svc-lwqfc
May 19 09:37:11.831: INFO: Got endpoints: latency-svc-lwqfc [53.243638ms]
May 19 09:37:11.842: INFO: Created: latency-svc-rwbkb
May 19 09:37:11.845: INFO: Got endpoints: latency-svc-rwbkb [67.445787ms]
May 19 09:37:11.855: INFO: Created: latency-svc-d6fxw
May 19 09:37:11.857: INFO: Got endpoints: latency-svc-d6fxw [79.530318ms]
May 19 09:37:11.867: INFO: Created: latency-svc-wt9qw
May 19 09:37:11.869: INFO: Got endpoints: latency-svc-wt9qw [91.438637ms]
May 19 09:37:11.878: INFO: Created: latency-svc-cnqjp
May 19 09:37:11.881: INFO: Got endpoints: latency-svc-cnqjp [102.566888ms]
May 19 09:37:11.890: INFO: Created: latency-svc-kz8nx
May 19 09:37:11.893: INFO: Got endpoints: latency-svc-kz8nx [114.547633ms]
May 19 09:37:11.901: INFO: Created: latency-svc-xjpv5
May 19 09:37:11.904: INFO: Got endpoints: latency-svc-xjpv5 [126.092215ms]
May 19 09:37:11.912: INFO: Created: latency-svc-r2qnv
May 19 09:37:11.916: INFO: Got endpoints: latency-svc-r2qnv [137.463028ms]
May 19 09:37:11.925: INFO: Created: latency-svc-nk8mq
May 19 09:37:11.927: INFO: Got endpoints: latency-svc-nk8mq [149.446148ms]
May 19 09:37:11.936: INFO: Created: latency-svc-hvmw4
May 19 09:37:11.948: INFO: Got endpoints: latency-svc-hvmw4 [169.963297ms]
May 19 09:37:11.952: INFO: Created: latency-svc-42vfr
May 19 09:37:11.955: INFO: Got endpoints: latency-svc-42vfr [176.763142ms]
May 19 09:37:11.966: INFO: Created: latency-svc-mzbrr
May 19 09:37:11.969: INFO: Got endpoints: latency-svc-mzbrr [191.193745ms]
May 19 09:37:11.977: INFO: Created: latency-svc-wv4cx
May 19 09:37:11.980: INFO: Got endpoints: latency-svc-wv4cx [183.259871ms]
May 19 09:37:11.989: INFO: Created: latency-svc-jc5c6
May 19 09:37:11.992: INFO: Got endpoints: latency-svc-jc5c6 [183.8228ms]
May 19 09:37:11.999: INFO: Created: latency-svc-qldc9
May 19 09:37:12.002: INFO: Got endpoints: latency-svc-qldc9 [182.330341ms]
May 19 09:37:12.010: INFO: Created: latency-svc-b7kz8
May 19 09:37:12.013: INFO: Got endpoints: latency-svc-b7kz8 [182.061587ms]
May 19 09:37:12.024: INFO: Created: latency-svc-7qs5t
May 19 09:37:12.028: INFO: Got endpoints: latency-svc-7qs5t [182.350734ms]
May 19 09:37:12.035: INFO: Created: latency-svc-crvg2
May 19 09:37:12.037: INFO: Got endpoints: latency-svc-crvg2 [179.794157ms]
May 19 09:37:12.046: INFO: Created: latency-svc-fjl9v
May 19 09:37:12.058: INFO: Got endpoints: latency-svc-fjl9v [188.132418ms]
May 19 09:37:12.062: INFO: Created: latency-svc-b7cs6
May 19 09:37:12.064: INFO: Got endpoints: latency-svc-b7cs6 [183.682208ms]
May 19 09:37:12.073: INFO: Created: latency-svc-q9nnw
May 19 09:37:12.076: INFO: Got endpoints: latency-svc-q9nnw [183.475544ms]
May 19 09:37:12.084: INFO: Created: latency-svc-jrqgh
May 19 09:37:12.086: INFO: Got endpoints: latency-svc-jrqgh [182.285711ms]
May 19 09:37:12.096: INFO: Created: latency-svc-25nrw
May 19 09:37:12.099: INFO: Got endpoints: latency-svc-25nrw [183.173057ms]
May 19 09:37:12.107: INFO: Created: latency-svc-nc6zb
May 19 09:37:12.110: INFO: Got endpoints: latency-svc-nc6zb [182.10922ms]
May 19 09:37:12.117: INFO: Created: latency-svc-2mc28
May 19 09:37:12.120: INFO: Got endpoints: latency-svc-2mc28 [172.142167ms]
May 19 09:37:12.128: INFO: Created: latency-svc-vkvnd
May 19 09:37:12.131: INFO: Got endpoints: latency-svc-vkvnd [176.075196ms]
May 19 09:37:12.139: INFO: Created: latency-svc-9jz87
May 19 09:37:12.141: INFO: Got endpoints: latency-svc-9jz87 [172.381027ms]
May 19 09:37:12.149: INFO: Created: latency-svc-qhdq6
May 19 09:37:12.152: INFO: Got endpoints: latency-svc-qhdq6 [171.610597ms]
May 19 09:37:12.167: INFO: Created: latency-svc-4ljcr
May 19 09:37:12.167: INFO: Got endpoints: latency-svc-4ljcr [175.420634ms]
May 19 09:37:12.177: INFO: Created: latency-svc-7mmwk
May 19 09:37:12.181: INFO: Got endpoints: latency-svc-7mmwk [178.978469ms]
May 19 09:37:12.188: INFO: Created: latency-svc-rdx4m
May 19 09:37:12.191: INFO: Got endpoints: latency-svc-rdx4m [177.919451ms]
May 19 09:37:12.200: INFO: Created: latency-svc-2dwkq
May 19 09:37:12.202: INFO: Got endpoints: latency-svc-2dwkq [174.582736ms]
May 19 09:37:12.213: INFO: Created: latency-svc-fwsvp
May 19 09:37:12.217: INFO: Got endpoints: latency-svc-fwsvp [179.29241ms]
May 19 09:37:12.225: INFO: Created: latency-svc-b4z9m
May 19 09:37:12.227: INFO: Got endpoints: latency-svc-b4z9m [169.871594ms]
May 19 09:37:12.235: INFO: Created: latency-svc-cq6l9
May 19 09:37:12.237: INFO: Got endpoints: latency-svc-cq6l9 [173.021551ms]
May 19 09:37:12.248: INFO: Created: latency-svc-wmc9k
May 19 09:37:12.264: INFO: Created: latency-svc-dts8x
May 19 09:37:12.277: INFO: Got endpoints: latency-svc-wmc9k [201.13775ms]
May 19 09:37:12.279: INFO: Created: latency-svc-qvxxf
May 19 09:37:12.288: INFO: Created: latency-svc-dbj6v
May 19 09:37:12.297: INFO: Created: latency-svc-d7ngf
May 19 09:37:12.308: INFO: Created: latency-svc-vmf8l
May 19 09:37:12.319: INFO: Created: latency-svc-2x8zj
May 19 09:37:12.328: INFO: Got endpoints: latency-svc-dts8x [241.304468ms]
May 19 09:37:12.329: INFO: Created: latency-svc-k2kw5
May 19 09:37:12.341: INFO: Created: latency-svc-rj2tx
May 19 09:37:12.351: INFO: Created: latency-svc-5mqdz
May 19 09:37:12.362: INFO: Created: latency-svc-dqtbn
May 19 09:37:12.372: INFO: Created: latency-svc-6cwnf
May 19 09:37:12.388: INFO: Got endpoints: latency-svc-qvxxf [289.357217ms]
May 19 09:37:12.391: INFO: Created: latency-svc-vl6ql
May 19 09:37:12.401: INFO: Created: latency-svc-xglbf
May 19 09:37:12.412: INFO: Created: latency-svc-v8689
May 19 09:37:12.423: INFO: Created: latency-svc-vv8pj
May 19 09:37:12.431: INFO: Got endpoints: latency-svc-dbj6v [321.485506ms]
May 19 09:37:12.433: INFO: Created: latency-svc-p6w8w
May 19 09:37:12.444: INFO: Created: latency-svc-6t9hf
May 19 09:37:12.454: INFO: Created: latency-svc-75pbt
May 19 09:37:12.477: INFO: Got endpoints: latency-svc-d7ngf [357.177848ms]
May 19 09:37:12.496: INFO: Created: latency-svc-bqlrc
May 19 09:37:12.528: INFO: Got endpoints: latency-svc-vmf8l [396.622815ms]
May 19 09:37:12.542: INFO: Created: latency-svc-hdt2l
May 19 09:37:12.579: INFO: Got endpoints: latency-svc-2x8zj [437.723184ms]
May 19 09:37:12.592: INFO: Created: latency-svc-c7kw4
May 19 09:37:12.628: INFO: Got endpoints: latency-svc-k2kw5 [475.761807ms]
May 19 09:37:12.642: INFO: Created: latency-svc-hn6sr
May 19 09:37:12.678: INFO: Got endpoints: latency-svc-rj2tx [510.568129ms]
May 19 09:37:12.692: INFO: Created: latency-svc-42dqq
May 19 09:37:12.727: INFO: Got endpoints: latency-svc-5mqdz [546.552202ms]
May 19 09:37:12.744: INFO: Created: latency-svc-jtbtl
May 19 09:37:12.782: INFO: Got endpoints: latency-svc-dqtbn [590.19029ms]
May 19 09:37:12.796: INFO: Created: latency-svc-vktc7
May 19 09:37:12.828: INFO: Got endpoints: latency-svc-6cwnf [625.107802ms]
May 19 09:37:12.841: INFO: Created: latency-svc-h2fm2
May 19 09:37:12.878: INFO: Got endpoints: latency-svc-vl6ql [661.440177ms]
May 19 09:37:12.892: INFO: Created: latency-svc-5hnx7
May 19 09:37:12.928: INFO: Got endpoints: latency-svc-xglbf [700.4531ms]
May 19 09:37:12.942: INFO: Created: latency-svc-c2qd2
May 19 09:37:12.978: INFO: Got endpoints: latency-svc-v8689 [740.723394ms]
May 19 09:37:12.992: INFO: Created: latency-svc-cw4kw
May 19 09:37:13.028: INFO: Got endpoints: latency-svc-vv8pj [750.360162ms]
May 19 09:37:13.042: INFO: Created: latency-svc-2v447
May 19 09:37:13.079: INFO: Got endpoints: latency-svc-p6w8w [751.580517ms]
May 19 09:37:13.093: INFO: Created: latency-svc-zz8bp
May 19 09:37:13.127: INFO: Got endpoints: latency-svc-6t9hf [738.928586ms]
May 19 09:37:13.141: INFO: Created: latency-svc-67gtv
May 19 09:37:13.178: INFO: Got endpoints: latency-svc-75pbt [747.322721ms]
May 19 09:37:13.192: INFO: Created: latency-svc-8t2vv
May 19 09:37:13.228: INFO: Got endpoints: latency-svc-bqlrc [750.226554ms]
May 19 09:37:13.242: INFO: Created: latency-svc-x482j
May 19 09:37:13.278: INFO: Got endpoints: latency-svc-hdt2l [750.855274ms]
May 19 09:37:13.291: INFO: Created: latency-svc-q8b9n
May 19 09:37:13.337: INFO: Got endpoints: latency-svc-c7kw4 [757.638147ms]
May 19 09:37:13.350: INFO: Created: latency-svc-65tmz
May 19 09:37:13.377: INFO: Got endpoints: latency-svc-hn6sr [749.790808ms]
May 19 09:37:13.390: INFO: Created: latency-svc-84mtg
May 19 09:37:13.427: INFO: Got endpoints: latency-svc-42dqq [749.002987ms]
May 19 09:37:13.440: INFO: Created: latency-svc-4c7xt
May 19 09:37:13.477: INFO: Got endpoints: latency-svc-jtbtl [750.215868ms]
May 19 09:37:13.493: INFO: Created: latency-svc-r7h5v
May 19 09:37:13.528: INFO: Got endpoints: latency-svc-vktc7 [746.419521ms]
May 19 09:37:13.544: INFO: Created: latency-svc-gj9jp
May 19 09:37:13.578: INFO: Got endpoints: latency-svc-h2fm2 [749.952214ms]
May 19 09:37:13.592: INFO: Created: latency-svc-dbqsz
May 19 09:37:13.627: INFO: Got endpoints: latency-svc-5hnx7 [749.027361ms]
May 19 09:37:13.653: INFO: Created: latency-svc-w28x9
May 19 09:37:13.677: INFO: Got endpoints: latency-svc-c2qd2 [749.09015ms]
May 19 09:37:13.691: INFO: Created: latency-svc-c8xp9
May 19 09:37:13.728: INFO: Got endpoints: latency-svc-cw4kw [749.669841ms]
May 19 09:37:13.741: INFO: Created: latency-svc-9czqv
May 19 09:37:13.777: INFO: Got endpoints: latency-svc-2v447 [749.712235ms]
May 19 09:37:13.792: INFO: Created: latency-svc-dgzc6
May 19 09:37:13.828: INFO: Got endpoints: latency-svc-zz8bp [748.339695ms]
May 19 09:37:13.842: INFO: Created: latency-svc-7bkkh
May 19 09:37:13.878: INFO: Got endpoints: latency-svc-67gtv [750.470374ms]
May 19 09:37:13.893: INFO: Created: latency-svc-kxbv9
May 19 09:37:13.928: INFO: Got endpoints: latency-svc-8t2vv [749.151751ms]
May 19 09:37:13.942: INFO: Created: latency-svc-wjcp6
May 19 09:37:13.978: INFO: Got endpoints: latency-svc-x482j [750.027434ms]
May 19 09:37:13.997: INFO: Created: latency-svc-sfdpw
May 19 09:37:14.027: INFO: Got endpoints: latency-svc-q8b9n [748.672982ms]
May 19 09:37:14.042: INFO: Created: latency-svc-dwg84
May 19 09:37:14.077: INFO: Got endpoints: latency-svc-65tmz [740.240993ms]
May 19 09:37:14.091: INFO: Created: latency-svc-qzcn4
May 19 09:37:14.131: INFO: Got endpoints: latency-svc-84mtg [753.38168ms]
May 19 09:37:14.144: INFO: Created: latency-svc-tgpvb
May 19 09:37:14.177: INFO: Got endpoints: latency-svc-4c7xt [750.46304ms]
May 19 09:37:14.192: INFO: Created: latency-svc-bwmdq
May 19 09:37:14.227: INFO: Got endpoints: latency-svc-r7h5v [749.831735ms]
May 19 09:37:14.244: INFO: Created: latency-svc-kw8x2
May 19 09:37:14.278: INFO: Got endpoints: latency-svc-gj9jp [750.058304ms]
May 19 09:37:14.293: INFO: Created: latency-svc-lw4cr
May 19 09:37:14.333: INFO: Got endpoints: latency-svc-dbqsz [755.350463ms]
May 19 09:37:14.347: INFO: Created: latency-svc-9w884
May 19 09:37:14.379: INFO: Got endpoints: latency-svc-w28x9 [751.628987ms]
May 19 09:37:14.393: INFO: Created: latency-svc-8wcdn
May 19 09:37:14.427: INFO: Got endpoints: latency-svc-c8xp9 [750.307292ms]
May 19 09:37:14.446: INFO: Created: latency-svc-4qkfv
May 19 09:37:14.477: INFO: Got endpoints: latency-svc-9czqv [749.560887ms]
May 19 09:37:14.494: INFO: Created: latency-svc-f7lbn
May 19 09:37:14.528: INFO: Got endpoints: latency-svc-dgzc6 [750.596858ms]
May 19 09:37:14.542: INFO: Created: latency-svc-sfmw6
May 19 09:37:14.578: INFO: Got endpoints: latency-svc-7bkkh [750.335229ms]
May 19 09:37:14.592: INFO: Created: latency-svc-t5w96
May 19 09:37:14.628: INFO: Got endpoints: latency-svc-kxbv9 [750.384537ms]
May 19 09:37:14.643: INFO: Created: latency-svc-6dkt5
May 19 09:37:14.677: INFO: Got endpoints: latency-svc-wjcp6 [749.35136ms]
May 19 09:37:14.694: INFO: Created: latency-svc-2lrqt
May 19 09:37:14.727: INFO: Got endpoints: latency-svc-sfdpw [749.161668ms]
May 19 09:37:14.763: INFO: Created: latency-svc-w4slt
May 19 09:37:14.777: INFO: Got endpoints: latency-svc-dwg84 [750.180947ms]
May 19 09:37:14.795: INFO: Created: latency-svc-cr62h
May 19 09:37:14.827: INFO: Got endpoints: latency-svc-qzcn4 [750.043358ms]
May 19 09:37:14.842: INFO: Created: latency-svc-2bg5g
May 19 09:37:14.877: INFO: Got endpoints: latency-svc-tgpvb [746.474626ms]
May 19 09:37:14.892: INFO: Created: latency-svc-7msjr
May 19 09:37:14.928: INFO: Got endpoints: latency-svc-bwmdq [750.226414ms]
May 19 09:37:14.943: INFO: Created: latency-svc-s85sw
May 19 09:37:14.978: INFO: Got endpoints: latency-svc-kw8x2 [750.3874ms]
May 19 09:37:14.991: INFO: Created: latency-svc-24mhk
May 19 09:37:15.027: INFO: Got endpoints: latency-svc-lw4cr [748.932516ms]
May 19 09:37:15.040: INFO: Created: latency-svc-rw55n
May 19 09:37:15.077: INFO: Got endpoints: latency-svc-9w884 [744.162916ms]
May 19 09:37:15.092: INFO: Created: latency-svc-q2sl7
May 19 09:37:15.129: INFO: Got endpoints: latency-svc-8wcdn [749.805475ms]
May 19 09:37:15.142: INFO: Created: latency-svc-zj64j
May 19 09:37:15.178: INFO: Got endpoints: latency-svc-4qkfv [750.482665ms]
May 19 09:37:15.192: INFO: Created: latency-svc-s7c9f
May 19 09:37:15.228: INFO: Got endpoints: latency-svc-f7lbn [750.362676ms]
May 19 09:37:15.241: INFO: Created: latency-svc-kvj4c
May 19 09:37:15.278: INFO: Got endpoints: latency-svc-sfmw6 [749.870428ms]
May 19 09:37:15.293: INFO: Created: latency-svc-pkbgb
May 19 09:37:15.329: INFO: Got endpoints: latency-svc-t5w96 [750.822518ms]
May 19 09:37:15.342: INFO: Created: latency-svc-klcn4
May 19 09:37:15.378: INFO: Got endpoints: latency-svc-6dkt5 [749.51095ms]
May 19 09:37:15.392: INFO: Created: latency-svc-nxxrl
May 19 09:37:15.427: INFO: Got endpoints: latency-svc-2lrqt [749.864002ms]
May 19 09:37:15.443: INFO: Created: latency-svc-lpmp6
May 19 09:37:15.478: INFO: Got endpoints: latency-svc-w4slt [751.110339ms]
May 19 09:37:15.491: INFO: Created: latency-svc-zgkvh
May 19 09:37:15.530: INFO: Got endpoints: latency-svc-cr62h [752.698133ms]
May 19 09:37:15.543: INFO: Created: latency-svc-2wljm
May 19 09:37:15.577: INFO: Got endpoints: latency-svc-2bg5g [749.890752ms]
May 19 09:37:15.590: INFO: Created: latency-svc-cgzcc
May 19 09:37:15.627: INFO: Got endpoints: latency-svc-7msjr [749.750439ms]
May 19 09:37:15.640: INFO: Created: latency-svc-blqhx
May 19 09:37:15.678: INFO: Got endpoints: latency-svc-s85sw [750.501663ms]
May 19 09:37:15.691: INFO: Created: latency-svc-7gk89
May 19 09:37:15.728: INFO: Got endpoints: latency-svc-24mhk [749.921553ms]
May 19 09:37:15.749: INFO: Created: latency-svc-z286n
May 19 09:37:15.777: INFO: Got endpoints: latency-svc-rw55n [750.238427ms]
May 19 09:37:15.791: INFO: Created: latency-svc-gp7tb
May 19 09:37:15.828: INFO: Got endpoints: latency-svc-q2sl7 [750.509066ms]
May 19 09:37:15.841: INFO: Created: latency-svc-28h9b
May 19 09:37:15.878: INFO: Got endpoints: latency-svc-zj64j [748.990276ms]
May 19 09:37:15.891: INFO: Created: latency-svc-htvcm
May 19 09:37:15.927: INFO: Got endpoints: latency-svc-s7c9f [749.441107ms]
May 19 09:37:15.941: INFO: Created: latency-svc-2fl9f
May 19 09:37:15.978: INFO: Got endpoints: latency-svc-kvj4c [749.83425ms]
May 19 09:37:15.992: INFO: Created: latency-svc-mk5wb
May 19 09:37:16.027: INFO: Got endpoints: latency-svc-pkbgb [749.295905ms]
May 19 09:37:16.042: INFO: Created: latency-svc-t2bv2
May 19 09:37:16.077: INFO: Got endpoints: latency-svc-klcn4 [748.299745ms]
May 19 09:37:16.090: INFO: Created: latency-svc-qvz6w
May 19 09:37:16.128: INFO: Got endpoints: latency-svc-nxxrl [750.193658ms]
May 19 09:37:16.141: INFO: Created: latency-svc-crsw2
May 19 09:37:16.178: INFO: Got endpoints: latency-svc-lpmp6 [751.05202ms]
May 19 09:37:16.191: INFO: Created: latency-svc-5gdcc
May 19 09:37:16.228: INFO: Got endpoints: latency-svc-zgkvh [750.058653ms]
May 19 09:37:16.243: INFO: Created: latency-svc-72b4p
May 19 09:37:16.278: INFO: Got endpoints: latency-svc-2wljm [748.216354ms]
May 19 09:37:16.292: INFO: Created: latency-svc-72wbl
May 19 09:37:16.328: INFO: Got endpoints: latency-svc-cgzcc [750.341025ms]
May 19 09:37:16.341: INFO: Created: latency-svc-89vng
May 19 09:37:16.378: INFO: Got endpoints: latency-svc-blqhx [750.43615ms]
May 19 09:37:16.391: INFO: Created: latency-svc-8fjfb
May 19 09:37:16.427: INFO: Got endpoints: latency-svc-7gk89 [748.87769ms]
May 19 09:37:16.441: INFO: Created: latency-svc-r4x5c
May 19 09:37:16.478: INFO: Got endpoints: latency-svc-z286n [749.940969ms]
May 19 09:37:16.491: INFO: Created: latency-svc-x55rj
May 19 09:37:16.529: INFO: Got endpoints: latency-svc-gp7tb [751.214962ms]
May 19 09:37:16.543: INFO: Created: latency-svc-zczr2
May 19 09:37:16.583: INFO: Got endpoints: latency-svc-28h9b [755.630182ms]
May 19 09:37:16.598: INFO: Created: latency-svc-rmmjm
May 19 09:37:16.627: INFO: Got endpoints: latency-svc-htvcm [749.589802ms]
May 19 09:37:16.641: INFO: Created: latency-svc-nds7k
May 19 09:37:16.678: INFO: Got endpoints: latency-svc-2fl9f [750.092945ms]
May 19 09:37:16.692: INFO: Created: latency-svc-nsr7p
May 19 09:37:16.728: INFO: Got endpoints: latency-svc-mk5wb [750.322587ms]
May 19 09:37:16.744: INFO: Created: latency-svc-ncvj9
May 19 09:37:16.779: INFO: Got endpoints: latency-svc-t2bv2 [751.822729ms]
May 19 09:37:16.794: INFO: Created: latency-svc-zbcvq
May 19 09:37:16.830: INFO: Got endpoints: latency-svc-qvz6w [752.793048ms]
May 19 09:37:16.844: INFO: Created: latency-svc-vmvd5
May 19 09:37:16.882: INFO: Got endpoints: latency-svc-crsw2 [753.909058ms]
May 19 09:37:16.898: INFO: Created: latency-svc-mrcjq
May 19 09:37:16.928: INFO: Got endpoints: latency-svc-5gdcc [749.949419ms]
May 19 09:37:16.941: INFO: Created: latency-svc-wk4ht
May 19 09:37:16.979: INFO: Got endpoints: latency-svc-72b4p [750.608382ms]
May 19 09:37:16.992: INFO: Created: latency-svc-zhb9h
May 19 09:37:17.027: INFO: Got endpoints: latency-svc-72wbl [748.745618ms]
May 19 09:37:17.040: INFO: Created: latency-svc-g22jf
May 19 09:37:17.079: INFO: Got endpoints: latency-svc-89vng [751.111455ms]
May 19 09:37:17.092: INFO: Created: latency-svc-5km64
May 19 09:37:17.127: INFO: Got endpoints: latency-svc-8fjfb [749.600068ms]
May 19 09:37:17.141: INFO: Created: latency-svc-7jqzw
May 19 09:37:17.178: INFO: Got endpoints: latency-svc-r4x5c [750.997054ms]
May 19 09:37:17.191: INFO: Created: latency-svc-xsblj
May 19 09:37:17.227: INFO: Got endpoints: latency-svc-x55rj [749.650564ms]
May 19 09:37:17.242: INFO: Created: latency-svc-8bvdp
May 19 09:37:17.279: INFO: Got endpoints: latency-svc-zczr2 [749.836764ms]
May 19 09:37:17.292: INFO: Created: latency-svc-n6fn2
May 19 09:37:17.328: INFO: Got endpoints: latency-svc-rmmjm [744.408203ms]
May 19 09:37:17.349: INFO: Created: latency-svc-r5665
May 19 09:37:17.377: INFO: Got endpoints: latency-svc-nds7k [750.131918ms]
May 19 09:37:17.391: INFO: Created: latency-svc-r9bxg
May 19 09:37:17.427: INFO: Got endpoints: latency-svc-nsr7p [749.281518ms]
May 19 09:37:17.441: INFO: Created: latency-svc-dvkd9
May 19 09:37:17.478: INFO: Got endpoints: latency-svc-ncvj9 [750.290878ms]
May 19 09:37:17.492: INFO: Created: latency-svc-tlsgg
May 19 09:37:17.527: INFO: Got endpoints: latency-svc-zbcvq [748.100415ms]
May 19 09:37:17.541: INFO: Created: latency-svc-zc7xw
May 19 09:37:17.579: INFO: Got endpoints: latency-svc-vmvd5 [748.636455ms]
May 19 09:37:17.592: INFO: Created: latency-svc-glzbx
May 19 09:37:17.628: INFO: Got endpoints: latency-svc-mrcjq [745.748895ms]
May 19 09:37:17.641: INFO: Created: latency-svc-76nms
May 19 09:37:17.678: INFO: Got endpoints: latency-svc-wk4ht [750.212864ms]
May 19 09:37:17.692: INFO: Created: latency-svc-nndz2
May 19 09:37:17.730: INFO: Got endpoints: latency-svc-zhb9h [750.762105ms]
May 19 09:37:17.743: INFO: Created: latency-svc-jfq9p
May 19 09:37:17.777: INFO: Got endpoints: latency-svc-g22jf [750.008436ms]
May 19 09:37:17.791: INFO: Created: latency-svc-qrrs5
May 19 09:37:17.828: INFO: Got endpoints: latency-svc-5km64 [749.446136ms]
May 19 09:37:17.844: INFO: Created: latency-svc-nllsd
May 19 09:37:17.877: INFO: Got endpoints: latency-svc-7jqzw [750.018145ms]
May 19 09:37:17.890: INFO: Created: latency-svc-dzq77
May 19 09:37:17.930: INFO: Got endpoints: latency-svc-xsblj [751.341516ms]
May 19 09:37:17.943: INFO: Created: latency-svc-skdcz
May 19 09:37:17.978: INFO: Got endpoints: latency-svc-8bvdp [750.116552ms]
May 19 09:37:17.990: INFO: Created: latency-svc-2q52t
May 19 09:37:18.028: INFO: Got endpoints: latency-svc-n6fn2 [749.071222ms]
May 19 09:37:18.044: INFO: Created: latency-svc-8gb64
May 19 09:37:18.078: INFO: Got endpoints: latency-svc-r5665 [749.679759ms]
May 19 09:37:18.091: INFO: Created: latency-svc-274s7
May 19 09:37:18.129: INFO: Got endpoints: latency-svc-r9bxg [751.636809ms]
May 19 09:37:18.147: INFO: Created: latency-svc-x68xb
May 19 09:37:18.178: INFO: Got endpoints: latency-svc-dvkd9 [751.27335ms]
May 19 09:37:18.195: INFO: Created: latency-svc-jk4pc
May 19 09:37:18.231: INFO: Got endpoints: latency-svc-tlsgg [752.619071ms]
May 19 09:37:18.252: INFO: Created: latency-svc-4fvcg
May 19 09:37:18.278: INFO: Got endpoints: latency-svc-zc7xw [750.512908ms]
May 19 09:37:18.292: INFO: Created: latency-svc-r5q2m
May 19 09:37:18.329: INFO: Got endpoints: latency-svc-glzbx [750.032602ms]
May 19 09:37:18.343: INFO: Created: latency-svc-k8t4w
May 19 09:37:18.378: INFO: Got endpoints: latency-svc-76nms [750.136597ms]
May 19 09:37:18.391: INFO: Created: latency-svc-jlksn
May 19 09:37:18.429: INFO: Got endpoints: latency-svc-nndz2 [750.292834ms]
May 19 09:37:18.442: INFO: Created: latency-svc-bb5t7
May 19 09:37:18.477: INFO: Got endpoints: latency-svc-jfq9p [747.797858ms]
May 19 09:37:18.492: INFO: Created: latency-svc-xbjhh
May 19 09:37:18.527: INFO: Got endpoints: latency-svc-qrrs5 [750.161181ms]
May 19 09:37:18.540: INFO: Created: latency-svc-n5vrr
May 19 09:37:18.578: INFO: Got endpoints: latency-svc-nllsd [749.626609ms]
May 19 09:37:18.591: INFO: Created: latency-svc-cb7b7
May 19 09:37:18.628: INFO: Got endpoints: latency-svc-dzq77 [750.492443ms]
May 19 09:37:18.641: INFO: Created: latency-svc-kh2rt
May 19 09:37:18.678: INFO: Got endpoints: latency-svc-skdcz [748.599368ms]
May 19 09:37:18.691: INFO: Created: latency-svc-6p6rr
May 19 09:37:18.728: INFO: Got endpoints: latency-svc-2q52t [750.820353ms]
May 19 09:37:18.741: INFO: Created: latency-svc-7kmqm
May 19 09:37:18.783: INFO: Got endpoints: latency-svc-8gb64 [755.213084ms]
May 19 09:37:18.795: INFO: Created: latency-svc-vxd79
May 19 09:37:18.828: INFO: Got endpoints: latency-svc-274s7 [750.060679ms]
May 19 09:37:18.840: INFO: Created: latency-svc-5s9cm
May 19 09:37:18.878: INFO: Got endpoints: latency-svc-x68xb [748.965622ms]
May 19 09:37:18.892: INFO: Created: latency-svc-nhdxn
May 19 09:37:18.928: INFO: Got endpoints: latency-svc-jk4pc [749.67466ms]
May 19 09:37:18.941: INFO: Created: latency-svc-ftbp7
May 19 09:37:18.980: INFO: Got endpoints: latency-svc-4fvcg [749.007038ms]
May 19 09:37:18.995: INFO: Created: latency-svc-5zc6d
May 19 09:37:19.028: INFO: Got endpoints: latency-svc-r5q2m [750.06438ms]
May 19 09:37:19.041: INFO: Created: latency-svc-gvczk
May 19 09:37:19.077: INFO: Got endpoints: latency-svc-k8t4w [748.388654ms]
May 19 09:37:19.090: INFO: Created: latency-svc-66wqs
May 19 09:37:19.128: INFO: Got endpoints: latency-svc-jlksn [750.466602ms]
May 19 09:37:19.141: INFO: Created: latency-svc-hv2ml
May 19 09:37:19.179: INFO: Got endpoints: latency-svc-bb5t7 [750.192331ms]
May 19 09:37:19.191: INFO: Created: latency-svc-899qv
May 19 09:37:19.232: INFO: Got endpoints: latency-svc-xbjhh [754.140446ms]
May 19 09:37:19.244: INFO: Created: latency-svc-k4h9h
May 19 09:37:19.278: INFO: Got endpoints: latency-svc-n5vrr [750.435661ms]
May 19 09:37:19.291: INFO: Created: latency-svc-zk2k5
May 19 09:37:19.328: INFO: Got endpoints: latency-svc-cb7b7 [749.632545ms]
May 19 09:37:19.341: INFO: Created: latency-svc-wc4n6
May 19 09:37:19.378: INFO: Got endpoints: latency-svc-kh2rt [750.332365ms]
May 19 09:37:19.391: INFO: Created: latency-svc-2psfs
May 19 09:37:19.428: INFO: Got endpoints: latency-svc-6p6rr [750.097346ms]
May 19 09:37:19.442: INFO: Created: latency-svc-f2576
May 19 09:37:19.478: INFO: Got endpoints: latency-svc-7kmqm [749.414008ms]
May 19 09:37:19.491: INFO: Created: latency-svc-qsk7r
May 19 09:37:19.527: INFO: Got endpoints: latency-svc-vxd79 [744.486496ms]
May 19 09:37:19.541: INFO: Created: latency-svc-9szbh
May 19 09:37:19.579: INFO: Got endpoints: latency-svc-5s9cm [751.227533ms]
May 19 09:37:19.592: INFO: Created: latency-svc-xc7f5
May 19 09:37:19.628: INFO: Got endpoints: latency-svc-nhdxn [749.858835ms]
May 19 09:37:19.678: INFO: Got endpoints: latency-svc-ftbp7 [749.71426ms]
May 19 09:37:19.728: INFO: Got endpoints: latency-svc-5zc6d [747.515556ms]
May 19 09:37:19.777: INFO: Got endpoints: latency-svc-gvczk [749.122487ms]
May 19 09:37:19.827: INFO: Got endpoints: latency-svc-66wqs [749.532601ms]
May 19 09:37:19.878: INFO: Got endpoints: latency-svc-hv2ml [750.024291ms]
May 19 09:37:19.927: INFO: Got endpoints: latency-svc-899qv [748.336552ms]
May 19 09:37:19.977: INFO: Got endpoints: latency-svc-k4h9h [745.398357ms]
May 19 09:37:20.027: INFO: Got endpoints: latency-svc-zk2k5 [749.128843ms]
May 19 09:37:20.077: INFO: Got endpoints: latency-svc-wc4n6 [749.401297ms]
May 19 09:37:20.128: INFO: Got endpoints: latency-svc-2psfs [749.38258ms]
May 19 09:37:20.178: INFO: Got endpoints: latency-svc-f2576 [750.074018ms]
May 19 09:37:20.227: INFO: Got endpoints: latency-svc-qsk7r [749.627447ms]
May 19 09:37:20.277: INFO: Got endpoints: latency-svc-9szbh [749.612291ms]
May 19 09:37:20.328: INFO: Got endpoints: latency-svc-xc7f5 [748.584003ms]
May 19 09:37:20.328: INFO: Latencies: [19.05996ms 29.92679ms 41.399783ms 53.243638ms 67.445787ms 79.530318ms 91.438637ms 102.566888ms 114.547633ms 126.092215ms 137.463028ms 149.446148ms 169.871594ms 169.963297ms 171.610597ms 172.142167ms 172.381027ms 173.021551ms 174.582736ms 175.420634ms 176.075196ms 176.763142ms 177.919451ms 178.978469ms 179.29241ms 179.794157ms 182.061587ms 182.10922ms 182.285711ms 182.330341ms 182.350734ms 183.173057ms 183.259871ms 183.475544ms 183.682208ms 183.8228ms 188.132418ms 191.193745ms 201.13775ms 241.304468ms 289.357217ms 321.485506ms 357.177848ms 396.622815ms 437.723184ms 475.761807ms 510.568129ms 546.552202ms 590.19029ms 625.107802ms 661.440177ms 700.4531ms 738.928586ms 740.240993ms 740.723394ms 744.162916ms 744.408203ms 744.486496ms 745.398357ms 745.748895ms 746.419521ms 746.474626ms 747.322721ms 747.515556ms 747.797858ms 748.100415ms 748.216354ms 748.299745ms 748.336552ms 748.339695ms 748.388654ms 748.584003ms 748.599368ms 748.636455ms 748.672982ms 748.745618ms 748.87769ms 748.932516ms 748.965622ms 748.990276ms 749.002987ms 749.007038ms 749.027361ms 749.071222ms 749.09015ms 749.122487ms 749.128843ms 749.151751ms 749.161668ms 749.281518ms 749.295905ms 749.35136ms 749.38258ms 749.401297ms 749.414008ms 749.441107ms 749.446136ms 749.51095ms 749.532601ms 749.560887ms 749.589802ms 749.600068ms 749.612291ms 749.626609ms 749.627447ms 749.632545ms 749.650564ms 749.669841ms 749.67466ms 749.679759ms 749.712235ms 749.71426ms 749.750439ms 749.790808ms 749.805475ms 749.831735ms 749.83425ms 749.836764ms 749.858835ms 749.864002ms 749.870428ms 749.890752ms 749.921553ms 749.940969ms 749.949419ms 749.952214ms 750.008436ms 750.018145ms 750.024291ms 750.027434ms 750.032602ms 750.043358ms 750.058304ms 750.058653ms 750.060679ms 750.06438ms 750.074018ms 750.092945ms 750.097346ms 750.116552ms 750.131918ms 750.136597ms 750.161181ms 750.180947ms 750.192331ms 750.193658ms 750.212864ms 750.215868ms 750.226414ms 750.226554ms 750.238427ms 750.290878ms 750.292834ms 750.307292ms 750.322587ms 750.332365ms 750.335229ms 750.341025ms 750.360162ms 750.362676ms 750.384537ms 750.3874ms 750.435661ms 750.43615ms 750.46304ms 750.466602ms 750.470374ms 750.482665ms 750.492443ms 750.501663ms 750.509066ms 750.512908ms 750.596858ms 750.608382ms 750.762105ms 750.820353ms 750.822518ms 750.855274ms 750.997054ms 751.05202ms 751.110339ms 751.111455ms 751.214962ms 751.227533ms 751.27335ms 751.341516ms 751.580517ms 751.628987ms 751.636809ms 751.822729ms 752.619071ms 752.698133ms 752.793048ms 753.38168ms 753.909058ms 754.140446ms 755.213084ms 755.350463ms 755.630182ms 757.638147ms]
May 19 09:37:20.328: INFO: 50 %ile: 749.589802ms
May 19 09:37:20.328: INFO: 90 %ile: 751.110339ms
May 19 09:37:20.328: INFO: 99 %ile: 755.630182ms
May 19 09:37:20.328: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:37:20.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2147" for this suite.
May 19 09:37:36.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:37:36.424: INFO: namespace svc-latency-2147 deletion completed in 16.09075832s

• [SLOW TEST:26.849 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:37:36.424: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:37:37.097: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:37:39.108: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477857, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477857, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477857, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725477857, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:37:42.129: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:37:42.133: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3718-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:37:43.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-878" for this suite.
May 19 09:37:49.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:37:49.345: INFO: namespace webhook-878 deletion completed in 6.095290044s
STEP: Destroying namespace "webhook-878-markers" for this suite.
May 19 09:37:55.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:37:55.436: INFO: namespace webhook-878-markers deletion completed in 6.09057583s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.024 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:37:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
May 19 09:38:01.511: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0519 09:38:01.511919      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:38:01.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3229" for this suite.
May 19 09:38:07.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:38:07.605: INFO: namespace gc-3229 deletion completed in 6.089485661s

• [SLOW TEST:12.156 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:38:07.605: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:39:07.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3943" for this suite.
May 19 09:39:35.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:39:35.742: INFO: namespace container-probe-3943 deletion completed in 28.092114566s

• [SLOW TEST:88.137 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:39:35.742: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:39:35.791: INFO: (0) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 17.725553ms)
May 19 09:39:35.795: INFO: (1) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.158201ms)
May 19 09:39:35.799: INFO: (2) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.004547ms)
May 19 09:39:35.803: INFO: (3) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.930654ms)
May 19 09:39:35.807: INFO: (4) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.019913ms)
May 19 09:39:35.811: INFO: (5) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.80857ms)
May 19 09:39:35.815: INFO: (6) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.045754ms)
May 19 09:39:35.819: INFO: (7) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.96816ms)
May 19 09:39:35.823: INFO: (8) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.890146ms)
May 19 09:39:35.827: INFO: (9) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.806754ms)
May 19 09:39:35.831: INFO: (10) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.826309ms)
May 19 09:39:35.835: INFO: (11) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.993861ms)
May 19 09:39:35.839: INFO: (12) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.080466ms)
May 19 09:39:35.843: INFO: (13) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.870869ms)
May 19 09:39:35.847: INFO: (14) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.966762ms)
May 19 09:39:35.850: INFO: (15) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.829103ms)
May 19 09:39:35.855: INFO: (16) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.095482ms)
May 19 09:39:35.859: INFO: (17) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.949092ms)
May 19 09:39:35.862: INFO: (18) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.913961ms)
May 19 09:39:35.866: INFO: (19) /api/v1/nodes/nchc-kubemaster01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.844119ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:39:35.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7329" for this suite.
May 19 09:39:41.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:39:41.959: INFO: namespace proxy-7329 deletion completed in 6.089383482s

• [SLOW TEST:6.217 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:39:41.960: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
May 19 09:39:41.994: INFO: Waiting up to 5m0s for pod "var-expansion-189e2371-d485-4a68-882a-c830f57a1fab" in namespace "var-expansion-7722" to be "success or failure"
May 19 09:39:41.997: INFO: Pod "var-expansion-189e2371-d485-4a68-882a-c830f57a1fab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225873ms
May 19 09:39:44.000: INFO: Pod "var-expansion-189e2371-d485-4a68-882a-c830f57a1fab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005050419s
May 19 09:39:46.004: INFO: Pod "var-expansion-189e2371-d485-4a68-882a-c830f57a1fab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009473656s
STEP: Saw pod success
May 19 09:39:46.004: INFO: Pod "var-expansion-189e2371-d485-4a68-882a-c830f57a1fab" satisfied condition "success or failure"
May 19 09:39:46.007: INFO: Trying to get logs from node nchc-worker03 pod var-expansion-189e2371-d485-4a68-882a-c830f57a1fab container dapi-container: <nil>
STEP: delete the pod
May 19 09:39:46.040: INFO: Waiting for pod var-expansion-189e2371-d485-4a68-882a-c830f57a1fab to disappear
May 19 09:39:46.042: INFO: Pod var-expansion-189e2371-d485-4a68-882a-c830f57a1fab no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:39:46.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7722" for this suite.
May 19 09:39:52.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:39:52.136: INFO: namespace var-expansion-7722 deletion completed in 6.089495788s

• [SLOW TEST:10.176 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:39:52.136: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:39:56.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9687" for this suite.
May 19 09:40:02.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:40:03.071: INFO: namespace watch-9687 deletion completed in 6.177374076s

• [SLOW TEST:10.934 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:40:03.071: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:40:03.611: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:40:05.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478003, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478003, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478003, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478003, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:40:08.639: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 19 09:40:12.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 attach --namespace=webhook-4238 to-be-attached-pod -i -c=container1'
May 19 09:40:12.806: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:40:12.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4238" for this suite.
May 19 09:40:24.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:40:24.913: INFO: namespace webhook-4238 deletion completed in 12.095584872s
STEP: Destroying namespace "webhook-4238-markers" for this suite.
May 19 09:40:30.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:40:31.005: INFO: namespace webhook-4238-markers deletion completed in 6.091493418s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.947 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:40:31.018: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 19 09:40:31.050: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:40:45.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7111" for this suite.
May 19 09:40:51.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:40:51.350: INFO: namespace pods-7111 deletion completed in 6.09015964s

• [SLOW TEST:20.332 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:40:51.350: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 19 09:40:51.378: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 19 09:40:51.391: INFO: Waiting for terminating namespaces to be deleted...
May 19 09:40:51.393: INFO: 
Logging pods the kubelet thinks is on node nchc-kubemaster01 before test
May 19 09:40:51.399: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-7hsd4 from sonobuoy started at 2020-05-19 08:18:56 +0000 UTC (2 container statuses recorded)
May 19 09:40:51.399: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 19 09:40:51.399: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 09:40:51.399: INFO: kube-proxy-qg52l from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 09:40:51.399: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 09:40:51.399: INFO: calico-node-7jgfk from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 09:40:51.399: INFO: 	Container calico-node ready: true, restart count 0
May 19 09:40:51.399: INFO: 
Logging pods the kubelet thinks is on node nchc-worker03 before test
May 19 09:40:51.405: INFO: kube-proxy-5l969 from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 09:40:51.405: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 09:40:51.405: INFO: sonobuoy from sonobuoy started at 2020-05-19 08:18:41 +0000 UTC (1 container statuses recorded)
May 19 09:40:51.405: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 19 09:40:51.405: INFO: calico-node-724vj from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 09:40:51.405: INFO: 	Container calico-node ready: true, restart count 0
May 19 09:40:51.405: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-hw9m8 from sonobuoy started at 2020-05-19 08:18:43 +0000 UTC (2 container statuses recorded)
May 19 09:40:51.405: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 19 09:40:51.405: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.161064931a93b4b9], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:40:52.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7589" for this suite.
May 19 09:40:58.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:40:58.527: INFO: namespace sched-pred-7589 deletion completed in 6.087372862s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.177 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:40:58.527: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 19 09:41:03.094: INFO: Successfully updated pod "annotationupdate5794c7a2-ad80-4692-bb1e-d98e5b355c76"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:41:05.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5627" for this suite.
May 19 09:41:33.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:41:33.213: INFO: namespace downward-api-5627 deletion completed in 28.094802515s

• [SLOW TEST:34.686 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:41:33.213: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:41:33.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 version'
May 19 09:41:33.334: INFO: stderr: ""
May 19 09:41:33.334: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:44:51Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:36:15Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:41:33.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3910" for this suite.
May 19 09:41:39.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:41:39.431: INFO: namespace kubectl-3910 deletion completed in 6.089812733s

• [SLOW TEST:6.218 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:41:39.432: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
May 19 09:41:39.468: INFO: Waiting up to 5m0s for pod "var-expansion-637e857d-8a0a-4d9b-80e2-b1c99e45fec8" in namespace "var-expansion-244" to be "success or failure"
May 19 09:41:39.470: INFO: Pod "var-expansion-637e857d-8a0a-4d9b-80e2-b1c99e45fec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.399641ms
May 19 09:41:41.474: INFO: Pod "var-expansion-637e857d-8a0a-4d9b-80e2-b1c99e45fec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005641425s
STEP: Saw pod success
May 19 09:41:41.474: INFO: Pod "var-expansion-637e857d-8a0a-4d9b-80e2-b1c99e45fec8" satisfied condition "success or failure"
May 19 09:41:41.476: INFO: Trying to get logs from node nchc-kubemaster01 pod var-expansion-637e857d-8a0a-4d9b-80e2-b1c99e45fec8 container dapi-container: <nil>
STEP: delete the pod
May 19 09:41:41.495: INFO: Waiting for pod var-expansion-637e857d-8a0a-4d9b-80e2-b1c99e45fec8 to disappear
May 19 09:41:41.497: INFO: Pod var-expansion-637e857d-8a0a-4d9b-80e2-b1c99e45fec8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:41:41.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-244" for this suite.
May 19 09:41:47.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:41:47.592: INFO: namespace var-expansion-244 deletion completed in 6.091105862s

• [SLOW TEST:8.161 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:41:47.593: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-1c251c10-a780-47c9-93ad-eff6aa672c47
STEP: Creating a pod to test consume configMaps
May 19 09:41:47.632: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0c567af-9435-4f22-8ab0-68204d984fad" in namespace "projected-2911" to be "success or failure"
May 19 09:41:47.634: INFO: Pod "pod-projected-configmaps-c0c567af-9435-4f22-8ab0-68204d984fad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309824ms
May 19 09:41:49.640: INFO: Pod "pod-projected-configmaps-c0c567af-9435-4f22-8ab0-68204d984fad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00743714s
STEP: Saw pod success
May 19 09:41:49.640: INFO: Pod "pod-projected-configmaps-c0c567af-9435-4f22-8ab0-68204d984fad" satisfied condition "success or failure"
May 19 09:41:49.643: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-configmaps-c0c567af-9435-4f22-8ab0-68204d984fad container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 19 09:41:49.662: INFO: Waiting for pod pod-projected-configmaps-c0c567af-9435-4f22-8ab0-68204d984fad to disappear
May 19 09:41:49.665: INFO: Pod pod-projected-configmaps-c0c567af-9435-4f22-8ab0-68204d984fad no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:41:49.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2911" for this suite.
May 19 09:41:55.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:41:55.762: INFO: namespace projected-2911 deletion completed in 6.093391242s

• [SLOW TEST:8.169 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:41:55.762: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:41:55.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62" in namespace "projected-2216" to be "success or failure"
May 19 09:41:55.800: INFO: Pod "downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.212325ms
May 19 09:41:57.803: INFO: Pod "downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005550072s
May 19 09:41:59.808: INFO: Pod "downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009917784s
STEP: Saw pod success
May 19 09:41:59.808: INFO: Pod "downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62" satisfied condition "success or failure"
May 19 09:41:59.811: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62 container client-container: <nil>
STEP: delete the pod
May 19 09:41:59.829: INFO: Waiting for pod downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62 to disappear
May 19 09:41:59.832: INFO: Pod downwardapi-volume-84852ce9-e053-4f2a-9c64-263f26a26e62 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:41:59.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2216" for this suite.
May 19 09:42:05.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:42:05.925: INFO: namespace projected-2216 deletion completed in 6.088991597s

• [SLOW TEST:10.163 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:42:05.925: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5134
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 19 09:42:05.953: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 19 09:42:32.018: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.161.118:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5134 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:42:32.018: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:42:32.230: INFO: Found all expected endpoints: [netserver-0]
May 19 09:42:32.233: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.148.81:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5134 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:42:32.233: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:42:32.453: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:42:32.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5134" for this suite.
May 19 09:42:44.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:42:44.548: INFO: namespace pod-network-test-5134 deletion completed in 12.089335245s

• [SLOW TEST:38.623 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:42:44.548: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-e1ea6cee-8204-4a6e-a9c6-c66f6f951caf
STEP: Creating a pod to test consume secrets
May 19 09:42:44.589: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fc3877e8-bf25-478a-86b4-6dce266e65b8" in namespace "projected-1724" to be "success or failure"
May 19 09:42:44.591: INFO: Pod "pod-projected-secrets-fc3877e8-bf25-478a-86b4-6dce266e65b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282725ms
May 19 09:42:46.595: INFO: Pod "pod-projected-secrets-fc3877e8-bf25-478a-86b4-6dce266e65b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005947265s
STEP: Saw pod success
May 19 09:42:46.595: INFO: Pod "pod-projected-secrets-fc3877e8-bf25-478a-86b4-6dce266e65b8" satisfied condition "success or failure"
May 19 09:42:46.597: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-secrets-fc3877e8-bf25-478a-86b4-6dce266e65b8 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 19 09:42:46.616: INFO: Waiting for pod pod-projected-secrets-fc3877e8-bf25-478a-86b4-6dce266e65b8 to disappear
May 19 09:42:46.618: INFO: Pod pod-projected-secrets-fc3877e8-bf25-478a-86b4-6dce266e65b8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:42:46.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1724" for this suite.
May 19 09:42:52.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:42:52.706: INFO: namespace projected-1724 deletion completed in 6.083507371s

• [SLOW TEST:8.157 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:42:52.706: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9925.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9925.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 09:42:54.778: INFO: DNS probes using dns-9925/dns-test-3794f3b5-d585-4674-95c8-26435df1721a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:42:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9925" for this suite.
May 19 09:43:00.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:43:00.881: INFO: namespace dns-9925 deletion completed in 6.089494601s

• [SLOW TEST:8.175 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:43:00.882: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:43:00.929: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b2f3c101-2e8e-4552-ba42-65a99b23cf64", Controller:(*bool)(0xc00082d14a), BlockOwnerDeletion:(*bool)(0xc00082d14b)}}
May 19 09:43:00.933: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9dc3801a-c2fc-4ff1-9b90-e63cf17dc20b", Controller:(*bool)(0xc00400686a), BlockOwnerDeletion:(*bool)(0xc00400686b)}}
May 19 09:43:00.937: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"844a079b-d9ba-4961-b666-2cdaacf16b19", Controller:(*bool)(0xc004006a0a), BlockOwnerDeletion:(*bool)(0xc004006a0b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:43:05.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6032" for this suite.
May 19 09:43:11.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:43:12.043: INFO: namespace gc-6032 deletion completed in 6.092259587s

• [SLOW TEST:11.161 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:43:12.043: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 19 09:43:12.666: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 19 09:43:14.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478192, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478192, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478192, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478192, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:43:17.698: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:43:17.702: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:43:18.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1919" for this suite.
May 19 09:43:24.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:43:24.952: INFO: namespace crd-webhook-1919 deletion completed in 6.092906327s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.922 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:43:24.966: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
May 19 09:43:24.995: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:43:30.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9401" for this suite.
May 19 09:43:36.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:43:36.177: INFO: namespace init-container-9401 deletion completed in 6.094829295s

• [SLOW TEST:11.211 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:43:36.177: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-1007/configmap-test-b09798b2-fc8f-48cf-b1ba-0c8b0dd2413b
STEP: Creating a pod to test consume configMaps
May 19 09:43:36.219: INFO: Waiting up to 5m0s for pod "pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02" in namespace "configmap-1007" to be "success or failure"
May 19 09:43:36.222: INFO: Pod "pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.938824ms
May 19 09:43:38.227: INFO: Pod "pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007632699s
May 19 09:43:40.231: INFO: Pod "pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012429382s
STEP: Saw pod success
May 19 09:43:40.232: INFO: Pod "pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02" satisfied condition "success or failure"
May 19 09:43:40.234: INFO: Trying to get logs from node nchc-worker03 pod pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02 container env-test: <nil>
STEP: delete the pod
May 19 09:43:40.265: INFO: Waiting for pod pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02 to disappear
May 19 09:43:40.267: INFO: Pod pod-configmaps-930e93c9-ec80-4ba3-97ce-834d07902c02 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:43:40.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1007" for this suite.
May 19 09:43:46.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:43:46.361: INFO: namespace configmap-1007 deletion completed in 6.089676889s

• [SLOW TEST:10.184 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:43:46.361: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-34cfd29c-f161-4904-b78e-6b21647e1329
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-34cfd29c-f161-4904-b78e-6b21647e1329
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:45:18.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6068" for this suite.
May 19 09:45:46.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:45:47.027: INFO: namespace configmap-6068 deletion completed in 28.087885614s

• [SLOW TEST:120.666 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:45:47.027: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:45:48.507: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:45:50.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478348, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478348, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478348, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478348, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:45:53.540: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:45:53.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7045" for this suite.
May 19 09:45:59.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:45:59.694: INFO: namespace webhook-7045 deletion completed in 6.085246654s
STEP: Destroying namespace "webhook-7045-markers" for this suite.
May 19 09:46:05.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:46:05.783: INFO: namespace webhook-7045-markers deletion completed in 6.089209365s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.769 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:46:05.796: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
May 19 09:46:05.824: INFO: Waiting up to 1m0s for all nodes to be ready
May 19 09:47:05.849: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:47:05.853: INFO: Starting informer...
STEP: Starting pods...
May 19 09:47:06.069: INFO: Pod1 is running on nchc-kubemaster01. Tainting Node
May 19 09:47:08.285: INFO: Pod2 is running on nchc-kubemaster01. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 19 09:47:25.253: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 19 09:47:35.302: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:47:35.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4242" for this suite.
May 19 09:47:41.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:47:41.420: INFO: namespace taint-multiple-pods-4242 deletion completed in 6.100244656s

• [SLOW TEST:95.624 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:47:41.420: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:47:42.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:47:45.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478463, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478463, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478463, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478462, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:47:48.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:47:48.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6872" for this suite.
May 19 09:47:54.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:47:54.127: INFO: namespace webhook-6872 deletion completed in 6.091198893s
STEP: Destroying namespace "webhook-6872-markers" for this suite.
May 19 09:48:00.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:48:00.218: INFO: namespace webhook-6872-markers deletion completed in 6.09147889s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.811 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:48:00.232: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c7817b72-1500-4729-b69d-b36ed4bc7fa7
STEP: Creating a pod to test consume secrets
May 19 09:48:00.272: INFO: Waiting up to 5m0s for pod "pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d" in namespace "secrets-6274" to be "success or failure"
May 19 09:48:00.275: INFO: Pod "pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295856ms
May 19 09:48:02.279: INFO: Pod "pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006335239s
May 19 09:48:04.282: INFO: Pod "pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009517727s
STEP: Saw pod success
May 19 09:48:04.282: INFO: Pod "pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d" satisfied condition "success or failure"
May 19 09:48:04.285: INFO: Trying to get logs from node nchc-worker03 pod pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d container secret-volume-test: <nil>
STEP: delete the pod
May 19 09:48:04.313: INFO: Waiting for pod pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d to disappear
May 19 09:48:04.316: INFO: Pod pod-secrets-6822d30a-7337-43fb-9226-77b39a271c2d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:48:04.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6274" for this suite.
May 19 09:48:10.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:48:10.406: INFO: namespace secrets-6274 deletion completed in 6.086366156s

• [SLOW TEST:10.174 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:48:10.406: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-4swn
STEP: Creating a pod to test atomic-volume-subpath
May 19 09:48:10.452: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4swn" in namespace "subpath-6627" to be "success or failure"
May 19 09:48:10.454: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24082ms
May 19 09:48:12.459: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006983026s
May 19 09:48:14.463: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 4.011259733s
May 19 09:48:16.468: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 6.015626886s
May 19 09:48:18.473: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 8.021216558s
May 19 09:48:20.478: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 10.025467773s
May 19 09:48:22.482: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 12.030086078s
May 19 09:48:24.487: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 14.034704314s
May 19 09:48:26.492: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 16.039444146s
May 19 09:48:28.497: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 18.044640117s
May 19 09:48:30.501: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 20.048811641s
May 19 09:48:32.505: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Running", Reason="", readiness=true. Elapsed: 22.053064742s
May 19 09:48:34.509: INFO: Pod "pod-subpath-test-secret-4swn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057250444s
STEP: Saw pod success
May 19 09:48:34.509: INFO: Pod "pod-subpath-test-secret-4swn" satisfied condition "success or failure"
May 19 09:48:34.512: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-subpath-test-secret-4swn container test-container-subpath-secret-4swn: <nil>
STEP: delete the pod
May 19 09:48:34.546: INFO: Waiting for pod pod-subpath-test-secret-4swn to disappear
May 19 09:48:34.548: INFO: Pod pod-subpath-test-secret-4swn no longer exists
STEP: Deleting pod pod-subpath-test-secret-4swn
May 19 09:48:34.548: INFO: Deleting pod "pod-subpath-test-secret-4swn" in namespace "subpath-6627"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:48:34.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6627" for this suite.
May 19 09:48:40.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:48:40.645: INFO: namespace subpath-6627 deletion completed in 6.089581624s

• [SLOW TEST:30.238 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:48:40.645: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
May 19 09:48:40.681: INFO: Waiting up to 5m0s for pod "pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f" in namespace "emptydir-2500" to be "success or failure"
May 19 09:48:40.684: INFO: Pod "pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.310173ms
May 19 09:48:42.689: INFO: Pod "pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007133117s
May 19 09:48:44.693: INFO: Pod "pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011214126s
STEP: Saw pod success
May 19 09:48:44.693: INFO: Pod "pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f" satisfied condition "success or failure"
May 19 09:48:44.695: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f container test-container: <nil>
STEP: delete the pod
May 19 09:48:44.713: INFO: Waiting for pod pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f to disappear
May 19 09:48:44.715: INFO: Pod pod-e67e99a7-7133-42f7-9682-5bc6e2455f2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:48:44.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2500" for this suite.
May 19 09:48:50.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:48:50.809: INFO: namespace emptydir-2500 deletion completed in 6.089985104s

• [SLOW TEST:10.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:48:50.810: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 19 09:48:50.837: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 19 09:48:50.848: INFO: Waiting for terminating namespaces to be deleted...
May 19 09:48:50.850: INFO: 
Logging pods the kubelet thinks is on node nchc-kubemaster01 before test
May 19 09:48:50.855: INFO: kube-proxy-qg52l from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 09:48:50.855: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 09:48:50.855: INFO: calico-node-7jgfk from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 09:48:50.855: INFO: 	Container calico-node ready: true, restart count 0
May 19 09:48:50.855: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-7hsd4 from sonobuoy started at 2020-05-19 08:18:56 +0000 UTC (2 container statuses recorded)
May 19 09:48:50.855: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 19 09:48:50.855: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 09:48:50.855: INFO: 
Logging pods the kubelet thinks is on node nchc-worker03 before test
May 19 09:48:50.861: INFO: calico-node-724vj from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 09:48:50.861: INFO: 	Container calico-node ready: true, restart count 0
May 19 09:48:50.861: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-hw9m8 from sonobuoy started at 2020-05-19 08:18:43 +0000 UTC (2 container statuses recorded)
May 19 09:48:50.861: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 19 09:48:50.861: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 09:48:50.861: INFO: kube-proxy-5l969 from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 09:48:50.861: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 09:48:50.861: INFO: sonobuoy from sonobuoy started at 2020-05-19 08:18:41 +0000 UTC (1 container statuses recorded)
May 19 09:48:50.861: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-613d8660-fab0-4801-a26f-bbe81bdaa8f0 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-613d8660-fab0-4801-a26f-bbe81bdaa8f0 off the node nchc-worker03
STEP: verifying the node doesn't have the label kubernetes.io/e2e-613d8660-fab0-4801-a26f-bbe81bdaa8f0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:53:58.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4389" for this suite.
May 19 09:54:16.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:54:17.035: INFO: namespace sched-pred-4389 deletion completed in 18.092075097s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:326.225 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:54:17.035: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:54:17.938: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 19 09:54:19.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478857, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478857, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478857, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725478857, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:54:22.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:54:23.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1908" for this suite.
May 19 09:54:35.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:54:35.111: INFO: namespace webhook-1908 deletion completed in 12.090002301s
STEP: Destroying namespace "webhook-1908-markers" for this suite.
May 19 09:54:41.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:54:41.199: INFO: namespace webhook-1908-markers deletion completed in 6.088056689s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:24.177 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:54:41.212: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 19 09:54:41.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7794'
May 19 09:54:41.438: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 19 09:54:41.438: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
May 19 09:54:43.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7794'
May 19 09:54:43.571: INFO: stderr: ""
May 19 09:54:43.571: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:54:43.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7794" for this suite.
May 19 09:55:11.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:55:11.662: INFO: namespace kubectl-7794 deletion completed in 28.086724668s

• [SLOW TEST:30.450 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:55:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:55:11.703: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025" in namespace "projected-8729" to be "success or failure"
May 19 09:55:11.705: INFO: Pod "downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.317228ms
May 19 09:55:13.709: INFO: Pod "downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006073887s
May 19 09:55:15.712: INFO: Pod "downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009795555s
STEP: Saw pod success
May 19 09:55:15.712: INFO: Pod "downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025" satisfied condition "success or failure"
May 19 09:55:15.715: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025 container client-container: <nil>
STEP: delete the pod
May 19 09:55:15.747: INFO: Waiting for pod downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025 to disappear
May 19 09:55:15.750: INFO: Pod downwardapi-volume-e4654dba-e1ab-4930-aa7a-1d24e6362025 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:55:15.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8729" for this suite.
May 19 09:55:21.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:55:21.845: INFO: namespace projected-8729 deletion completed in 6.090628143s

• [SLOW TEST:10.182 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:55:21.845: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1996
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1996
STEP: Creating statefulset with conflicting port in namespace statefulset-1996
STEP: Waiting until pod test-pod will start running in namespace statefulset-1996
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1996
May 19 09:55:25.923: INFO: Observed stateful pod in namespace: statefulset-1996, name: ss-0, uid: 16bb1199-cc06-4893-b857-9238c03ad051, status phase: Pending. Waiting for statefulset controller to delete.
May 19 09:55:26.297: INFO: Observed stateful pod in namespace: statefulset-1996, name: ss-0, uid: 16bb1199-cc06-4893-b857-9238c03ad051, status phase: Failed. Waiting for statefulset controller to delete.
May 19 09:55:26.302: INFO: Observed stateful pod in namespace: statefulset-1996, name: ss-0, uid: 16bb1199-cc06-4893-b857-9238c03ad051, status phase: Failed. Waiting for statefulset controller to delete.
May 19 09:55:26.306: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1996
STEP: Removing pod with conflicting port in namespace statefulset-1996
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1996 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 19 09:55:30.326: INFO: Deleting all statefulset in ns statefulset-1996
May 19 09:55:30.329: INFO: Scaling statefulset ss to 0
May 19 09:55:40.345: INFO: Waiting for statefulset status.replicas updated to 0
May 19 09:55:40.348: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:55:40.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1996" for this suite.
May 19 09:55:46.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:55:46.460: INFO: namespace statefulset-1996 deletion completed in 6.095826784s

• [SLOW TEST:24.615 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:55:46.460: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-9836
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9836
STEP: Deleting pre-stop pod
May 19 09:55:57.528: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:55:57.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9836" for this suite.
May 19 09:56:41.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:56:41.623: INFO: namespace prestop-9836 deletion completed in 44.084903735s

• [SLOW TEST:55.163 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:56:41.623: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:56:41.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7112" for this suite.
May 19 09:56:47.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:56:47.772: INFO: namespace resourcequota-7112 deletion completed in 6.085344573s

• [SLOW TEST:6.148 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:56:47.772: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
May 19 09:56:47.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-7247'
May 19 09:56:48.306: INFO: stderr: ""
May 19 09:56:48.306: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 19 09:56:48.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7247'
May 19 09:56:48.407: INFO: stderr: ""
May 19 09:56:48.407: INFO: stdout: "update-demo-nautilus-vhlt8 update-demo-nautilus-vxdpr "
May 19 09:56:48.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vhlt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:56:48.510: INFO: stderr: ""
May 19 09:56:48.510: INFO: stdout: ""
May 19 09:56:48.510: INFO: update-demo-nautilus-vhlt8 is created but not running
May 19 09:56:53.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7247'
May 19 09:56:53.631: INFO: stderr: ""
May 19 09:56:53.631: INFO: stdout: "update-demo-nautilus-vhlt8 update-demo-nautilus-vxdpr "
May 19 09:56:53.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vhlt8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:56:53.729: INFO: stderr: ""
May 19 09:56:53.729: INFO: stdout: "true"
May 19 09:56:53.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vhlt8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:56:53.825: INFO: stderr: ""
May 19 09:56:53.825: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:56:53.825: INFO: validating pod update-demo-nautilus-vhlt8
May 19 09:56:53.831: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:56:53.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:56:53.831: INFO: update-demo-nautilus-vhlt8 is verified up and running
May 19 09:56:53.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:56:53.933: INFO: stderr: ""
May 19 09:56:53.933: INFO: stdout: "true"
May 19 09:56:53.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:56:54.037: INFO: stderr: ""
May 19 09:56:54.037: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:56:54.037: INFO: validating pod update-demo-nautilus-vxdpr
May 19 09:56:54.043: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:56:54.043: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:56:54.043: INFO: update-demo-nautilus-vxdpr is verified up and running
STEP: scaling down the replication controller
May 19 09:56:54.045: INFO: scanned /root for discovery docs: <nil>
May 19 09:56:54.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7247'
May 19 09:56:55.166: INFO: stderr: ""
May 19 09:56:55.166: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 19 09:56:55.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7247'
May 19 09:56:55.268: INFO: stderr: ""
May 19 09:56:55.268: INFO: stdout: "update-demo-nautilus-vhlt8 update-demo-nautilus-vxdpr "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 19 09:57:00.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7247'
May 19 09:57:00.378: INFO: stderr: ""
May 19 09:57:00.378: INFO: stdout: "update-demo-nautilus-vxdpr "
May 19 09:57:00.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:00.478: INFO: stderr: ""
May 19 09:57:00.478: INFO: stdout: "true"
May 19 09:57:00.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:00.579: INFO: stderr: ""
May 19 09:57:00.579: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:57:00.579: INFO: validating pod update-demo-nautilus-vxdpr
May 19 09:57:00.583: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:57:00.583: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:57:00.583: INFO: update-demo-nautilus-vxdpr is verified up and running
STEP: scaling up the replication controller
May 19 09:57:00.585: INFO: scanned /root for discovery docs: <nil>
May 19 09:57:00.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7247'
May 19 09:57:01.707: INFO: stderr: ""
May 19 09:57:01.707: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 19 09:57:01.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7247'
May 19 09:57:01.809: INFO: stderr: ""
May 19 09:57:01.809: INFO: stdout: "update-demo-nautilus-vxdpr update-demo-nautilus-xzhp4 "
May 19 09:57:01.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:01.908: INFO: stderr: ""
May 19 09:57:01.908: INFO: stdout: "true"
May 19 09:57:01.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:02.007: INFO: stderr: ""
May 19 09:57:02.007: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:57:02.007: INFO: validating pod update-demo-nautilus-vxdpr
May 19 09:57:02.012: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:57:02.012: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:57:02.012: INFO: update-demo-nautilus-vxdpr is verified up and running
May 19 09:57:02.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-xzhp4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:02.112: INFO: stderr: ""
May 19 09:57:02.112: INFO: stdout: ""
May 19 09:57:02.112: INFO: update-demo-nautilus-xzhp4 is created but not running
May 19 09:57:07.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7247'
May 19 09:57:07.228: INFO: stderr: ""
May 19 09:57:07.228: INFO: stdout: "update-demo-nautilus-vxdpr update-demo-nautilus-xzhp4 "
May 19 09:57:07.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:07.327: INFO: stderr: ""
May 19 09:57:07.327: INFO: stdout: "true"
May 19 09:57:07.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-vxdpr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:07.428: INFO: stderr: ""
May 19 09:57:07.428: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:57:07.428: INFO: validating pod update-demo-nautilus-vxdpr
May 19 09:57:07.433: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:57:07.433: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:57:07.433: INFO: update-demo-nautilus-vxdpr is verified up and running
May 19 09:57:07.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-xzhp4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:07.532: INFO: stderr: ""
May 19 09:57:07.532: INFO: stdout: "true"
May 19 09:57:07.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods update-demo-nautilus-xzhp4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7247'
May 19 09:57:07.629: INFO: stderr: ""
May 19 09:57:07.629: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 19 09:57:07.629: INFO: validating pod update-demo-nautilus-xzhp4
May 19 09:57:07.635: INFO: got data: {
  "image": "nautilus.jpg"
}

May 19 09:57:07.635: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 19 09:57:07.635: INFO: update-demo-nautilus-xzhp4 is verified up and running
STEP: using delete to clean up resources
May 19 09:57:07.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-7247'
May 19 09:57:07.740: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 09:57:07.740: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 19 09:57:07.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7247'
May 19 09:57:07.845: INFO: stderr: "No resources found in kubectl-7247 namespace.\n"
May 19 09:57:07.845: INFO: stdout: ""
May 19 09:57:07.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -l name=update-demo --namespace=kubectl-7247 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 19 09:57:07.945: INFO: stderr: ""
May 19 09:57:07.945: INFO: stdout: "update-demo-nautilus-vxdpr\nupdate-demo-nautilus-xzhp4\n"
May 19 09:57:08.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7247'
May 19 09:57:08.567: INFO: stderr: "No resources found in kubectl-7247 namespace.\n"
May 19 09:57:08.567: INFO: stdout: ""
May 19 09:57:08.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pods -l name=update-demo --namespace=kubectl-7247 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 19 09:57:08.670: INFO: stderr: ""
May 19 09:57:08.670: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:57:08.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7247" for this suite.
May 19 09:57:20.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:57:20.761: INFO: namespace kubectl-7247 deletion completed in 12.086394039s

• [SLOW TEST:32.989 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:57:20.761: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-9a33296c-af50-4c48-bae2-b2ec283797be
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:57:20.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2575" for this suite.
May 19 09:57:26.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:57:26.884: INFO: namespace configmap-2575 deletion completed in 6.090226689s

• [SLOW TEST:6.124 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:57:26.885: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 19 09:57:32.949: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:32.950: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:33.130: INFO: Exec stderr: ""
May 19 09:57:33.130: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:33.130: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:33.310: INFO: Exec stderr: ""
May 19 09:57:33.310: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:33.310: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:33.470: INFO: Exec stderr: ""
May 19 09:57:33.470: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:33.470: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:33.655: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 19 09:57:33.655: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:33.655: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:33.810: INFO: Exec stderr: ""
May 19 09:57:33.810: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:33.810: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:33.982: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 19 09:57:33.982: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:34.384: INFO: Exec stderr: ""
May 19 09:57:34.384: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:34.384: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:34.882: INFO: Exec stderr: ""
May 19 09:57:34.882: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:34.883: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:35.216: INFO: Exec stderr: ""
May 19 09:57:35.216: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1973 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 19 09:57:35.216: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
May 19 09:57:35.613: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:57:35.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1973" for this suite.
May 19 09:58:25.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:58:25.708: INFO: namespace e2e-kubelet-etc-hosts-1973 deletion completed in 50.089896636s

• [SLOW TEST:58.824 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:58:25.709: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 09:58:25.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97d17a57-a101-4d9e-bb90-c519feca2002" in namespace "downward-api-1488" to be "success or failure"
May 19 09:58:25.751: INFO: Pod "downwardapi-volume-97d17a57-a101-4d9e-bb90-c519feca2002": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24103ms
May 19 09:58:27.755: INFO: Pod "downwardapi-volume-97d17a57-a101-4d9e-bb90-c519feca2002": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006068928s
STEP: Saw pod success
May 19 09:58:27.755: INFO: Pod "downwardapi-volume-97d17a57-a101-4d9e-bb90-c519feca2002" satisfied condition "success or failure"
May 19 09:58:27.758: INFO: Trying to get logs from node nchc-kubemaster01 pod downwardapi-volume-97d17a57-a101-4d9e-bb90-c519feca2002 container client-container: <nil>
STEP: delete the pod
May 19 09:58:27.787: INFO: Waiting for pod downwardapi-volume-97d17a57-a101-4d9e-bb90-c519feca2002 to disappear
May 19 09:58:27.790: INFO: Pod downwardapi-volume-97d17a57-a101-4d9e-bb90-c519feca2002 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:58:27.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1488" for this suite.
May 19 09:58:33.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:58:33.882: INFO: namespace downward-api-1488 deletion completed in 6.088536436s

• [SLOW TEST:8.173 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:58:33.882: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 19 09:58:33.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-1846'
May 19 09:58:34.027: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 19 09:58:34.027: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
May 19 09:58:34.032: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-rq6wm]
May 19 09:58:34.032: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-rq6wm" in namespace "kubectl-1846" to be "running and ready"
May 19 09:58:34.034: INFO: Pod "e2e-test-httpd-rc-rq6wm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429603ms
May 19 09:58:36.038: INFO: Pod "e2e-test-httpd-rc-rq6wm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006161538s
May 19 09:58:36.038: INFO: Pod "e2e-test-httpd-rc-rq6wm" satisfied condition "running and ready"
May 19 09:58:36.038: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-rq6wm]
May 19 09:58:36.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 logs rc/e2e-test-httpd-rc --namespace=kubectl-1846'
May 19 09:58:36.167: INFO: stderr: ""
May 19 09:58:36.167: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.161.84. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.161.84. Set the 'ServerName' directive globally to suppress this message\n[Tue May 19 09:58:35.267770 2020] [mpm_event:notice] [pid 1:tid 140179385019240] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue May 19 09:58:35.267830 2020] [core:notice] [pid 1:tid 140179385019240] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
May 19 09:58:36.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete rc e2e-test-httpd-rc --namespace=kubectl-1846'
May 19 09:58:36.276: INFO: stderr: ""
May 19 09:58:36.276: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:58:36.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1846" for this suite.
May 19 09:58:42.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:58:42.372: INFO: namespace kubectl-1846 deletion completed in 6.091352338s

• [SLOW TEST:8.490 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:58:42.373: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 09:58:42.938: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 09:58:44.948: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479122, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479122, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479122, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479122, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 09:58:47.968: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:58:48.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-715" for this suite.
May 19 09:58:54.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:58:54.103: INFO: namespace webhook-715 deletion completed in 6.085849882s
STEP: Destroying namespace "webhook-715-markers" for this suite.
May 19 09:59:00.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:59:00.190: INFO: namespace webhook-715-markers deletion completed in 6.087293523s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.830 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:59:00.204: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 09:59:00.242: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 19 09:59:05.246: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 19 09:59:05.246: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 19 09:59:07.251: INFO: Creating deployment "test-rollover-deployment"
May 19 09:59:07.258: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 19 09:59:09.264: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 19 09:59:09.270: INFO: Ensure that both replica sets have 1 created replica
May 19 09:59:09.276: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 19 09:59:09.284: INFO: Updating deployment test-rollover-deployment
May 19 09:59:09.284: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 19 09:59:11.291: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 19 09:59:11.296: INFO: Make sure deployment "test-rollover-deployment" is complete
May 19 09:59:11.302: INFO: all replica sets need to contain the pod-template-hash label
May 19 09:59:11.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479147, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479147, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479149, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479147, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 19 09:59:13.310: INFO: 
May 19 09:59:13.310: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
May 19 09:59:13.318: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3688 /apis/apps/v1/namespaces/deployment-3688/deployments/test-rollover-deployment 4f434adf-a4b8-45ef-825c-9c718af4eb9c 1123431 2 2020-05-19 09:59:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00327bbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-05-19 09:59:07 +0000 UTC,LastTransitionTime:2020-05-19 09:59:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-05-19 09:59:11 +0000 UTC,LastTransitionTime:2020-05-19 09:59:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 19 09:59:13.321: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-3688 /apis/apps/v1/namespaces/deployment-3688/replicasets/test-rollover-deployment-7d7dc6548c 4621d88d-4bad-42e2-8a7b-c6d79584039f 1123420 2 2020-05-19 09:59:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4f434adf-a4b8-45ef-825c-9c718af4eb9c 0xc003ea4077 0xc003ea4078}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ea40d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 19 09:59:13.321: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 19 09:59:13.322: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3688 /apis/apps/v1/namespaces/deployment-3688/replicasets/test-rollover-controller 259f4675-1081-4364-9e56-e33a5c073640 1123430 2 2020-05-19 09:59:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4f434adf-a4b8-45ef-825c-9c718af4eb9c 0xc00327bfa7 0xc00327bfa8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ea4008 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 19 09:59:13.322: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3688 /apis/apps/v1/namespaces/deployment-3688/replicasets/test-rollover-deployment-f6c94f66c 47983f0b-4df7-45e0-8124-3983a85ed5e6 1123397 2 2020-05-19 09:59:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4f434adf-a4b8-45ef-825c-9c718af4eb9c 0xc003ea4140 0xc003ea4141}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ea41b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 19 09:59:13.325: INFO: Pod "test-rollover-deployment-7d7dc6548c-scs2v" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-scs2v test-rollover-deployment-7d7dc6548c- deployment-3688 /api/v1/namespaces/deployment-3688/pods/test-rollover-deployment-7d7dc6548c-scs2v 377036c6-d886-4e65-94a0-2bd01c0c5e94 1123419 0 2020-05-19 09:59:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:10.244.148.95/32] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 4621d88d-4bad-42e2-8a7b-c6d79584039f 0xc003ea4747 0xc003ea4748}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-75mql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-75mql,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-75mql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nchc-worker03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-05-19 09:59:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.181,PodIP:10.244.148.95,StartTime:2020-05-19 09:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-05-19 09:58:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://6a774b521cf396b2e4187af6742ca62dc832f54ca983ae6823d63045c08e3fb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.148.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:59:13.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3688" for this suite.
May 19 09:59:19.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:59:19.424: INFO: namespace deployment-3688 deletion completed in 6.095366174s

• [SLOW TEST:19.220 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:59:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
May 19 09:59:19.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-8079'
May 19 09:59:19.753: INFO: stderr: ""
May 19 09:59:19.753: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 19 09:59:20.758: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:59:20.758: INFO: Found 0 / 1
May 19 09:59:21.758: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:59:21.758: INFO: Found 0 / 1
May 19 09:59:22.757: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:59:22.757: INFO: Found 1 / 1
May 19 09:59:22.757: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 19 09:59:22.760: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:59:22.760: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 19 09:59:22.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 patch pod redis-master-gh7wg --namespace=kubectl-8079 -p {"metadata":{"annotations":{"x":"y"}}}'
May 19 09:59:22.876: INFO: stderr: ""
May 19 09:59:22.876: INFO: stdout: "pod/redis-master-gh7wg patched\n"
STEP: checking annotations
May 19 09:59:22.880: INFO: Selector matched 1 pods for map[app:redis]
May 19 09:59:22.880: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 09:59:22.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8079" for this suite.
May 19 09:59:50.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 09:59:50.973: INFO: namespace kubectl-8079 deletion completed in 28.088930916s

• [SLOW TEST:31.548 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 09:59:50.974: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8044
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
May 19 09:59:51.013: INFO: Found 0 stateful pods, waiting for 3
May 19 10:00:01.018: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 19 10:00:01.018: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 19 10:00:01.018: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 19 10:00:01.045: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 19 10:00:11.079: INFO: Updating stateful set ss2
May 19 10:00:11.085: INFO: Waiting for Pod statefulset-8044/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 19 10:00:21.119: INFO: Found 2 stateful pods, waiting for 3
May 19 10:00:31.125: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 19 10:00:31.125: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 19 10:00:31.125: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 19 10:00:31.149: INFO: Updating stateful set ss2
May 19 10:00:31.155: INFO: Waiting for Pod statefulset-8044/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 19 10:00:41.181: INFO: Updating stateful set ss2
May 19 10:00:41.187: INFO: Waiting for StatefulSet statefulset-8044/ss2 to complete update
May 19 10:00:41.187: INFO: Waiting for Pod statefulset-8044/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
May 19 10:00:51.196: INFO: Deleting all statefulset in ns statefulset-8044
May 19 10:00:51.199: INFO: Scaling statefulset ss2 to 0
May 19 10:01:21.214: INFO: Waiting for statefulset status.replicas updated to 0
May 19 10:01:21.218: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:01:21.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8044" for this suite.
May 19 10:01:27.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:01:27.327: INFO: namespace statefulset-8044 deletion completed in 6.093575418s

• [SLOW TEST:96.354 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:01:27.328: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-856d3931-e023-4eed-9a90-b5c7481e01d2
STEP: Creating a pod to test consume secrets
May 19 10:01:27.367: INFO: Waiting up to 5m0s for pod "pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae" in namespace "secrets-5130" to be "success or failure"
May 19 10:01:27.369: INFO: Pod "pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071243ms
May 19 10:01:29.372: INFO: Pod "pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005510469s
May 19 10:01:31.378: INFO: Pod "pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010925044s
STEP: Saw pod success
May 19 10:01:31.378: INFO: Pod "pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae" satisfied condition "success or failure"
May 19 10:01:31.381: INFO: Trying to get logs from node nchc-worker03 pod pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae container secret-volume-test: <nil>
STEP: delete the pod
May 19 10:01:31.415: INFO: Waiting for pod pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae to disappear
May 19 10:01:31.418: INFO: Pod pod-secrets-71d00c65-c6ab-452e-b609-f6eaa9203bae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:01:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5130" for this suite.
May 19 10:01:37.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:01:37.509: INFO: namespace secrets-5130 deletion completed in 6.087258811s

• [SLOW TEST:10.181 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:01:37.509: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 10:01:41.574: INFO: Waiting up to 5m0s for pod "client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7" in namespace "pods-6420" to be "success or failure"
May 19 10:01:41.576: INFO: Pod "client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267988ms
May 19 10:01:43.580: INFO: Pod "client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005726281s
May 19 10:01:45.584: INFO: Pod "client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009860438s
STEP: Saw pod success
May 19 10:01:45.584: INFO: Pod "client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7" satisfied condition "success or failure"
May 19 10:01:45.587: INFO: Trying to get logs from node nchc-worker03 pod client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7 container env3cont: <nil>
STEP: delete the pod
May 19 10:01:45.605: INFO: Waiting for pod client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7 to disappear
May 19 10:01:45.607: INFO: Pod client-envvars-5591fd4e-485f-4448-85b7-3224ab4d6cf7 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:01:45.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6420" for this suite.
May 19 10:01:57.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:01:57.697: INFO: namespace pods-6420 deletion completed in 12.086049088s

• [SLOW TEST:20.189 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:01:57.698: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 19 10:02:02.758: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:02:03.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2787" for this suite.
May 19 10:02:31.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:02:31.866: INFO: namespace replicaset-2787 deletion completed in 28.091516547s

• [SLOW TEST:34.169 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:02:31.867: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
May 19 10:02:31.903: INFO: Waiting up to 5m0s for pod "pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a" in namespace "emptydir-1054" to be "success or failure"
May 19 10:02:31.905: INFO: Pod "pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.212883ms
May 19 10:02:33.909: INFO: Pod "pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006460185s
May 19 10:02:35.913: INFO: Pod "pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010326147s
STEP: Saw pod success
May 19 10:02:35.913: INFO: Pod "pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a" satisfied condition "success or failure"
May 19 10:02:35.916: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a container test-container: <nil>
STEP: delete the pod
May 19 10:02:35.946: INFO: Waiting for pod pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a to disappear
May 19 10:02:35.948: INFO: Pod pod-80c65ff9-0622-4f03-ab5e-7f6cb414a32a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:02:35.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1054" for this suite.
May 19 10:02:41.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:02:42.041: INFO: namespace emptydir-1054 deletion completed in 6.089664738s

• [SLOW TEST:10.175 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:02:42.041: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-96ebff78-b996-4e99-885e-e3f5839ab1aa
STEP: Creating a pod to test consume configMaps
May 19 10:02:42.083: INFO: Waiting up to 5m0s for pod "pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193" in namespace "configmap-6873" to be "success or failure"
May 19 10:02:42.086: INFO: Pod "pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210579ms
May 19 10:02:44.089: INFO: Pod "pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006027651s
May 19 10:02:46.094: INFO: Pod "pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010228019s
STEP: Saw pod success
May 19 10:02:46.094: INFO: Pod "pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193" satisfied condition "success or failure"
May 19 10:02:46.096: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193 container configmap-volume-test: <nil>
STEP: delete the pod
May 19 10:02:46.114: INFO: Waiting for pod pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193 to disappear
May 19 10:02:46.117: INFO: Pod pod-configmaps-76c235a0-2816-4776-b4b7-dc5b78856193 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:02:46.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6873" for this suite.
May 19 10:02:52.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:02:52.207: INFO: namespace configmap-6873 deletion completed in 6.086515829s

• [SLOW TEST:10.165 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:02:52.207: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-7f93745d-bd4b-4377-b3a1-4bb986a200a3
STEP: Creating a pod to test consume secrets
May 19 10:02:52.247: INFO: Waiting up to 5m0s for pod "pod-secrets-21503ad6-b052-4019-bb3f-fe98d303a8cd" in namespace "secrets-7201" to be "success or failure"
May 19 10:02:52.249: INFO: Pod "pod-secrets-21503ad6-b052-4019-bb3f-fe98d303a8cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.278605ms
May 19 10:02:54.254: INFO: Pod "pod-secrets-21503ad6-b052-4019-bb3f-fe98d303a8cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006961443s
STEP: Saw pod success
May 19 10:02:54.254: INFO: Pod "pod-secrets-21503ad6-b052-4019-bb3f-fe98d303a8cd" satisfied condition "success or failure"
May 19 10:02:54.257: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-secrets-21503ad6-b052-4019-bb3f-fe98d303a8cd container secret-volume-test: <nil>
STEP: delete the pod
May 19 10:02:54.277: INFO: Waiting for pod pod-secrets-21503ad6-b052-4019-bb3f-fe98d303a8cd to disappear
May 19 10:02:54.279: INFO: Pod pod-secrets-21503ad6-b052-4019-bb3f-fe98d303a8cd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:02:54.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7201" for this suite.
May 19 10:03:00.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:03:00.371: INFO: namespace secrets-7201 deletion completed in 6.088232901s

• [SLOW TEST:8.164 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:03:00.372: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-1d305295-db91-49ff-b6cb-0169bb0ce8f3
STEP: Creating a pod to test consume configMaps
May 19 10:03:00.411: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-839f1fca-6f3e-4fcf-9bb7-713e8ba652ce" in namespace "projected-5251" to be "success or failure"
May 19 10:03:00.414: INFO: Pod "pod-projected-configmaps-839f1fca-6f3e-4fcf-9bb7-713e8ba652ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.189835ms
May 19 10:03:02.417: INFO: Pod "pod-projected-configmaps-839f1fca-6f3e-4fcf-9bb7-713e8ba652ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005782085s
STEP: Saw pod success
May 19 10:03:02.417: INFO: Pod "pod-projected-configmaps-839f1fca-6f3e-4fcf-9bb7-713e8ba652ce" satisfied condition "success or failure"
May 19 10:03:02.420: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-configmaps-839f1fca-6f3e-4fcf-9bb7-713e8ba652ce container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 19 10:03:02.437: INFO: Waiting for pod pod-projected-configmaps-839f1fca-6f3e-4fcf-9bb7-713e8ba652ce to disappear
May 19 10:03:02.439: INFO: Pod pod-projected-configmaps-839f1fca-6f3e-4fcf-9bb7-713e8ba652ce no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:03:02.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5251" for this suite.
May 19 10:03:08.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:03:08.534: INFO: namespace projected-5251 deletion completed in 6.091572481s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:03:08.535: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:03:19.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4485" for this suite.
May 19 10:03:25.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:03:25.693: INFO: namespace resourcequota-4485 deletion completed in 6.088478886s

• [SLOW TEST:17.159 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:03:25.694: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-29582f07-8927-47a4-88c5-ba3c01136301
STEP: Creating secret with name secret-projected-all-test-volume-61df83af-e135-49dc-a24d-6a5c21b0c0b9
STEP: Creating a pod to test Check all projections for projected volume plugin
May 19 10:03:25.737: INFO: Waiting up to 5m0s for pod "projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25" in namespace "projected-3835" to be "success or failure"
May 19 10:03:25.739: INFO: Pod "projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279443ms
May 19 10:03:27.743: INFO: Pod "projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25": Phase="Running", Reason="", readiness=true. Elapsed: 2.006136326s
May 19 10:03:29.747: INFO: Pod "projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010278025s
STEP: Saw pod success
May 19 10:03:29.748: INFO: Pod "projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25" satisfied condition "success or failure"
May 19 10:03:29.750: INFO: Trying to get logs from node nchc-kubemaster01 pod projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25 container projected-all-volume-test: <nil>
STEP: delete the pod
May 19 10:03:29.770: INFO: Waiting for pod projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25 to disappear
May 19 10:03:29.772: INFO: Pod projected-volume-bee6c15b-2183-4711-9d88-14fdf618ca25 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:03:29.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3835" for this suite.
May 19 10:03:35.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:03:35.864: INFO: namespace projected-3835 deletion completed in 6.087270195s

• [SLOW TEST:10.169 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:03:35.864: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:03:46.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-160" for this suite.
May 19 10:03:52.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:03:53.054: INFO: namespace resourcequota-160 deletion completed in 6.091133801s

• [SLOW TEST:17.190 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:03:53.054: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-2941
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2941 to expose endpoints map[]
May 19 10:03:53.107: INFO: Get endpoints failed (2.503986ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May 19 10:03:54.110: INFO: successfully validated that service multi-endpoint-test in namespace services-2941 exposes endpoints map[] (1.005993228s elapsed)
STEP: Creating pod pod1 in namespace services-2941
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2941 to expose endpoints map[pod1:[100]]
May 19 10:03:57.144: INFO: successfully validated that service multi-endpoint-test in namespace services-2941 exposes endpoints map[pod1:[100]] (3.026112469s elapsed)
STEP: Creating pod pod2 in namespace services-2941
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2941 to expose endpoints map[pod1:[100] pod2:[101]]
May 19 10:04:00.184: INFO: successfully validated that service multi-endpoint-test in namespace services-2941 exposes endpoints map[pod1:[100] pod2:[101]] (3.035979788s elapsed)
STEP: Deleting pod pod1 in namespace services-2941
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2941 to expose endpoints map[pod2:[101]]
May 19 10:04:01.202: INFO: successfully validated that service multi-endpoint-test in namespace services-2941 exposes endpoints map[pod2:[101]] (1.012749981s elapsed)
STEP: Deleting pod pod2 in namespace services-2941
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2941 to expose endpoints map[]
May 19 10:04:02.217: INFO: successfully validated that service multi-endpoint-test in namespace services-2941 exposes endpoints map[] (1.009226996s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:04:02.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2941" for this suite.
May 19 10:04:30.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:04:30.333: INFO: namespace services-2941 deletion completed in 28.089235917s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:37.279 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:04:30.334: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3330.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3330.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 10:04:34.391: INFO: DNS probes using dns-test-500035c5-75ec-4ca9-b02e-802722ac4ee8 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3330.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3330.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 10:04:38.438: INFO: File wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:38.441: INFO: File jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:38.441: INFO: Lookups using dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 failed for: [wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local]

May 19 10:04:43.449: INFO: File wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:43.453: INFO: File jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:43.453: INFO: Lookups using dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 failed for: [wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local]

May 19 10:04:48.446: INFO: File wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:48.450: INFO: File jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:48.450: INFO: Lookups using dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 failed for: [wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local]

May 19 10:04:53.448: INFO: File wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:53.452: INFO: File jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:53.452: INFO: Lookups using dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 failed for: [wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local]

May 19 10:04:58.448: INFO: File wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:58.452: INFO: File jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:04:58.452: INFO: Lookups using dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 failed for: [wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local]

May 19 10:05:03.448: INFO: File wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:05:03.452: INFO: File jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local from pod  dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 19 10:05:03.452: INFO: Lookups using dns-3330/dns-test-b579833c-96e3-4974-a522-ff175dddde81 failed for: [wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local]

May 19 10:05:08.451: INFO: DNS probes using dns-test-b579833c-96e3-4974-a522-ff175dddde81 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3330.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3330.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3330.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3330.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 19 10:05:10.510: INFO: DNS probes using dns-test-2728527f-0ed6-4c28-a319-3668f3948d0c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:05:10.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3330" for this suite.
May 19 10:05:16.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:05:16.633: INFO: namespace dns-3330 deletion completed in 6.08858351s

• [SLOW TEST:46.300 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:05:16.634: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 10:05:16.672: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-cb7e569c-7965-4c6b-982c-bf4031358415" in namespace "security-context-test-4240" to be "success or failure"
May 19 10:05:16.675: INFO: Pod "busybox-privileged-false-cb7e569c-7965-4c6b-982c-bf4031358415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.43149ms
May 19 10:05:18.678: INFO: Pod "busybox-privileged-false-cb7e569c-7965-4c6b-982c-bf4031358415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006220486s
May 19 10:05:18.678: INFO: Pod "busybox-privileged-false-cb7e569c-7965-4c6b-982c-bf4031358415" satisfied condition "success or failure"
May 19 10:05:18.698: INFO: Got logs for pod "busybox-privileged-false-cb7e569c-7965-4c6b-982c-bf4031358415": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:05:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4240" for this suite.
May 19 10:05:24.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:05:24.788: INFO: namespace security-context-test-4240 deletion completed in 6.085359031s

• [SLOW TEST:8.154 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:05:24.788: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 10:05:24.824: INFO: Waiting up to 5m0s for pod "downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64" in namespace "downward-api-1377" to be "success or failure"
May 19 10:05:24.827: INFO: Pod "downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160571ms
May 19 10:05:26.831: INFO: Pod "downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006349205s
May 19 10:05:28.836: INFO: Pod "downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011160901s
STEP: Saw pod success
May 19 10:05:28.836: INFO: Pod "downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64" satisfied condition "success or failure"
May 19 10:05:28.839: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64 container client-container: <nil>
STEP: delete the pod
May 19 10:05:28.869: INFO: Waiting for pod downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64 to disappear
May 19 10:05:28.871: INFO: Pod downwardapi-volume-289071d3-9bdb-444c-bdc1-74d9934ffa64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:05:28.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1377" for this suite.
May 19 10:05:34.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:05:34.969: INFO: namespace downward-api-1377 deletion completed in 6.094192195s

• [SLOW TEST:10.181 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:05:34.969: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 19 10:05:35.006: INFO: Pod name pod-release: Found 0 pods out of 1
May 19 10:05:40.012: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:05:41.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4672" for this suite.
May 19 10:05:47.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:05:47.117: INFO: namespace replication-controller-4672 deletion completed in 6.087140079s

• [SLOW TEST:12.148 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:05:47.118: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 10:05:47.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597" in namespace "projected-9110" to be "success or failure"
May 19 10:05:47.156: INFO: Pod "downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712326ms
May 19 10:05:49.159: INFO: Pod "downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006055169s
May 19 10:05:51.168: INFO: Pod "downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015013612s
STEP: Saw pod success
May 19 10:05:51.168: INFO: Pod "downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597" satisfied condition "success or failure"
May 19 10:05:51.172: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597 container client-container: <nil>
STEP: delete the pod
May 19 10:05:51.190: INFO: Waiting for pod downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597 to disappear
May 19 10:05:51.192: INFO: Pod downwardapi-volume-b6644dbb-b3e4-4ddf-9b6a-7181ce11f597 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:05:51.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9110" for this suite.
May 19 10:05:57.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:05:57.286: INFO: namespace projected-9110 deletion completed in 6.088920499s

• [SLOW TEST:10.169 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:05:57.287: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 19 10:06:03.373: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 19 10:06:03.375: INFO: Pod pod-with-poststart-exec-hook still exists
May 19 10:06:05.376: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 19 10:06:05.380: INFO: Pod pod-with-poststart-exec-hook still exists
May 19 10:06:07.376: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 19 10:06:07.380: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:06:07.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8542" for this suite.
May 19 10:06:35.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:06:35.476: INFO: namespace container-lifecycle-hook-8542 deletion completed in 28.090800105s

• [SLOW TEST:38.189 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:06:35.476: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
May 19 10:06:40.030: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-88 pod-service-account-4c363911-622e-4efd-9bc0-b4193c735388 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 19 10:06:40.554: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-88 pod-service-account-4c363911-622e-4efd-9bc0-b4193c735388 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 19 10:06:40.982: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-88 pod-service-account-4c363911-622e-4efd-9bc0-b4193c735388 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:06:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-88" for this suite.
May 19 10:06:47.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:06:47.543: INFO: namespace svcaccounts-88 deletion completed in 6.090891379s

• [SLOW TEST:12.067 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:06:47.543: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-d2b373a3-9693-45ec-95f9-7fab38333c8c in namespace container-probe-772
May 19 10:06:49.587: INFO: Started pod busybox-d2b373a3-9693-45ec-95f9-7fab38333c8c in namespace container-probe-772
STEP: checking the pod's current state and verifying that restartCount is present
May 19 10:06:49.589: INFO: Initial restart count of pod busybox-d2b373a3-9693-45ec-95f9-7fab38333c8c is 0
May 19 10:07:39.703: INFO: Restart count of pod container-probe-772/busybox-d2b373a3-9693-45ec-95f9-7fab38333c8c is now 1 (50.11421663s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:07:39.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-772" for this suite.
May 19 10:07:45.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:07:45.808: INFO: namespace container-probe-772 deletion completed in 6.089927345s

• [SLOW TEST:58.265 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:07:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
May 19 10:07:45.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb" in namespace "downward-api-6266" to be "success or failure"
May 19 10:07:45.846: INFO: Pod "downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185645ms
May 19 10:07:47.851: INFO: Pod "downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006514801s
May 19 10:07:49.855: INFO: Pod "downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011213373s
STEP: Saw pod success
May 19 10:07:49.855: INFO: Pod "downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb" satisfied condition "success or failure"
May 19 10:07:49.858: INFO: Trying to get logs from node nchc-worker03 pod downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb container client-container: <nil>
STEP: delete the pod
May 19 10:07:49.890: INFO: Waiting for pod downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb to disappear
May 19 10:07:49.892: INFO: Pod downwardapi-volume-8f175a1d-1766-41cc-8d63-58656795f7eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:07:49.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6266" for this suite.
May 19 10:07:55.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:07:55.983: INFO: namespace downward-api-6266 deletion completed in 6.087281632s

• [SLOW TEST:10.175 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:07:55.984: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-84529d1e-2e89-42db-83af-d2500536dea2
STEP: Creating a pod to test consume secrets
May 19 10:07:56.052: INFO: Waiting up to 5m0s for pod "pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e" in namespace "secrets-536" to be "success or failure"
May 19 10:07:56.054: INFO: Pod "pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20101ms
May 19 10:07:58.058: INFO: Pod "pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006326592s
May 19 10:08:00.063: INFO: Pod "pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011373882s
STEP: Saw pod success
May 19 10:08:00.063: INFO: Pod "pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e" satisfied condition "success or failure"
May 19 10:08:00.066: INFO: Trying to get logs from node nchc-worker03 pod pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e container secret-volume-test: <nil>
STEP: delete the pod
May 19 10:08:00.086: INFO: Waiting for pod pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e to disappear
May 19 10:08:00.088: INFO: Pod pod-secrets-a16bcd72-7f57-407d-91aa-ff3de18b4a9e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:08:00.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-536" for this suite.
May 19 10:08:06.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:08:06.181: INFO: namespace secrets-536 deletion completed in 6.088891078s
STEP: Destroying namespace "secret-namespace-4040" for this suite.
May 19 10:08:12.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:08:12.268: INFO: namespace secret-namespace-4040 deletion completed in 6.087587332s

• [SLOW TEST:16.285 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:08:12.269: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 19 10:08:14.846: INFO: Successfully updated pod "annotationupdate8d44c80d-7a72-4de1-8933-338d19884110"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:08:16.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-139" for this suite.
May 19 10:08:44.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:08:44.958: INFO: namespace projected-139 deletion completed in 28.089507615s

• [SLOW TEST:32.689 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:08:44.959: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vxf7p in namespace proxy-4811
I0519 10:08:45.006870      25 runners.go:184] Created replication controller with name: proxy-service-vxf7p, namespace: proxy-4811, replica count: 1
I0519 10:08:46.057303      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0519 10:08:47.057549      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:48.057767      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:49.057993      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:50.058213      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:51.058432      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:52.058741      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:53.059033      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:54.059225      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0519 10:08:55.059419      25 runners.go:184] proxy-service-vxf7p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 19 10:08:55.063: INFO: setup took 10.075961378s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 19 10:08:55.067: INFO: (0) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.089267ms)
May 19 10:08:55.067: INFO: (0) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.595413ms)
May 19 10:08:55.068: INFO: (0) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.828338ms)
May 19 10:08:55.069: INFO: (0) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 5.871852ms)
May 19 10:08:55.069: INFO: (0) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 5.709259ms)
May 19 10:08:55.069: INFO: (0) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 6.192986ms)
May 19 10:08:55.069: INFO: (0) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 6.099188ms)
May 19 10:08:55.069: INFO: (0) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.065594ms)
May 19 10:08:55.070: INFO: (0) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.90866ms)
May 19 10:08:55.071: INFO: (0) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 7.685447ms)
May 19 10:08:55.071: INFO: (0) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 7.589833ms)
May 19 10:08:55.076: INFO: (0) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 13.509037ms)
May 19 10:08:55.076: INFO: (0) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 13.620855ms)
May 19 10:08:55.076: INFO: (0) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 13.717237ms)
May 19 10:08:55.076: INFO: (0) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 13.597946ms)
May 19 10:08:55.076: INFO: (0) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 13.638105ms)
May 19 10:08:55.080: INFO: (1) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.313039ms)
May 19 10:08:55.080: INFO: (1) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 3.641997ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.990442ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 5.046874ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 5.090526ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 5.044989ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 5.202203ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 5.112387ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 5.212611ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 5.272046ms)
May 19 10:08:55.082: INFO: (1) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 5.438759ms)
May 19 10:08:55.083: INFO: (1) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.500921ms)
May 19 10:08:55.083: INFO: (1) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.62545ms)
May 19 10:08:55.083: INFO: (1) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.608338ms)
May 19 10:08:55.083: INFO: (1) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.766951ms)
May 19 10:08:55.084: INFO: (1) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.996033ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.182366ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.241803ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.344471ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.243478ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.298375ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.307524ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.350128ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.549737ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.435265ms)
May 19 10:08:55.088: INFO: (2) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.533254ms)
May 19 10:08:55.089: INFO: (2) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.158553ms)
May 19 10:08:55.090: INFO: (2) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.12943ms)
May 19 10:08:55.090: INFO: (2) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.153945ms)
May 19 10:08:55.090: INFO: (2) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.07635ms)
May 19 10:08:55.090: INFO: (2) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.22693ms)
May 19 10:08:55.090: INFO: (2) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.31975ms)
May 19 10:08:55.093: INFO: (3) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 2.973396ms)
May 19 10:08:55.094: INFO: (3) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.054555ms)
May 19 10:08:55.094: INFO: (3) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.108054ms)
May 19 10:08:55.094: INFO: (3) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.063844ms)
May 19 10:08:55.094: INFO: (3) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.111337ms)
May 19 10:08:55.094: INFO: (3) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.063984ms)
May 19 10:08:55.094: INFO: (3) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.165465ms)
May 19 10:08:55.094: INFO: (3) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.175871ms)
May 19 10:08:55.095: INFO: (3) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.303124ms)
May 19 10:08:55.095: INFO: (3) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.404116ms)
May 19 10:08:55.096: INFO: (3) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.460759ms)
May 19 10:08:55.096: INFO: (3) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.154014ms)
May 19 10:08:55.097: INFO: (3) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.351319ms)
May 19 10:08:55.097: INFO: (3) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.255495ms)
May 19 10:08:55.097: INFO: (3) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.323591ms)
May 19 10:08:55.097: INFO: (3) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.757731ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.668885ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.60498ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 3.774766ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 3.962084ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.022079ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.099114ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.227205ms)
May 19 10:08:55.101: INFO: (4) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.18935ms)
May 19 10:08:55.102: INFO: (4) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.208696ms)
May 19 10:08:55.102: INFO: (4) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.270437ms)
May 19 10:08:55.102: INFO: (4) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 4.957685ms)
May 19 10:08:55.102: INFO: (4) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 5.216941ms)
May 19 10:08:55.103: INFO: (4) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.380581ms)
May 19 10:08:55.103: INFO: (4) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 5.342727ms)
May 19 10:08:55.103: INFO: (4) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.105404ms)
May 19 10:08:55.104: INFO: (4) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.3421ms)
May 19 10:08:55.107: INFO: (5) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 2.977517ms)
May 19 10:08:55.107: INFO: (5) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.71554ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 3.933938ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.011183ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.989602ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.097089ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.13222ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.502104ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.555045ms)
May 19 10:08:55.108: INFO: (5) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.570271ms)
May 19 10:08:55.109: INFO: (5) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.091852ms)
May 19 10:08:55.109: INFO: (5) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.719525ms)
May 19 10:08:55.109: INFO: (5) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.833718ms)
May 19 10:08:55.110: INFO: (5) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.332391ms)
May 19 10:08:55.110: INFO: (5) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.393992ms)
May 19 10:08:55.110: INFO: (5) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.495613ms)
May 19 10:08:55.113: INFO: (6) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 2.765965ms)
May 19 10:08:55.114: INFO: (6) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 3.540446ms)
May 19 10:08:55.114: INFO: (6) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 3.876457ms)
May 19 10:08:55.114: INFO: (6) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 3.748017ms)
May 19 10:08:55.114: INFO: (6) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.923251ms)
May 19 10:08:55.114: INFO: (6) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 3.909213ms)
May 19 10:08:55.115: INFO: (6) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.310457ms)
May 19 10:08:55.115: INFO: (6) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.496796ms)
May 19 10:08:55.115: INFO: (6) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.332877ms)
May 19 10:08:55.115: INFO: (6) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.364026ms)
May 19 10:08:55.115: INFO: (6) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 4.795442ms)
May 19 10:08:55.116: INFO: (6) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.446861ms)
May 19 10:08:55.116: INFO: (6) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.487021ms)
May 19 10:08:55.116: INFO: (6) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.820098ms)
May 19 10:08:55.116: INFO: (6) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 5.804175ms)
May 19 10:08:55.118: INFO: (6) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 7.903494ms)
May 19 10:08:55.121: INFO: (7) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 2.895731ms)
May 19 10:08:55.122: INFO: (7) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.673775ms)
May 19 10:08:55.122: INFO: (7) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.727833ms)
May 19 10:08:55.122: INFO: (7) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 3.76003ms)
May 19 10:08:55.122: INFO: (7) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.746411ms)
May 19 10:08:55.123: INFO: (7) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.660786ms)
May 19 10:08:55.123: INFO: (7) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.829036ms)
May 19 10:08:55.123: INFO: (7) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.88051ms)
May 19 10:08:55.123: INFO: (7) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.883164ms)
May 19 10:08:55.123: INFO: (7) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.974448ms)
May 19 10:08:55.123: INFO: (7) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 4.965648ms)
May 19 10:08:55.124: INFO: (7) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.557701ms)
May 19 10:08:55.124: INFO: (7) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.629639ms)
May 19 10:08:55.124: INFO: (7) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.560006ms)
May 19 10:08:55.124: INFO: (7) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 5.913687ms)
May 19 10:08:55.125: INFO: (7) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.157576ms)
May 19 10:08:55.128: INFO: (8) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 2.940989ms)
May 19 10:08:55.128: INFO: (8) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.553716ms)
May 19 10:08:55.128: INFO: (8) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 3.790412ms)
May 19 10:08:55.129: INFO: (8) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 3.864374ms)
May 19 10:08:55.129: INFO: (8) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.829314ms)
May 19 10:08:55.129: INFO: (8) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 3.892521ms)
May 19 10:08:55.129: INFO: (8) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.22099ms)
May 19 10:08:55.129: INFO: (8) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.37094ms)
May 19 10:08:55.129: INFO: (8) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.48185ms)
May 19 10:08:55.129: INFO: (8) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.483177ms)
May 19 10:08:55.130: INFO: (8) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 4.801868ms)
May 19 10:08:55.130: INFO: (8) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.621955ms)
May 19 10:08:55.130: INFO: (8) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.538564ms)
May 19 10:08:55.131: INFO: (8) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.154993ms)
May 19 10:08:55.131: INFO: (8) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.189285ms)
May 19 10:08:55.131: INFO: (8) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.243412ms)
May 19 10:08:55.134: INFO: (9) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.403416ms)
May 19 10:08:55.135: INFO: (9) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.223294ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.598906ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.743898ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.775048ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.689421ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.690609ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.741384ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.762616ms)
May 19 10:08:55.136: INFO: (9) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.864795ms)
May 19 10:08:55.137: INFO: (9) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 5.623283ms)
May 19 10:08:55.138: INFO: (9) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.809484ms)
May 19 10:08:55.138: INFO: (9) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.693337ms)
May 19 10:08:55.138: INFO: (9) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.720645ms)
May 19 10:08:55.138: INFO: (9) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.800614ms)
May 19 10:08:55.138: INFO: (9) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.869339ms)
May 19 10:08:55.142: INFO: (10) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.739426ms)
May 19 10:08:55.142: INFO: (10) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.005456ms)
May 19 10:08:55.142: INFO: (10) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 3.956916ms)
May 19 10:08:55.142: INFO: (10) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.107635ms)
May 19 10:08:55.142: INFO: (10) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.084797ms)
May 19 10:08:55.143: INFO: (10) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.363957ms)
May 19 10:08:55.143: INFO: (10) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 4.544569ms)
May 19 10:08:55.143: INFO: (10) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.541216ms)
May 19 10:08:55.143: INFO: (10) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.824846ms)
May 19 10:08:55.143: INFO: (10) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.838325ms)
May 19 10:08:55.143: INFO: (10) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.856065ms)
May 19 10:08:55.143: INFO: (10) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.23035ms)
May 19 10:08:55.144: INFO: (10) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.87248ms)
May 19 10:08:55.144: INFO: (10) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.81926ms)
May 19 10:08:55.144: INFO: (10) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 5.927167ms)
May 19 10:08:55.144: INFO: (10) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 5.901534ms)
May 19 10:08:55.147: INFO: (11) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 2.762263ms)
May 19 10:08:55.148: INFO: (11) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 3.535277ms)
May 19 10:08:55.148: INFO: (11) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.443504ms)
May 19 10:08:55.148: INFO: (11) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.59157ms)
May 19 10:08:55.149: INFO: (11) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.658271ms)
May 19 10:08:55.149: INFO: (11) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.717218ms)
May 19 10:08:55.149: INFO: (11) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.904326ms)
May 19 10:08:55.149: INFO: (11) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.895107ms)
May 19 10:08:55.149: INFO: (11) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 5.046945ms)
May 19 10:08:55.149: INFO: (11) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.989813ms)
May 19 10:08:55.149: INFO: (11) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 5.131802ms)
May 19 10:08:55.150: INFO: (11) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.071321ms)
May 19 10:08:55.150: INFO: (11) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.017054ms)
May 19 10:08:55.150: INFO: (11) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.053511ms)
May 19 10:08:55.150: INFO: (11) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.024457ms)
May 19 10:08:55.150: INFO: (11) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.069784ms)
May 19 10:08:55.153: INFO: (12) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 2.943014ms)
May 19 10:08:55.154: INFO: (12) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 3.513906ms)
May 19 10:08:55.154: INFO: (12) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 3.816742ms)
May 19 10:08:55.154: INFO: (12) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 3.780005ms)
May 19 10:08:55.154: INFO: (12) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 3.822888ms)
May 19 10:08:55.154: INFO: (12) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.920458ms)
May 19 10:08:55.154: INFO: (12) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.970534ms)
May 19 10:08:55.155: INFO: (12) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.136061ms)
May 19 10:08:55.155: INFO: (12) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.185649ms)
May 19 10:08:55.155: INFO: (12) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.232792ms)
May 19 10:08:55.156: INFO: (12) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.544221ms)
May 19 10:08:55.157: INFO: (12) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.692638ms)
May 19 10:08:55.157: INFO: (12) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.761293ms)
May 19 10:08:55.157: INFO: (12) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.700531ms)
May 19 10:08:55.157: INFO: (12) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.800545ms)
May 19 10:08:55.157: INFO: (12) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.82031ms)
May 19 10:08:55.160: INFO: (13) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 2.72804ms)
May 19 10:08:55.161: INFO: (13) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 3.456077ms)
May 19 10:08:55.161: INFO: (13) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 3.534859ms)
May 19 10:08:55.161: INFO: (13) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 3.647235ms)
May 19 10:08:55.161: INFO: (13) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.80424ms)
May 19 10:08:55.162: INFO: (13) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.239707ms)
May 19 10:08:55.162: INFO: (13) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.425767ms)
May 19 10:08:55.162: INFO: (13) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.540517ms)
May 19 10:08:55.162: INFO: (13) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.527038ms)
May 19 10:08:55.162: INFO: (13) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.544219ms)
May 19 10:08:55.162: INFO: (13) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.024315ms)
May 19 10:08:55.163: INFO: (13) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 5.632362ms)
May 19 10:08:55.163: INFO: (13) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.767577ms)
May 19 10:08:55.164: INFO: (13) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.462019ms)
May 19 10:08:55.164: INFO: (13) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.499594ms)
May 19 10:08:55.164: INFO: (13) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.491004ms)
May 19 10:08:55.167: INFO: (14) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.123696ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 5.065103ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 5.211772ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 5.107358ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 5.153174ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 5.182438ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 5.321843ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 5.413407ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 5.452239ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 5.513211ms)
May 19 10:08:55.169: INFO: (14) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 5.401534ms)
May 19 10:08:55.170: INFO: (14) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.771419ms)
May 19 10:08:55.170: INFO: (14) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 5.833578ms)
May 19 10:08:55.170: INFO: (14) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.996729ms)
May 19 10:08:55.170: INFO: (14) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.283641ms)
May 19 10:08:55.170: INFO: (14) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.303687ms)
May 19 10:08:55.173: INFO: (15) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 2.885954ms)
May 19 10:08:55.174: INFO: (15) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.44106ms)
May 19 10:08:55.174: INFO: (15) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.523265ms)
May 19 10:08:55.174: INFO: (15) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 3.599184ms)
May 19 10:08:55.174: INFO: (15) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.60945ms)
May 19 10:08:55.174: INFO: (15) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 3.747249ms)
May 19 10:08:55.175: INFO: (15) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.440015ms)
May 19 10:08:55.175: INFO: (15) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.429958ms)
May 19 10:08:55.175: INFO: (15) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.451329ms)
May 19 10:08:55.175: INFO: (15) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.476053ms)
May 19 10:08:55.175: INFO: (15) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 4.813531ms)
May 19 10:08:55.176: INFO: (15) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.662534ms)
May 19 10:08:55.176: INFO: (15) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.632851ms)
May 19 10:08:55.177: INFO: (15) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.311649ms)
May 19 10:08:55.177: INFO: (15) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.331623ms)
May 19 10:08:55.177: INFO: (15) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.354462ms)
May 19 10:08:55.180: INFO: (16) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.041702ms)
May 19 10:08:55.181: INFO: (16) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.770017ms)
May 19 10:08:55.181: INFO: (16) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 3.834692ms)
May 19 10:08:55.181: INFO: (16) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.220081ms)
May 19 10:08:55.181: INFO: (16) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.188093ms)
May 19 10:08:55.181: INFO: (16) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.346845ms)
May 19 10:08:55.181: INFO: (16) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.464738ms)
May 19 10:08:55.182: INFO: (16) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.733073ms)
May 19 10:08:55.182: INFO: (16) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 4.867379ms)
May 19 10:08:55.182: INFO: (16) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.825404ms)
May 19 10:08:55.182: INFO: (16) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 5.193892ms)
May 19 10:08:55.183: INFO: (16) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.268555ms)
May 19 10:08:55.183: INFO: (16) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.365567ms)
May 19 10:08:55.183: INFO: (16) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.389173ms)
May 19 10:08:55.183: INFO: (16) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.555468ms)
May 19 10:08:55.183: INFO: (16) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.518801ms)
May 19 10:08:55.186: INFO: (17) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 2.658197ms)
May 19 10:08:55.187: INFO: (17) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.447276ms)
May 19 10:08:55.187: INFO: (17) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 3.663228ms)
May 19 10:08:55.187: INFO: (17) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.909283ms)
May 19 10:08:55.188: INFO: (17) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.245853ms)
May 19 10:08:55.188: INFO: (17) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.20737ms)
May 19 10:08:55.188: INFO: (17) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.434357ms)
May 19 10:08:55.188: INFO: (17) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.439805ms)
May 19 10:08:55.188: INFO: (17) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 4.476751ms)
May 19 10:08:55.188: INFO: (17) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.702063ms)
May 19 10:08:55.189: INFO: (17) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 4.922555ms)
May 19 10:08:55.189: INFO: (17) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.578863ms)
May 19 10:08:55.189: INFO: (17) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 5.564895ms)
May 19 10:08:55.190: INFO: (17) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.02292ms)
May 19 10:08:55.190: INFO: (17) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 6.015866ms)
May 19 10:08:55.190: INFO: (17) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.203044ms)
May 19 10:08:55.193: INFO: (18) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 2.888258ms)
May 19 10:08:55.193: INFO: (18) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.447276ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 3.624745ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.810316ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 3.980104ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 3.96341ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.030319ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.30445ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.45084ms)
May 19 10:08:55.194: INFO: (18) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.380579ms)
May 19 10:08:55.195: INFO: (18) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 4.748298ms)
May 19 10:08:55.195: INFO: (18) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 5.508532ms)
May 19 10:08:55.196: INFO: (18) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 5.512722ms)
May 19 10:08:55.196: INFO: (18) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 6.048204ms)
May 19 10:08:55.196: INFO: (18) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.060984ms)
May 19 10:08:55.196: INFO: (18) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 6.235171ms)
May 19 10:08:55.199: INFO: (19) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">test<... (200; 2.879249ms)
May 19 10:08:55.200: INFO: (19) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.544846ms)
May 19 10:08:55.200: INFO: (19) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:162/proxy/: bar (200; 3.649609ms)
May 19 10:08:55.200: INFO: (19) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 3.745852ms)
May 19 10:08:55.201: INFO: (19) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs:160/proxy/: foo (200; 4.509578ms)
May 19 10:08:55.201: INFO: (19) /api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/proxy-service-vxf7p-8wghs/proxy/rewriteme">test</a> (200; 4.424719ms)
May 19 10:08:55.201: INFO: (19) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:443/proxy/tlsrewritem... (200; 4.7476ms)
May 19 10:08:55.201: INFO: (19) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:462/proxy/: tls qux (200; 4.791461ms)
May 19 10:08:55.201: INFO: (19) /api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/: <a href="/api/v1/namespaces/proxy-4811/pods/http:proxy-service-vxf7p-8wghs:1080/proxy/rewriteme">... (200; 4.862909ms)
May 19 10:08:55.201: INFO: (19) /api/v1/namespaces/proxy-4811/pods/https:proxy-service-vxf7p-8wghs:460/proxy/: tls baz (200; 4.775257ms)
May 19 10:08:55.201: INFO: (19) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname2/proxy/: tls qux (200; 4.970816ms)
May 19 10:08:55.202: INFO: (19) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname1/proxy/: foo (200; 5.838536ms)
May 19 10:08:55.202: INFO: (19) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname1/proxy/: foo (200; 5.927446ms)
May 19 10:08:55.202: INFO: (19) /api/v1/namespaces/proxy-4811/services/https:proxy-service-vxf7p:tlsportname1/proxy/: tls baz (200; 6.091436ms)
May 19 10:08:55.203: INFO: (19) /api/v1/namespaces/proxy-4811/services/proxy-service-vxf7p:portname2/proxy/: bar (200; 6.279731ms)
May 19 10:08:55.203: INFO: (19) /api/v1/namespaces/proxy-4811/services/http:proxy-service-vxf7p:portname2/proxy/: bar (200; 6.25361ms)
STEP: deleting ReplicationController proxy-service-vxf7p in namespace proxy-4811, will wait for the garbage collector to delete the pods
May 19 10:08:55.262: INFO: Deleting ReplicationController proxy-service-vxf7p took: 6.732728ms
May 19 10:08:55.562: INFO: Terminating ReplicationController proxy-service-vxf7p pods took: 300.27355ms
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:08:58.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4811" for this suite.
May 19 10:09:04.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:09:04.256: INFO: namespace proxy-4811 deletion completed in 6.08813706s

• [SLOW TEST:19.297 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:09:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 19 10:09:04.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6590'
May 19 10:09:04.398: INFO: stderr: ""
May 19 10:09:04.398: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 19 10:09:09.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 get pod e2e-test-httpd-pod --namespace=kubectl-6590 -o json'
May 19 10:09:09.558: INFO: stderr: ""
May 19 10:09:09.558: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.161.109/32\"\n        },\n        \"creationTimestamp\": \"2020-05-19T10:09:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6590\",\n        \"resourceVersion\": \"1126019\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6590/pods/e2e-test-httpd-pod\",\n        \"uid\": \"890f1907-b974-4b20-b681-170bf5e62376\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-cdgcb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"nchc-kubemaster01\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-cdgcb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-cdgcb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-19T10:09:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-19T10:09:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-19T10:09:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-05-19T10:09:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://8e292d05642cd09d951200d60a811576a66acc50aef7198cce875f616884475d\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-05-19T10:09:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.78.26.171\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.161.109\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.161.109\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-05-19T10:09:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 19 10:09:09.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 replace -f - --namespace=kubectl-6590'
May 19 10:09:09.846: INFO: stderr: ""
May 19 10:09:09.846: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
May 19 10:09:09.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete pods e2e-test-httpd-pod --namespace=kubectl-6590'
May 19 10:09:15.255: INFO: stderr: ""
May 19 10:09:15.255: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:09:15.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6590" for this suite.
May 19 10:09:21.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:09:21.354: INFO: namespace kubectl-6590 deletion completed in 6.092931245s

• [SLOW TEST:17.097 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:09:21.354: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:09:21.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2747" for this suite.
May 19 10:09:27.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:09:27.475: INFO: namespace services-2747 deletion completed in 6.086123926s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.120 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:09:27.475: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-f02e94df-8fdc-4feb-a080-7ac4b61d7ebb
STEP: Creating a pod to test consume secrets
May 19 10:09:27.515: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51" in namespace "projected-6125" to be "success or failure"
May 19 10:09:27.517: INFO: Pod "pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.301862ms
May 19 10:09:29.522: INFO: Pod "pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006933661s
May 19 10:09:31.527: INFO: Pod "pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011796707s
STEP: Saw pod success
May 19 10:09:31.527: INFO: Pod "pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51" satisfied condition "success or failure"
May 19 10:09:31.530: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 19 10:09:31.550: INFO: Waiting for pod pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51 to disappear
May 19 10:09:31.553: INFO: Pod pod-projected-secrets-68c7dc81-cc4a-40d8-9d15-f8d4bae7da51 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:09:31.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6125" for this suite.
May 19 10:09:37.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:09:37.644: INFO: namespace projected-6125 deletion completed in 6.087200755s

• [SLOW TEST:10.169 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:09:37.644: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
May 19 10:09:37.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=kubectl-4636 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 19 10:09:40.431: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 19 10:09:40.431: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:09:42.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4636" for this suite.
May 19 10:09:48.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:09:48.533: INFO: namespace kubectl-4636 deletion completed in 6.088984736s

• [SLOW TEST:10.889 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:09:48.533: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 10:09:48.577: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 19 10:09:48.585: INFO: Number of nodes with available pods: 0
May 19 10:09:48.585: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 19 10:09:48.599: INFO: Number of nodes with available pods: 0
May 19 10:09:48.599: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 10:09:49.602: INFO: Number of nodes with available pods: 0
May 19 10:09:49.602: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 10:09:50.603: INFO: Number of nodes with available pods: 1
May 19 10:09:50.603: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 19 10:09:50.617: INFO: Number of nodes with available pods: 1
May 19 10:09:50.617: INFO: Number of running nodes: 0, number of available pods: 1
May 19 10:09:51.621: INFO: Number of nodes with available pods: 0
May 19 10:09:51.621: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 19 10:09:51.628: INFO: Number of nodes with available pods: 0
May 19 10:09:51.628: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 10:09:52.633: INFO: Number of nodes with available pods: 0
May 19 10:09:52.633: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 10:09:53.633: INFO: Number of nodes with available pods: 0
May 19 10:09:53.633: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 10:09:54.632: INFO: Number of nodes with available pods: 0
May 19 10:09:54.632: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 10:09:55.632: INFO: Number of nodes with available pods: 0
May 19 10:09:55.632: INFO: Node nchc-kubemaster01 is running more than one daemon pod
May 19 10:09:56.632: INFO: Number of nodes with available pods: 1
May 19 10:09:56.632: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1032, will wait for the garbage collector to delete the pods
May 19 10:09:56.697: INFO: Deleting DaemonSet.extensions daemon-set took: 7.197807ms
May 19 10:09:56.998: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.250154ms
May 19 10:10:05.303: INFO: Number of nodes with available pods: 0
May 19 10:10:05.303: INFO: Number of running nodes: 0, number of available pods: 0
May 19 10:10:05.305: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1032/daemonsets","resourceVersion":"1126295"},"items":null}

May 19 10:10:05.308: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1032/pods","resourceVersion":"1126295"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:10:05.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1032" for this suite.
May 19 10:10:11.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:10:11.420: INFO: namespace daemonsets-1032 deletion completed in 6.092511493s

• [SLOW TEST:22.887 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:10:11.420: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 10:10:12.913: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 10:10:14.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479812, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479812, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479812, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479812, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 10:10:17.944: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:10:17.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7234" for this suite.
May 19 10:10:23.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:10:24.072: INFO: namespace webhook-7234 deletion completed in 6.088572667s
STEP: Destroying namespace "webhook-7234-markers" for this suite.
May 19 10:10:30.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:10:30.160: INFO: namespace webhook-7234-markers deletion completed in 6.087799373s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.752 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:10:30.173: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
May 19 10:10:30.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 api-versions'
May 19 10:10:30.291: INFO: stderr: ""
May 19 10:10:30.291: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:10:30.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8054" for this suite.
May 19 10:10:36.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:10:36.382: INFO: namespace kubectl-8054 deletion completed in 6.086238049s

• [SLOW TEST:6.209 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:10:36.382: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
May 19 10:10:36.414: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 19 10:10:36.426: INFO: Waiting for terminating namespaces to be deleted...
May 19 10:10:36.428: INFO: 
Logging pods the kubelet thinks is on node nchc-kubemaster01 before test
May 19 10:10:36.434: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-7hsd4 from sonobuoy started at 2020-05-19 08:18:56 +0000 UTC (2 container statuses recorded)
May 19 10:10:36.434: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 19 10:10:36.434: INFO: 	Container systemd-logs ready: true, restart count 0
May 19 10:10:36.434: INFO: kube-proxy-qg52l from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 10:10:36.434: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 10:10:36.434: INFO: calico-node-7jgfk from kube-system started at 2020-05-13 03:42:35 +0000 UTC (1 container statuses recorded)
May 19 10:10:36.434: INFO: 	Container calico-node ready: true, restart count 0
May 19 10:10:36.434: INFO: 
Logging pods the kubelet thinks is on node nchc-worker03 before test
May 19 10:10:36.451: INFO: kube-proxy-5l969 from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 10:10:36.451: INFO: 	Container kube-proxy ready: true, restart count 0
May 19 10:10:36.451: INFO: sonobuoy from sonobuoy started at 2020-05-19 08:18:41 +0000 UTC (1 container statuses recorded)
May 19 10:10:36.451: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 19 10:10:36.451: INFO: calico-node-724vj from kube-system started at 2020-05-13 06:57:26 +0000 UTC (1 container statuses recorded)
May 19 10:10:36.451: INFO: 	Container calico-node ready: true, restart count 0
May 19 10:10:36.451: INFO: sonobuoy-systemd-logs-daemon-set-13aa31fbfd5a45c5-hw9m8 from sonobuoy started at 2020-05-19 08:18:43 +0000 UTC (2 container statuses recorded)
May 19 10:10:36.451: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 19 10:10:36.451: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b6db3c9e-3543-4bfc-9e71-7b04a98733f1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b6db3c9e-3543-4bfc-9e71-7b04a98733f1 off the node nchc-worker03
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b6db3c9e-3543-4bfc-9e71-7b04a98733f1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:10:44.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9692" for this suite.
May 19 10:10:54.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:10:54.604: INFO: namespace sched-pred-9692 deletion completed in 10.084117212s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:18.222 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:10:54.605: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 10:10:54.632: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 19 10:10:59.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-9258 create -f -'
May 19 10:10:59.778: INFO: stderr: ""
May 19 10:10:59.778: INFO: stdout: "e2e-test-crd-publish-openapi-7688-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 19 10:10:59.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-9258 delete e2e-test-crd-publish-openapi-7688-crds test-cr'
May 19 10:10:59.885: INFO: stderr: ""
May 19 10:10:59.885: INFO: stdout: "e2e-test-crd-publish-openapi-7688-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 19 10:10:59.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-9258 apply -f -'
May 19 10:11:00.092: INFO: stderr: ""
May 19 10:11:00.092: INFO: stdout: "e2e-test-crd-publish-openapi-7688-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 19 10:11:00.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-9258 delete e2e-test-crd-publish-openapi-7688-crds test-cr'
May 19 10:11:00.199: INFO: stderr: ""
May 19 10:11:00.199: INFO: stdout: "e2e-test-crd-publish-openapi-7688-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 19 10:11:00.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-7688-crds'
May 19 10:11:00.472: INFO: stderr: ""
May 19 10:11:00.472: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7688-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:11:05.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9258" for this suite.
May 19 10:11:11.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:11:11.117: INFO: namespace crd-publish-openapi-9258 deletion completed in 6.091258872s

• [SLOW TEST:16.512 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:11:11.117: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 10:11:11.149: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 19 10:11:15.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-8054 create -f -'
May 19 10:11:15.598: INFO: stderr: ""
May 19 10:11:15.598: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 19 10:11:15.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-8054 delete e2e-test-crd-publish-openapi-6273-crds test-cr'
May 19 10:11:15.719: INFO: stderr: ""
May 19 10:11:15.719: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 19 10:11:15.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-8054 apply -f -'
May 19 10:11:16.143: INFO: stderr: ""
May 19 10:11:16.143: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 19 10:11:16.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 --namespace=crd-publish-openapi-8054 delete e2e-test-crd-publish-openapi-6273-crds test-cr'
May 19 10:11:16.251: INFO: stderr: ""
May 19 10:11:16.251: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 19 10:11:16.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 explain e2e-test-crd-publish-openapi-6273-crds'
May 19 10:11:16.534: INFO: stderr: ""
May 19 10:11:16.534: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6273-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:11:20.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8054" for this suite.
May 19 10:11:26.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:11:26.589: INFO: namespace crd-publish-openapi-8054 deletion completed in 6.091050253s

• [SLOW TEST:15.471 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:11:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
May 19 10:11:26.626: INFO: Waiting up to 5m0s for pod "downward-api-e28073bf-68e8-4aaf-8710-0d4d19653149" in namespace "downward-api-7466" to be "success or failure"
May 19 10:11:26.628: INFO: Pod "downward-api-e28073bf-68e8-4aaf-8710-0d4d19653149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331546ms
May 19 10:11:28.634: INFO: Pod "downward-api-e28073bf-68e8-4aaf-8710-0d4d19653149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007743972s
STEP: Saw pod success
May 19 10:11:28.634: INFO: Pod "downward-api-e28073bf-68e8-4aaf-8710-0d4d19653149" satisfied condition "success or failure"
May 19 10:11:28.637: INFO: Trying to get logs from node nchc-kubemaster01 pod downward-api-e28073bf-68e8-4aaf-8710-0d4d19653149 container dapi-container: <nil>
STEP: delete the pod
May 19 10:11:28.657: INFO: Waiting for pod downward-api-e28073bf-68e8-4aaf-8710-0d4d19653149 to disappear
May 19 10:11:28.660: INFO: Pod downward-api-e28073bf-68e8-4aaf-8710-0d4d19653149 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:11:28.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7466" for this suite.
May 19 10:11:34.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:11:34.753: INFO: namespace downward-api-7466 deletion completed in 6.089350361s

• [SLOW TEST:8.164 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:11:34.753: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 19 10:11:37.309: INFO: Successfully updated pod "pod-update-cc4f7057-06bf-4837-94bb-0a7ea7656388"
STEP: verifying the updated pod is in kubernetes
May 19 10:11:37.314: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:11:37.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7411" for this suite.
May 19 10:12:05.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:12:05.405: INFO: namespace pods-7411 deletion completed in 28.087225029s

• [SLOW TEST:30.652 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:12:05.406: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 10:12:06.063: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 10:12:08.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479926, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479926, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479926, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479926, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 10:12:11.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 19 10:12:11.120: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:12:11.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7463" for this suite.
May 19 10:12:17.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:12:17.231: INFO: namespace webhook-7463 deletion completed in 6.09327487s
STEP: Destroying namespace "webhook-7463-markers" for this suite.
May 19 10:12:23.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:12:23.317: INFO: namespace webhook-7463-markers deletion completed in 6.086363345s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.924 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:12:23.331: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 19 10:12:23.970: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 19 10:12:25.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479943, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479943, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479943, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63725479943, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 19 10:12:29.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:12:29.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7375" for this suite.
May 19 10:12:35.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:12:35.150: INFO: namespace webhook-7375 deletion completed in 6.088497935s
STEP: Destroying namespace "webhook-7375-markers" for this suite.
May 19 10:12:41.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:12:41.238: INFO: namespace webhook-7375-markers deletion completed in 6.087944366s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.920 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:12:41.251: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-e0d95818-d65b-436b-9fdf-7fc3cbabf392 in namespace container-probe-912
May 19 10:12:43.293: INFO: Started pod busybox-e0d95818-d65b-436b-9fdf-7fc3cbabf392 in namespace container-probe-912
STEP: checking the pod's current state and verifying that restartCount is present
May 19 10:12:43.296: INFO: Initial restart count of pod busybox-e0d95818-d65b-436b-9fdf-7fc3cbabf392 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:16:43.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-912" for this suite.
May 19 10:16:49.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:16:49.943: INFO: namespace container-probe-912 deletion completed in 6.092383403s

• [SLOW TEST:248.693 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:16:49.944: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 10:16:49.981: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e5a4ed32-e628-4812-9f49-52bff768fe22" in namespace "security-context-test-405" to be "success or failure"
May 19 10:16:49.983: INFO: Pod "busybox-user-65534-e5a4ed32-e628-4812-9f49-52bff768fe22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229576ms
May 19 10:16:51.988: INFO: Pod "busybox-user-65534-e5a4ed32-e628-4812-9f49-52bff768fe22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006875692s
May 19 10:16:53.992: INFO: Pod "busybox-user-65534-e5a4ed32-e628-4812-9f49-52bff768fe22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011313818s
May 19 10:16:53.993: INFO: Pod "busybox-user-65534-e5a4ed32-e628-4812-9f49-52bff768fe22" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:16:53.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-405" for this suite.
May 19 10:17:00.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:17:00.089: INFO: namespace security-context-test-405 deletion completed in 6.091353298s

• [SLOW TEST:10.145 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:17:00.089: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
May 19 10:17:00.126: INFO: Waiting up to 5m0s for pod "client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd" in namespace "containers-6492" to be "success or failure"
May 19 10:17:00.128: INFO: Pod "client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.249342ms
May 19 10:17:02.132: INFO: Pod "client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00627889s
May 19 10:17:04.136: INFO: Pod "client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009965792s
STEP: Saw pod success
May 19 10:17:04.136: INFO: Pod "client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd" satisfied condition "success or failure"
May 19 10:17:04.139: INFO: Trying to get logs from node nchc-worker03 pod client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd container test-container: <nil>
STEP: delete the pod
May 19 10:17:04.175: INFO: Waiting for pod client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd to disappear
May 19 10:17:04.177: INFO: Pod client-containers-1637a8b9-463d-4045-acbc-23bf6cc421cd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:17:04.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6492" for this suite.
May 19 10:17:10.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:17:10.273: INFO: namespace containers-6492 deletion completed in 6.091995778s

• [SLOW TEST:10.185 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:17:10.274: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 19 10:17:10.309: INFO: Waiting up to 5m0s for pod "pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e" in namespace "emptydir-307" to be "success or failure"
May 19 10:17:10.312: INFO: Pod "pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341323ms
May 19 10:17:12.316: INFO: Pod "pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006280635s
May 19 10:17:14.319: INFO: Pod "pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009475848s
STEP: Saw pod success
May 19 10:17:14.319: INFO: Pod "pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e" satisfied condition "success or failure"
May 19 10:17:14.321: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e container test-container: <nil>
STEP: delete the pod
May 19 10:17:14.352: INFO: Waiting for pod pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e to disappear
May 19 10:17:14.354: INFO: Pod pod-57f91cce-f3f1-4fd8-b9a9-02998a3b430e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:17:14.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-307" for this suite.
May 19 10:17:20.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:17:20.444: INFO: namespace emptydir-307 deletion completed in 6.086331777s

• [SLOW TEST:10.170 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:17:20.444: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-446912a8-a095-4d01-93b6-f0bee97fa938
STEP: Creating a pod to test consume configMaps
May 19 10:17:20.484: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339" in namespace "projected-9371" to be "success or failure"
May 19 10:17:20.486: INFO: Pod "pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188998ms
May 19 10:17:22.490: INFO: Pod "pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006274699s
May 19 10:17:24.494: INFO: Pod "pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009981088s
STEP: Saw pod success
May 19 10:17:24.494: INFO: Pod "pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339" satisfied condition "success or failure"
May 19 10:17:24.497: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 19 10:17:24.514: INFO: Waiting for pod pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339 to disappear
May 19 10:17:24.517: INFO: Pod pod-projected-configmaps-05ae2718-ab66-47ec-95df-4407398f2339 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:17:24.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9371" for this suite.
May 19 10:17:30.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:17:30.614: INFO: namespace projected-9371 deletion completed in 6.092854629s

• [SLOW TEST:10.169 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:17:30.614: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
May 19 10:17:35.190: INFO: Successfully updated pod "labelsupdate915fdb5e-0627-4f2a-bf1f-4a4b555aba36"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:17:37.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1581" for this suite.
May 19 10:18:05.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:18:05.305: INFO: namespace projected-1581 deletion completed in 28.092307523s

• [SLOW TEST:34.691 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:18:05.306: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 19 10:18:05.360: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9639 /api/v1/namespaces/watch-9639/configmaps/e2e-watch-test-label-changed 14814885-a56a-455b-9627-8dd2aeedbb4f 1127847 0 2020-05-19 10:18:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 19 10:18:05.360: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9639 /api/v1/namespaces/watch-9639/configmaps/e2e-watch-test-label-changed 14814885-a56a-455b-9627-8dd2aeedbb4f 1127848 0 2020-05-19 10:18:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 19 10:18:05.360: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9639 /api/v1/namespaces/watch-9639/configmaps/e2e-watch-test-label-changed 14814885-a56a-455b-9627-8dd2aeedbb4f 1127849 0 2020-05-19 10:18:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 19 10:18:15.387: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9639 /api/v1/namespaces/watch-9639/configmaps/e2e-watch-test-label-changed 14814885-a56a-455b-9627-8dd2aeedbb4f 1127871 0 2020-05-19 10:18:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 19 10:18:15.387: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9639 /api/v1/namespaces/watch-9639/configmaps/e2e-watch-test-label-changed 14814885-a56a-455b-9627-8dd2aeedbb4f 1127872 0 2020-05-19 10:18:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 19 10:18:15.387: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9639 /api/v1/namespaces/watch-9639/configmaps/e2e-watch-test-label-changed 14814885-a56a-455b-9627-8dd2aeedbb4f 1127873 0 2020-05-19 10:18:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:18:15.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9639" for this suite.
May 19 10:18:21.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:18:21.476: INFO: namespace watch-9639 deletion completed in 6.085155423s

• [SLOW TEST:16.170 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:18:21.476: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 19 10:18:21.532: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-689 /api/v1/namespaces/watch-689/configmaps/e2e-watch-test-resource-version f3adfbf1-cd05-47dd-8710-9370bc1ba525 1127897 0 2020-05-19 10:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 19 10:18:21.533: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-689 /api/v1/namespaces/watch-689/configmaps/e2e-watch-test-resource-version f3adfbf1-cd05-47dd-8710-9370bc1ba525 1127898 0 2020-05-19 10:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:18:21.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-689" for this suite.
May 19 10:18:27.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:18:27.625: INFO: namespace watch-689 deletion completed in 6.088964412s

• [SLOW TEST:6.149 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:18:27.626: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:18:31.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4879" for this suite.
May 19 10:19:15.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:19:15.774: INFO: namespace kubelet-test-4879 deletion completed in 44.089964012s

• [SLOW TEST:48.149 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:19:15.774: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
May 19 10:19:15.801: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:19:20.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8283" for this suite.
May 19 10:20:04.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:20:04.339: INFO: namespace pods-8283 deletion completed in 44.094879163s

• [SLOW TEST:48.564 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:20:04.339: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
May 19 10:20:44.400: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0519 10:20:44.399980      25 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 19 10:20:44.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5579" for this suite.
May 19 10:20:50.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:20:50.502: INFO: namespace gc-5579 deletion completed in 6.097890538s

• [SLOW TEST:46.163 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:20:50.502: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
May 19 10:20:50.530: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 19 10:20:50.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-44'
May 19 10:20:50.821: INFO: stderr: ""
May 19 10:20:50.821: INFO: stdout: "service/redis-slave created\n"
May 19 10:20:50.821: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 19 10:20:50.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-44'
May 19 10:20:51.043: INFO: stderr: ""
May 19 10:20:51.043: INFO: stdout: "service/redis-master created\n"
May 19 10:20:51.043: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 19 10:20:51.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-44'
May 19 10:20:51.360: INFO: stderr: ""
May 19 10:20:51.360: INFO: stdout: "service/frontend created\n"
May 19 10:20:51.360: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 19 10:20:51.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-44'
May 19 10:20:51.563: INFO: stderr: ""
May 19 10:20:51.563: INFO: stdout: "deployment.apps/frontend created\n"
May 19 10:20:51.563: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 19 10:20:51.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-44'
May 19 10:20:51.761: INFO: stderr: ""
May 19 10:20:51.761: INFO: stdout: "deployment.apps/redis-master created\n"
May 19 10:20:51.761: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 19 10:20:51.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 create -f - --namespace=kubectl-44'
May 19 10:20:51.980: INFO: stderr: ""
May 19 10:20:51.980: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 19 10:20:51.980: INFO: Waiting for all frontend pods to be Running.
May 19 10:21:02.031: INFO: Waiting for frontend to serve content.
May 19 10:21:02.052: INFO: Trying to add a new entry to the guestbook.
May 19 10:21:02.070: INFO: Verifying that added entry can be retrieved.
May 19 10:21:02.088: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
May 19 10:21:07.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-44'
May 19 10:21:07.226: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 10:21:07.226: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 19 10:21:07.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-44'
May 19 10:21:07.340: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 10:21:07.340: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 19 10:21:07.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-44'
May 19 10:21:07.457: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 10:21:07.457: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 19 10:21:07.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-44'
May 19 10:21:07.566: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 10:21:07.566: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 19 10:21:07.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-44'
May 19 10:21:07.671: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 10:21:07.671: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 19 10:21:07.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-934715428 delete --grace-period=0 --force -f - --namespace=kubectl-44'
May 19 10:21:07.774: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 19 10:21:07.774: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:21:07.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-44" for this suite.
May 19 10:21:19.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:21:19.870: INFO: namespace kubectl-44 deletion completed in 12.090227712s

• [SLOW TEST:29.368 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:21:19.870: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 19 10:21:20.128: INFO: Pod name wrapped-volume-race-78054ce7-43a5-49f2-ab20-5fc08089c08c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-78054ce7-43a5-49f2-ab20-5fc08089c08c in namespace emptydir-wrapper-2196, will wait for the garbage collector to delete the pods
May 19 10:21:34.253: INFO: Deleting ReplicationController wrapped-volume-race-78054ce7-43a5-49f2-ab20-5fc08089c08c took: 7.572233ms
May 19 10:21:34.554: INFO: Terminating ReplicationController wrapped-volume-race-78054ce7-43a5-49f2-ab20-5fc08089c08c pods took: 300.283608ms
STEP: Creating RC which spawns configmap-volume pods
May 19 10:22:15.371: INFO: Pod name wrapped-volume-race-d3895711-3e74-4733-a783-264653de5cd7: Found 0 pods out of 5
May 19 10:22:20.379: INFO: Pod name wrapped-volume-race-d3895711-3e74-4733-a783-264653de5cd7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d3895711-3e74-4733-a783-264653de5cd7 in namespace emptydir-wrapper-2196, will wait for the garbage collector to delete the pods
May 19 10:22:30.460: INFO: Deleting ReplicationController wrapped-volume-race-d3895711-3e74-4733-a783-264653de5cd7 took: 7.89218ms
May 19 10:22:30.760: INFO: Terminating ReplicationController wrapped-volume-race-d3895711-3e74-4733-a783-264653de5cd7 pods took: 300.307424ms
STEP: Creating RC which spawns configmap-volume pods
May 19 10:23:06.179: INFO: Pod name wrapped-volume-race-c584f1c6-1c95-466f-b86a-60303562fd0a: Found 0 pods out of 5
May 19 10:23:11.187: INFO: Pod name wrapped-volume-race-c584f1c6-1c95-466f-b86a-60303562fd0a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c584f1c6-1c95-466f-b86a-60303562fd0a in namespace emptydir-wrapper-2196, will wait for the garbage collector to delete the pods
May 19 10:23:21.270: INFO: Deleting ReplicationController wrapped-volume-race-c584f1c6-1c95-466f-b86a-60303562fd0a took: 8.841476ms
May 19 10:23:21.570: INFO: Terminating ReplicationController wrapped-volume-race-c584f1c6-1c95-466f-b86a-60303562fd0a pods took: 300.283957ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:23:57.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2196" for this suite.
May 19 10:24:03.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:24:03.745: INFO: namespace emptydir-wrapper-2196 deletion completed in 6.087645231s

• [SLOW TEST:163.875 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:24:03.745: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 19 10:24:05.791: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:24:05.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8410" for this suite.
May 19 10:24:11.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:24:11.894: INFO: namespace container-runtime-8410 deletion completed in 6.08663126s

• [SLOW TEST:8.149 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:24:11.894: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-lv9r
STEP: Creating a pod to test atomic-volume-subpath
May 19 10:24:11.937: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lv9r" in namespace "subpath-9128" to be "success or failure"
May 19 10:24:11.939: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138711ms
May 19 10:24:13.943: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005776863s
May 19 10:24:15.947: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 4.009903004s
May 19 10:24:17.952: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 6.015327652s
May 19 10:24:19.957: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 8.01986691s
May 19 10:24:21.961: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 10.024481807s
May 19 10:24:23.965: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 12.028501926s
May 19 10:24:25.969: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 14.032353796s
May 19 10:24:27.975: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 16.037901926s
May 19 10:24:29.979: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 18.042275448s
May 19 10:24:31.984: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Running", Reason="", readiness=true. Elapsed: 20.046853607s
May 19 10:24:33.987: INFO: Pod "pod-subpath-test-downwardapi-lv9r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.050485474s
STEP: Saw pod success
May 19 10:24:33.987: INFO: Pod "pod-subpath-test-downwardapi-lv9r" satisfied condition "success or failure"
May 19 10:24:33.990: INFO: Trying to get logs from node nchc-kubemaster01 pod pod-subpath-test-downwardapi-lv9r container test-container-subpath-downwardapi-lv9r: <nil>
STEP: delete the pod
May 19 10:24:34.022: INFO: Waiting for pod pod-subpath-test-downwardapi-lv9r to disappear
May 19 10:24:34.024: INFO: Pod pod-subpath-test-downwardapi-lv9r no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lv9r
May 19 10:24:34.024: INFO: Deleting pod "pod-subpath-test-downwardapi-lv9r" in namespace "subpath-9128"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:24:34.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9128" for this suite.
May 19 10:24:40.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:24:40.118: INFO: namespace subpath-9128 deletion completed in 6.087349518s

• [SLOW TEST:28.223 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
May 19 10:24:40.118: INFO: >>> kubeConfig: /tmp/kubeconfig-934715428
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
May 19 10:24:46.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-299" for this suite.
May 19 10:24:52.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:24:52.317: INFO: namespace namespaces-299 deletion completed in 6.088584959s
STEP: Destroying namespace "nsdeletetest-1025" for this suite.
May 19 10:24:52.320: INFO: Namespace nsdeletetest-1025 was already deleted
STEP: Destroying namespace "nsdeletetest-8137" for this suite.
May 19 10:24:58.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 19 10:24:58.401: INFO: namespace nsdeletetest-8137 deletion completed in 6.081300338s

• [SLOW TEST:18.283 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSMay 19 10:24:58.401: INFO: Running AfterSuite actions on all nodes
May 19 10:24:58.401: INFO: Running AfterSuite actions on node 1
May 19 10:24:58.401: INFO: Skipping dumping logs from cluster

Ran 276 of 4732 Specs in 7554.541 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4456 Skipped
PASS

Ginkgo ran 1 suite in 2h5m56.443263355s
Test Suite Passed
